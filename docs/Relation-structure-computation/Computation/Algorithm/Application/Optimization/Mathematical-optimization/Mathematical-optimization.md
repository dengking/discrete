[TOC]

# [Mathematical optimization](https://en.wikipedia.org/wiki/Mathematical_optimization)

**Mathematical optimization** (alternatively spelled *optimisation*) or **mathematical programming** is the selection of a best element (with regard to some criterion) from some set of available alternatives.[[1\]](https://en.wikipedia.org/wiki/Mathematical_optimization#cite_note-1) Optimization problems of sorts arise in all quantitative disciplines from [computer science](https://en.wikipedia.org/wiki/Computer_science) and [engineering](https://en.wikipedia.org/wiki/Engineering) to [operations research](https://en.wikipedia.org/wiki/Operations_research) and [economics](https://en.wikipedia.org/wiki/Economics), and the development of solution methods has been of interest in [mathematics](https://en.wikipedia.org/wiki/Mathematics) for centuries.[[2\]](https://en.wikipedia.org/wiki/Mathematical_optimization#cite_note-2)

In the simplest case, an [optimization problem](https://en.wikipedia.org/wiki/Optimization_problem) consists of [maximizing or minimizing](https://en.wikipedia.org/wiki/Maxima_and_minima) a [real function](https://en.wikipedia.org/wiki/Function_of_a_real_variable) by systematically choosing [input](https://en.wikipedia.org/wiki/Argument_of_a_function) values from within an allowed set and computing the [value](https://en.wikipedia.org/wiki/Value_(mathematics)) of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of [applied mathematics](https://en.wikipedia.org/wiki/Applied_mathematics). More generally, optimization includes finding "best available" values of some objective function given a defined [domain](https://en.wikipedia.org/wiki/Domain_of_a_function) (or input), including a variety of different types of objective functions and different types of domains.





## Major subfields

- [Convex programming](https://en.wikipedia.org/wiki/Convex_programming) studies the case when the objective function is convex (minimization) or concave (maximization) and the constraint set is convex. This can be viewed as a particular case of nonlinear programming or as generalization of linear or convex quadratic programming.
  - [Linear programming](https://en.wikipedia.org/wiki/Linear_programming) (LP), a type of convex programming, studies the case in which the objective function *f* is linear and the constraints are specified using only linear equalities and inequalities. Such a constraint set is called a [polyhedron](https://en.wikipedia.org/wiki/Polyhedron) or a [polytope](https://en.wikipedia.org/wiki/Polytope) if it is [bounded](https://en.wikipedia.org/wiki/Bounded_set).
  - [Second order cone programming](https://en.wikipedia.org/wiki/Second_order_cone_programming) (SOCP) is a convex program, and includes certain types of quadratic programs.
  - [Semidefinite programming](https://en.wikipedia.org/wiki/Semidefinite_programming) (SDP) is a subfield of convex optimization where the underlying variables are [semidefinite](https://en.wikipedia.org/wiki/Semidefinite) [matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)). It is a generalization of linear and convex quadratic programming.
  - [Conic programming](https://en.wikipedia.org/wiki/Conic_programming) is a general form of convex programming. LP, SOCP and SDP can all be viewed as conic programs with the appropriate type of cone.
  - [Geometric programming](https://en.wikipedia.org/wiki/Geometric_programming) is a technique whereby objective and inequality constraints expressed as [posynomials](https://en.wikipedia.org/wiki/Posynomials) and equality constraints as [monomials](https://en.wikipedia.org/wiki/Monomials) can be transformed into a convex program.
- [Integer programming](https://en.wikipedia.org/wiki/Integer_programming) studies linear programs in which some or all variables are constrained to take on [integer](https://en.wikipedia.org/wiki/Integer) values. This is not convex, and in general much more difficult than regular linear programming.
- [Quadratic programming](https://en.wikipedia.org/wiki/Quadratic_programming) allows the objective function to have quadratic terms, while the feasible set must be specified with linear equalities and inequalities. For specific forms of the quadratic term, this is a type of convex programming.
- [Fractional programming](https://en.wikipedia.org/wiki/Fractional_programming) studies optimization of ratios of two nonlinear functions. The special class of concave fractional programs can be transformed to a convex optimization problem.
- [Nonlinear programming](https://en.wikipedia.org/wiki/Nonlinear_programming) studies the general case in which the objective function or the constraints or both contain nonlinear parts. This may or may not be a convex program. In general, whether the program is convex affects the difficulty of solving it.
- [Stochastic programming](https://en.wikipedia.org/wiki/Stochastic_programming) studies the case in which some of the constraints or parameters depend on [random variables](https://en.wikipedia.org/wiki/Random_variable).
- [Robust programming](https://en.wikipedia.org/wiki/Robust_optimization) is, like stochastic programming, an attempt to capture uncertainty in the data underlying the optimization problem. Robust optimization aims to find solutions that are valid under all possible realizations of the uncertainties.
- [Combinatorial optimization](https://en.wikipedia.org/wiki/Combinatorial_optimization) is concerned with problems where the set of feasible solutions is discrete or can be reduced to a [discrete](https://en.wikipedia.org/wiki/Discrete_mathematics) one.
- [Stochastic optimization](https://en.wikipedia.org/wiki/Stochastic_optimization) is used with random (noisy) function measurements or random inputs in the search process.
- [Infinite-dimensional optimization](https://en.wikipedia.org/wiki/Infinite-dimensional_optimization) studies the case when the set of feasible solutions is a subset of an infinite-[dimensional](https://en.wikipedia.org/wiki/Dimension) space, such as a space of functions.
- [Heuristics](https://en.wikipedia.org/wiki/Heuristic_(computer_science)) and [metaheuristics](https://en.wikipedia.org/wiki/Metaheuristic) make few or no assumptions about the problem being optimized. Usually, heuristics do not guarantee that any optimal solution need be found. On the other hand, heuristics are used to find approximate solutions for many complicated optimization problems.
- Constraint satisfaction studies the case in which the objective function f is constant (this is used in artificial intelligence, particularly in automated reasoning).
  - [Constraint programming](https://en.wikipedia.org/wiki/Constraint_programming) is a programming paradigm wherein relations between variables are stated in the form of constraints.
- Disjunctive programming is used where at least one constraint must be satisfied but not all. It is of particular use in scheduling.
- [Space mapping](https://en.wikipedia.org/wiki/Space_mapping) is a concept for modeling and optimization of an engineering system to high-fidelity (fine) model accuracy exploiting a suitable physically meaningful coarse or [surrogate model](https://en.wikipedia.org/wiki/Surrogate_model).

In a number of subfields, the techniques are designed primarily for optimization in dynamic contexts (that is, decision making over time):

- [Calculus of variations](https://en.wikipedia.org/wiki/Calculus_of_variations) seeks to optimize an action integral over some space to an extremum by varying a function of the coordinates.
- [Optimal control](https://en.wikipedia.org/wiki/Optimal_control) theory is a generalization of the calculus of variations which introduces control policies.
- [Dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) is the approach to solve the [stochastic optimization](https://en.wikipedia.org/wiki/Stochastic_optimization) problem with stochastic, randomness, and unknown model parameters. It studies the case in which the optimization strategy is based on splitting the problem into smaller subproblems. The equation that describes the relationship between these subproblems is called the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation).
- [Mathematical programming with equilibrium constraints](https://en.wikipedia.org/wiki/Mathematical_programming_with_equilibrium_constraints) is where the constraints include [variational inequalities](https://en.wikipedia.org/wiki/Variational_inequalities) or [complementarities](https://en.wikipedia.org/wiki/Complementarity_theory).