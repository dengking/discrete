{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u5de5\u7a0b Computer scientific is discrete \u4f5c\u4e3a\u4e00\u4e2asoftware engineer\uff0c\u6211\u8d8a\u6765\u8d8a\u89c9\u5f97\uff1a Computer science is discrete. computer science\u4e2d\u4e24\u4e2a\u975e\u5e38\u57fa\u672c\u7684\u95ee\u9898\u662f\uff1a 1) representation \u5173\u4e8erepresentation\uff0c\u5728\u6587\u7ae0Language\u4ecb\u7ecd\u4e86\u4e0e\u5b83\u76f8\u5173\u7684\u5185\u5bb9\uff1b\u5f80\u5177\u4f53\u8bf4\uff0c\u5b83\u5305\u542bdata structure\u3002 2) computation computation\u5f80\u5177\u4f53\u8bf4\uff0c\u5b83\u5305\u542balgorithm\u3002 representation\u548ccomputation\u5bc6\u5207\u76f8\u5173\uff1a 1) \u89e3\u51b3\u8bbe\u8ba1\u95ee\u9898\u7684\u7b2c\u4e00\u6b65\u5f80\u5f80\u662f\uff1a\u7ed9\u51fa\u5b83\u7684\u7ed3\u6784\u5316\u7684representation\uff0c\u5173\u4e8e\u7ed3\u6784\u5316representation\uff0c\u53c2\u89c1 Relation-structure-computation\\Representation-and-computation\\Structured-representation \u3002 2) \u597d\u7684representation\u662f\u9ad8\u6548\u5730\u5b9e\u73b0computation\u7684\u524d\u63d0\u3002 representation\u548ccomputation\u90fd\u5177\u5907**discrete**\u7279\u6027\uff0c\u53ef\u4ee5\u4f7f\u7528discrete math\u4e2d\u7684\u7406\u8bba\u3001\u601d\u60f3\u6765\u8fdb\u884c\u7406\u89e3\u548c\u63cf\u8ff0\uff0c\u6b63\u5982\u5728wikipedia Discrete mathematics \u4e2d\u6240\u8ff0\uff1a Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science , such as computer algorithms , programming languages , cryptography , automated theorem proving , and software development . Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems, such as in operations research . \u5173\u4e8e\u6b64\u7684\u601d\u8003\uff0c\u89e6\u53d1\u4e86\u6211\u5199\u4f5c\u672c\u4e66\uff0c\u6211\u7684\u5199\u4f5c\u76ee\u7684\u4e3b\u8981\u662f\uff1a\u68b3\u7406discrete math\u7684\u7406\u8bba\u77e5\u8bc6\uff0c\u7528\u8fd9\u4e9b\u77e5\u8bc6\uff0c\u66f4\u597d\u5730\u89e3\u51b3computer scientific\u4e2drepresentation\u3001computation\uff0c\u5f80\u66f4\u52a0\u5177\u4f53\u6765\u8bf4\u662f\uff1a \u5bf9\u4e8ediscrete objects\u76f8\u5173\u7684computation\u95ee\u9898\uff0c\u5982\u4f55\u8bbe\u8ba1algorithm\uff1f\u5982\u4f55\u8bbe\u8ba1representation\uff1f \u5728 Discrete-math\\Why-we-need-discrete-math \u4e2d\uff0c\u4e5f\u9610\u8ff0\u4e86\u6211\u7684\u5199\u4f5c\u52a8\u673a\u3002 \u5173\u4e8ediscrete math\uff0c\u6211\u60f3\u8d77\u4e86\u5728\u5927\u5b66\u4e8c\u5e74\u7ea7\u65f6\u5b66\u4e60discrete math\u8bfe\u7a0b\uff0c\u5f53\u65f6\u4f7f\u7528\u7684\u6559\u6750\u662f\u975e\u5e38\u7ecf\u5178\u7684 Discrete Mathematics and Its Applications \uff0c\u56e0\u6b64\u8fd9\u4e2a\u5de5\u7a0b\u4e2d\u7684\u4e00\u4e9b\u5185\u5bb9\u662f\u6e90\u81ea\u4e8e\u8fd9\u672c\u6559\u6750\u3002 \u5728 Computer-science-is-discrete \u4e2d\u7ed9\u51fa\u4e86\u8bba\u8bc1\"Computer scientific is discrete\"\u89c2\u70b9\u7684\u4f8b\u5b50\u3002 \u4e66\u5199\u601d\u8def \u9996\u5148\u8bf4\u660e\u4ec0\u4e48\u662fdiscrete\u3001discrete math\uff0c\u7136\u540e\u63cf\u8ff0discrete objects\u7684\u7279\u6027\uff0cdiscrete objects\u7684\u8fd9\u4e9b\u7279\u6027\u4f7f\u5f97\u5b83\u662f**computable**\u7684\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u8bbe\u8ba1algorithm\u6765\u89e3\u51b3\u4e0e\u5b83\u76f8\u5173\u7684\u95ee\u9898\u3002discrete math\u4e2d\u7684\u7406\u8bba\u77e5\u8bc6\u662f\u540e\u7eedalgorithm\u3001data-structure\u7684\u57fa\u7840\u3002 Discrete VS continuous \u9996\u5148\uff0c\u6765\u601d\u8003discrete\u7684\u542b\u4e49\u3002 \u201cdiscrete\u201d\u5373\u201c\u79bb\u6563\u201d\uff0c\u201ccontinuous\u201d\u5373\u201c\u8fde\u7eed\u201d\uff0c\u4e24\u8005\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u5bf9\u53cd\u4e49\u8bcd\uff0c\u5982\u679c\u662fdiscrete\u7684\u8bdd\uff0c\u5219\u5fc5\u7136\u5c31\u4e0d\u662fcontinuous\u7684\u3002 \u5173\u4e8e\u201cdiscrete\u201d\u7684\u6982\u5ff5\uff0c\u7ef4\u57fa\u767e\u79d1 Discrete space \u7ed9\u51fa\u4e86\u89e3\u91ca\u3002\u6211\u7684\u76f4\u89c2\u7406\u89e3\u5c31\u662f\u4e0d\u76f8\u8fde\u7684\u3001\u662f\u4e00\u4e2a\u4e00\u4e2anode\uff0c\u540e\u9762\u6211\u4eec\u5c06\u5b83\u79f0\u4e3a\u201cdiscrete object\u201d\u3002 \u5173\u4e8e\u201ccontinuous\u201d\u7684\u6982\u5ff5\uff0c\u7ef4\u57fa\u767e\u79d1 Continuous function \u7ed9\u51fa\u4e86\u89e3\u91ca\uff0c\u6211\u7684\u76f4\u89c2\u7406\u89e3\u662f\u76f8\u8fde\u7684\u3001\u662f\uff08\u5e73\u6ed1\u7684\uff09\u66f2\u7ebf\u3002 \u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u7ef4\u57fa\u767e\u79d1 Discrete mathematics \u7684\u7b2c\u4e00\u6bb5\u7684\u8bba\u8ff0\u4e5f\u662f\u6bd4\u8f83\u597d\u7684\u3002 \u6839\u636e\u8fd9\u4e24\u4e2a\u6027\u8d28\uff0c\u53ef\u4ee5\u5c06\u6570\u5b66\u5b66\u79d1\u8fdb\u884c\u5206\u7c7b\u3002 Property of discrete objects \u672c\u8282\u603b\u7ed3\u7531discrete\u884d\u751f\u51fa\u6765\u7684\u4e00\u7cfb\u5217\u7279\u6027\uff0c\u663e\u7136discrete objects\u9664\u4e86\u5177\u5907discrete\u7279\u6027\uff0c\u8fd8\u5177\u5907\u5982\u4e0b\u7279\u6027\u3002 Enumerable \u201cenumerable\u201d\u5373\u201c\u53ef\u679a\u4e3e\u7684\u201d\uff0c\u8fd9\u4e2a\u7279\u6027\u975e\u5e38\u91cd\u8981\u3002 \u7ef4\u57fa\u767e\u79d1 Discrete mathematics \uff1a Discrete objects can often be enumerated by integers. Countable \u5173\u4e8e\u201ccountable\u201d\uff0c\u53ef\u4ee5\u53c2\u89c1wikipedia Countable set \u3002 discrete objects\u4e00\u822c\u662fcountable\u7684\uff08\u6709\u7684\u662f\u65e0\u6cd5count\u7684\uff09\u3002 Computable \u5173\u4e8e\u201ccomputable\u201d\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Computability theory \u3001\u7ef4\u57fa\u767e\u79d1 Computability \u3002 discrete objects\uff0c\u4e00\u822c\u662fcomputable\u7684\uff0c\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u5c31\u9700\u8981\u601d\u8003\uff1a\u5982\u4f55\u6765\u5b9e\u73b0\u201cComputation on discrete objects\u201d\uff1b\u663e\u7136\u4e0e\u6b64\u76f8\u5173\u7684\u4e00\u4e2a\u91cd\u8981\u8bfe\u9898\u5c31\u662f\uff1a\u7b97\u6cd5\uff0c\u8fd9\u5728\u540e\u7eed\u7684\u7ae0\u8282\u4e2d\u4f1a\u8fdb\u884c\u4ecb\u7ecd\u3002Countable\u662f\u5bf9computation\u8fdb\u884c\u91cf\u5316\u8fdb\u800c\u8fdb\u884c**\u7b97\u6cd5\u590d\u6742\u5ea6\u5206\u6790**\u7684\u524d\u63d0\u3002","title":"Home"},{"location":"#_1","text":"","title":"\u5173\u4e8e\u672c\u5de5\u7a0b"},{"location":"#computer#scientific#is#discrete","text":"\u4f5c\u4e3a\u4e00\u4e2asoftware engineer\uff0c\u6211\u8d8a\u6765\u8d8a\u89c9\u5f97\uff1a Computer science is discrete. computer science\u4e2d\u4e24\u4e2a\u975e\u5e38\u57fa\u672c\u7684\u95ee\u9898\u662f\uff1a 1) representation \u5173\u4e8erepresentation\uff0c\u5728\u6587\u7ae0Language\u4ecb\u7ecd\u4e86\u4e0e\u5b83\u76f8\u5173\u7684\u5185\u5bb9\uff1b\u5f80\u5177\u4f53\u8bf4\uff0c\u5b83\u5305\u542bdata structure\u3002 2) computation computation\u5f80\u5177\u4f53\u8bf4\uff0c\u5b83\u5305\u542balgorithm\u3002 representation\u548ccomputation\u5bc6\u5207\u76f8\u5173\uff1a 1) \u89e3\u51b3\u8bbe\u8ba1\u95ee\u9898\u7684\u7b2c\u4e00\u6b65\u5f80\u5f80\u662f\uff1a\u7ed9\u51fa\u5b83\u7684\u7ed3\u6784\u5316\u7684representation\uff0c\u5173\u4e8e\u7ed3\u6784\u5316representation\uff0c\u53c2\u89c1 Relation-structure-computation\\Representation-and-computation\\Structured-representation \u3002 2) \u597d\u7684representation\u662f\u9ad8\u6548\u5730\u5b9e\u73b0computation\u7684\u524d\u63d0\u3002 representation\u548ccomputation\u90fd\u5177\u5907**discrete**\u7279\u6027\uff0c\u53ef\u4ee5\u4f7f\u7528discrete math\u4e2d\u7684\u7406\u8bba\u3001\u601d\u60f3\u6765\u8fdb\u884c\u7406\u89e3\u548c\u63cf\u8ff0\uff0c\u6b63\u5982\u5728wikipedia Discrete mathematics \u4e2d\u6240\u8ff0\uff1a Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science , such as computer algorithms , programming languages , cryptography , automated theorem proving , and software development . Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems, such as in operations research . \u5173\u4e8e\u6b64\u7684\u601d\u8003\uff0c\u89e6\u53d1\u4e86\u6211\u5199\u4f5c\u672c\u4e66\uff0c\u6211\u7684\u5199\u4f5c\u76ee\u7684\u4e3b\u8981\u662f\uff1a\u68b3\u7406discrete math\u7684\u7406\u8bba\u77e5\u8bc6\uff0c\u7528\u8fd9\u4e9b\u77e5\u8bc6\uff0c\u66f4\u597d\u5730\u89e3\u51b3computer scientific\u4e2drepresentation\u3001computation\uff0c\u5f80\u66f4\u52a0\u5177\u4f53\u6765\u8bf4\u662f\uff1a \u5bf9\u4e8ediscrete objects\u76f8\u5173\u7684computation\u95ee\u9898\uff0c\u5982\u4f55\u8bbe\u8ba1algorithm\uff1f\u5982\u4f55\u8bbe\u8ba1representation\uff1f \u5728 Discrete-math\\Why-we-need-discrete-math \u4e2d\uff0c\u4e5f\u9610\u8ff0\u4e86\u6211\u7684\u5199\u4f5c\u52a8\u673a\u3002 \u5173\u4e8ediscrete math\uff0c\u6211\u60f3\u8d77\u4e86\u5728\u5927\u5b66\u4e8c\u5e74\u7ea7\u65f6\u5b66\u4e60discrete math\u8bfe\u7a0b\uff0c\u5f53\u65f6\u4f7f\u7528\u7684\u6559\u6750\u662f\u975e\u5e38\u7ecf\u5178\u7684 Discrete Mathematics and Its Applications \uff0c\u56e0\u6b64\u8fd9\u4e2a\u5de5\u7a0b\u4e2d\u7684\u4e00\u4e9b\u5185\u5bb9\u662f\u6e90\u81ea\u4e8e\u8fd9\u672c\u6559\u6750\u3002 \u5728 Computer-science-is-discrete \u4e2d\u7ed9\u51fa\u4e86\u8bba\u8bc1\"Computer scientific is discrete\"\u89c2\u70b9\u7684\u4f8b\u5b50\u3002","title":"Computer scientific is discrete"},{"location":"#_2","text":"\u9996\u5148\u8bf4\u660e\u4ec0\u4e48\u662fdiscrete\u3001discrete math\uff0c\u7136\u540e\u63cf\u8ff0discrete objects\u7684\u7279\u6027\uff0cdiscrete objects\u7684\u8fd9\u4e9b\u7279\u6027\u4f7f\u5f97\u5b83\u662f**computable**\u7684\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u8bbe\u8ba1algorithm\u6765\u89e3\u51b3\u4e0e\u5b83\u76f8\u5173\u7684\u95ee\u9898\u3002discrete math\u4e2d\u7684\u7406\u8bba\u77e5\u8bc6\u662f\u540e\u7eedalgorithm\u3001data-structure\u7684\u57fa\u7840\u3002","title":"\u4e66\u5199\u601d\u8def"},{"location":"#discrete#vs#continuous","text":"\u9996\u5148\uff0c\u6765\u601d\u8003discrete\u7684\u542b\u4e49\u3002 \u201cdiscrete\u201d\u5373\u201c\u79bb\u6563\u201d\uff0c\u201ccontinuous\u201d\u5373\u201c\u8fde\u7eed\u201d\uff0c\u4e24\u8005\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u5bf9\u53cd\u4e49\u8bcd\uff0c\u5982\u679c\u662fdiscrete\u7684\u8bdd\uff0c\u5219\u5fc5\u7136\u5c31\u4e0d\u662fcontinuous\u7684\u3002 \u5173\u4e8e\u201cdiscrete\u201d\u7684\u6982\u5ff5\uff0c\u7ef4\u57fa\u767e\u79d1 Discrete space \u7ed9\u51fa\u4e86\u89e3\u91ca\u3002\u6211\u7684\u76f4\u89c2\u7406\u89e3\u5c31\u662f\u4e0d\u76f8\u8fde\u7684\u3001\u662f\u4e00\u4e2a\u4e00\u4e2anode\uff0c\u540e\u9762\u6211\u4eec\u5c06\u5b83\u79f0\u4e3a\u201cdiscrete object\u201d\u3002 \u5173\u4e8e\u201ccontinuous\u201d\u7684\u6982\u5ff5\uff0c\u7ef4\u57fa\u767e\u79d1 Continuous function \u7ed9\u51fa\u4e86\u89e3\u91ca\uff0c\u6211\u7684\u76f4\u89c2\u7406\u89e3\u662f\u76f8\u8fde\u7684\u3001\u662f\uff08\u5e73\u6ed1\u7684\uff09\u66f2\u7ebf\u3002 \u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u7ef4\u57fa\u767e\u79d1 Discrete mathematics \u7684\u7b2c\u4e00\u6bb5\u7684\u8bba\u8ff0\u4e5f\u662f\u6bd4\u8f83\u597d\u7684\u3002 \u6839\u636e\u8fd9\u4e24\u4e2a\u6027\u8d28\uff0c\u53ef\u4ee5\u5c06\u6570\u5b66\u5b66\u79d1\u8fdb\u884c\u5206\u7c7b\u3002","title":"Discrete VS continuous"},{"location":"#property#of#discrete#objects","text":"\u672c\u8282\u603b\u7ed3\u7531discrete\u884d\u751f\u51fa\u6765\u7684\u4e00\u7cfb\u5217\u7279\u6027\uff0c\u663e\u7136discrete objects\u9664\u4e86\u5177\u5907discrete\u7279\u6027\uff0c\u8fd8\u5177\u5907\u5982\u4e0b\u7279\u6027\u3002","title":"Property of discrete objects"},{"location":"#enumerable","text":"\u201cenumerable\u201d\u5373\u201c\u53ef\u679a\u4e3e\u7684\u201d\uff0c\u8fd9\u4e2a\u7279\u6027\u975e\u5e38\u91cd\u8981\u3002 \u7ef4\u57fa\u767e\u79d1 Discrete mathematics \uff1a Discrete objects can often be enumerated by integers.","title":"Enumerable"},{"location":"#countable","text":"\u5173\u4e8e\u201ccountable\u201d\uff0c\u53ef\u4ee5\u53c2\u89c1wikipedia Countable set \u3002 discrete objects\u4e00\u822c\u662fcountable\u7684\uff08\u6709\u7684\u662f\u65e0\u6cd5count\u7684\uff09\u3002","title":"Countable"},{"location":"#computable","text":"\u5173\u4e8e\u201ccomputable\u201d\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Computability theory \u3001\u7ef4\u57fa\u767e\u79d1 Computability \u3002 discrete objects\uff0c\u4e00\u822c\u662fcomputable\u7684\uff0c\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u5c31\u9700\u8981\u601d\u8003\uff1a\u5982\u4f55\u6765\u5b9e\u73b0\u201cComputation on discrete objects\u201d\uff1b\u663e\u7136\u4e0e\u6b64\u76f8\u5173\u7684\u4e00\u4e2a\u91cd\u8981\u8bfe\u9898\u5c31\u662f\uff1a\u7b97\u6cd5\uff0c\u8fd9\u5728\u540e\u7eed\u7684\u7ae0\u8282\u4e2d\u4f1a\u8fdb\u884c\u4ecb\u7ecd\u3002Countable\u662f\u5bf9computation\u8fdb\u884c\u91cf\u5316\u8fdb\u800c\u8fdb\u884c**\u7b97\u6cd5\u590d\u6742\u5ea6\u5206\u6790**\u7684\u524d\u63d0\u3002","title":"Computable"},{"location":"Computer-science-is-discrete/","text":"Computer science is discrete \u4e0b\u9762\u662f\u8bba\u8bc1\u201ccomputer science is discrete\u201d\u7684\u4f8b\u5b50\uff1a One-by-one model \u5728 Relation-structure-computation\\Computation\\index.md \u4e2d\u63cf\u8ff0\u7684one-by-one model\u4f53\u73b0\u4e86\u201cComputer science is discrete\"\u3002one-by-one\u662f\u5178\u578b\u7684\u79bb\u6563\u7684\uff0c\u800c\u4e0d\u662f\u8fde\u7eed\u7684\u3002 Recursive definition and discrete \u4efb\u4f55recursive definition\uff0c\u90fd\u662f\u79bb\u6563\u7684\uff0c\u90fd\u53ef\u4ee5\u8fdb\u884c\u8ba1\u7b97 \u51fd\u6570\u7684\u6267\u884c\u662f\u79bb\u6563\u7684 \u540d\u79f0\u4e2d\u7684sub\uff0c\u8868\u793a\u5176\u5177\u5907**\u5305\u542b\u5173\u7cfb**\u3002 \u5c06\u51fd\u6570\u62bd\u8c61\u6210\u4e00\u4e2a\u4e00\u4e2a\u7684\u8282\u70b9\uff0c\u5219\u6211\u4eec\u6240\u63cf\u8ff0\u7684\u662f\u4e00\u5f20\u56fe\u3002 \u663e\u7136\u5b83\u662f\u5177\u5907\u79bb\u6563\u7279\u6027\u7684\u3002 \u5b83\u7684\u6267\u884c\u6a21\u578b\u662f\u79bb\u6563\u7684 \u5c06\u8c03\u7528\u4e00\u4e2a\u51fd\u6570\u770b\u505a\u662f\u753b\u4e00\u4e2a\u70b9\u7684\u8bdd\uff0c\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u5c06\u5448\u73b0\u6811\u7ed3\u6784\uff0c\u5b83\u662f**\u5305\u542b\u5173\u7cfb**\uff08\u5728 Relation-structure-computation \u7ae0\u8282\u4f1a\u5bf9\u6b64\u8fdb\u884c\u63cf\u8ff0\uff09\u3002 0 and 1 everything in computer is 0 or 1. \u6570\u5b57\u7535\u8def computer\u5f80\u5f80\u91c7\u7528\u7684\u662f\u6570\u5b57\u7535\u8def","title":"Computer-science-is-discrete"},{"location":"Computer-science-is-discrete/#computer#science#is#discrete","text":"\u4e0b\u9762\u662f\u8bba\u8bc1\u201ccomputer science is discrete\u201d\u7684\u4f8b\u5b50\uff1a","title":"Computer science is discrete"},{"location":"Computer-science-is-discrete/#one-by-one#model","text":"\u5728 Relation-structure-computation\\Computation\\index.md \u4e2d\u63cf\u8ff0\u7684one-by-one model\u4f53\u73b0\u4e86\u201cComputer science is discrete\"\u3002one-by-one\u662f\u5178\u578b\u7684\u79bb\u6563\u7684\uff0c\u800c\u4e0d\u662f\u8fde\u7eed\u7684\u3002","title":"One-by-one model"},{"location":"Computer-science-is-discrete/#recursive#definition#and#discrete","text":"\u4efb\u4f55recursive definition\uff0c\u90fd\u662f\u79bb\u6563\u7684\uff0c\u90fd\u53ef\u4ee5\u8fdb\u884c\u8ba1\u7b97","title":"Recursive definition and discrete"},{"location":"Computer-science-is-discrete/#_1","text":"\u540d\u79f0\u4e2d\u7684sub\uff0c\u8868\u793a\u5176\u5177\u5907**\u5305\u542b\u5173\u7cfb**\u3002 \u5c06\u51fd\u6570\u62bd\u8c61\u6210\u4e00\u4e2a\u4e00\u4e2a\u7684\u8282\u70b9\uff0c\u5219\u6211\u4eec\u6240\u63cf\u8ff0\u7684\u662f\u4e00\u5f20\u56fe\u3002 \u663e\u7136\u5b83\u662f\u5177\u5907\u79bb\u6563\u7279\u6027\u7684\u3002 \u5b83\u7684\u6267\u884c\u6a21\u578b\u662f\u79bb\u6563\u7684 \u5c06\u8c03\u7528\u4e00\u4e2a\u51fd\u6570\u770b\u505a\u662f\u753b\u4e00\u4e2a\u70b9\u7684\u8bdd\uff0c\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u5c06\u5448\u73b0\u6811\u7ed3\u6784\uff0c\u5b83\u662f**\u5305\u542b\u5173\u7cfb**\uff08\u5728 Relation-structure-computation \u7ae0\u8282\u4f1a\u5bf9\u6b64\u8fdb\u884c\u63cf\u8ff0\uff09\u3002","title":"\u51fd\u6570\u7684\u6267\u884c\u662f\u79bb\u6563\u7684"},{"location":"Computer-science-is-discrete/#0#and#1","text":"everything in computer is 0 or 1.","title":"0 and 1"},{"location":"Computer-science-is-discrete/#_2","text":"computer\u5f80\u5f80\u91c7\u7528\u7684\u662f\u6570\u5b57\u7535\u8def","title":"\u6570\u5b57\u7535\u8def"},{"location":"Discrete-math/","text":"Discrete math \u672c\u6587\u56de\u7b54\u201cwhat is discrete math\u201d\uff0c\u4ee5\u53ca\u770b\u770btopics in discrete mathematics\u3002 wikipedia Discrete mathematics Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous . In contrast to real numbers that have the property of varying \"smoothly\", the objects studied in discrete mathematics \u2013 such as integers , graphs , and statements in logic \u2013 do not vary smoothly in this way, but have distinct, separated values. Discrete mathematics therefore excludes topics in \"continuous mathematics\" such as calculus or Euclidean geometry . Discrete objects can often be enumerated by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (finite sets or sets with the same cardinality as the natural numbers). NOTE: \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u4e00\u6bb5\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002 Topics in discrete mathematics","title":"Introduction"},{"location":"Discrete-math/#discrete#math","text":"\u672c\u6587\u56de\u7b54\u201cwhat is discrete math\u201d\uff0c\u4ee5\u53ca\u770b\u770btopics in discrete mathematics\u3002","title":"Discrete math"},{"location":"Discrete-math/#wikipedia#discrete#mathematics","text":"Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous . In contrast to real numbers that have the property of varying \"smoothly\", the objects studied in discrete mathematics \u2013 such as integers , graphs , and statements in logic \u2013 do not vary smoothly in this way, but have distinct, separated values. Discrete mathematics therefore excludes topics in \"continuous mathematics\" such as calculus or Euclidean geometry . Discrete objects can often be enumerated by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (finite sets or sets with the same cardinality as the natural numbers). NOTE: \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u4e00\u6bb5\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002","title":"wikipedia Discrete mathematics"},{"location":"Discrete-math/#topics#in#discrete#mathematics","text":"","title":"Topics in discrete mathematics"},{"location":"Discrete-math/Why-we-need-discrete-math/","text":"Why need discrete math \u672c\u6587\u89e3\u7b54\u4e3a\u4ec0\u4e48\u6211\u4eec\u9700\u8981\u79bb\u6563\u6570\u5b66\uff0c\u672c\u6587\u622a\u53d6\u81eaBook-Discrete-Mathematics-and-Its-Applications\u7684Preface\uff0c \u539f\u4e66\u7684\u8fd9\u4e00\u6bb5\u6211\u89c9\u5f97\u603b\u7ed3\u5730\u975e\u5e38\u597d\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48software engineer\u9700\u8981\u5b66\u4e60discrete math\u3002 Goals of a Discrete Mathematics Course A discrete mathematics course has more than one purpose. Students should learn a particular set of mathematical facts and how to apply them; more importantly, such a course should teach students how to think logically and mathematically . To achieve these goals, this text stresses mathematical reasoning and the different ways problems are solved. Five important themes are interwoven in this text: mathematical reasoning , combinatorial analysis , discrete structures , algorithmic thinking , and applications and modeling . A successful discrete mathematics course should carefully blend\uff08\u6df7\u5408\uff09 and balance all five themes. Mathematical Reasoning NOTE: \u6570\u5b66\u63a8\u7406 Students must understand mathematical reasoning in order to read, comprehend, and construct mathematical arguments. This text starts with a discussion of mathematical logic , which serves as the foundation for the subsequent discussions of methods of proof . Both the science and the art of constructing proofs are addressed. The technique of mathematical induction is stressed through many different types of examples of such proofs and a careful explanation of why mathematical induction is a valid proof technique. Combinatorial Analysis NOTE: \u7ec4\u5408\u5206\u6790 An important problem-solving skill is the ability to count or enumerate objects. The discussion of enumeration in this book begins with the basic techniques of counting . The stress is on performing combinatorial analysis to solve counting problems and analyze algorithms, not on applying formulae\uff08\u516c\u5f0f\uff09. Discrete Structures NOTE: \u7ed3\u6784\u5316\u601d\u7ef4 A course in discrete mathematics should teach students how to work with discrete structures , which are the abstract mathematical structures used to represent discrete objects and relationships between these objects. These discrete structures include sets , permutations , relations , graphs , trees , and finite-state machines . Algorithmic Thinking Certain classes of problems are solved by the specification of an algorithm. After an algorithm has been described, a computer program can be constructed implementing it. The mathematical portions of this activity, which include the specification of the algorithm, the verification that it works properly, and the analysis of the computer memory and time required to perform it,are all covered in this text. Algorithms are described using both English and an easily understood form of pseudocode. Applications and Modeling NOTE: \u5efa\u6a21 Discrete mathematics has applications to almost every conceivable area of study. There are many applications to computer science and data networking in this text, as well as applications to such diverse areas as chemistry, biology, linguistics, geography, business, and the Internet. These applications are natural and important uses of discrete mathematics and are not contrived. Modeling with discrete mathematics is an extremely important problem-solving skill, which students have the opportunity to develop by constructing their own models in some of the exercises.","title":"Why-we-need-discrete-math"},{"location":"Discrete-math/Why-we-need-discrete-math/#why#need#discrete#math","text":"\u672c\u6587\u89e3\u7b54\u4e3a\u4ec0\u4e48\u6211\u4eec\u9700\u8981\u79bb\u6563\u6570\u5b66\uff0c\u672c\u6587\u622a\u53d6\u81eaBook-Discrete-Mathematics-and-Its-Applications\u7684Preface\uff0c \u539f\u4e66\u7684\u8fd9\u4e00\u6bb5\u6211\u89c9\u5f97\u603b\u7ed3\u5730\u975e\u5e38\u597d\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48software engineer\u9700\u8981\u5b66\u4e60discrete math\u3002","title":"Why need discrete math"},{"location":"Discrete-math/Why-we-need-discrete-math/#goals#of#a#discrete#mathematics#course","text":"A discrete mathematics course has more than one purpose. Students should learn a particular set of mathematical facts and how to apply them; more importantly, such a course should teach students how to think logically and mathematically . To achieve these goals, this text stresses mathematical reasoning and the different ways problems are solved. Five important themes are interwoven in this text: mathematical reasoning , combinatorial analysis , discrete structures , algorithmic thinking , and applications and modeling . A successful discrete mathematics course should carefully blend\uff08\u6df7\u5408\uff09 and balance all five themes.","title":"Goals of a Discrete Mathematics Course"},{"location":"Discrete-math/Why-we-need-discrete-math/#mathematical#reasoning","text":"NOTE: \u6570\u5b66\u63a8\u7406 Students must understand mathematical reasoning in order to read, comprehend, and construct mathematical arguments. This text starts with a discussion of mathematical logic , which serves as the foundation for the subsequent discussions of methods of proof . Both the science and the art of constructing proofs are addressed. The technique of mathematical induction is stressed through many different types of examples of such proofs and a careful explanation of why mathematical induction is a valid proof technique.","title":"Mathematical Reasoning"},{"location":"Discrete-math/Why-we-need-discrete-math/#combinatorial#analysis","text":"NOTE: \u7ec4\u5408\u5206\u6790 An important problem-solving skill is the ability to count or enumerate objects. The discussion of enumeration in this book begins with the basic techniques of counting . The stress is on performing combinatorial analysis to solve counting problems and analyze algorithms, not on applying formulae\uff08\u516c\u5f0f\uff09.","title":"Combinatorial Analysis"},{"location":"Discrete-math/Why-we-need-discrete-math/#discrete#structures","text":"NOTE: \u7ed3\u6784\u5316\u601d\u7ef4 A course in discrete mathematics should teach students how to work with discrete structures , which are the abstract mathematical structures used to represent discrete objects and relationships between these objects. These discrete structures include sets , permutations , relations , graphs , trees , and finite-state machines .","title":"Discrete Structures"},{"location":"Discrete-math/Why-we-need-discrete-math/#algorithmic#thinking","text":"Certain classes of problems are solved by the specification of an algorithm. After an algorithm has been described, a computer program can be constructed implementing it. The mathematical portions of this activity, which include the specification of the algorithm, the verification that it works properly, and the analysis of the computer memory and time required to perform it,are all covered in this text. Algorithms are described using both English and an easily understood form of pseudocode.","title":"Algorithmic Thinking"},{"location":"Discrete-math/Why-we-need-discrete-math/#applications#and#modeling","text":"NOTE: \u5efa\u6a21 Discrete mathematics has applications to almost every conceivable area of study. There are many applications to computer science and data networking in this text, as well as applications to such diverse areas as chemistry, biology, linguistics, geography, business, and the Internet. These applications are natural and important uses of discrete mathematics and are not contrived. Modeling with discrete mathematics is an extremely important problem-solving skill, which students have the opportunity to develop by constructing their own models in some of the exercises.","title":"Applications and Modeling"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u662f\u6211\u9605\u8bfb Discrete Mathematics and Its Applications \u7684\u7b14\u8bb0\u3002","title":"Introduction"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/#_1","text":"\u672c\u7ae0\u662f\u6211\u9605\u8bfb Discrete Mathematics and Its Applications \u7684\u7b14\u8bb0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chapter-2-Basic-Structures/","text":"2 Basic Structures: Sets, Functions, Sequences, Sums, and Matrices Much of discrete mathematics is devoted to the study of discrete structures, used to represent discrete objects . Many important discrete structures are built using sets, which are collections of objects. Among the discrete structures built from sets are combinations ,unordered collections of objects used extensively in counting; relations , sets of ordered pairs that represent relationships between objects; graphs , sets of vertices and edges that connect vertices; and finite state machines, used to model computing machines. These are some of the topics we will study in later chapters.","title":"Chapter-2-Basic-Structures"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chapter-2-Basic-Structures/#2#basic#structures#sets#functions#sequences#sums#and#matrices","text":"Much of discrete mathematics is devoted to the study of discrete structures, used to represent discrete objects . Many important discrete structures are built using sets, which are collections of objects. Among the discrete structures built from sets are combinations ,unordered collections of objects used extensively in counting; relations , sets of ordered pairs that represent relationships between objects; graphs , sets of vertices and edges that connect vertices; and finite state machines, used to model computing machines. These are some of the topics we will study in later chapters.","title":"2 Basic Structures: Sets, Functions, Sequences, Sums, and Matrices"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chapter-5-Induction-and-Recursion/","text":"5 Induction and Recursion Proofs using mathematical induction have two parts. First, they show that the statement holds for the positive integer 1. Second, they show that if the statement holds for a positive integer then it must also hold for the next larger integer. Mathematical induction is based on the rule of inference that tells us that if P(1) and \u2200k(P(k) \u2192 P(k + 1)) are true for the domain of positive integers,then \u2200nP(n) is true. Mathematical induction can be used to prove a tremendous variety of results. Understanding how to read and construct proofs by mathematical induction is a key goal of learning discrete mathematics. In Chapter 2 we explicitly defined sets and functions. That is, we described sets by listing their elements or by giving some property that characterizes these elements. We gave formulae for the values of functions. There is another important way to define such objects, based on mathematical induction. To define functions, some initial terms are specified, and a rule is given for finding subsequent values from values already known. (We briefly touched on this sort of definition in Chapter 2 when we showed how sequences can be defined using recurrence relations.)Sets can be defined by listing some of the i r elements and giving rules for constructing elements from those already known to be in the set. Such definitions,called recursive definitions, are used throughout discrete mathematics and computer science. Once we have defined a set recursively, we can use a proof method called structural induction to prove results about this set.","title":"Chapter-5-Induction-and-Recursion"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chapter-5-Induction-and-Recursion/#5#induction#and#recursion","text":"Proofs using mathematical induction have two parts. First, they show that the statement holds for the positive integer 1. Second, they show that if the statement holds for a positive integer then it must also hold for the next larger integer. Mathematical induction is based on the rule of inference that tells us that if P(1) and \u2200k(P(k) \u2192 P(k + 1)) are true for the domain of positive integers,then \u2200nP(n) is true. Mathematical induction can be used to prove a tremendous variety of results. Understanding how to read and construct proofs by mathematical induction is a key goal of learning discrete mathematics. In Chapter 2 we explicitly defined sets and functions. That is, we described sets by listing their elements or by giving some property that characterizes these elements. We gave formulae for the values of functions. There is another important way to define such objects, based on mathematical induction. To define functions, some initial terms are specified, and a rule is given for finding subsequent values from values already known. (We briefly touched on this sort of definition in Chapter 2 when we showed how sequences can be defined using recurrence relations.)Sets can be defined by listing some of the i r elements and giving rules for constructing elements from those already known to be in the set. Such definitions,called recursive definitions, are used throughout discrete mathematics and computer science. Once we have defined a set recursively, we can use a proof method called structural induction to prove results about this set.","title":"5 Induction and Recursion"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chapter-6-Counting/","text":"6 Counting Combinatorics, the study of arrangements of objects, is an important part of discrete mathematics. This subject was studied as long ago as the seventeenth century, when combinatorial questions arose in the study of gambling games. Enumeration, the counting of objects with certain properties, is an important part of combinatorics. We must count objects to solve many different types of problems. For instance, counting is used to determine the complexity of algorithms. Counting is also required to determine whether there are enough telephone numbers or Internet protocol addresses to meet demand. Recently,it has played a key role in mathematical biology, especially in sequencing DNA. Furthermore, counting techniques are used extensively when probabilities of events are computed. 6.1 The Basics of Counting Tree Diagrams Counting problems can be solved using tree diagrams. A tree consists of a root, a number of branches leaving the root, and possible additional branches leaving the endpoints of other branches. (We will study trees in detail in Chapter 11.)To use trees in counting, we use a branch to represent each possible choice. We represent the possible outcomes by the leaves , which are the endpoints of branches not having other branches starting at them. NOTE: \u5f53\u4f7f\u7528tree diagram\u6765\u89e3\u51b3counting problem\u7684\u65f6\u5019\uff0c\u91c7\u7528\u7684\u662f\u81ea\u9876\u5411\u4e0b\u7b56\u7565\u6765\u6784\u5efatree\u3002\u5728\u4e0a\u9762\u8fd9\u6bb5\u4e2d\u63cf\u8ff0\u7684\u8fd9\u79cd\u5173\u7cfb\uff0c\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4f7f\u7528tree model\u7684nesting\u5173\u7cfb\u6765\u8fdb\u884c\u8868\u8fbe\u3002 \u5173\u4e8e\u201c\u81ea\u9876\u5411\u4e0b\u7b56\u7565\u201d\u3001\u201cnesting\u5173\u7cfb\u201d\uff0c\u53c2\u89c1\u5de5\u7a0b data-structure \u7684 Tree \u7ae0\u8282\u3002 Note that when a tree diagram is used to solve a counting problem, the number of choices of which branch to follow to reach a leaf can vary (see Example 21, for example). EXAMPLE 22 A play off between two teams consists of at most five games. The first team that wins three games wins the playoff. In how many different ways can the playoff occur? Solution : The tree diagram in Figure 3 displays all the ways the playoff can proceed, with the winner of each game shown. We see that there are 20 different ways for the playoff to occur.","title":"Chapter-6-Counting"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chapter-6-Counting/#6#counting","text":"Combinatorics, the study of arrangements of objects, is an important part of discrete mathematics. This subject was studied as long ago as the seventeenth century, when combinatorial questions arose in the study of gambling games. Enumeration, the counting of objects with certain properties, is an important part of combinatorics. We must count objects to solve many different types of problems. For instance, counting is used to determine the complexity of algorithms. Counting is also required to determine whether there are enough telephone numbers or Internet protocol addresses to meet demand. Recently,it has played a key role in mathematical biology, especially in sequencing DNA. Furthermore, counting techniques are used extensively when probabilities of events are computed.","title":"6 Counting"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chapter-6-Counting/#61#the#basics#of#counting","text":"","title":"6.1 The Basics of Counting"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chapter-6-Counting/#tree#diagrams","text":"Counting problems can be solved using tree diagrams. A tree consists of a root, a number of branches leaving the root, and possible additional branches leaving the endpoints of other branches. (We will study trees in detail in Chapter 11.)To use trees in counting, we use a branch to represent each possible choice. We represent the possible outcomes by the leaves , which are the endpoints of branches not having other branches starting at them. NOTE: \u5f53\u4f7f\u7528tree diagram\u6765\u89e3\u51b3counting problem\u7684\u65f6\u5019\uff0c\u91c7\u7528\u7684\u662f\u81ea\u9876\u5411\u4e0b\u7b56\u7565\u6765\u6784\u5efatree\u3002\u5728\u4e0a\u9762\u8fd9\u6bb5\u4e2d\u63cf\u8ff0\u7684\u8fd9\u79cd\u5173\u7cfb\uff0c\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4f7f\u7528tree model\u7684nesting\u5173\u7cfb\u6765\u8fdb\u884c\u8868\u8fbe\u3002 \u5173\u4e8e\u201c\u81ea\u9876\u5411\u4e0b\u7b56\u7565\u201d\u3001\u201cnesting\u5173\u7cfb\u201d\uff0c\u53c2\u89c1\u5de5\u7a0b data-structure \u7684 Tree \u7ae0\u8282\u3002 Note that when a tree diagram is used to solve a counting problem, the number of choices of which branch to follow to reach a leaf can vary (see Example 21, for example). EXAMPLE 22 A play off between two teams consists of at most five games. The first team that wins three games wins the playoff. In how many different ways can the playoff occur? Solution : The tree diagram in Figure 3 displays all the ways the playoff can proceed, with the winner of each game shown. We see that there are 20 different ways for the playoff to occur.","title":"Tree Diagrams"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chpater-9-Relations/","text":"9 Relations","title":"Chpater-9-Relations"},{"location":"Discrete-math/Book-Discrete-Mathematics-and-Its-Applications/Chpater-9-Relations/#9#relations","text":"","title":"9 Relations"},{"location":"Discrete-math/Number-theory/Ordinal-number/","text":"Ordinal number","title":"Ordinal-number"},{"location":"Discrete-math/Number-theory/Ordinal-number/#ordinal#number","text":"","title":"Ordinal number"},{"location":"Discrete-math/Number-theory/Prime-number/","text":"Prime number","title":"Prime-number"},{"location":"Discrete-math/Number-theory/Prime-number/#prime#number","text":"","title":"Prime number"},{"location":"Discrete-math/Set-theory/","text":"\u5173\u4e8e\u672c\u7ae0 \u201cset\u201d\u662f\u6570\u5b66\u7684\u57fa\u77f3\uff0c\u6570\u5b66\u4e2d\u7684\u5927\u90e8\u5206\u6982\u5ff5\u90fd\u662f\u5efa\u7acb\u4e8eset\u4e4b\u4e0a\u7684\u3002 \u5f88\u591amath object\uff08\u6570\u5b66\u5bf9\u8c61\uff09\u7684\u672c\u8d28\u90fd\u662f\uff1a relation function ...... set\u662f\u679a\u4e3e\u3001\u6982\u62ec\u3002","title":"Introduction"},{"location":"Discrete-math/Set-theory/#_1","text":"\u201cset\u201d\u662f\u6570\u5b66\u7684\u57fa\u77f3\uff0c\u6570\u5b66\u4e2d\u7684\u5927\u90e8\u5206\u6982\u5ff5\u90fd\u662f\u5efa\u7acb\u4e8eset\u4e4b\u4e0a\u7684\u3002 \u5f88\u591amath object\uff08\u6570\u5b66\u5bf9\u8c61\uff09\u7684\u672c\u8d28\u90fd\u662f\uff1a relation function ...... set\u662f\u679a\u4e3e\u3001\u6982\u62ec\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Discrete-math/Set-theory/Power-set/","text":"Power set","title":"Power-set"},{"location":"Discrete-math/Set-theory/Power-set/#power#set","text":"","title":"Power set"},{"location":"Relation-structure-computation/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u7684\u6807\u9898\u662f\"Relation-structure-computation\"\uff0c\u5b83\u7684\u542b\u4e49\u662f\u5173\u7cfb\u3001\u7ed3\u6784\u3001\u8ba1\u7b97\u201d\u3002\u672c\u6587\u5bf9\u672c\u7ae0\u7684\u5185\u5bb9\u8fdb\u884c\u7efc\u8ff0\uff0c\u4ee5\u4ece\u4e00\u4e2a\u8f83\u9ad8\u7684\u89d2\u5ea6\u6765\u638c\u63e1\u672c\u7ae0\u7684\u5185\u5bb9\u3002 \u7efc\u8ff0: relation-structure-computation \u5982\u679cdiscrete object\u4e4b\u95f4\u6ca1\u6709relation\uff0c\u90a3\u4e48\u5b83\u4eec\u5c31\u662f\u4e00\u5806\u6742\u4e71\u65e0\u7ae0\u3001\u6beb\u65e0\u89c4\u5f8b\u53ef\u8a00\u7684\u6df7\u6c8c\u7cfb\u7edf\uff0c\u65e0\u6cd5\u4f7f\u7528\u7cbe\u7b80\u7684\u6570\u5b66\u8bed\u8a00\u5bf9\u5176\u8fdb\u884c\u63cf\u8ff0\uff08 formal description \uff09\uff0c\u6211\u4eec\u6240\u5173\u6ce8\u7684\u662f\u90a3\u4e9b\u5177\u5907\u4e2d**relation**\u7684discrete objects\u3002 \u5bf9\u4e8e\u5177\u5907relation\u7684discrete objects\uff08\u53ef\u4ee5\u770b\u505a\u662f**node**\uff09\uff0c\u901a\u8fc7relation\u5c06discrete objects\u8fdb\u884c\u5173\u8054\uff0c\u663e\u7136\u5b83\u4eec\u5f62\u6210\u4e86\u4e00\u4e2a\u4e00\u4e2a\u7684**ordered pair**\uff08\u53ef\u4ee5\u770b\u505a\u662f**edge**\uff09\uff0c\u8fd9\u4e9b**ordered pair**\u5c31\u5f62\u6210\u4e86\u4e00\u5b9a\u7684**structure**\u3002\u6211\u4eec\u5c06\u5728 Structure \u7ae0\u8282\u8ba8\u8bbarelation and structure\uff0c\u6211\u4eec\u5c06\u4f7f\u7528relation\u7684\u7406\u8bba\u77e5\u8bc6\u6765\u63cf\u8ff0structure\u3002 NOTE: \u201cordered\u201d\u8bf4\u660erelation\u662f\u5177\u5907**\u65b9\u5411\u6027**\u7684 relation**\u51b3\u5b9a**structure \uff0c\u76f8\u540c\u7684**relation**\u51b3\u5b9a\u4e86\u5b83\u4eec\u5177\u5907\u76f8\u540c\u7684**structure**\uff0c\u8fdb\u800c\u53ef\u4ee5\u4f7f\u7528\u76f8\u540c\u7684algorithm\u6765\u8fdb\u884c\u8ba1\u7b97( computation )\u3002 \u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff1aclass hierarchy\u3001grammar tree\u90fd\u662f**containing\u5173\u7cfb**\uff0c\u8fdb\u800c\u51b3\u5b9a\u4e86\u5bf9\u5b83\u4eec\u8fdb\u884c\u641c\u7d22\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u91c7\u7528\u76f8\u540c\u7684\u7b97\u6cd5\u3002 \u6240\u4ee5\uff0c relation\u662f\u6838\u5fc3\u6240\u5728 : relation\u51b3\u5b9astructure -> \u51b3\u5b9a\u91c7\u7528\u7684algorithm( computation )\u3002\u56e0\u6b64\uff0c\u6211\u4eec**\u57fa\u4e8erelation\u6765\u5efa\u7acbmodel**\uff08\u6a21\u578b\uff09\uff1a \u8bbe\u8ba1\u51fa\u975e\u5e38\u9ad8\u6548\u7684algorithm\u6765\u5bf9\u5b83\u76f8\u5173\u7684\u95ee\u9898\u8fdb\u884ccomputation\uff08\u5178\u578b\u7684\u4f8b\u5b50containing\u5173\u7cfb\uff0c\u5728computer science\u4e2d\uff0cCFG\u3001class inheritance\u90fd\u670d\u4ece\u8fd9\u79cd\u5173\u7cfb\uff09 \u540e\u7eed\uff0c\u6211\u4f1a\u5bf9computer science\u4e2d\u5e38\u89c1\u7684relation\u4f1a\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u4f7f\u7528\u5df2\u6709\u7684model\u6765\u89e3\u51b3\u95ee\u9898\u3002 \u5728 Relation-structure-computation\\Model \u4e2d\u5bf9model\u8fdb\u884c\u63cf\u8ff0\u3002 \u5728 Relation-structure-computation\\Model\\Containing-relation-model.md \u4e2d\u5bf9**containing\u5173\u7cfb**\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 NOTE: \u4e0a\u8ff0\u8fd9\u79cd\u5efa\u7acbmodel\u7684\u601d\u60f3\u5728science\u4e2d\u662f\u5e7f\u6cdb\u5b58\u5728\u7684\uff0c\u6bd4\u5982\u5728\u6982\u7387\u8bba\u4e2d\uff0c\u6709\u7740\u975e\u5e38\u591a\u7684\u6982\u7387\u6a21\u578b\uff0c\u6570\u5b66\u5bb6\u4f7f\u7528\u8fd9\u4e9b\u6982\u7387\u6a21\u578b\u6765\u63cf\u8ff0\u5b9e\u9645\u95ee\u9898\uff0c\u6bd4\u5982\uff1a \u6b63\u6001\u5206\u5e03 \u4e8c\u9879\u5f0f\u53d1\u5e03 computation\u662f\u6307\u4e3a\u89e3\u51b3\u95ee\u9898\u91c7\u7528\u7684algorithm\uff1b Relation and structure are abstraction Relation \u548c structure \u90fd\u662f abstraction\uff0crelation \u662f\u4ece concrete \u4e2d\u62bd\u8c61\u51fa\u6765\u7684\uff0c\u4e8e\u6b64\u7c7b\u4f3c\u7684\u662f structure \u4e5f\u662f\u5982\u6b64\u7684\u3002 Graph and relation **graph**\u662f\u8868\u793arelation\u7684\u6709\u6548\u5de5\u5177\uff0c\u540e\u9762\u6211\u4eec\u4f1a\u770b\u5230graph\u7684\u5b9a\u4e49\u548crelation\u7684\u5b9a\u4e49\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\uff0c\u57fa\u4e8egraph\u7684algorithm\uff0c\u53ef\u4ee5\u89e3\u51b3\u5f88\u591arelation\u7684\u95ee\u9898\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u6709\u5fc5\u8981\u5b66\u4e60graph-theory\u7684\u77e5\u8bc6\u3002 Computation \u5728discrete\u9886\u57df\uff0c\u6211\u4eec\u8003\u8651\u7684\u4e3b\u8981\u662f\uff1a\u5bf9\u4e8e\u5177\u5907relation\u7684discrete objects\u7684computation\uff0c\u540e\u9762\u6211\u4eec\u5c06\u770b\u5230\uff0c\u5bf9\u4e8e\u67d0\u4e9brelation\uff0c\u6211\u4eec\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u975e\u5e38\u9ad8\u6548\u7684algorithm\u6765\u5bf9\u5b83\u4eec\u8fdb\u884ccomputation\uff0c\u6211\u4eec\u5c06\u5728 Computation \u7ae0\u8282\u6765\u8ba8\u8bbacomputation\u3002 \u7ae0\u8282\u8bf4\u660e \u201crelation\u201d\u5c06\u5728 Relation \u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\uff1b \u201cstructure\u201d\u5c06\u5728 Structure \u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\uff1b \u201ccomputation\u201d\u5c06\u5728 Computation \u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\uff1b Book \u300aDiscrete Computational Structures\u300b https://books.google.com/books/about/Discrete_Computational_Structures.html?id=KrLiBQAAQBAJ&source=kp_book_description https://www.elsevier.com/books/discrete-computational-structures/korfhage/978-0-12-420850-6","title":"Introduction"},{"location":"Relation-structure-computation/#_1","text":"\u672c\u7ae0\u7684\u6807\u9898\u662f\"Relation-structure-computation\"\uff0c\u5b83\u7684\u542b\u4e49\u662f\u5173\u7cfb\u3001\u7ed3\u6784\u3001\u8ba1\u7b97\u201d\u3002\u672c\u6587\u5bf9\u672c\u7ae0\u7684\u5185\u5bb9\u8fdb\u884c\u7efc\u8ff0\uff0c\u4ee5\u4ece\u4e00\u4e2a\u8f83\u9ad8\u7684\u89d2\u5ea6\u6765\u638c\u63e1\u672c\u7ae0\u7684\u5185\u5bb9\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/#relation-structure-computation","text":"\u5982\u679cdiscrete object\u4e4b\u95f4\u6ca1\u6709relation\uff0c\u90a3\u4e48\u5b83\u4eec\u5c31\u662f\u4e00\u5806\u6742\u4e71\u65e0\u7ae0\u3001\u6beb\u65e0\u89c4\u5f8b\u53ef\u8a00\u7684\u6df7\u6c8c\u7cfb\u7edf\uff0c\u65e0\u6cd5\u4f7f\u7528\u7cbe\u7b80\u7684\u6570\u5b66\u8bed\u8a00\u5bf9\u5176\u8fdb\u884c\u63cf\u8ff0\uff08 formal description \uff09\uff0c\u6211\u4eec\u6240\u5173\u6ce8\u7684\u662f\u90a3\u4e9b\u5177\u5907\u4e2d**relation**\u7684discrete objects\u3002 \u5bf9\u4e8e\u5177\u5907relation\u7684discrete objects\uff08\u53ef\u4ee5\u770b\u505a\u662f**node**\uff09\uff0c\u901a\u8fc7relation\u5c06discrete objects\u8fdb\u884c\u5173\u8054\uff0c\u663e\u7136\u5b83\u4eec\u5f62\u6210\u4e86\u4e00\u4e2a\u4e00\u4e2a\u7684**ordered pair**\uff08\u53ef\u4ee5\u770b\u505a\u662f**edge**\uff09\uff0c\u8fd9\u4e9b**ordered pair**\u5c31\u5f62\u6210\u4e86\u4e00\u5b9a\u7684**structure**\u3002\u6211\u4eec\u5c06\u5728 Structure \u7ae0\u8282\u8ba8\u8bbarelation and structure\uff0c\u6211\u4eec\u5c06\u4f7f\u7528relation\u7684\u7406\u8bba\u77e5\u8bc6\u6765\u63cf\u8ff0structure\u3002 NOTE: \u201cordered\u201d\u8bf4\u660erelation\u662f\u5177\u5907**\u65b9\u5411\u6027**\u7684 relation**\u51b3\u5b9a**structure \uff0c\u76f8\u540c\u7684**relation**\u51b3\u5b9a\u4e86\u5b83\u4eec\u5177\u5907\u76f8\u540c\u7684**structure**\uff0c\u8fdb\u800c\u53ef\u4ee5\u4f7f\u7528\u76f8\u540c\u7684algorithm\u6765\u8fdb\u884c\u8ba1\u7b97( computation )\u3002 \u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff1aclass hierarchy\u3001grammar tree\u90fd\u662f**containing\u5173\u7cfb**\uff0c\u8fdb\u800c\u51b3\u5b9a\u4e86\u5bf9\u5b83\u4eec\u8fdb\u884c\u641c\u7d22\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u91c7\u7528\u76f8\u540c\u7684\u7b97\u6cd5\u3002 \u6240\u4ee5\uff0c relation\u662f\u6838\u5fc3\u6240\u5728 : relation\u51b3\u5b9astructure -> \u51b3\u5b9a\u91c7\u7528\u7684algorithm( computation )\u3002\u56e0\u6b64\uff0c\u6211\u4eec**\u57fa\u4e8erelation\u6765\u5efa\u7acbmodel**\uff08\u6a21\u578b\uff09\uff1a \u8bbe\u8ba1\u51fa\u975e\u5e38\u9ad8\u6548\u7684algorithm\u6765\u5bf9\u5b83\u76f8\u5173\u7684\u95ee\u9898\u8fdb\u884ccomputation\uff08\u5178\u578b\u7684\u4f8b\u5b50containing\u5173\u7cfb\uff0c\u5728computer science\u4e2d\uff0cCFG\u3001class inheritance\u90fd\u670d\u4ece\u8fd9\u79cd\u5173\u7cfb\uff09 \u540e\u7eed\uff0c\u6211\u4f1a\u5bf9computer science\u4e2d\u5e38\u89c1\u7684relation\u4f1a\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u4f7f\u7528\u5df2\u6709\u7684model\u6765\u89e3\u51b3\u95ee\u9898\u3002 \u5728 Relation-structure-computation\\Model \u4e2d\u5bf9model\u8fdb\u884c\u63cf\u8ff0\u3002 \u5728 Relation-structure-computation\\Model\\Containing-relation-model.md \u4e2d\u5bf9**containing\u5173\u7cfb**\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 NOTE: \u4e0a\u8ff0\u8fd9\u79cd\u5efa\u7acbmodel\u7684\u601d\u60f3\u5728science\u4e2d\u662f\u5e7f\u6cdb\u5b58\u5728\u7684\uff0c\u6bd4\u5982\u5728\u6982\u7387\u8bba\u4e2d\uff0c\u6709\u7740\u975e\u5e38\u591a\u7684\u6982\u7387\u6a21\u578b\uff0c\u6570\u5b66\u5bb6\u4f7f\u7528\u8fd9\u4e9b\u6982\u7387\u6a21\u578b\u6765\u63cf\u8ff0\u5b9e\u9645\u95ee\u9898\uff0c\u6bd4\u5982\uff1a \u6b63\u6001\u5206\u5e03 \u4e8c\u9879\u5f0f\u53d1\u5e03 computation\u662f\u6307\u4e3a\u89e3\u51b3\u95ee\u9898\u91c7\u7528\u7684algorithm\uff1b","title":"\u7efc\u8ff0: relation-structure-computation"},{"location":"Relation-structure-computation/#relation#and#structure#are#abstraction","text":"Relation \u548c structure \u90fd\u662f abstraction\uff0crelation \u662f\u4ece concrete \u4e2d\u62bd\u8c61\u51fa\u6765\u7684\uff0c\u4e8e\u6b64\u7c7b\u4f3c\u7684\u662f structure \u4e5f\u662f\u5982\u6b64\u7684\u3002","title":"Relation and structure are abstraction"},{"location":"Relation-structure-computation/#graph#and#relation","text":"**graph**\u662f\u8868\u793arelation\u7684\u6709\u6548\u5de5\u5177\uff0c\u540e\u9762\u6211\u4eec\u4f1a\u770b\u5230graph\u7684\u5b9a\u4e49\u548crelation\u7684\u5b9a\u4e49\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\uff0c\u57fa\u4e8egraph\u7684algorithm\uff0c\u53ef\u4ee5\u89e3\u51b3\u5f88\u591arelation\u7684\u95ee\u9898\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u6709\u5fc5\u8981\u5b66\u4e60graph-theory\u7684\u77e5\u8bc6\u3002","title":"Graph and relation"},{"location":"Relation-structure-computation/#computation","text":"\u5728discrete\u9886\u57df\uff0c\u6211\u4eec\u8003\u8651\u7684\u4e3b\u8981\u662f\uff1a\u5bf9\u4e8e\u5177\u5907relation\u7684discrete objects\u7684computation\uff0c\u540e\u9762\u6211\u4eec\u5c06\u770b\u5230\uff0c\u5bf9\u4e8e\u67d0\u4e9brelation\uff0c\u6211\u4eec\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u975e\u5e38\u9ad8\u6548\u7684algorithm\u6765\u5bf9\u5b83\u4eec\u8fdb\u884ccomputation\uff0c\u6211\u4eec\u5c06\u5728 Computation \u7ae0\u8282\u6765\u8ba8\u8bbacomputation\u3002","title":"Computation"},{"location":"Relation-structure-computation/#_2","text":"\u201crelation\u201d\u5c06\u5728 Relation \u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\uff1b \u201cstructure\u201d\u5c06\u5728 Structure \u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\uff1b \u201ccomputation\u201d\u5c06\u5728 Computation \u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\uff1b","title":"\u7ae0\u8282\u8bf4\u660e"},{"location":"Relation-structure-computation/#book#discrete#computational#structures","text":"https://books.google.com/books/about/Discrete_Computational_Structures.html?id=KrLiBQAAQBAJ&source=kp_book_description https://www.elsevier.com/books/discrete-computational-structures/korfhage/978-0-12-420850-6","title":"Book \u300aDiscrete Computational Structures\u300b"},{"location":"Relation-structure-computation/Computation/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bba\u201cComputation on discrete objects\u201c\u3002 \u8ba1\u7b97\u601d\u7ef4 \u5728 index \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86discrete objects\u7684computable\u7684\u7279\u6027\uff0c\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u9700\u8981\u601d\u8003: \u5982\u4f55\u6765**\u5b9e\u73b0Computation** \uff1f\u5f53\u6211\u4eec\u5f00\u59cb\u601d\u8003\u201cComputation on discrete objects\u201d\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u5c31\u5df2\u7ecf\u840c\u53d1\u4e86\"\u8ba1\u7b97\u601d\u7ef4\"\uff0c\u5373\u5f00\u59cb\u601d\u8003\u201c\u54ea\u4e9b\u95ee\u9898\u662f\u53ef\u8ba1\u7b97\u7684\uff1f\u201d\u3001\u201c\u8be5\u5982\u4f55\u5b9e\u73b0\u8ba1\u7b97\u201d\u3002\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u95ee\u9898\uff0c\u6211\u53ea\u80fd\u57fa\u4e8e\u5bf9\u73b0\u6709\u7406\u8bba\u7684\u89e3\u8bfb\u3001\u5177\u4f53\u7684\u7ecf\u9a8c\u6765\u8fdb\u884c\u603b\u7ed3\uff0c\u672c\u7ae0\u5c31\u662f\u63cf\u8ff0\u7684\u8fd9\u4e2a\u4e3b\u9898\u3002 Make it computational \u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u95ee\u9898\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b9e\u73b0**\u8ba1\u7b97**\u7684\u7b2c\u4e00\u6b65\u662f\u4f7f\u7528\u4e00\u79cdlanguage/representation\u6765\u5bf9\u95ee\u9898\u8fdb\u884c\u63cf\u8ff0\uff0c\u8fd9\u79cdlanguage/representation\u9700\u8981\u662fcomputer\u80fd\u591f\u7406\u89e3\u7684\u3001\u662f\u9002\u5408\u8ba1\u7b97\u7684\u3002\u8fd9\u662f\u5b9e\u73b0\u8ba1\u7b97\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u53ea\u6709\u8fd9\u6837\u8fd9\u6837\u6211\u4eec\u624d\u80fd\u591f\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1algorithm\u3001\u5b9e\u73b0computation\u3002\u5728 ./Make-it-computational \u7ae0\u8282\u4e2d\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u8fdb\u884c\u8ba8\u8bba\u3002 \u8ba1\u7b97\u6a21\u578b: one-by-one NOTE: \"one-by-one\"\u5176\u5b9e\u5c31\u662fTuring machine\uff0c\u672c\u8282\u7684\u5185\u5bb9\u5176\u5b9e\u6240\u6d89\u53ca\u7684\u5c31\u662ftheory of computation\u3002 \u5728\u8bb2\u89e3\u5177\u4f53\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e4b\u524d\uff0c\u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u4e00\u79cd\u975e\u5e38\u62bd\u8c61\u7684\u7684\u8ba1\u7b97\u6a21\u578b\uff1aone-by-one\u3002\u4e0b\u9762\u5217\u4e3e\u4e86\u6848\u4f8b \u987a\u5e8f\u6267\u884c\uff0c\u76f4\u81f3\u7ec8\u70b9 unit \u89e3\u91ca CPU instruction CPU\u7684\u6267\u884c\u8fc7\u7a0b\u662fone instruction by another\uff0c\u76f4\u81f3\u5230\u8fbe\u7ec8\u70b9 Turing-machine instruction Turing-machine\u7684computation\u4e5f\u662fone-by-one\u3002 function subfunction \u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u53ef\u4ee5\u7528one-by-one\u6765\u8fdb\u884c\u63cf\u8ff0\uff1a \u5c06\u6574\u4e2aprogram\u7b80\u5316\u4e3a\u7531function\u7ec4\u6210\uff0c \u5c06\u51fd\u6570\u7b80\u5316\u4e3a\u7684\u8282\u70b9\uff0c \u5b58\u5728\u8c03\u7528\u5173\u7cfb\u7684\u51fd\u6570\u4e4b\u95f4\uff0c\u4f7f\u7528\u8fde\u7ebf \u5219\u5f62\u6210\u4e86\u4e00\u4e2agraph\u3002 \u5219\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\uff1a one subfunction by another subfunction one node by another \u57fa\u4e8erelation\u8fdb\u884c\u641c\u7d22\uff0c\u76f4\u81f3\u76ee\u6807\u70b9 unit \u89e3\u91ca graph node \u57fa\u4e8egraph\u7684algorithm\u7684\u6267\u884c\u8fc7\u7a0b\u5f80\u5f80\u662fone node by another node\uff0c\u76f4\u81f3\u7ec8\u6b62\u6761\u4ef6 relation-based algorithm model node \u5728\u4e0b\u9762\u7ae0\u8282\u4f1a\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd relation\u53ef\u4ee5\u4f7f\u7528graph\u6765\u5b9e\u73b0\uff0c\u6240\u4ee5\u4e0a\u8ff0\u4e24\u8005\uff0c\u5176\u5b9e\u672c\u8d28\u4e0a\u662f\u76f8\u540c\u7684\u3002 Iterative method \u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Iterative method \u7ef4\u57fa\u767e\u79d1 Mathematical optimization # Computational optimization techniques \uff0c \u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50\uff1a Gradient descent \u4e3b\u8981\u7528\u4e8e\u89e3\u51b3 Continuous optimization problem\uff0c\u53c2\u89c1 Relation-structure-computation\\Computation\\Algorithm\\Application\\Optimization \u3002 \u9010\u6b65\u5411\u76ee\u6807\u9760\u8fd1 \u5f88\u591a\u6c42\u6700\u503c\u7684algorithm\u90fd\u53ef\u4ee5\u770b\u505a\u662f\uff1a\u9010\u6b65\u5411\u76ee\u6807\u9760\u8fd1\uff0c\u5373\u91cd\u590d\u6267\u884c\u7740 \u8d2a\u5fc3\u9009\u62e9 \u8fc7\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0 \u4e0d\u65ad\u7684 \u5411 \u76ee\u6807 \u9760\u8fd1 \uff0c\u663e\u7136\u5b83\u662f\u7b26\u5408one-by-one\u7684\uff0c\u4e0b\u9762\u662f\u8fd9\u4e9b\u4f8b\u5b50\uff1a best-first search Iterative method greedy algorithm \u8fd9\u4e9balgorithm\u7684\u53e6\u5916\u4e00\u4e2a\u5171\u6027\u662f\uff1a - \u53ef\u80fd\u65e0\u6cd5\u83b7\u5f97\u5168\u5c40\u6700\u4f18 \u4ecesearch algorithm\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u201c\u9010\u6b65\u5411\u76ee\u6807\u9760\u8fd1\u201d\u4e5f\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cdsearch\uff0c\u5173\u4e8esearch algorithm\uff0c\u53c2\u89c1\uff1a Relation-structure-computation\\Computation\\Algorithm\\Application\\Search Computation\u7684\u5ea6\u91cf NOTE: Turing\u7684\u53ef\u8ba1\u7b97\u7406\u8bba\u662f\u5bf9\u6b64\u7684\u7814\u7a76\u3002 \u6240\u6709\u7684one-by-one\uff0c\u90fd\u662fenumerable\u3001countable\u3002 unit unit\u8868\u793a\u7684\u662f\u8ba1\u7b97\u5355\u4f4d\uff0c\u5728\u6587\u7ae0 Unit.md \u4e2d\u5bf9\u4e00\u4e9b\u5e38\u89c1\u7684unit\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 computation\u7684\u5ea6\u91cf \u6211\u4eec\u53ef\u4ee5\u57fa\u4e8eunit\u6765\u5bf9computation\u8fdb\u884c\u5ea6\u91cf\u3002\u901a\u8fc7\u5ea6\u91cfcomputation\u7684\u91cf\uff0c\u53ef\u4ee5\u8ba1\u7b97\u5f97\u5230\u7b97\u6cd5\u590d\u6742\u5ea6\u3002 \u53ef\u8ba1\u7b97\u6027 \u518d\u590d\u6742\u7684\u95ee\u9898\u4e5f\u9700\u8981\u8f6c\u6362\u4e3aone-by-one\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u5373\u4f7f\u590d\u6742\u5982back-prop\u4e5f\u662f\u5982\u6b64\uff0c\u5426\u5219\u662f\u4e0d\u53ef\u8ba1\u7b97\u7684\u3002 \u5982\u4f55\u7f16\u7a0b\u5b9e\u73b0\u8ba1\u7b97? \u524d\u9762\u4ecb\u7ecd\u4e86ono-by-one model\u5176\u5b9e\u66f4\u52a0\u504f\u5411\u4e8e\u7406\u8bba\u7684\u6a21\u578b\uff0c\u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u6765\u7f16\u7a0b\u8fdb\u884c\u5b9e\u73b0\u3002 Repetition \u4ece\u524d\u9762\u63cf\u8ff0\u7684one-by-one computation\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u51fa: 1) iterative method \u548c greedy algorithm\u5c31\u662f\u91cd\u590d\u6267\u884c\u7740 \u8d2a\u5fc3\u9009\u62e9 \u8fc7\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0 \u4e0d\u65ad\u7684 \u5411 \u76ee\u6807 \u9760\u8fd1 2) relation-based algorithm: \u91cd\u590d\u6267\u884crelation\u5bf9\u5e94\u7684computation \u663e\u7136\uff0c\u5b83\u4eec\u90fd\u662f\u5178\u578b\u7684\"\u91cd\u590d\u6267\u884c\u67d0\u4e2acomputation\"\uff0c\u6211\u4eec\u7b80\u79f0\u4e3a\"repetition\"\uff0c\u5177\u5907\"repetition\"\u7279\u5f81\u7684computation\u662f\u975e\u5e38\u5bb9\u6613\u7f16\u7a0b\u5b9e\u73b0\u7684\uff0c\u8fd9\u4e5f\u662f\u8fd9\u79cdcomputation\u5177\u5907\u91cd\u8981\u4ef7\u503c\u7684\u539f\u56e0\u3002\u524d\u9762\u6240\u63cf\u8ff0\u7684one-by-one model\uff0c\u5176\u5b9e\u662f\u504f\u5411\u4e8e\u7406\u8bba\u3001\u592a\u8fc7\u62bd\u8c61\uff0crepetition computation\u5219\u8ba9\u6211\u4eec\u80fd\u591f\u7f16\u7a0b\u5b9e\u73b0\u3002\u5728computer science\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230repetition\u662f\u65e0\u5904\u4e0d\u5728\u7684\u3002 \u5728 ./Repetition \u7ae0\u8282\uff0c\u5c06\u5bf9\u5b83\u8fdb\u884c\u8be6\u7ec6\u8bf4\u660e\u3002 \u8ba1\u7b97\u7684\u65b9\u5411 \u6cbf\u7740\u5173\u7cfb\u3001\u7ed3\u6784\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u4e00\u822c\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4e24\u4e2a\u4e0d\u540c\u7684\u65b9\u5411\uff0c\u8fd9\u5c06\u5728 ./Computation-direction \u7ae0\u8282\u4e2d\u8fdb\u884c\u4ecb\u7ecd\u3002 \u5982\u4f55\u8bbe\u8ba1algorithm\uff1f \u8bbe\u8ba1algorithm\u662f\u5b9e\u73b0computation\u7684\u975e\u5e38\u91cd\u8981\u7684\u4e00\u73af\uff0c\u662f\u4e00\u4e2a\u975e\u5e38\u590d\u6742\u7684\u95ee\u9898\uff0c\u6709\u7740\u975e\u5e38\u591a\u7684\u6280\u5de7\uff0c\u6211\u4eec\u5c06\u5728 Relation-structure-computation\\Computation\\Algorithm \u4e2d\u8fdb\u884c\u8ba8\u8bba\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/#_1","text":"\u672c\u7ae0\u8ba8\u8bba\u201cComputation on discrete objects\u201c\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/#_2","text":"\u5728 index \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86discrete objects\u7684computable\u7684\u7279\u6027\uff0c\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u9700\u8981\u601d\u8003: \u5982\u4f55\u6765**\u5b9e\u73b0Computation** \uff1f\u5f53\u6211\u4eec\u5f00\u59cb\u601d\u8003\u201cComputation on discrete objects\u201d\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u5c31\u5df2\u7ecf\u840c\u53d1\u4e86\"\u8ba1\u7b97\u601d\u7ef4\"\uff0c\u5373\u5f00\u59cb\u601d\u8003\u201c\u54ea\u4e9b\u95ee\u9898\u662f\u53ef\u8ba1\u7b97\u7684\uff1f\u201d\u3001\u201c\u8be5\u5982\u4f55\u5b9e\u73b0\u8ba1\u7b97\u201d\u3002\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u95ee\u9898\uff0c\u6211\u53ea\u80fd\u57fa\u4e8e\u5bf9\u73b0\u6709\u7406\u8bba\u7684\u89e3\u8bfb\u3001\u5177\u4f53\u7684\u7ecf\u9a8c\u6765\u8fdb\u884c\u603b\u7ed3\uff0c\u672c\u7ae0\u5c31\u662f\u63cf\u8ff0\u7684\u8fd9\u4e2a\u4e3b\u9898\u3002","title":"\u8ba1\u7b97\u601d\u7ef4"},{"location":"Relation-structure-computation/Computation/#make#it#computational","text":"\u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u95ee\u9898\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b9e\u73b0**\u8ba1\u7b97**\u7684\u7b2c\u4e00\u6b65\u662f\u4f7f\u7528\u4e00\u79cdlanguage/representation\u6765\u5bf9\u95ee\u9898\u8fdb\u884c\u63cf\u8ff0\uff0c\u8fd9\u79cdlanguage/representation\u9700\u8981\u662fcomputer\u80fd\u591f\u7406\u89e3\u7684\u3001\u662f\u9002\u5408\u8ba1\u7b97\u7684\u3002\u8fd9\u662f\u5b9e\u73b0\u8ba1\u7b97\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u53ea\u6709\u8fd9\u6837\u8fd9\u6837\u6211\u4eec\u624d\u80fd\u591f\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1algorithm\u3001\u5b9e\u73b0computation\u3002\u5728 ./Make-it-computational \u7ae0\u8282\u4e2d\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u8fdb\u884c\u8ba8\u8bba\u3002","title":"Make it computational"},{"location":"Relation-structure-computation/Computation/#one-by-one","text":"NOTE: \"one-by-one\"\u5176\u5b9e\u5c31\u662fTuring machine\uff0c\u672c\u8282\u7684\u5185\u5bb9\u5176\u5b9e\u6240\u6d89\u53ca\u7684\u5c31\u662ftheory of computation\u3002 \u5728\u8bb2\u89e3\u5177\u4f53\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e4b\u524d\uff0c\u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u4e00\u79cd\u975e\u5e38\u62bd\u8c61\u7684\u7684\u8ba1\u7b97\u6a21\u578b\uff1aone-by-one\u3002\u4e0b\u9762\u5217\u4e3e\u4e86\u6848\u4f8b","title":"\u8ba1\u7b97\u6a21\u578b: one-by-one"},{"location":"Relation-structure-computation/Computation/#_3","text":"unit \u89e3\u91ca CPU instruction CPU\u7684\u6267\u884c\u8fc7\u7a0b\u662fone instruction by another\uff0c\u76f4\u81f3\u5230\u8fbe\u7ec8\u70b9 Turing-machine instruction Turing-machine\u7684computation\u4e5f\u662fone-by-one\u3002 function subfunction \u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u53ef\u4ee5\u7528one-by-one\u6765\u8fdb\u884c\u63cf\u8ff0\uff1a \u5c06\u6574\u4e2aprogram\u7b80\u5316\u4e3a\u7531function\u7ec4\u6210\uff0c \u5c06\u51fd\u6570\u7b80\u5316\u4e3a\u7684\u8282\u70b9\uff0c \u5b58\u5728\u8c03\u7528\u5173\u7cfb\u7684\u51fd\u6570\u4e4b\u95f4\uff0c\u4f7f\u7528\u8fde\u7ebf \u5219\u5f62\u6210\u4e86\u4e00\u4e2agraph\u3002 \u5219\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\uff1a one subfunction by another subfunction one node by another","title":"\u987a\u5e8f\u6267\u884c\uff0c\u76f4\u81f3\u7ec8\u70b9"},{"location":"Relation-structure-computation/Computation/#relation","text":"unit \u89e3\u91ca graph node \u57fa\u4e8egraph\u7684algorithm\u7684\u6267\u884c\u8fc7\u7a0b\u5f80\u5f80\u662fone node by another node\uff0c\u76f4\u81f3\u7ec8\u6b62\u6761\u4ef6 relation-based algorithm model node \u5728\u4e0b\u9762\u7ae0\u8282\u4f1a\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd relation\u53ef\u4ee5\u4f7f\u7528graph\u6765\u5b9e\u73b0\uff0c\u6240\u4ee5\u4e0a\u8ff0\u4e24\u8005\uff0c\u5176\u5b9e\u672c\u8d28\u4e0a\u662f\u76f8\u540c\u7684\u3002","title":"\u57fa\u4e8erelation\u8fdb\u884c\u641c\u7d22\uff0c\u76f4\u81f3\u76ee\u6807\u70b9"},{"location":"Relation-structure-computation/Computation/#iterative#method","text":"\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Iterative method \u7ef4\u57fa\u767e\u79d1 Mathematical optimization # Computational optimization techniques \uff0c \u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50\uff1a Gradient descent \u4e3b\u8981\u7528\u4e8e\u89e3\u51b3 Continuous optimization problem\uff0c\u53c2\u89c1 Relation-structure-computation\\Computation\\Algorithm\\Application\\Optimization \u3002","title":"Iterative method"},{"location":"Relation-structure-computation/Computation/#_4","text":"\u5f88\u591a\u6c42\u6700\u503c\u7684algorithm\u90fd\u53ef\u4ee5\u770b\u505a\u662f\uff1a\u9010\u6b65\u5411\u76ee\u6807\u9760\u8fd1\uff0c\u5373\u91cd\u590d\u6267\u884c\u7740 \u8d2a\u5fc3\u9009\u62e9 \u8fc7\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0 \u4e0d\u65ad\u7684 \u5411 \u76ee\u6807 \u9760\u8fd1 \uff0c\u663e\u7136\u5b83\u662f\u7b26\u5408one-by-one\u7684\uff0c\u4e0b\u9762\u662f\u8fd9\u4e9b\u4f8b\u5b50\uff1a best-first search Iterative method greedy algorithm \u8fd9\u4e9balgorithm\u7684\u53e6\u5916\u4e00\u4e2a\u5171\u6027\u662f\uff1a - \u53ef\u80fd\u65e0\u6cd5\u83b7\u5f97\u5168\u5c40\u6700\u4f18 \u4ecesearch algorithm\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u201c\u9010\u6b65\u5411\u76ee\u6807\u9760\u8fd1\u201d\u4e5f\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cdsearch\uff0c\u5173\u4e8esearch algorithm\uff0c\u53c2\u89c1\uff1a Relation-structure-computation\\Computation\\Algorithm\\Application\\Search","title":"\u9010\u6b65\u5411\u76ee\u6807\u9760\u8fd1"},{"location":"Relation-structure-computation/Computation/#computation","text":"NOTE: Turing\u7684\u53ef\u8ba1\u7b97\u7406\u8bba\u662f\u5bf9\u6b64\u7684\u7814\u7a76\u3002 \u6240\u6709\u7684one-by-one\uff0c\u90fd\u662fenumerable\u3001countable\u3002","title":"Computation\u7684\u5ea6\u91cf"},{"location":"Relation-structure-computation/Computation/#unit","text":"unit\u8868\u793a\u7684\u662f\u8ba1\u7b97\u5355\u4f4d\uff0c\u5728\u6587\u7ae0 Unit.md \u4e2d\u5bf9\u4e00\u4e9b\u5e38\u89c1\u7684unit\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"unit"},{"location":"Relation-structure-computation/Computation/#computation_1","text":"\u6211\u4eec\u53ef\u4ee5\u57fa\u4e8eunit\u6765\u5bf9computation\u8fdb\u884c\u5ea6\u91cf\u3002\u901a\u8fc7\u5ea6\u91cfcomputation\u7684\u91cf\uff0c\u53ef\u4ee5\u8ba1\u7b97\u5f97\u5230\u7b97\u6cd5\u590d\u6742\u5ea6\u3002","title":"computation\u7684\u5ea6\u91cf"},{"location":"Relation-structure-computation/Computation/#_5","text":"\u518d\u590d\u6742\u7684\u95ee\u9898\u4e5f\u9700\u8981\u8f6c\u6362\u4e3aone-by-one\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u5373\u4f7f\u590d\u6742\u5982back-prop\u4e5f\u662f\u5982\u6b64\uff0c\u5426\u5219\u662f\u4e0d\u53ef\u8ba1\u7b97\u7684\u3002","title":"\u53ef\u8ba1\u7b97\u6027"},{"location":"Relation-structure-computation/Computation/#_6","text":"\u524d\u9762\u4ecb\u7ecd\u4e86ono-by-one model\u5176\u5b9e\u66f4\u52a0\u504f\u5411\u4e8e\u7406\u8bba\u7684\u6a21\u578b\uff0c\u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u6765\u7f16\u7a0b\u8fdb\u884c\u5b9e\u73b0\u3002","title":"\u5982\u4f55\u7f16\u7a0b\u5b9e\u73b0\u8ba1\u7b97?"},{"location":"Relation-structure-computation/Computation/#repetition","text":"\u4ece\u524d\u9762\u63cf\u8ff0\u7684one-by-one computation\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u51fa: 1) iterative method \u548c greedy algorithm\u5c31\u662f\u91cd\u590d\u6267\u884c\u7740 \u8d2a\u5fc3\u9009\u62e9 \u8fc7\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0 \u4e0d\u65ad\u7684 \u5411 \u76ee\u6807 \u9760\u8fd1 2) relation-based algorithm: \u91cd\u590d\u6267\u884crelation\u5bf9\u5e94\u7684computation \u663e\u7136\uff0c\u5b83\u4eec\u90fd\u662f\u5178\u578b\u7684\"\u91cd\u590d\u6267\u884c\u67d0\u4e2acomputation\"\uff0c\u6211\u4eec\u7b80\u79f0\u4e3a\"repetition\"\uff0c\u5177\u5907\"repetition\"\u7279\u5f81\u7684computation\u662f\u975e\u5e38\u5bb9\u6613\u7f16\u7a0b\u5b9e\u73b0\u7684\uff0c\u8fd9\u4e5f\u662f\u8fd9\u79cdcomputation\u5177\u5907\u91cd\u8981\u4ef7\u503c\u7684\u539f\u56e0\u3002\u524d\u9762\u6240\u63cf\u8ff0\u7684one-by-one model\uff0c\u5176\u5b9e\u662f\u504f\u5411\u4e8e\u7406\u8bba\u3001\u592a\u8fc7\u62bd\u8c61\uff0crepetition computation\u5219\u8ba9\u6211\u4eec\u80fd\u591f\u7f16\u7a0b\u5b9e\u73b0\u3002\u5728computer science\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230repetition\u662f\u65e0\u5904\u4e0d\u5728\u7684\u3002 \u5728 ./Repetition \u7ae0\u8282\uff0c\u5c06\u5bf9\u5b83\u8fdb\u884c\u8be6\u7ec6\u8bf4\u660e\u3002","title":"Repetition"},{"location":"Relation-structure-computation/Computation/#_7","text":"\u6cbf\u7740\u5173\u7cfb\u3001\u7ed3\u6784\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u4e00\u822c\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4e24\u4e2a\u4e0d\u540c\u7684\u65b9\u5411\uff0c\u8fd9\u5c06\u5728 ./Computation-direction \u7ae0\u8282\u4e2d\u8fdb\u884c\u4ecb\u7ecd\u3002","title":"\u8ba1\u7b97\u7684\u65b9\u5411"},{"location":"Relation-structure-computation/Computation/#algorithm","text":"\u8bbe\u8ba1algorithm\u662f\u5b9e\u73b0computation\u7684\u975e\u5e38\u91cd\u8981\u7684\u4e00\u73af\uff0c\u662f\u4e00\u4e2a\u975e\u5e38\u590d\u6742\u7684\u95ee\u9898\uff0c\u6709\u7740\u975e\u5e38\u591a\u7684\u6280\u5de7\uff0c\u6211\u4eec\u5c06\u5728 Relation-structure-computation\\Computation\\Algorithm \u4e2d\u8fdb\u884c\u8ba8\u8bba\u3002","title":"\u5982\u4f55\u8bbe\u8ba1algorithm\uff1f"},{"location":"Relation-structure-computation/Computation/Algorithm/","text":"\u5173\u4e8e\u672c\u7ae0 Implementation martinbroadhurst http://www.martinbroadhurst.com cppsecrets C++ algorithm","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/#_1","text":"","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Algorithm/#implementation","text":"","title":"Implementation"},{"location":"Relation-structure-computation/Computation/Algorithm/#martinbroadhurst","text":"http://www.martinbroadhurst.com","title":"martinbroadhurst"},{"location":"Relation-structure-computation/Computation/Algorithm/#cppsecrets#c#algorithm","text":"","title":"cppsecrets C++ algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Computational-problem/","text":"Computational problem In theoretical computer science , a computational problem is a mathematical object representing a collection of questions that computers might be able to solve. For example, the problem of factoring \"Given a positive integer n , find a nontrivial prime factor of n .\" is a computational problem. Computational problems are one of the main objects of study in theoretical computer science. The field of algorithms studies methods of solving computational problems efficiently. The complementary field of computational complexity attempts to explain why certain computational problems are intractable for computers. A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. For example, in the factoring problem, the instances are the integers n , and solutions are prime numbers p that describe nontrivial prime factors of n . It is conventional to represent both instances and solutions by binary strings , namely elements of {0, 1}*. For example, numbers can be represented as binary strings using the binary encoding. (For readability, we identify numbers with their binary encodings in the examples below.) Types of computational problems decision problem search problem counting problem optimization problem combinatorial optimization function problem","title":"Computational-problem"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Computational-problem/#computational#problem","text":"In theoretical computer science , a computational problem is a mathematical object representing a collection of questions that computers might be able to solve. For example, the problem of factoring \"Given a positive integer n , find a nontrivial prime factor of n .\" is a computational problem. Computational problems are one of the main objects of study in theoretical computer science. The field of algorithms studies methods of solving computational problems efficiently. The complementary field of computational complexity attempts to explain why certain computational problems are intractable for computers. A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. For example, in the factoring problem, the instances are the integers n , and solutions are prime numbers p that describe nontrivial prime factors of n . It is conventional to represent both instances and solutions by binary strings , namely elements of {0, 1}*. For example, numbers can be represented as binary strings using the binary encoding. (For readability, we identify numbers with their binary encodings in the examples below.)","title":"Computational problem"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Computational-problem/#types#of#computational#problems","text":"decision problem search problem counting problem optimization problem combinatorial optimization function problem","title":"Types of computational problems"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/","text":"Optimization \u5728computer science\u4e2d\uff0coptimization\u662f\u4e00\u7c7b\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u3002\u4ecewikipedia Mathematical optimization \u7684\u5185\u5bb9\u53ef\u77e5\uff0coptimization\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u95ee\u9898\u3002\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u9700\u8981\u5173\u6ce8\u7684\u662falgorithm implementation\u3002 wikipedia Mathematical optimization Mathematical optimization (alternatively spelled optimisation ) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives.[ 1] Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics , and the development of solution methods has been of interest in mathematics for centuries.[ 2] In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics . More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains. Optimization problems NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\uff0c\u4e3b\u8981\u662f\u53c2\u8003\u7ef4\u57fa\u767e\u79d1 Optimization problem Computational optimization techniques NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u4f7f\u7528program\u6765\u89e3\u51b3optimization\u95ee\u9898\uff0c\u8fd9\u662f\u6211\u4eec\u4f5c\u4e3aprogrammer\u9700\u8981\u91cd\u70b9\u5b66\u4e60\u7684\u3002 To solve problems, researchers may use algorithms that terminate in a finite number of steps, or iterative methods that converge to a solution (on some specified class of problems), or heuristics that may provide approximate solutions to some problems (although their iterates need not converge). NOTE: \u5f88\u591aoptimization\uff0c\u65e0\u6cd5\u4f7f\u7528program\u6c42\u5f97\u6700\u4f18\u503c\uff0c\u800c\u662f\u6c42\u5f97\u4e00\u4e2a\u8fd1\u4f3c\u503c Classification of optimization problem \u4ee5 wikipedia Optimization problem \u4e2d\u63cf\u8ff0\u7684\u5206\u7c7b\u65b9\u6cd5\u4e3a\u4e3b\uff0c\u7136\u540e\u8865\u5145wikipedia Mathematical optimization \uff08Optimization**:** Algorithms , methods , and heuristics \uff09\u4e2d\u63d0\u4f9b\u7684\u5404\u79cd\u66f4\u52a0\u5177\u4f53\u7684\u7c7b\u522b\u3002 wikipedia Optimization problem In mathematics and computer science , an optimization problem is the problem of finding the best solution from all feasible solutions . Optimization problems can be divided into two categories depending on whether the variables are continuous or discrete . An optimization problem with discrete variables is known as a discrete optimization . In a discrete optimization problem, we are looking for an object such as an integer , permutation or graph from a countable set . A problem with continuous variables is known as a continuous optimization , in which an optimal value from a continuous function must be found. They can include constrained problems and multimodal problems. NOTE: \u5728\u6570\u5b66\u4e2d\uff0c\u6839\u636ediscrete\u3001continuous\u6765\u8fdb\u884c\u5206\u7c7b\u7684\u65b9\u5f0f\u666e\u904d\u5b58\u5728\uff0c\u6bd4\u5982\uff1a Continuous optimization problem The standard form of a continuous optimization problem is[ 1] where $ f:\\mathbb {R} ^{n}\\to \\mathbb {R} $ is the objective function to be minimized over the n -variable vector $ x $, $ g_{i}(x)\\leq 0 $ are called inequality constraints $ h_{j}(x)=0 $ are called equality constraints , and $ m\\geq 0 and p\\geq 0 $. If $ m $ and $ p $ equal 0, the problem is an unconstrained optimization problem. By convention, the standard form defines a minimization problem . A maximization problem can be treated by negating the objective function. Combinatorial optimization problem Main article: Combinatorial optimization Formally, a combinatorial optimization problem $ A $ is a quadruple[ citation needed ] $ (I,f,m,g) $, where $ I $ is a set of instances; given an instance $ x\\in I $, $ f(x) $ is the set of feasible solutions; given an instance $ x $ and a feasible solution $ y $ of $ x $, $ m(x,y) $ denotes the measure of $ y $, which is usually a positive real . $ g $ is the goal function, and is either $ \\min $ or $ \\max $. The goal is then to find for some instance $ x $ an optimal solution , that is, a feasible solution $ y $ with $$ m(x,y)=g{\\bigl {}m(x,y')\\mid y'\\in f(x){\\bigr }} $$ For each combinatorial optimization problem, there is a corresponding decision problem that asks whether there is a feasible solution for some particular measure $ m_{0} $. For example, if there is a graph $ G $ which contains vertices $ u $ and $ v $, an optimization problem might be \"find a path from $ u $ to $ v $ that uses the fewest edges\". This problem might have an answer of, say, 4. A corresponding decision problem would be \"is there a path from $ u $ to $ v $ that uses 10 or fewer edges?\" This problem can be answered with a simple 'yes' or 'no'. Summary \u7c7b\u522b \u5b50\u7c7b\u522b \u65b9\u6cd5\u8bba Continuous optimization problem - \u7ef4\u57fa\u767e\u79d1 Continuous optimization - Unconstrained nonlinear - Constrained nonlinear - Convex optimization \u6bd4\u8f83\u5178\u578b\u7684\u6709: - Iterative method Combinatorial optimization problem - \u7ef4\u57fa\u767e\u79d1 Discrete optimization - \u7ef4\u57fa\u767e\u79d1 Combinatorial optimization \u6bd4\u8f83\u5178\u578b\u7684\u6709: - relation-based algorithm model \u65b9\u6cd5\u8bba \u4e00\u822c\uff0c\u5728\u5b9e\u8df5\u4e2d\uff0c \u4e0a\u8ff0\u4e24\u79cd\u7c7b\u522b\u7684optimization problem\u91c7\u7528\u7684\u662f\u4e0d\u540c\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002 \u4e00\u822c\u8981\u6211\u4eec\u89e3\u51b3\u7684optimization\u95ee\u9898\u4e2d\u5f80\u5f80\u53ea\u5305\u542b\u4e00\u4e2a\u6700\u503c\uff0c\u5982\uff1a All nearest smaller values \u7684\u6700\u503c\u662fnearest Maximum subarray problem \u7684\u6700\u503c\u662flargest \u6240\u4ee5\u5728\u6c42\u89e3\u6700\u503c\u95ee\u9898\u7684\u65f6\u5019\uff0c\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u524d\u63d0\u662f\u8981\u641e\u6e05\u695a\u5b83\u7684\u6700\u503c\u662f\u4ec0\u4e48\u3002 \u5176\u6b21\uff0c\u6211\u4eec\u5f80\u5f80\u662f\u57fa\u4e8e**\u6bd4\u8f83**\uff08\u6253\u64c2\u53f0\uff09\u6765\u6c42\u89e3\u6700\u503c\uff0c\u4f46\u662f\u6709\u6709\u4e9b\u662f\u53ef\u4ee5\u76f4\u63a5\u8fdb\u884c\u6bd4\u8f83\u7684\uff0c\u6bd4\u5982\u6570\u503c\uff0c\u4f46\u662f\u6709\u4e9b\u662f\u65e0\u6cd5\u76f4\u63a5\u8fdb\u884c\u6bd4\u8f83\u7684\uff0c\u6bd4\u5982\u5728 All nearest smaller values \u4e2d\u6700\u503c\u662fnearest\uff0c\u9664\u975e\u8bb0\u5f55\u6bcf\u4e2a\u5143\u7d20\u7684\u4f4d\u7f6e\uff0c\u5426\u5219\u53ea\u80fd\u591f\u501f\u52a9\u4e00\u4e2astack\u6765\u5b9e\u73b0nearest\uff1b","title":"Optimization"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#optimization","text":"\u5728computer science\u4e2d\uff0coptimization\u662f\u4e00\u7c7b\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u3002\u4ecewikipedia Mathematical optimization \u7684\u5185\u5bb9\u53ef\u77e5\uff0coptimization\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u95ee\u9898\u3002\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u9700\u8981\u5173\u6ce8\u7684\u662falgorithm implementation\u3002","title":"Optimization"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#wikipedia#mathematical#optimization","text":"Mathematical optimization (alternatively spelled optimisation ) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives.[ 1] Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics , and the development of solution methods has been of interest in mathematics for centuries.[ 2] In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics . More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.","title":"wikipedia Mathematical optimization"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#optimization#problems","text":"NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\uff0c\u4e3b\u8981\u662f\u53c2\u8003\u7ef4\u57fa\u767e\u79d1 Optimization problem","title":"Optimization problems"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#computational#optimization#techniques","text":"NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u4f7f\u7528program\u6765\u89e3\u51b3optimization\u95ee\u9898\uff0c\u8fd9\u662f\u6211\u4eec\u4f5c\u4e3aprogrammer\u9700\u8981\u91cd\u70b9\u5b66\u4e60\u7684\u3002 To solve problems, researchers may use algorithms that terminate in a finite number of steps, or iterative methods that converge to a solution (on some specified class of problems), or heuristics that may provide approximate solutions to some problems (although their iterates need not converge). NOTE: \u5f88\u591aoptimization\uff0c\u65e0\u6cd5\u4f7f\u7528program\u6c42\u5f97\u6700\u4f18\u503c\uff0c\u800c\u662f\u6c42\u5f97\u4e00\u4e2a\u8fd1\u4f3c\u503c","title":"Computational optimization techniques"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#classification#of#optimization#problem","text":"\u4ee5 wikipedia Optimization problem \u4e2d\u63cf\u8ff0\u7684\u5206\u7c7b\u65b9\u6cd5\u4e3a\u4e3b\uff0c\u7136\u540e\u8865\u5145wikipedia Mathematical optimization \uff08Optimization**:** Algorithms , methods , and heuristics \uff09\u4e2d\u63d0\u4f9b\u7684\u5404\u79cd\u66f4\u52a0\u5177\u4f53\u7684\u7c7b\u522b\u3002","title":"Classification of optimization problem"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#wikipedia#optimization#problem","text":"In mathematics and computer science , an optimization problem is the problem of finding the best solution from all feasible solutions . Optimization problems can be divided into two categories depending on whether the variables are continuous or discrete . An optimization problem with discrete variables is known as a discrete optimization . In a discrete optimization problem, we are looking for an object such as an integer , permutation or graph from a countable set . A problem with continuous variables is known as a continuous optimization , in which an optimal value from a continuous function must be found. They can include constrained problems and multimodal problems. NOTE: \u5728\u6570\u5b66\u4e2d\uff0c\u6839\u636ediscrete\u3001continuous\u6765\u8fdb\u884c\u5206\u7c7b\u7684\u65b9\u5f0f\u666e\u904d\u5b58\u5728\uff0c\u6bd4\u5982\uff1a","title":"wikipedia Optimization problem"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#continuous#optimization#problem","text":"The standard form of a continuous optimization problem is[ 1] where $ f:\\mathbb {R} ^{n}\\to \\mathbb {R} $ is the objective function to be minimized over the n -variable vector $ x $, $ g_{i}(x)\\leq 0 $ are called inequality constraints $ h_{j}(x)=0 $ are called equality constraints , and $ m\\geq 0 and p\\geq 0 $. If $ m $ and $ p $ equal 0, the problem is an unconstrained optimization problem. By convention, the standard form defines a minimization problem . A maximization problem can be treated by negating the objective function.","title":"Continuous optimization problem"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#combinatorial#optimization#problem","text":"Main article: Combinatorial optimization Formally, a combinatorial optimization problem $ A $ is a quadruple[ citation needed ] $ (I,f,m,g) $, where $ I $ is a set of instances; given an instance $ x\\in I $, $ f(x) $ is the set of feasible solutions; given an instance $ x $ and a feasible solution $ y $ of $ x $, $ m(x,y) $ denotes the measure of $ y $, which is usually a positive real . $ g $ is the goal function, and is either $ \\min $ or $ \\max $. The goal is then to find for some instance $ x $ an optimal solution , that is, a feasible solution $ y $ with $$ m(x,y)=g{\\bigl {}m(x,y')\\mid y'\\in f(x){\\bigr }} $$ For each combinatorial optimization problem, there is a corresponding decision problem that asks whether there is a feasible solution for some particular measure $ m_{0} $. For example, if there is a graph $ G $ which contains vertices $ u $ and $ v $, an optimization problem might be \"find a path from $ u $ to $ v $ that uses the fewest edges\". This problem might have an answer of, say, 4. A corresponding decision problem would be \"is there a path from $ u $ to $ v $ that uses 10 or fewer edges?\" This problem can be answered with a simple 'yes' or 'no'.","title":"Combinatorial optimization problem"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#summary","text":"\u7c7b\u522b \u5b50\u7c7b\u522b \u65b9\u6cd5\u8bba Continuous optimization problem - \u7ef4\u57fa\u767e\u79d1 Continuous optimization - Unconstrained nonlinear - Constrained nonlinear - Convex optimization \u6bd4\u8f83\u5178\u578b\u7684\u6709: - Iterative method Combinatorial optimization problem - \u7ef4\u57fa\u767e\u79d1 Discrete optimization - \u7ef4\u57fa\u767e\u79d1 Combinatorial optimization \u6bd4\u8f83\u5178\u578b\u7684\u6709: - relation-based algorithm model","title":"Summary"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Optimization/#_1","text":"\u4e00\u822c\uff0c\u5728\u5b9e\u8df5\u4e2d\uff0c \u4e0a\u8ff0\u4e24\u79cd\u7c7b\u522b\u7684optimization problem\u91c7\u7528\u7684\u662f\u4e0d\u540c\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002 \u4e00\u822c\u8981\u6211\u4eec\u89e3\u51b3\u7684optimization\u95ee\u9898\u4e2d\u5f80\u5f80\u53ea\u5305\u542b\u4e00\u4e2a\u6700\u503c\uff0c\u5982\uff1a All nearest smaller values \u7684\u6700\u503c\u662fnearest Maximum subarray problem \u7684\u6700\u503c\u662flargest \u6240\u4ee5\u5728\u6c42\u89e3\u6700\u503c\u95ee\u9898\u7684\u65f6\u5019\uff0c\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u524d\u63d0\u662f\u8981\u641e\u6e05\u695a\u5b83\u7684\u6700\u503c\u662f\u4ec0\u4e48\u3002 \u5176\u6b21\uff0c\u6211\u4eec\u5f80\u5f80\u662f\u57fa\u4e8e**\u6bd4\u8f83**\uff08\u6253\u64c2\u53f0\uff09\u6765\u6c42\u89e3\u6700\u503c\uff0c\u4f46\u662f\u6709\u6709\u4e9b\u662f\u53ef\u4ee5\u76f4\u63a5\u8fdb\u884c\u6bd4\u8f83\u7684\uff0c\u6bd4\u5982\u6570\u503c\uff0c\u4f46\u662f\u6709\u4e9b\u662f\u65e0\u6cd5\u76f4\u63a5\u8fdb\u884c\u6bd4\u8f83\u7684\uff0c\u6bd4\u5982\u5728 All nearest smaller values \u4e2d\u6700\u503c\u662fnearest\uff0c\u9664\u975e\u8bb0\u5f55\u6bcf\u4e2a\u5143\u7d20\u7684\u4f4d\u7f6e\uff0c\u5426\u5219\u53ea\u80fd\u591f\u501f\u52a9\u4e00\u4e2astack\u6765\u5b9e\u73b0nearest\uff1b","title":"\u65b9\u6cd5\u8bba"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorial-optimization/","text":"Combinatorial optimization \u201cCombinatorial optimization\u201d\u5373\u201c\u7ec4\u5408\u4f18\u5316\u201d\uff0c\u6211\u662f\u5728\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1 Search algorithm \u7684\u65f6\u5019\u53d1\u73b0\u5b83\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1aCombinatorial optimization\u662f\u7efc\u5408\u4e86 Search algorithm \u548coptimization\u601d\u60f3\u7684\uff0c\u5bfb\u627e\u6700\u4f18\u89e3\u7684\u8fc7\u7a0b\u5176\u5b9e\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2asearch\u7684\u8fc7\u7a0b\uff1b\u7ecf\u8fc7\u4e00\u756a\u601d\u8003\uff0c\u51b3\u5b9a\u5c06\u5b83\u653e\u5230 optimization \u7ae0\u8282\u3002 Combinatorial optimization\u57fa\u672c\u4e0a\u53ef\u4ee5\u91c7\u7528relation-based algorithm model\uff08\u53c2\u89c1 Relation-structure-computation\\Computation\\index.md \uff09\u3002 \u7ef4\u57fa\u767e\u79d1 Combinatorial optimization Example \u6211\u4eec\u5e73\u65f6\u6240\u9047\u89c1\u7684\u5927\u591a\u6570optimization\u95ee\u9898\uff0c\u90fd\u662f\u53ef\u4ee5\u5f52\u5165combinatorial optimization\u8303\u8f74\uff0c\u6bd4\u5982\uff1a travelling salesman problem minimum spanning tree problem (\"MST\") knapsack problem","title":"Combinatorial-optimization"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorial-optimization/#combinatorial#optimization","text":"\u201cCombinatorial optimization\u201d\u5373\u201c\u7ec4\u5408\u4f18\u5316\u201d\uff0c\u6211\u662f\u5728\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1 Search algorithm \u7684\u65f6\u5019\u53d1\u73b0\u5b83\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1aCombinatorial optimization\u662f\u7efc\u5408\u4e86 Search algorithm \u548coptimization\u601d\u60f3\u7684\uff0c\u5bfb\u627e\u6700\u4f18\u89e3\u7684\u8fc7\u7a0b\u5176\u5b9e\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2asearch\u7684\u8fc7\u7a0b\uff1b\u7ecf\u8fc7\u4e00\u756a\u601d\u8003\uff0c\u51b3\u5b9a\u5c06\u5b83\u653e\u5230 optimization \u7ae0\u8282\u3002 Combinatorial optimization\u57fa\u672c\u4e0a\u53ef\u4ee5\u91c7\u7528relation-based algorithm model\uff08\u53c2\u89c1 Relation-structure-computation\\Computation\\index.md \uff09\u3002","title":"Combinatorial optimization"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorial-optimization/#combinatorial#optimization_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Combinatorial optimization"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorial-optimization/#example","text":"\u6211\u4eec\u5e73\u65f6\u6240\u9047\u89c1\u7684\u5927\u591a\u6570optimization\u95ee\u9898\uff0c\u90fd\u662f\u53ef\u4ee5\u5f52\u5165combinatorial optimization\u8303\u8f74\uff0c\u6bd4\u5982\uff1a travelling salesman problem minimum spanning tree problem (\"MST\") knapsack problem","title":"Example"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/","text":"\u5173\u4e8e\u672c\u7ae0 \u201ccombinatorics\u201d\u5373\u201c\u7ec4\u5408\u5b66\u201d\uff0c\u662fdiscrete math\u7684\u4e00\u4e2a\u91cd\u8981\u5206\u652f\u3002\u5728 Discrete Mathematics and Its Applications \u7684 Preface \u4e2d\u63d0\u53ca\u7684\u201cCombinatorial Analysis\u201d\u5176\u5b9e\u5c31\u662f\u8fd0\u7528combinatorics\u4e2d\u7684\u7406\u8bba\u4e0e\u65b9\u6cd5\u3002\u5728 Discrete Mathematics and Its Applications \u7684\u201cCombinatorial Analysis\u201d\u5c31\u5c5e\u4e8ecombinatorics\u3002 Discrete Mathematics and Its Applications \u7684 chapter 6 \u4e2d\u5bf9\u5b83\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 \u53ea\u6709\u79bb\u6563\u624d\u80fd\u591f\u8ba1\u6570\uff0c\u8fd9\u5c31\u884d\u751f\u51fa\u4e86 Combinatorics \u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/#_1","text":"\u201ccombinatorics\u201d\u5373\u201c\u7ec4\u5408\u5b66\u201d\uff0c\u662fdiscrete math\u7684\u4e00\u4e2a\u91cd\u8981\u5206\u652f\u3002\u5728 Discrete Mathematics and Its Applications \u7684 Preface \u4e2d\u63d0\u53ca\u7684\u201cCombinatorial Analysis\u201d\u5176\u5b9e\u5c31\u662f\u8fd0\u7528combinatorics\u4e2d\u7684\u7406\u8bba\u4e0e\u65b9\u6cd5\u3002\u5728 Discrete Mathematics and Its Applications \u7684\u201cCombinatorial Analysis\u201d\u5c31\u5c5e\u4e8ecombinatorics\u3002 Discrete Mathematics and Its Applications \u7684 chapter 6 \u4e2d\u5bf9\u5b83\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 \u53ea\u6709\u79bb\u6563\u624d\u80fd\u591f\u8ba1\u6570\uff0c\u8fd9\u5c31\u884d\u751f\u51fa\u4e86 Combinatorics \u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Combinatorics/","text":"Combinatorics Combinatorics is an area of mathematics primarily concerned with counting\uff08\u8ba1\u6570\uff09, both as a means and an end in obtaining results, and certain properties of finite structures \uff08\u6709\u9650\u7ed3\u6784\uff09. It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics , from evolutionary biology to computer science , etc. To fully understand the scope of combinatorics requires a great deal of further amplification\uff08\u653e\u5927\uff09, the details of which are not universally agreed upon.[ 1] According to H.J. Ryser , a definition of the subject is difficult because it crosses so many mathematical subdivisions.[ 2] Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with the enumeration (counting) of specified structures, sometimes referred to as arrangements or configurations in a very general sense, associated with finite systems, the existence of such structures that satisfy certain given criteria, the construction of these structures, perhaps in many ways, and optimization , finding the \"best\" structure or solution among several possibilities, be it the \"largest\", \"smallest\" or satisfying some other optimality criterion. Leon Mirsky has said: \"combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.\"[ 3] One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella.[ 4] Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable ) but discrete setting. Combinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics , notably in algebra , probability theory , topology , and geometry ,[ 5] as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right.[ 6] One of the oldest and most accessible parts of combinatorics is graph theory , which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms . A mathematician who studies combinatorics is called a combinatorialist . Approaches and subfields of combinatorics Enumerative combinatorics Main article: Enumerative combinatorics Enumerative combinatorics is the most classical area of combinatorics and concentrates on counting the number of certain combinatorial objects . Although counting the number of elements in a set is a rather broad mathematical problem , many of the problems that arise in applications have a relatively simple combinatorial description. Fibonacci numbers is the basic example of a problem in enumerative combinatorics. The twelvefold way provides a unified framework for counting permutations , combinations and partitions . Five binary trees on three vertices , an example of Catalan numbers . Analytic combinatorics Main article: Analytic combinatorics Analytic combinatorics concerns the enumeration of combinatorial structures using tools from complex analysis and probability theory . In contrast with enumerative combinatorics, which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae . Partition theory Main article: Partition theory Partition theory studies various enumeration and asymptotic problems related to integer partitions , and is closely related to q-series , special functions and orthogonal polynomials . Originally a part of number theory and analysis , it is now considered a part of combinatorics or an independent field. It incorporates the bijective approach and various tools in analysis and analytic number theory and has connections with statistical mechanics . Related fields Combinatorial optimization Combinatorial optimization is the study of optimization on discrete and combinatorial objects. It started as a part of combinatorics and graph theory, but is now viewed as a branch of applied mathematics and computer science, related to operations research , algorithm theory and computational complexity theory .","title":"Combinatorics"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Combinatorics/#combinatorics","text":"Combinatorics is an area of mathematics primarily concerned with counting\uff08\u8ba1\u6570\uff09, both as a means and an end in obtaining results, and certain properties of finite structures \uff08\u6709\u9650\u7ed3\u6784\uff09. It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics , from evolutionary biology to computer science , etc. To fully understand the scope of combinatorics requires a great deal of further amplification\uff08\u653e\u5927\uff09, the details of which are not universally agreed upon.[ 1] According to H.J. Ryser , a definition of the subject is difficult because it crosses so many mathematical subdivisions.[ 2] Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with the enumeration (counting) of specified structures, sometimes referred to as arrangements or configurations in a very general sense, associated with finite systems, the existence of such structures that satisfy certain given criteria, the construction of these structures, perhaps in many ways, and optimization , finding the \"best\" structure or solution among several possibilities, be it the \"largest\", \"smallest\" or satisfying some other optimality criterion. Leon Mirsky has said: \"combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.\"[ 3] One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella.[ 4] Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable ) but discrete setting. Combinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics , notably in algebra , probability theory , topology , and geometry ,[ 5] as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right.[ 6] One of the oldest and most accessible parts of combinatorics is graph theory , which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms . A mathematician who studies combinatorics is called a combinatorialist .","title":"Combinatorics"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Combinatorics/#approaches#and#subfields#of#combinatorics","text":"","title":"Approaches and subfields of combinatorics"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Combinatorics/#enumerative#combinatorics","text":"Main article: Enumerative combinatorics Enumerative combinatorics is the most classical area of combinatorics and concentrates on counting the number of certain combinatorial objects . Although counting the number of elements in a set is a rather broad mathematical problem , many of the problems that arise in applications have a relatively simple combinatorial description. Fibonacci numbers is the basic example of a problem in enumerative combinatorics. The twelvefold way provides a unified framework for counting permutations , combinations and partitions . Five binary trees on three vertices , an example of Catalan numbers .","title":"Enumerative combinatorics"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Combinatorics/#analytic#combinatorics","text":"Main article: Analytic combinatorics Analytic combinatorics concerns the enumeration of combinatorial structures using tools from complex analysis and probability theory . In contrast with enumerative combinatorics, which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae .","title":"Analytic combinatorics"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Combinatorics/#partition#theory","text":"Main article: Partition theory Partition theory studies various enumeration and asymptotic problems related to integer partitions , and is closely related to q-series , special functions and orthogonal polynomials . Originally a part of number theory and analysis , it is now considered a part of combinatorics or an independent field. It incorporates the bijective approach and various tools in analysis and analytic number theory and has connections with statistical mechanics .","title":"Partition theory"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Combinatorics/#related#fields","text":"","title":"Related fields"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Combinatorics/#combinatorial#optimization","text":"Combinatorial optimization is the study of optimization on discrete and combinatorial objects. It started as a part of combinatorics and graph theory, but is now viewed as a branch of applied mathematics and computer science, related to operations research , algorithm theory and computational complexity theory .","title":"Combinatorial optimization"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Enumerative-combinatorics/","text":"Enumerative combinatorics Enumerative combinatorics is an area of combinatorics that deals with the number of ways that certain patterns can be formed. Two examples of this type of problem are counting combinations and counting permutations . More generally, given an infinite collection of finite sets S_i S_i indexed by the natural numbers , enumerative combinatorics seeks to describe a counting function which counts the number of objects in S_n S_n for each n . Although counting the number of elements in a set is a rather broad mathematical problem , many of the problems that arise in applications have a relatively simple combinatorial description. The twelvefold way provides a unified framework for counting permutations , combinations and partitions . SUMMARY : \u5bf9\u4e8eprogram\u800c\u8a00\uff0c Enumeration \u6bd4 counting \u66f4\u52a0\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728optimization\u95ee\u9898\u4e2d\uff0c\u53ea\u6709\u7ecf\u8fc7 Enumeration \u3001\u6bd4\u8f83\u624d\u80fd\u591f\u5f97\u5230\u6700\u4f18\u7684\u89e3\uff1b counting \u5f80\u5f80\u662f\u5728\u8fdb\u884ccomplexity\u5206\u6790\u7684\u65f6\u5019\u9700\u8981\u4f7f\u7528\u7684\uff1b The simplest such functions are closed formulas , which can be expressed as a composition of elementary functions such as factorials , powers, and so on. For instance, as shown below, the number of different possible orderings of a deck of n cards is f ( n ) = n !. The problem of finding a closed formula is known as algebraic enumeration , and frequently involves deriving a recurrence relation or generating function and using this to arrive at the desired closed form. Often, a complicated closed formula yields little insight into the behavior of the counting function as the number of counted objects grows. In these cases, a simple asymptotic approximation may be preferable. A function $ g(n) $ is an asymptotic approximation to $ f(n) $ if $ f(n)/g(n)\\rightarrow 1 $ as $ n\\rightarrow \\infty $. In this case, we write $ f(n)\\sim g(n).\\, $ Generating functions Generating functions are used to describe families of combinatorial objects. Let $ {\\mathcal {F}} $ denote the family of objects and let F ( x ) be its generating function. Then $ F(x)=\\sum {n=0}^{\\infty }f {n}x^{n} $ where $ f_{n} $ denotes the number of combinatorial objects of size n . The number of combinatorial objects of size n is therefore given by the coefficient of $ x^{n} $. Some common operation on families of combinatorial objects and its effect on the generating function will now be developed. The exponential generating function is also sometimes used. In this case it would have the form $ F(x)=\\sum {n=0}^{\\infty }f {n}{\\frac {x^{n}}{n!}} $ Once determined, the generating function yields the information given by the previous approaches. In addition, the various natural operations on generating functions such as addition, multiplication, differentiation, etc., have a combinatorial significance; this allows one to extend results from one combinatorial problem in order to solve others. Union Given two combinatorial families, $ {\\mathcal {F}} $ and $ {\\mathcal {G}} $ with generating functions F ( x ) and G ( x ) respectively, the disjoint union of the two families ($ {\\mathcal {F}}\\cup {\\mathcal {G}} $) has generating function F ( x ) + G ( x ). Pairs For two combinatorial families as above the Cartesian product (pair) of the two families ($ {\\mathcal {F}}\\times {\\mathcal {G}} $) has generating function F ( x ) G ( x ). Sequences A sequence generalizes the idea of the pair as defined above. Sequences are arbitrary Cartesian products of a combinatorial object with itself. Formally: $ {\\mbox{Seq}}({\\mathcal {F}})=\\epsilon \\cup {\\mathcal {F}} \\cup {\\mathcal {F}}\\times {\\mathcal {F}} \\cup {\\mathcal {F}}\\times {\\mathcal {F}}\\times {\\mathcal {F}} \\cup \\cdots $ To put the above in words: An empty sequence or a sequence of one element or a sequence of two elements or a sequence of three elements, etc. The generating function would be: $ 1+F(x)+[F(x)] {2}+[F(x)] {3}+\\cdots ={\\frac {1}{1-F(x)}} $ Combinatorial structures The above operations can now be used to enumerate common combinatorial objects including trees (binary and plane), Dyck paths and cycles. A combinatorial structure is composed of atoms. For example, with trees the atoms would be the nodes. The atoms which compose the object can either be labeled or unlabeled. Unlabeled atoms are indistinguishable from each other, while labelled atoms are distinct. Therefore, for a combinatorial object consisting of labeled atoms a new object can be formed by simply swapping two or more atoms. Binary and plane trees Binary and plane trees are examples of an unlabeled combinatorial structure. Trees consist of nodes linked by edges in such a way that there are no cycles. There is generally a node called the root, which has no parent node. In Plane trees each node can have an arbitrary number of children. In binary trees, a special case of plane trees, each node can have either two or no children. Let $ {\\mathcal {P}} $ denote the family of all plane trees. Then this family can be recursively defined as follows: $ {\\mathcal {P}}={\\bullet }\\times {\\mbox{Seq}}({\\mathcal {P}}) $ In this case $ {\\bullet } $ represents the family of objects consisting of one node. This has generating function x . Let P ( x ) denote the generating function $ {\\mathcal {P}} $. Putting the above description in words: A plane tree consists of a node to which is attached an arbitrary number of subtrees, each of which is also a plane tree. Using the operation on families of combinatorial structures developed earlier this translates to a recursive generating function: $ P(x)=x{\\frac {1}{1-P(x)}} $ After solving for P ( x ): $ P(x)={\\frac {1-{\\sqrt {1-4x}}}{2}} $ An explicit formula for the number of plane trees of size n can now be determined by extracting the coefficient of x^n x^n . $ {\\begin{aligned}p_{n}&=[x {n}]P(x)=[x {n}]{\\frac {1-{\\sqrt {1-4x}}}{2}}\\[6pt]&=[x^{n}]{\\frac {1}{2}}-[x^{n}]{\\frac {1}{2}}{\\sqrt {1-4x}}\\[6pt]&=-{\\frac {1}{2}}[x^{n}]\\sum _{k=0}^{\\infty }{{\\frac {1}{2}} \\choose k}(-4x)^{k}\\[6pt]&=-{\\frac {1}{2}}{{\\frac {1}{2}} \\choose n}(-4)^{n}\\[6pt]&={\\frac {1}{n}}{2n-2 \\choose n-1}\\end{aligned}} $ Note: The notation [ x**n ] f ( x ) refers to the coefficient of x**n in f ( x ). The series expansion of the square root is based on Newton's generalization of the binomial theorem . To get from the fourth to fifth line manipulations using the generalized binomial coefficient is needed. The expression on the last line is equal to the ( n \u2212 1)th Catalan number . Therefore p**n = c**n \u22121.","title":"Enumerative-combinatorics"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Enumerative-combinatorics/#enumerative#combinatorics","text":"Enumerative combinatorics is an area of combinatorics that deals with the number of ways that certain patterns can be formed. Two examples of this type of problem are counting combinations and counting permutations . More generally, given an infinite collection of finite sets S_i S_i indexed by the natural numbers , enumerative combinatorics seeks to describe a counting function which counts the number of objects in S_n S_n for each n . Although counting the number of elements in a set is a rather broad mathematical problem , many of the problems that arise in applications have a relatively simple combinatorial description. The twelvefold way provides a unified framework for counting permutations , combinations and partitions . SUMMARY : \u5bf9\u4e8eprogram\u800c\u8a00\uff0c Enumeration \u6bd4 counting \u66f4\u52a0\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728optimization\u95ee\u9898\u4e2d\uff0c\u53ea\u6709\u7ecf\u8fc7 Enumeration \u3001\u6bd4\u8f83\u624d\u80fd\u591f\u5f97\u5230\u6700\u4f18\u7684\u89e3\uff1b counting \u5f80\u5f80\u662f\u5728\u8fdb\u884ccomplexity\u5206\u6790\u7684\u65f6\u5019\u9700\u8981\u4f7f\u7528\u7684\uff1b The simplest such functions are closed formulas , which can be expressed as a composition of elementary functions such as factorials , powers, and so on. For instance, as shown below, the number of different possible orderings of a deck of n cards is f ( n ) = n !. The problem of finding a closed formula is known as algebraic enumeration , and frequently involves deriving a recurrence relation or generating function and using this to arrive at the desired closed form. Often, a complicated closed formula yields little insight into the behavior of the counting function as the number of counted objects grows. In these cases, a simple asymptotic approximation may be preferable. A function $ g(n) $ is an asymptotic approximation to $ f(n) $ if $ f(n)/g(n)\\rightarrow 1 $ as $ n\\rightarrow \\infty $. In this case, we write $ f(n)\\sim g(n).\\, $","title":"Enumerative combinatorics"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Enumerative-combinatorics/#generating#functions","text":"Generating functions are used to describe families of combinatorial objects. Let $ {\\mathcal {F}} $ denote the family of objects and let F ( x ) be its generating function. Then $ F(x)=\\sum {n=0}^{\\infty }f {n}x^{n} $ where $ f_{n} $ denotes the number of combinatorial objects of size n . The number of combinatorial objects of size n is therefore given by the coefficient of $ x^{n} $. Some common operation on families of combinatorial objects and its effect on the generating function will now be developed. The exponential generating function is also sometimes used. In this case it would have the form $ F(x)=\\sum {n=0}^{\\infty }f {n}{\\frac {x^{n}}{n!}} $ Once determined, the generating function yields the information given by the previous approaches. In addition, the various natural operations on generating functions such as addition, multiplication, differentiation, etc., have a combinatorial significance; this allows one to extend results from one combinatorial problem in order to solve others.","title":"Generating functions"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Enumerative-combinatorics/#union","text":"Given two combinatorial families, $ {\\mathcal {F}} $ and $ {\\mathcal {G}} $ with generating functions F ( x ) and G ( x ) respectively, the disjoint union of the two families ($ {\\mathcal {F}}\\cup {\\mathcal {G}} $) has generating function F ( x ) + G ( x ).","title":"Union"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Enumerative-combinatorics/#pairs","text":"For two combinatorial families as above the Cartesian product (pair) of the two families ($ {\\mathcal {F}}\\times {\\mathcal {G}} $) has generating function F ( x ) G ( x ).","title":"Pairs"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Enumerative-combinatorics/#sequences","text":"A sequence generalizes the idea of the pair as defined above. Sequences are arbitrary Cartesian products of a combinatorial object with itself. Formally: $ {\\mbox{Seq}}({\\mathcal {F}})=\\epsilon \\cup {\\mathcal {F}} \\cup {\\mathcal {F}}\\times {\\mathcal {F}} \\cup {\\mathcal {F}}\\times {\\mathcal {F}}\\times {\\mathcal {F}} \\cup \\cdots $ To put the above in words: An empty sequence or a sequence of one element or a sequence of two elements or a sequence of three elements, etc. The generating function would be: $ 1+F(x)+[F(x)] {2}+[F(x)] {3}+\\cdots ={\\frac {1}{1-F(x)}} $","title":"Sequences"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Enumerative-combinatorics/#combinatorial#structures","text":"The above operations can now be used to enumerate common combinatorial objects including trees (binary and plane), Dyck paths and cycles. A combinatorial structure is composed of atoms. For example, with trees the atoms would be the nodes. The atoms which compose the object can either be labeled or unlabeled. Unlabeled atoms are indistinguishable from each other, while labelled atoms are distinct. Therefore, for a combinatorial object consisting of labeled atoms a new object can be formed by simply swapping two or more atoms.","title":"Combinatorial structures"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Enumerative-combinatorics/#binary#and#plane#trees","text":"Binary and plane trees are examples of an unlabeled combinatorial structure. Trees consist of nodes linked by edges in such a way that there are no cycles. There is generally a node called the root, which has no parent node. In Plane trees each node can have an arbitrary number of children. In binary trees, a special case of plane trees, each node can have either two or no children. Let $ {\\mathcal {P}} $ denote the family of all plane trees. Then this family can be recursively defined as follows: $ {\\mathcal {P}}={\\bullet }\\times {\\mbox{Seq}}({\\mathcal {P}}) $ In this case $ {\\bullet } $ represents the family of objects consisting of one node. This has generating function x . Let P ( x ) denote the generating function $ {\\mathcal {P}} $. Putting the above description in words: A plane tree consists of a node to which is attached an arbitrary number of subtrees, each of which is also a plane tree. Using the operation on families of combinatorial structures developed earlier this translates to a recursive generating function: $ P(x)=x{\\frac {1}{1-P(x)}} $ After solving for P ( x ): $ P(x)={\\frac {1-{\\sqrt {1-4x}}}{2}} $ An explicit formula for the number of plane trees of size n can now be determined by extracting the coefficient of x^n x^n . $ {\\begin{aligned}p_{n}&=[x {n}]P(x)=[x {n}]{\\frac {1-{\\sqrt {1-4x}}}{2}}\\[6pt]&=[x^{n}]{\\frac {1}{2}}-[x^{n}]{\\frac {1}{2}}{\\sqrt {1-4x}}\\[6pt]&=-{\\frac {1}{2}}[x^{n}]\\sum _{k=0}^{\\infty }{{\\frac {1}{2}} \\choose k}(-4x)^{k}\\[6pt]&=-{\\frac {1}{2}}{{\\frac {1}{2}} \\choose n}(-4)^{n}\\[6pt]&={\\frac {1}{n}}{2n-2 \\choose n-1}\\end{aligned}} $ Note: The notation [ x**n ] f ( x ) refers to the coefficient of x**n in f ( x ). The series expansion of the square root is based on Newton's generalization of the binomial theorem . To get from the fourth to fifth line manipulations using the generalized binomial coefficient is needed. The expression on the last line is equal to the ( n \u2212 1)th Catalan number . Therefore p**n = c**n \u22121.","title":"Binary and plane trees"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/Catalan%27s-triangle/","text":"wikipedia Catalan's triangle In combinatorial mathematics , Catalan's triangle is a number triangle whose entries $ C(n,k) $ give the number of strings consisting of n X's and k Y's such that no initial segment of the string has more Y's than X's. It is a generalization of the Catalan numbers , and is named after Eug\u00e8ne Charles Catalan . Bailey[ 1] shows that $ C(n,k) $ satisfy the following properties: $ C(n,0)=1{\\text{ for }}n\\geq 0 $. $ C(n,1)=n{\\text{ for }}n\\geq 1 $. $ C(n+1,k)=C(n+1,k-1)+C(n,k){\\text{ for }}1<k<n+1 $ $ C(n+1,n+1)=C(n+1,n){\\text{ for }}n\\geq 1 $.","title":"Catalan's triangle"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/Catalan%27s-triangle/#wikipedia#catalans#triangle","text":"In combinatorial mathematics , Catalan's triangle is a number triangle whose entries $ C(n,k) $ give the number of strings consisting of n X's and k Y's such that no initial segment of the string has more Y's than X's. It is a generalization of the Catalan numbers , and is named after Eug\u00e8ne Charles Catalan . Bailey[ 1] shows that $ C(n,k) $ satisfy the following properties: $ C(n,0)=1{\\text{ for }}n\\geq 0 $. $ C(n,1)=n{\\text{ for }}n\\geq 1 $. $ C(n+1,k)=C(n+1,k-1)+C(n,k){\\text{ for }}1<k<n+1 $ $ C(n+1,n+1)=C(n+1,n){\\text{ for }}n\\geq 1 $.","title":"wikipedia Catalan's triangle"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/Catalan-number/","text":"Catalan number In combinatorial mathematics , the Catalan numbers form a sequence of natural numbers that occur in various counting problems , often involving recursively -defined objects. They are named after the Belgian mathematician Eug\u00e8ne Charles Catalan (1814\u20131894). The *n*th Catalan number is given directly in terms of binomial coefficients by $ C_{n}={\\frac {1}{n+1}}{2n \\choose n}={\\frac {(2n)!}{(n+1)!\\,n!}}=\\prod \\limits _{k=2}^{n}{\\frac {n+k}{k}}\\qquad {\\text{for }}n\\geq 0. $ SUMMARY : \u6ce8\u610f\u662f\u8fde\u4e58\uff0c\u4e0a\u5f0f\u7ed9\u51fa\u7684\u662fCatalan number\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u4f46\u662f\u5728\u5b9e\u9645\u5730\u89e3\u51b3\u95ee\u9898\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u66f4\u591a\u7684\u662f\u4f7f\u7528Catalan number\u7684\u9012\u5f52\u8868\u8fbe\u5f0f\uff1b The first Catalan numbers for n = 0, 1, 2, 3, ... are 1 , 1, 2 , 5 , 14 , 42 , 132 , 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440, 9694845, 35357670, 129644790, 477638700, 1767263190, 6564120420, 24466267020, 91482563640, 343059613650, 1289904147324, 4861946401452, ... (sequence A000108 in the OEIS ). The C_5 = 42 C_5 = 42 noncrossing partitions of a 5-element set (below, the other 10 of the 52 partitions ) Properties An alternative expression for C_n C_n is $ C_{n}={2n \\choose n}-{2n \\choose n+1}={1 \\over n+1}{2n \\choose n}\\quad {\\text{ for }}n\\geq 0, $ which is equivalent to the expression given above because $ {\\tbinom {2n}{n+1}}={\\tfrac {n}{n+1}}{\\tbinom {2n}{n}} $. This shows that C_n C_n is an integer , which is not immediately obvious from the first formula given. This expression forms the basis for a proof of the correctness of the formula . The Catalan numbers satisfy the recurrence relations [ 1] $ C_{0}=1\\quad {\\text{and}}\\quad C_{n+1}=\\sum {i=0}^{n}C {i}\\,C_{n-i}\\quad {\\text{for }}n\\geq 0, $ SUMMARY : \u4e3a\u4ec0\u4e48\u662f C_{i} C_{i} \u4e58\u4ee5 C_{n-i} C_{n-i} \uff0c\u800c\u4e0d\u662f\u76f8\u52a0\u5462\uff1f\u5982\u4f55\u6765\u7ed3\u548c\u5177\u4f53\u6848\u4f8b\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u5206\u6790\uff1f $ \\sum {i {1}+\\cdots +i_{m}=n,i_{1},\\ldots ,i_{m}\\geq 0}C_{i_{1}}\\cdots C_{i_{m}}={\\begin{cases}{\\dfrac {m(n+1)(n+2)\\cdots (n+m/2-1)}{2(n+m/2+2)(n+m/2+3)\\cdots (n+m)}}C_{n+m/2},&m{\\text{ even}}\\[5pt]{\\dfrac {m(n+1)(n+2)\\cdots (n+(m-1)/2)}{(n+(m+3)/2)(n+(m+3)/2+1)\\cdots (n+m)}}C_{n+(m-1)/2},&m{\\text{ odd,}}\\end{cases}} $ and $ C_{0}=1\\quad {\\text{and}}\\quad C_{n+1}={\\frac {2(2n+1)}{n+2}}C_{n}. $ Asymptotically, the Catalan numbers grow as $ C_{n}\\sim {\\frac {4 {n}}{n {3/2}{\\sqrt {\\pi }}}} $ in the sense that the quotient of the n*th Catalan number and the expression on the right tends towards 1 as *n approaches infinity. This can be proved by using Stirling's approximation for n ! or via generating functions; see the Asymptotic growth of the Catalan numbers section of the Generating function article. The only Catalan numbers C_n C_n that are odd are those for which n = 2^k \u2212 1 n = 2^k \u2212 1 ; all others are even. The only prime Catalan numbers are C_2 = 2 C_2 = 2 and C_3 = 5 C_3 = 5 .[ 2] The Catalan numbers have an integral representation $ C_{n}=\\int _{0} {4}x {n}\\rho (x)\\,dx, $ where $ \\rho (x)={\\tfrac {1}{2\\pi }}{\\sqrt {\\tfrac {4-x}{x}}}. $ This means that the Catalan numbers are a solution of the Hausdorff moment problem on the interval [0, 4] instead of [0, 1]. The orthogonal polynomials having the weight function $ \\rho (x) $ on $ [0,4] $ are $ H_{n}(x)=\\sum _{k=0}^{n}{n+k \\choose n-k}(-x)^{k}. $ Applications in combinatorics There are many counting problems in combinatorics whose solution is given by the Catalan numbers . The book Enumerative Combinatorics: Volume 2 by combinatorialist Richard P. Stanley contains a set of exercises which describe 66 different interpretations\uff08\u89e3\u91ca\uff09 of the Catalan numbers . Following are some examples, with illustrations of the cases C_3 = 5 C_3 = 5 and C_4 = 14 C_4 = 14 . C_n C_n is the number of Dyck words [ 3] of length 2n 2n . A Dyck word is a string consisting of n X's and n Y's such that no initial segment of the string has more Y's than X's. For example, the following are the Dyck words of length 6: XXXYYY XYXXYY XYXYXY XXYYXY XXYXYY. Re-interpreting the symbol X as an open parenthesis and Y as a close parenthesis, C_n C_n counts the number of expressions containing n pairs of parentheses which are correctly matched: ((())) ()(()) ()()() (())() (()()) C_n C_n is the number of different ways n + 1 factors can be completely parenthesized (or the number of ways of associating n applications of a binary operator ). For n = 3, for example, we have the following five different parenthesizations of four factors: ((ab)c)d (a(bc))d (ab)(cd) a((bc)d) a(b(cd)) Successive applications of a binary operator can be represented in terms of a full binary tree . (A rooted binary tree is full if every vertex has either two children or no children.) It follows that C_n C_n is the number of full binary trees with n + 1 leaves: SUMMARY : \u5982\u679c\u5c06 ((ab)c)d (a(bc))d (ab)(cd) a((bc)d) a(b(cd)) \u4e2d\u7684\u5b57\u7b26\u770b\u505a\u4e0a\u9762\u6811\u4e2d\u7684leaf node\u7684\u8bdd\uff0c\u5219\u5b83\u4eec\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\uff0c\u8fd9\u8bf4\u660e\u5b83\u4eec\u672c\u8d28\u4e0a\u662f\u540c\u4e00\u7c7b\u95ee\u9898\uff1b\u8fd9\u8bf4\u660ecatalan number\u548c\u4e8c\u53c9\u6811\u4e4b\u95f4\u4e5f\u662f\u5b58\u5728\u7740\u4e00\u5b9a\u7684\u5173\u8054\u7684\uff1b C_n C_n is the number of non-isomorphic ordered trees with n + 1 vertices. (An ordered tree is a rooted tree in which the children of each vertex are given a fixed left-to-right order.)[ 4] C_n C_n is the number of monotonic lattice paths along the edges of a grid with n \u00d7 n square cells, which do not pass above the diagonal. A monotonic path is one which starts in the lower left corner, finishes in the upper right corner, and consists entirely of edges pointing rightwards or upwards. Counting such paths is equivalent to counting Dyck words: X stands for \"move right\" and Y stands for \"move up\". Proof of the formula There are several ways of explaining why the formula $ C_{n}={\\frac {1}{n+1}}{2n \\choose n} $ Generalizations The two-parameter sequence of non-negative integers $ {\\frac {(2m)!(2n)!}{(m+n)!m!n!}} $ is a generalization of the Catalan numbers. These are named super-Catalan numbers , by Ira Gessel . These number should not confused with the Schr\u00f6der\u2013Hipparchus numbers , which sometimes are also called super-Catalan numbers. For $ m=1 $, this is just two times the ordinary Catalan numbers, and for $ m=n $, the numbers have an easy combinatorial description. However, other combinatorial descriptions are only known[ 15] for $ m=2 $ and $ m=3 $, and it is an open problem to find a general combinatorial interpretation. Sergey Fomin and Nathan Reading have given a generalized Catalan number associated to any finite crystalographic Coxeter group , namely the number of fully commutative elements of the group; in terms of the associated root system , it is the number of anti-chains (or order ideals) in the poset of positive roots. The classical Catalan number $ C_{n} $ corresponds to the root system of type $ A_{n} $. The classical recurrence relation generalizes: the Catalan number of a Coxeter diagram is equal to the sum of the Catalan numbers of all its maximal proper sub-diagrams.[ 16]","title":"Catalan-number"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/Catalan-number/#catalan#number","text":"In combinatorial mathematics , the Catalan numbers form a sequence of natural numbers that occur in various counting problems , often involving recursively -defined objects. They are named after the Belgian mathematician Eug\u00e8ne Charles Catalan (1814\u20131894). The *n*th Catalan number is given directly in terms of binomial coefficients by $ C_{n}={\\frac {1}{n+1}}{2n \\choose n}={\\frac {(2n)!}{(n+1)!\\,n!}}=\\prod \\limits _{k=2}^{n}{\\frac {n+k}{k}}\\qquad {\\text{for }}n\\geq 0. $ SUMMARY : \u6ce8\u610f\u662f\u8fde\u4e58\uff0c\u4e0a\u5f0f\u7ed9\u51fa\u7684\u662fCatalan number\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u4f46\u662f\u5728\u5b9e\u9645\u5730\u89e3\u51b3\u95ee\u9898\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u66f4\u591a\u7684\u662f\u4f7f\u7528Catalan number\u7684\u9012\u5f52\u8868\u8fbe\u5f0f\uff1b The first Catalan numbers for n = 0, 1, 2, 3, ... are 1 , 1, 2 , 5 , 14 , 42 , 132 , 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440, 9694845, 35357670, 129644790, 477638700, 1767263190, 6564120420, 24466267020, 91482563640, 343059613650, 1289904147324, 4861946401452, ... (sequence A000108 in the OEIS ). The C_5 = 42 C_5 = 42 noncrossing partitions of a 5-element set (below, the other 10 of the 52 partitions )","title":"Catalan number"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/Catalan-number/#properties","text":"An alternative expression for C_n C_n is $ C_{n}={2n \\choose n}-{2n \\choose n+1}={1 \\over n+1}{2n \\choose n}\\quad {\\text{ for }}n\\geq 0, $ which is equivalent to the expression given above because $ {\\tbinom {2n}{n+1}}={\\tfrac {n}{n+1}}{\\tbinom {2n}{n}} $. This shows that C_n C_n is an integer , which is not immediately obvious from the first formula given. This expression forms the basis for a proof of the correctness of the formula . The Catalan numbers satisfy the recurrence relations [ 1] $ C_{0}=1\\quad {\\text{and}}\\quad C_{n+1}=\\sum {i=0}^{n}C {i}\\,C_{n-i}\\quad {\\text{for }}n\\geq 0, $ SUMMARY : \u4e3a\u4ec0\u4e48\u662f C_{i} C_{i} \u4e58\u4ee5 C_{n-i} C_{n-i} \uff0c\u800c\u4e0d\u662f\u76f8\u52a0\u5462\uff1f\u5982\u4f55\u6765\u7ed3\u548c\u5177\u4f53\u6848\u4f8b\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u5206\u6790\uff1f $ \\sum {i {1}+\\cdots +i_{m}=n,i_{1},\\ldots ,i_{m}\\geq 0}C_{i_{1}}\\cdots C_{i_{m}}={\\begin{cases}{\\dfrac {m(n+1)(n+2)\\cdots (n+m/2-1)}{2(n+m/2+2)(n+m/2+3)\\cdots (n+m)}}C_{n+m/2},&m{\\text{ even}}\\[5pt]{\\dfrac {m(n+1)(n+2)\\cdots (n+(m-1)/2)}{(n+(m+3)/2)(n+(m+3)/2+1)\\cdots (n+m)}}C_{n+(m-1)/2},&m{\\text{ odd,}}\\end{cases}} $ and $ C_{0}=1\\quad {\\text{and}}\\quad C_{n+1}={\\frac {2(2n+1)}{n+2}}C_{n}. $ Asymptotically, the Catalan numbers grow as $ C_{n}\\sim {\\frac {4 {n}}{n {3/2}{\\sqrt {\\pi }}}} $ in the sense that the quotient of the n*th Catalan number and the expression on the right tends towards 1 as *n approaches infinity. This can be proved by using Stirling's approximation for n ! or via generating functions; see the Asymptotic growth of the Catalan numbers section of the Generating function article. The only Catalan numbers C_n C_n that are odd are those for which n = 2^k \u2212 1 n = 2^k \u2212 1 ; all others are even. The only prime Catalan numbers are C_2 = 2 C_2 = 2 and C_3 = 5 C_3 = 5 .[ 2] The Catalan numbers have an integral representation $ C_{n}=\\int _{0} {4}x {n}\\rho (x)\\,dx, $ where $ \\rho (x)={\\tfrac {1}{2\\pi }}{\\sqrt {\\tfrac {4-x}{x}}}. $ This means that the Catalan numbers are a solution of the Hausdorff moment problem on the interval [0, 4] instead of [0, 1]. The orthogonal polynomials having the weight function $ \\rho (x) $ on $ [0,4] $ are $ H_{n}(x)=\\sum _{k=0}^{n}{n+k \\choose n-k}(-x)^{k}. $","title":"Properties"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/Catalan-number/#applications#in#combinatorics","text":"There are many counting problems in combinatorics whose solution is given by the Catalan numbers . The book Enumerative Combinatorics: Volume 2 by combinatorialist Richard P. Stanley contains a set of exercises which describe 66 different interpretations\uff08\u89e3\u91ca\uff09 of the Catalan numbers . Following are some examples, with illustrations of the cases C_3 = 5 C_3 = 5 and C_4 = 14 C_4 = 14 . C_n C_n is the number of Dyck words [ 3] of length 2n 2n . A Dyck word is a string consisting of n X's and n Y's such that no initial segment of the string has more Y's than X's. For example, the following are the Dyck words of length 6: XXXYYY XYXXYY XYXYXY XXYYXY XXYXYY. Re-interpreting the symbol X as an open parenthesis and Y as a close parenthesis, C_n C_n counts the number of expressions containing n pairs of parentheses which are correctly matched: ((())) ()(()) ()()() (())() (()()) C_n C_n is the number of different ways n + 1 factors can be completely parenthesized (or the number of ways of associating n applications of a binary operator ). For n = 3, for example, we have the following five different parenthesizations of four factors: ((ab)c)d (a(bc))d (ab)(cd) a((bc)d) a(b(cd)) Successive applications of a binary operator can be represented in terms of a full binary tree . (A rooted binary tree is full if every vertex has either two children or no children.) It follows that C_n C_n is the number of full binary trees with n + 1 leaves: SUMMARY : \u5982\u679c\u5c06 ((ab)c)d (a(bc))d (ab)(cd) a((bc)d) a(b(cd)) \u4e2d\u7684\u5b57\u7b26\u770b\u505a\u4e0a\u9762\u6811\u4e2d\u7684leaf node\u7684\u8bdd\uff0c\u5219\u5b83\u4eec\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\uff0c\u8fd9\u8bf4\u660e\u5b83\u4eec\u672c\u8d28\u4e0a\u662f\u540c\u4e00\u7c7b\u95ee\u9898\uff1b\u8fd9\u8bf4\u660ecatalan number\u548c\u4e8c\u53c9\u6811\u4e4b\u95f4\u4e5f\u662f\u5b58\u5728\u7740\u4e00\u5b9a\u7684\u5173\u8054\u7684\uff1b C_n C_n is the number of non-isomorphic ordered trees with n + 1 vertices. (An ordered tree is a rooted tree in which the children of each vertex are given a fixed left-to-right order.)[ 4] C_n C_n is the number of monotonic lattice paths along the edges of a grid with n \u00d7 n square cells, which do not pass above the diagonal. A monotonic path is one which starts in the lower left corner, finishes in the upper right corner, and consists entirely of edges pointing rightwards or upwards. Counting such paths is equivalent to counting Dyck words: X stands for \"move right\" and Y stands for \"move up\".","title":"Applications in combinatorics"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/Catalan-number/#proof#of#the#formula","text":"There are several ways of explaining why the formula $ C_{n}={\\frac {1}{n+1}}{2n \\choose n} $","title":"Proof of the formula"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/Catalan-number/#generalizations","text":"The two-parameter sequence of non-negative integers $ {\\frac {(2m)!(2n)!}{(m+n)!m!n!}} $ is a generalization of the Catalan numbers. These are named super-Catalan numbers , by Ira Gessel . These number should not confused with the Schr\u00f6der\u2013Hipparchus numbers , which sometimes are also called super-Catalan numbers. For $ m=1 $, this is just two times the ordinary Catalan numbers, and for $ m=n $, the numbers have an easy combinatorial description. However, other combinatorial descriptions are only known[ 15] for $ m=2 $ and $ m=3 $, and it is an open problem to find a general combinatorial interpretation. Sergey Fomin and Nathan Reading have given a generalized Catalan number associated to any finite crystalographic Coxeter group , namely the number of fully commutative elements of the group; in terms of the associated root system , it is the number of anti-chains (or order ideals) in the poset of positive roots. The classical Catalan number $ C_{n} $ corresponds to the root system of type $ A_{n} $. The classical recurrence relation generalizes: the Catalan number of a Coxeter diagram is equal to the sum of the Catalan numbers of all its maximal proper sub-diagrams.[ 16]","title":"Generalizations"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/","text":"baike catalan **\u5361\u5854\u5170\u6570**\u662f \u7ec4\u5408\u6570\u5b66 \u4e2d\u4e00\u4e2a\u5e38\u5728\u5404\u79cd\u8ba1\u6570\u95ee\u9898\u4e2d\u51fa\u73b0\u7684 \u6570\u5217 \u3002\u4ee5 \u6bd4\u5229\u65f6 \u7684\u6570\u5b66\u5bb6\u6b27\u4ec1\u00b7\u67e5\u7406\u00b7\u5361\u5854\u5170\uff081814\u20131894\uff09\u547d\u540d\u3002\u5386\u53f2\u4e0a\uff0c\u6e05\u4ee3\u6570\u5b66\u5bb6 \u660e\u5b89\u56fe (1692\u5e74\uff0d1763\u5e74)\u5728\u5176\u300a \u5272\u571c\u5bc6\u7387\u6377\u6cd5 \u300b\u6700\u65e9\u7528\u5230\u201c\u5361\u5854\u5170\u6570\u201d\uff0c\u8fdc\u8fdc\u65e9\u4e8e\u5361\u5854\u5170\u3002\u6709\u4e2d\u56fd\u5b66\u8005\u5efa\u8bae\u5c06\u6b64\u6570\u547d\u540d\u4e3a\u201c\u660e\u5b89\u56fe\u6570\u201d\u6216\u201c\u660e\u5b89\u56fe-\u5361\u5854\u5170\u6570\u201d\u3002\u5361\u5854\u5170\u6570\u7684\u4e00\u822c\u516c\u5f0f\u4e3a C(2n,n)/(n+1)\u3002 \u6027\u8d28 \u4ee4h(0)=1,h(1)=1\uff0c\u5361\u5854\u5170\u6570\u6ee1\u8db3 \u9012\u5f52 \u5f0f\uff1a h(n)= h(0)*h(n-1) + h(1)*h(n-2) + ... + h(n-1)h(0) (\u5176\u4e2dn>=2),\u8fd9\u662fn\u9636 \u9012\u63a8 \u5173\u7cfb; \u8fd8\u53ef\u4ee5\u5316\u7b80\u4e3a1\u9636\u9012\u63a8\u5173\u7cfb: \u5982h(n)=(4n-2)/(n+1)*h(n-1)(n>1) h(0)=1 \u8be5\u9012\u63a8\u5173\u7cfb\u7684\u89e3\u4e3a\uff1a h(n)=****C(2n,n)/(n+1)=P(2n,n)/(n+1)!=(2n)!/(n!*(n+1)!) (n=1,2,3,...) \u5361\u5854\u5170\u6570\u5217\u7684\u524d\u51e0\u9879\u4e3a(sequence A 0 0 0 1 0 8 in OEIS) [\u6ce8\uff1a n = 0, 1, 2, 3, \u2026 n] \u7406\u89e3\u5e94\u7528\u548c\u516c\u5f0f\u63a8\u5bfc \u51fa\u6808\u6b21\u5e8f\u95ee\u9898 \u4e00\u4e2a\u6808( \u65e0\u7a77\u5927 )\u7684\u8fdb\u6808\u5e8f\u5217\u4e3a1,2,3,..n,\u6709\u591a\u5c11\u4e2a\u4e0d\u540c\u7684\u51fa\u6808\u5e8f\u5217? \u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\u76845\u79cd\u53d8\u578b (2).\u627e\u96f6\u94b1\uff08\u627e\u4e00\u534a\uff09 \u67092n\u4e2a\u4eba\u6392\u6210\u4e00\u884c\u8fdb\u5165\u5267\u573a\u3002\u5165\u573a\u8d395\u5143\u3002\u5176\u4e2d\u53ea\u6709n\u4e2a\u4eba\u6709\u4e00\u5f205\u5143\u949e\u7968\uff0c\u53e6\u5916n\u4eba\u53ea\u670910\u5143\u949e\u7968\uff0c\u5267\u9662\u65e0\u5176\u5b83\u949e\u7968\uff0c\u95ee\u6709\u591a\u5c11\u4e2d\u65b9\u6cd5\u4f7f\u5f97\u53ea\u8981\u670910\u5143\u7684\u4eba\u4e70\u7968\uff0c\u552e\u7968\u5904\u5c31\u67095\u5143\u7684\u949e\u7968\u627e\u96f6\uff1f (3).\u4e09\u89d2\u7f51\u683c \u5f62\u5982\u8fd9\u6837\u7684\u76f4\u89d2\u4e09\u89d2\u5f62\u7f51\u683c\uff0c\u4ece\u5de6\u4e0a\u89d2\u5f00\u59cb\uff0c\u53ea\u80fd\u5411\u53f3\u8d70\u548c\u5411\u4e0b\u8d70\uff0c\u95ee\u603b\u5171\u6709\u591a\u5c11\u79cd\u8d70\u6cd5\uff1f (4).\u62ec\u53f7\u6392\u5217 \u77e9\u9635\u8fde\u4e58\uff1a \uff0c\u5171\u6709\uff08n+1\uff09\u9879\uff0c\u4f9d\u636e \u4e58\u6cd5\u7ed3\u5408\u5f8b","title":"baike [catalan](https://baike.baidu.com/item/catalan/7605685?fr=aladdin)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/#baike#catalan","text":"**\u5361\u5854\u5170\u6570**\u662f \u7ec4\u5408\u6570\u5b66 \u4e2d\u4e00\u4e2a\u5e38\u5728\u5404\u79cd\u8ba1\u6570\u95ee\u9898\u4e2d\u51fa\u73b0\u7684 \u6570\u5217 \u3002\u4ee5 \u6bd4\u5229\u65f6 \u7684\u6570\u5b66\u5bb6\u6b27\u4ec1\u00b7\u67e5\u7406\u00b7\u5361\u5854\u5170\uff081814\u20131894\uff09\u547d\u540d\u3002\u5386\u53f2\u4e0a\uff0c\u6e05\u4ee3\u6570\u5b66\u5bb6 \u660e\u5b89\u56fe (1692\u5e74\uff0d1763\u5e74)\u5728\u5176\u300a \u5272\u571c\u5bc6\u7387\u6377\u6cd5 \u300b\u6700\u65e9\u7528\u5230\u201c\u5361\u5854\u5170\u6570\u201d\uff0c\u8fdc\u8fdc\u65e9\u4e8e\u5361\u5854\u5170\u3002\u6709\u4e2d\u56fd\u5b66\u8005\u5efa\u8bae\u5c06\u6b64\u6570\u547d\u540d\u4e3a\u201c\u660e\u5b89\u56fe\u6570\u201d\u6216\u201c\u660e\u5b89\u56fe-\u5361\u5854\u5170\u6570\u201d\u3002\u5361\u5854\u5170\u6570\u7684\u4e00\u822c\u516c\u5f0f\u4e3a C(2n,n)/(n+1)\u3002","title":"baike catalan"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/#_1","text":"\u4ee4h(0)=1,h(1)=1\uff0c\u5361\u5854\u5170\u6570\u6ee1\u8db3 \u9012\u5f52 \u5f0f\uff1a h(n)= h(0)*h(n-1) + h(1)*h(n-2) + ... + h(n-1)h(0) (\u5176\u4e2dn>=2),\u8fd9\u662fn\u9636 \u9012\u63a8 \u5173\u7cfb; \u8fd8\u53ef\u4ee5\u5316\u7b80\u4e3a1\u9636\u9012\u63a8\u5173\u7cfb: \u5982h(n)=(4n-2)/(n+1)*h(n-1)(n>1) h(0)=1 \u8be5\u9012\u63a8\u5173\u7cfb\u7684\u89e3\u4e3a\uff1a h(n)=****C(2n,n)/(n+1)=P(2n,n)/(n+1)!=(2n)!/(n!*(n+1)!) (n=1,2,3,...) \u5361\u5854\u5170\u6570\u5217\u7684\u524d\u51e0\u9879\u4e3a(sequence A 0 0 0 1 0 8 in OEIS) [\u6ce8\uff1a n = 0, 1, 2, 3, \u2026 n]","title":"\u6027\u8d28"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/#_2","text":"","title":"\u7406\u89e3\u5e94\u7528\u548c\u516c\u5f0f\u63a8\u5bfc"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/#_3","text":"\u4e00\u4e2a\u6808( \u65e0\u7a77\u5927 )\u7684\u8fdb\u6808\u5e8f\u5217\u4e3a1,2,3,..n,\u6709\u591a\u5c11\u4e2a\u4e0d\u540c\u7684\u51fa\u6808\u5e8f\u5217?","title":"\u51fa\u6808\u6b21\u5e8f\u95ee\u9898"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/#5","text":"","title":"\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\u76845\u79cd\u53d8\u578b"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/#2","text":"\u67092n\u4e2a\u4eba\u6392\u6210\u4e00\u884c\u8fdb\u5165\u5267\u573a\u3002\u5165\u573a\u8d395\u5143\u3002\u5176\u4e2d\u53ea\u6709n\u4e2a\u4eba\u6709\u4e00\u5f205\u5143\u949e\u7968\uff0c\u53e6\u5916n\u4eba\u53ea\u670910\u5143\u949e\u7968\uff0c\u5267\u9662\u65e0\u5176\u5b83\u949e\u7968\uff0c\u95ee\u6709\u591a\u5c11\u4e2d\u65b9\u6cd5\u4f7f\u5f97\u53ea\u8981\u670910\u5143\u7684\u4eba\u4e70\u7968\uff0c\u552e\u7968\u5904\u5c31\u67095\u5143\u7684\u949e\u7968\u627e\u96f6\uff1f","title":"(2).\u627e\u96f6\u94b1\uff08\u627e\u4e00\u534a\uff09"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/#3","text":"\u5f62\u5982\u8fd9\u6837\u7684\u76f4\u89d2\u4e09\u89d2\u5f62\u7f51\u683c\uff0c\u4ece\u5de6\u4e0a\u89d2\u5f00\u59cb\uff0c\u53ea\u80fd\u5411\u53f3\u8d70\u548c\u5411\u4e0b\u8d70\uff0c\u95ee\u603b\u5171\u6709\u591a\u5c11\u79cd\u8d70\u6cd5\uff1f","title":"(3).\u4e09\u89d2\u7f51\u683c"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-catalan/#4","text":"\u77e9\u9635\u8fde\u4e58\uff1a \uff0c\u5171\u6709\uff08n+1\uff09\u9879\uff0c\u4f9d\u636e \u4e58\u6cd5\u7ed3\u5408\u5f8b","title":"(4).\u62ec\u53f7\u6392\u5217"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/","text":"baike \u5361\u7279\u5170\u6570 \u5361\u7279\u5170\u6570\u53c8\u79f0\u5361\u5854\u5170\u6570\uff0c\u5361\u7279\u5170\u6570\u662f \u7ec4\u5408\u6570\u5b66 \u4e2d\u4e00\u4e2a\u5e38\u51fa\u73b0\u5728\u5404\u79cd \u8ba1\u6570 \u95ee\u9898\u4e2d\u7684 \u6570\u5217 \u3002\u4ee5 \u6bd4\u5229\u65f6 \u7684\u6570\u5b66\u5bb6\u6b27\u4ec1\u00b7\u67e5\u7406\u00b7\u5361\u5854\u5170 (1814\u20131894)\u7684\u540d\u5b57\u6765 \u547d\u540d \u3002 \u5e94\u7528 \u5b9e\u8d28\u4e0a\u90fd\u662f**\u9012\u63a8\u7b49\u5f0f**\u7684\u5e94\u7528 \u62ec\u53f7\u5316 \u77e9\u9635\u8fde\u4e58\uff1a P=a1\u00d7a2\u00d7a3\u00d7\u2026\u2026\u00d7an\uff0c\u4f9d\u636e\u4e58\u6cd5\u7ed3\u5408\u5f8b\uff0c\u4e0d\u6539\u53d8\u5176\u987a\u5e8f\uff0c\u53ea\u7528\u62ec\u53f7\u8868\u793a\u6210\u5bf9\u7684\u4e58\u79ef\uff0c\u8bd5\u95ee\u6709\u51e0\u79cd\u62ec\u53f7\u5316\u7684\u65b9\u6848\uff1f(h(n)\u79cd) [3] \u51fa\u6808\u6b21\u5e8f \u4e00\u4e2a\u6808(\u65e0\u7a77\u5927)\u7684 \u8fdb\u6808 \u5e8f\u5217\u4e3a1\uff0c2\uff0c3\uff0c\u2026\uff0cn\uff0c\u6709\u591a\u5c11\u4e2a\u4e0d\u540c\u7684 \u51fa\u6808 \u5e8f\u5217? \u5e38\u89c4\u5206\u6790 \u9996\u5148\uff0c\u6211\u4eec\u8bbe f\uff08n\uff09=\u5e8f\u5217\u4e2a\u6570\u4e3an\u7684\u51fa\u6808\u5e8f\u5217\u79cd\u6570 \u3002\uff08\u6211\u4eec\u5047\u5b9a\uff0c\u6700\u540e\u51fa\u6808\u7684\u5143\u7d20\u4e3ak\uff08\u663e\u7136k\u7684\u53d6\u503c\u8303\u56f4\u662f 1-n \uff09\uff0c\u663e\u7136\uff0c k \u53d6\u4e0d\u540c\u503c\u65f6\u7684\u60c5\u51b5\u662f**\u76f8\u4e92\u72ec\u7acb**\u7684\uff0c\u4e5f\u5c31\u662f\u6c42\u51fa\u6bcf\u79cd k \u6700\u540e\u51fa\u6808\u7684\u60c5\u51b5\u6570\u540e\u53ef\u7528**\u52a0\u6cd5\u539f\u5219**\uff0c\u7531\u4e8e k \u6700\u540e\u51fa\u6808\uff0c\u56e0\u6b64\uff0c\u5728 k \u5165\u6808\u4e4b\u524d\uff0c\u6bd4 k \u5c0f\u7684\u503c\u5747\u51fa\u6808\uff08\u56e0\u4e3a\u6bd4 k \u5c0f\u7684\u6570\u80af\u5b9a\u662f\u6bd4 k \u5148\u5165\u6808\u7684\uff09\uff0c\u6b64\u5904\u60c5\u51b5\u6709 f(k-1) \u79cd\uff0c\u800c\u4e4b\u540e\u6bd4 k \u5927\u7684\u503c\u5165\u6808\uff0c\u4e14\u90fd\u5728 k \u4e4b\u524d\u51fa\u6808\uff0c\u56e0\u6b64\u6709 f(n-k) \u79cd\u65b9\u5f0f\uff0c\u7531\u4e8e\u6bd4 k \u5c0f\u548c\u6bd4 k \u5927\u7684\u503c\u5165\u6808\u51fa\u6808\u60c5\u51b5\u662f**\u76f8\u4e92\u72ec\u7acb**\u7684\uff0c\u6b64\u5904\u53ef\u7528**\u4e58\u6cd5\u539f\u5219**\uff0c f(n-k)*f(k-1) \u79cd\uff0c\u6c42\u548c\u4fbf\u662fCatalan\u9012\u5f52\u5f0f\u3002ps.author.\u9676\u767e\u767e\uff09 SUMMARY : k \u7684\u53d6\u503c\u8303\u56f4\u662f 1-n \uff0c\u5373\u5171\u6709 n \u79cd\u53ef\u80fd\u6027\uff1b\u90a3\u6bcf\u79cd\u53ef\u80fd\u6027\u4e2d\uff0c\u6709\u591a\u5c11\u79cd\u7ec4\u5408\u60c5\u51b5\u5462\uff1f\u662f f(n-k)*f(k-1) \u56e0\u4e3a\u9700\u8981\u5c06\u524d k-1 \u4e2a\u5143\u7d20\u7684\u51fa\u6808\u60c5\u51b5\u548c\u540e n-k \u4e2a\u5143\u7d20\u7684\u51fa\u6808\u60c5\u51b5\u7ec4\u5408\u8d77\u6765\u624d\u80fd\u591f\u5f97\u5230\u8fd9\u4e2an\u4e2a\u5143\u7d20\u7684\u6700\u7ec8\u7684\u51fa\u6808\u60c5\u51b5\uff0c f(n-k) \u4e2d\u7684\u6bcf\u4e00\u79cd\u60c5\u51b5\u90fd\u53ef\u4ee5\u548c f(k-1) \u4e2d\u7684\u6bcf\u4e00\u79cd\u60c5\u51b5\u8fdb\u884c\u7ec4\u5408\uff0c\u6bcf\u4e00\u79cd\u7ec4\u5408\u90fd\u662f\u4e00\u79cd\u5408\u7406\u7684\uff0c\u6240\u4ee5\u9700\u8981\u4f7f\u7528**\u4e58\u6cd5**\uff0c\u800c\u4e0d\u662f\u4f7f\u7528**\u52a0\u6cd5**\uff1b\u6240\u4ee5\u6700\u7ec8\u7684\u516c\u5f0f\u5c31\u662f: $ C_{0}=1\\quad {\\text{and}}\\quad C_{n+1}=\\sum {i=0}^{n}C {i}\\,C_{n-i}\\quad {\\text{for }}n\\geq 0, $ \u6b64\u65f6\uff0c\u6211\u4eec\u82e5\u628a k \u89c6\u4e3a\u786e\u5b9a\u4e00\u4e2a\u5e8f\u6570\uff0c\u90a3\u4e48\u6839\u636e \u4e58\u6cd5\u539f\u7406 \uff0c f\uff08n\uff09 \u7684\u95ee\u9898\u5c31\u7b49\u4ef7\u4e8e\u2014\u2014\u5e8f\u5217\u4e2a\u6570\u4e3a k-1 \u7684\u51fa\u6808\u5e8f\u5217\u79cd\u6570\u4e58\u4ee5\u5e8f\u5217\u4e2a\u6570\u4e3a n - k \u7684\u51fa\u6808\u5e8f\u5217\u79cd\u6570\uff0c\u5373\u9009\u62e9 k \u8fd9\u4e2a\u5e8f\u6570\u7684 f\uff08n\uff09=f\uff08k-1\uff09\u00d7f\uff08n-k\uff09 \u3002\u800ck\u53ef\u4ee5\u90091\u5230n\uff0c\u6240\u4ee5\u518d\u6839\u636e \u52a0\u6cd5\u539f\u7406 \uff0c\u5c06 k \u53d6\u4e0d\u540c\u503c\u7684\u5e8f\u5217\u79cd\u6570\u76f8\u52a0\uff0c\u5f97\u5230\u7684\u603b\u5e8f\u5217\u79cd\u6570\u4e3a\uff1a f\uff08n\uff09=f\uff080\uff09f\uff08n-1\uff09+f\uff081\uff09f\uff08n-2\uff09+\u2026\u2026+f\uff08n-1\uff09f\uff080\uff09 \u3002 \u770b\u5230\u6b64\u5904\uff0c\u518d\u770b\u770b\u5361\u7279\u5170\u6570\u7684\u9012\u63a8\u5f0f\uff0c\u7b54\u6848\u4e0d\u8a00\u800c\u55bb\uff0c\u5373\u4e3a f\uff08n\uff09=h\uff08n\uff09= C\uff082n,n\uff09/\uff08n+1\uff09= c\uff082n,n\uff09-c\uff082n,n-1\uff09\uff08n=0\uff0c1\uff0c2\uff0c\u2026\u2026\uff09 \u3002 \u6700\u540e\uff0c\u4ee4 f\uff080\uff09=1\uff0cf\uff081\uff09=1 \u3002 SUMMARY : \u4e0a\u8ff0\u9012\u5f52\u89e3\u91ca\u975e\u5e38\u597d\uff1b \u975e\u5e38\u89c4\u5206\u6790 \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u6570\u6765\u8bf4\uff0c\u5fc5\u987b\u8fdb\u6808\u4e00\u6b21\u3001\u51fa\u6808\u4e00\u6b21\u3002\u6211\u4eec\u628a \u8fdb\u6808 \u8bbe\u4e3a\u72b6\u6001\u20181\u2019\uff0c\u51fa\u6808\u8bbe\u4e3a\u72b6\u6001\u20180\u2019\u3002n\u4e2a\u6570\u7684\u6240\u6709\u72b6\u6001\u5bf9\u5e94n\u4e2a1\u548cn\u4e2a0\u7ec4\u6210\u76842n\u4f4d \u4e8c\u8fdb\u5236\u6570 \u3002\u7531\u4e8e\u7b49\u5f85\u5165\u6808\u7684\u64cd\u4f5c\u6570\u6309\u71671\u2025n\u7684\u987a\u5e8f\u6392\u5217\u3001\u5165\u6808\u7684\u64cd\u4f5c\u6570 b \u5927\u4e8e\u7b49\u4e8e \u51fa\u6808 \u7684\u64cd\u4f5c\u6570 a ( a\u2264b )\uff08 a \u8981\u51fa\u6808\uff0c\u8bf4\u660e a \u5df2\u7ecf\u5165\u8fc7\u6808\u7684\uff09\uff0c\u56e0\u6b64\u8f93\u51fa\u5e8f\u5217\u7684\u603b\u6570\u76ee=\u7531\u5de6\u800c\u53f3\u626b\u63cf\u7531 n \u4e2a1\u548c n \u4e2a0\u7ec4\u6210\u7684 2n \u4f4d\u4e8c\u8fdb\u5236\u6570\uff0c1\uff08\u8868\u793a\u8fdb\u6808\uff09\u7684\u7d2f\u8ba1\u6570\u4e0d\u5c0f\u4e8e0\uff08\u8868\u793a\u51fa\u6808\uff09\uff08\u8868\u793a\u51fa\u6808\uff09\u7684\u7d2f\u8ba1\u6570\u7684\u65b9\u6848\u79cd\u6570\u3002 SUMMARY : \u5bf9\u4e8e\u672c\u95ee\u9898\uff0c\u9650\u5236\u6761\u4ef6\u662f\uff1a 1 \uff08\u8868\u793a\u8fdb\u6808\uff09\u7684\u7d2f\u8ba1\u6570\u4e0d\u5c0f\u4e8e 0 \uff08\u8868\u793a\u51fa\u6808\uff09\uff08\u8868\u793a\u51fa\u6808\uff09\u7684\u7d2f\u8ba1\u6570 SUMMARY : \u8fd9\u662f\u8fd9\u7c7b\u95ee\u9898\u7684\u5e38\u89c4\u601d\u8003\u65b9\u5f0f\uff0c\u5b83\u5b9e\u9645\u662f bijection \uff1b \u5728 2n \u4f4d\u4e8c\u8fdb\u5236\u6570\u4e2d\u586b\u5165n\u4e2a1\u7684\u65b9\u6848\u6570\u4e3a c(2n,n) ,\u4e0d\u586b1\u7684\u5176\u4f59n\u4f4d\u81ea\u52a8\u586b0\u3002\u4ece\u4e2d\u51cf\u53bb\u4e0d\u7b26\u5408\u8981\u6c42\uff08\u7531\u5de6\u800c\u53f3\u626b\u63cf\uff0c0\u7684\u7d2f\u8ba1\u6570\u5927\u4e8e1\u7684\u7d2f\u8ba1\u6570\uff09\u7684\u65b9\u6848\u6570\u5373\u4e3a\u6240\u6c42\u3002 \u4e0d\u7b26\u5408\u8981\u6c42\u7684\u6570\u7684\u7279\u5f81\u662f\u7531\u5de6\u800c\u53f3\u626b\u63cf\u65f6\uff0c\u5fc5\u7136\u5728\u67d0\u4e00\u5947\u6570\u4f4d 2m+1 \u4f4d\u4e0a\u9996\u5148\u51fa\u73b0 m+1 \u4e2a0\u7684\u7d2f\u8ba1\u6570\u548c m \u4e2a1\u7684\u7d2f\u8ba1\u6570\uff0c\u6b64\u540e\u7684 2(n-m)-1 \u4f4d\u4e0a\u6709 n-m \u4e2a 1\u548c n-m-1 \u4e2a0\u3002\u5982\u82e5\u628a\u540e\u9762\u8fd9 2(n-m)-1 \u4f4d\u4e0a\u76840\u548c1\u4e92\u6362\uff0c\u4f7f\u4e4b\u6210\u4e3a n-m \u4e2a0\u548c n-m-1 \u4e2a1\uff0c\u7ed3\u679c\u5f971\u4e2a\u7531 n+1 \u4e2a0\u548c n-1 \u4e2a1\u7ec4\u6210\u7684 2n \u4f4d\u6570\uff0c\u5373\u4e00\u4e2a\u4e0d\u5408\u8981\u6c42\u7684\u6570\u5bf9\u5e94\u4e8e\u4e00\u4e2a\u7531 n+1 \u4e2a0\u548c n-1 \u4e2a1\u7ec4\u6210\u7684\u6392\u5217\u3002 \u53cd\u8fc7\u6765\uff0c\u4efb\u4f55\u4e00\u4e2a\u7531n+1\u4e2a0\u548cn-1\u4e2a1\u7ec4\u6210\u76842n\u4f4d \u4e8c\u8fdb\u5236\u6570 \uff0c\u7531\u4e8e0\u7684\u4e2a\u6570\u591a2\u4e2a\uff0c2n\u4e3a \u5076\u6570 \uff0c\u6545\u5fc5\u5728\u67d0\u4e00\u4e2a\u5947\u6570\u4f4d\u4e0a\u51fa\u73b00\u7684\u7d2f\u8ba1\u6570\u8d85\u8fc71\u7684\u7d2f\u8ba1\u6570\u3002\u540c\u6837\u5728\u540e\u9762\u90e8\u52060\u548c1\u4e92\u6362\uff0c\u4f7f\u4e4b\u6210\u4e3a\u7531n\u4e2a0\u548cn\u4e2a1\u7ec4\u6210\u76842n\u4f4d\u6570\uff0c\u5373n+1\u4e2a0\u548cn-1\u4e2a1\u7ec4\u6210\u76842n\u4f4d\u6570\u5fc5\u5bf9\u5e94\u4e00\u4e2a\u4e0d\u7b26\u5408\u8981\u6c42\u7684\u6570\u3002 \u56e0\u800c\u4e0d\u5408\u8981\u6c42\u76842n\u4f4d\u6570\u4e0en+1\u4e2a0\uff0cn\uff0d1\u4e2a1\u7ec4\u6210\u7684\u6392\u5217\u4e00\u4e00\u5bf9\u5e94\u3002 \u663e\u7136\uff0c\u4e0d\u7b26\u5408\u8981\u6c42\u7684\u65b9\u6848\u6570\u4e3ac(2n,n+1)\u3002\u7531\u6b64\u5f97\u51fa \u8f93\u51fa\u5e8f\u5217\u7684\u603b\u6570\u76ee=c(2n,n)-c(2n,n-1)=c(2n,n)/(n+1)=h(n) \u3002 SUMMARY : \u4e0a\u8ff0\u5bf9\u6b64\u95ee\u9898\u7684\u89e3\u91ca\u662f\u4e0d\u597d\u7684\uff0c\u53c2\u8003\u5982\u4e0b\u6587\u7ae0\uff1a \u6298\u73b0\u6cd5\u2014\u2014\u5361\u7279\u5170\u6570\u8bc1\u660e \u4e70\u7968\u627e\u96f6 \u67092n\u4e2a\u4eba\u6392\u6210\u4e00\u884c\u8fdb\u5165\u5267\u573a\u3002\u5165\u573a\u8d395\u5143\u3002\u5176\u4e2d\u53ea\u6709n\u4e2a\u4eba\u6709\u4e00\u5f205\u5143\u949e\u7968\uff0c\u53e6\u5916n\u4eba\u53ea\u670910\u5143\u949e\u7968\uff0c\u5267\u9662\u65e0\u5176\u5b83\u949e\u7968\uff0c\u95ee\u6709\u591a\u5c11\u79cd\u65b9\u6cd5\u4f7f\u5f97\u53ea\u8981\u670910\u5143\u7684\u4eba\u4e70\u7968\uff0c\u552e\u7968\u5904\u5c31\u67095\u5143\u7684\u949e\u7968\u627e\u96f6\uff1f(\u5c06\u63015\u5143\u8005\u5230\u8fbe\u89c6\u4f5c\u5c065\u5143\u5165\u6808\uff0c\u630110\u5143\u8005\u5230\u8fbe\u89c6\u4f5c\u4f7f\u6808\u4e2d\u67d05\u5143\u51fa\u6808) SUMMARY : \u51f8\u591a\u8fb9\u5f62\u4e09\u89d2\u5212\u5206 \u5728\u4e00\u4e2a \u51f8\u591a\u8fb9\u5f62 \u4e2d\uff0c\u901a\u8fc7\u82e5\u5e72\u6761\u4e92\u4e0d\u76f8\u4ea4\u7684\u5bf9\u89d2\u7ebf\uff0c\u628a\u8fd9\u4e2a\u591a\u8fb9\u5f62\u5212\u5206\u6210\u4e86\u82e5\u5e72\u4e2a\u4e09\u89d2\u5f62\u3002\u4efb\u52a1\u662f\u952e\u76d8\u4e0a\u8f93\u5165\u51f8\u591a\u8fb9\u5f62\u7684\u8fb9\u6570n\uff0c\u6c42\u4e0d\u540c\u5212\u5206\u7684\u65b9\u6848\u6570f\uff08n\uff09\u3002\u6bd4\u5982\u5f53n=6\u65f6\uff0cf\uff086\uff09=14\u3002 \u5206\u6790 \u5982\u679c\u7eaf\u7cb9\u4ecef\uff084\uff09=2\uff0cf\uff085\uff09=5\uff0cf\uff086\uff09=14\uff0c\u2026\u2026\uff0cf\uff08n\uff09=n\u6162\u6162\u53bb\u5f52\u7eb3\uff0c\u6050\u6015\u5f88\u96be\u627e\u5230\u95ee\u9898\u7684\u9012\u63a8\u5f0f\uff0c\u6211\u4eec\u5fc5\u987b\u4ece\u4e00\u822c\u60c5\u51b5\u51fa\u53d1\u53bb\u627e\u89c4\u5f8b\u3002 \u56e0\u4e3a\u51f8\u591a\u8fb9\u5f62\u7684\u4efb\u610f\u4e00\u6761\u8fb9\u5fc5\u5b9a\u5c5e\u4e8e\u67d0\u4e00\u4e2a\u4e09\u89d2\u5f62\uff0c\u6240\u4ee5\u6211\u4eec\u4ee5\u67d0\u4e00\u6761\u8fb9\u4e3a\u57fa\u51c6\uff0c\u4ee5\u8fd9\u6761\u8fb9\u7684\u4e24\u4e2a\u9876\u70b9\u4e3a\u8d77\u70b9P1\u548c\u7ec8\u70b9Pn\uff08P\u5373Point\uff09\uff0c\u5c06\u8be5\u51f8\u591a\u8fb9\u5f62\u7684\u9876\u70b9\u4f9d\u5e8f\u6807\u8bb0\u4e3aP1\u3001P2\u3001\u2026\u2026\u3001Pn\uff0c\u518d\u5728\u8be5\u51f8\u591a\u8fb9\u5f62\u4e2d\u627e\u4efb\u610f\u4e00\u4e2a\u4e0d\u5c5e\u4e8e\u8fd9\u4e24\u4e2a\u70b9\u7684\u9876\u70b9Pk\uff082<=k<=n-1\uff09\uff0c\u6765\u6784\u6210\u4e00\u4e2a\u4e09\u89d2\u5f62\uff0c\u7528\u8fd9\u4e2a\u4e09\u89d2\u5f62\u628a\u4e00\u4e2a\u51f8\u591a\u8fb9\u5f62\u5212\u5206\u6210\u4e24\u4e2a\u51f8\u591a\u8fb9\u5f62\uff0c\u5176\u4e2d\u4e00\u4e2a\u51f8\u591a\u8fb9\u5f62\uff0c\u662f\u7531P1\uff0cP2\uff0c\u2026\u2026\uff0cPk\u6784\u6210\u7684\u51f8k\u8fb9\u5f62\uff08\u9876\u70b9\u6570\u5373\u662f\u8fb9\u6570\uff09\uff0c\u53e6\u4e00\u4e2a\u51f8\u591a\u8fb9\u5f62\uff0c\u662f\u7531Pk\uff0cPk+1\uff0c\u2026\u2026\uff0cPn\u6784\u6210\u7684\u51f8n-k+1\u8fb9\u5f62\u3002 \u6b64\u65f6\uff0c\u6211\u4eec\u82e5\u628aPk\u89c6\u4e3a\u786e\u5b9a\u4e00\u70b9\uff0c\u90a3\u4e48\u6839\u636e \u4e58\u6cd5\u539f\u7406 \uff08\u4e24\u4e2a\u5b50\u95ee\u9898\u662f\u5f7c\u6b64\u72ec\u7acb\u7684\uff09\uff0cf\uff08n\uff09\u7684\u95ee\u9898\u5c31\u7b49\u4ef7\u4e8e\u2014\u2014\u51f8k\u591a\u8fb9\u5f62\u7684\u5212\u5206\u65b9\u6848\u6570\u4e58\u4ee5\u51f8n-k+1\u591a\u8fb9\u5f62\u7684\u5212\u5206\u65b9\u6848\u6570\uff0c\u5373\u9009\u62e9Pk\u8fd9\u4e2a\u9876\u70b9\u7684f\uff08n\uff09=f\uff08k\uff09\u00d7f\uff08n-k+1\uff09\u3002\u800ck\u53ef\u4ee5\u90092\u5230n-1\uff0c\u6240\u4ee5\u518d\u6839\u636e\u52a0\u6cd5\u539f\u7406\uff0c\u5c06k\u53d6\u4e0d\u540c\u503c\u7684\u5212\u5206\u65b9\u6848\u76f8\u52a0\uff0c\u5f97\u5230\u7684\u603b\u65b9\u6848\u6570\u4e3a\uff1af\uff08n\uff09=f\uff082\uff09f\uff08n-2+1\uff09+f\uff083\uff09f\uff08n-3+1\uff09+\u2026\u2026+f\uff08n-1\uff09f\uff082\uff09\u3002\u770b\u5230\u6b64\u5904\uff0c\u518d\u770b\u770b\u5361\u7279\u5170\u6570\u7684\u9012\u63a8\u5f0f\uff0c\u7b54\u6848\u4e0d\u8a00\u800c\u55bb\uff0c\u5373\u4e3af\uff08n\uff09=h\uff08n-2\uff09 \uff08n=2\uff0c3\uff0c4\uff0c\u2026\u2026\uff09\u3002 SUMMARY : \u4e0a\u8ff0\u5206\u6790\u65b9\u6848\u662f\u975e\u5e38\u597d\u7684\uff0c\u5b83\u548c\u300a\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790 \u7b2c\u56db\u7248 \u738b\u6653\u4e1c\u300b\u4e2d\u5206\u6790\u77e9\u9635\u8fde\u4e58\u95ee\u9898\u7684\u601d\u8def\u662f\u4e00\u6a21\u4e00\u6837\u7684\uff0c\u7531\u6b64\u4e5f\u53ef\u4ee5\u770b\u51fa**\u51f8\u591a\u8fb9\u5f62\u4e09\u89d2\u5212\u5206\u95ee\u9898**\u548c**\u77e9\u9635\u8fde\u4e58\u95ee\u9898**\u672c\u8d28\u4e0a\u662f\u7c7b\u4f3c\u7684\uff1b SUMMARY : \u5728\u4f7f\u7528\u5206\u800c\u6cbb\u4e4b\u601d\u60f3\u6765\u89e3\u91ca\u8bf8\u5982**\u51fa\u6808\u6b21\u5e8f\u95ee\u9898**\u3001**\u51f8\u591a\u8fb9\u5f62\u4e09\u89d2\u5212\u5206\u95ee\u9898**\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u8fd9\u4e9b\u5b83\u4eec\u4e4b\u95f4\u7684\u4e00\u4e2a\u5171\u6027\u662f\u5b50\u95ee\u9898\u4e4b\u95f4\u662f**\u5f7c\u6b64\u72ec\u7acb**\u7684\uff0c\u5728**\u51f8\u591a\u8fb9\u5f62\u4e09\u89d2\u5212\u5206\u95ee\u9898**\u4e2d\uff0c\u8fd9\u610f\u5473\u4e2d\u4e24\u4e2a\u5b50\u591a\u8fb9\u5f62\u4e4b\u95f4\u662f\u4e0d\u53ef\u80fd\u5b58\u5728\u8282\u70b9\u4e4b\u95f4\u7684\u76f8\u4e92\u8fde\u63a5\u7684\uff0c\u5728**\u51fa\u6808\u6b21\u5e8f\u95ee\u9898**\u4e2d\u610f\u5473\u7740\u524dk-1\u4e2a\u5143\u7d20\u5df2\u7ecf\u5b8c\u5168\u51fa\u6808\u4e86\uff0c\u6240\u4ee5\u5b83\u538b\u6839\u5c31\u4e0d\u4f1a\u5f71\u54cd\u7b2ck\u4e2a\u5143\u7d20\uff08\u8fd9\u4e2a\u6bd4\u8f83\u597d\u7406\u89e3\uff09\uff1b \u6700\u540e\uff0c\u4ee4f\uff082\uff09=1\uff0cf\uff083\uff09=1\u3002 \u6b64\u5904f\uff082\uff09=1\u548cf\uff083\uff09=1\u7684\u5177\u4f53\u7f18\u7531\u987b\u53c2\u8003\u8be6\u5c3d\u7684\u201c\u5361\u7279\u5170\u6570\u201d\uff0c\u4e5f\u8bb8\u53ef\u4ece \u51f8\u56db\u8fb9\u5f62 f\uff084\uff09=f\uff082\uff09f\uff083\uff09+ f\uff083\uff09f\uff082\uff09=2\u00d7f\uff082\uff09f\uff083\uff09\u5012\u63a8\uff0c\u56db\u8fb9\u5f62\u7684\u5212\u5206\u65b9\u6848\u4e0d\u7528\u89c4\u5f8b\u63a8\u5bfc\u90fd\u53ef\u4ee5\u77e5\u9053\u662f2\uff0c\u90a3\u4e482\u00d7f\uff082\uff09f\uff083\uff09=2\uff0c\u5219f\uff082\uff09f\uff083\uff09=1\uff0c\u53c8f\uff082\uff09\u548cf\uff083\uff09\u82e5\u5b58\u5728\u7684\u8bdd\u4e00\u5b9a\u662f\u6574\u6570\uff0c\u5219f\uff082\uff09=1\uff0cf\uff083\uff09=1\u3002\uff08\u56e0\u4e3a\u6211\u6ca1\u7814\u7a76\u8fc7\u5361\u7279\u5170\u6570\u7684\u7531\u6765\uff0c\u6b64\u5904\u4ec5\u4f5c\u5218\u629f\u7fbd\u7684\u81c6\u6d4b\uff09\u3002 \u7c7b\u4f3c\u95ee\u9898 \u4e00\u4f4d\u5927\u57ce\u5e02\u7684\u5f8b\u5e08\u5728\u5979\u4f4f\u6240\u4ee5\u5317n\u4e2a\u8857\u533a\u548c\u4ee5\u4e1cn\u4e2a\u8857\u533a\u5904\u5de5\u4f5c\u3002\u6bcf\u5929\u5979\u8d702n\u4e2a\u8857\u533a\u53bb\u4e0a\u73ed\u3002\u5982\u679c\u5979\u4ece\u4e0d\u7a7f\u8d8a\uff08\u4f46\u53ef\u4ee5\u78b0\u5230\uff09\u4ece\u5bb6\u5230\u529e\u516c\u5ba4\u7684\u5bf9\u89d2\u7ebf\uff0c\u90a3\u4e48\u6709\u591a\u5c11\u6761\u53ef\u80fd\u7684\u9053\u8def\uff1f \u7ed9\u5b9a\u8282\u70b9\u7ec4\u6210\u4e8c\u53c9\u641c\u7d22\u6811 \u7ed9\u5b9aN\u4e2a \u8282\u70b9 \uff0c\u80fd\u6784\u6210\u591a\u5c11\u79cd\u4e0d\u540c\u7684 \u4e8c\u53c9\u641c\u7d22\u6811 \uff1f \uff08\u80fd\u6784\u6210h\uff08N\uff09\u4e2a\uff09 \uff08\u8fd9\u4e2a\u516c\u5f0f\u7684\u4e0b\u6807\u662f\u4eceh(0)=1\u5f00\u59cb\u7684\uff09 n\u5bf9\u62ec\u53f7\u6b63\u786e\u5339\u914d\u6570\u76ee \u7ed9\u5b9an\u5bf9\u62ec\u53f7\uff0c\u6c42\u62ec\u53f7\u6b63\u786e\u914d\u5bf9\u7684\u5b57\u7b26\u4e32\u6570\uff0c\u4f8b\u5982\uff1a 0\u5bf9\u62ec\u53f7\uff1a[\u7a7a\u5e8f\u5217] 1\u79cd\u53ef\u80fd 1\u5bf9\u62ec\u53f7\uff1a() 1\u79cd\u53ef\u80fd 2\u5bf9\u62ec\u53f7\uff1a()() (()) 2\u79cd\u53ef\u80fd 3\u5bf9\u62ec\u53f7\uff1a((())) ()(()) ()()() (())() (()()) 5\u79cd\u53ef\u80fd \u90a3\u4e48\u95ee\u9898\u6765\u4e86\uff0cn\u5bf9\u62ec\u53f7\u6709\u591a\u5c11\u79cd\u6b63\u786e\u914d\u5bf9\u7684\u53ef\u80fd\u5462\uff1f \u8003\u8651n\u5bf9\u62ec\u53f7\u65f6\u7684\u4efb\u610f\u4e00\u79cd\u914d\u5bf9\u65b9\u6848\uff0c\u6700\u540e\u4e00\u4e2a\u53f3\u62ec\u53f7\u6709\u552f\u4e00\u7684\u4e0e\u4e4b\u5339\u914d\u7684\u5de6\u62ec\u53f7\uff0c\u4e8e\u662f\u6709\u552f\u4e00\u7684\u8868\u793aA(B)\uff0c\u5176\u4e2dA\u548cB\u4e5f\u662f\u5408\u6cd5\u7684\u62ec\u53f7\u5339\u914d\u5e8f\u5217 \u5047\u8bbeS(n)\u4e3an\u5bf9\u62ec\u53f7\u7684\u6b63\u786e\u914d\u5bf9\u6570\u76ee\uff0c\u90a3\u4e48\u6709\u9012\u63a8\u5173\u7cfbS(n)=S(0)S(n-1)+S(1)S(n-2) +...+S(n-1)S(0)\uff0c\u663e\u7136S(n)\u662f\u5361\u7279\u5170\u6570\u3002","title":"baike [\u5361\u7279\u5170\u6570](https://baike.baidu.com/item/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/6125746)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#baike","text":"\u5361\u7279\u5170\u6570\u53c8\u79f0\u5361\u5854\u5170\u6570\uff0c\u5361\u7279\u5170\u6570\u662f \u7ec4\u5408\u6570\u5b66 \u4e2d\u4e00\u4e2a\u5e38\u51fa\u73b0\u5728\u5404\u79cd \u8ba1\u6570 \u95ee\u9898\u4e2d\u7684 \u6570\u5217 \u3002\u4ee5 \u6bd4\u5229\u65f6 \u7684\u6570\u5b66\u5bb6\u6b27\u4ec1\u00b7\u67e5\u7406\u00b7\u5361\u5854\u5170 (1814\u20131894)\u7684\u540d\u5b57\u6765 \u547d\u540d \u3002","title":"baike \u5361\u7279\u5170\u6570"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_1","text":"\u5b9e\u8d28\u4e0a\u90fd\u662f**\u9012\u63a8\u7b49\u5f0f**\u7684\u5e94\u7528","title":"\u5e94\u7528"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_2","text":"\u77e9\u9635\u8fde\u4e58\uff1a P=a1\u00d7a2\u00d7a3\u00d7\u2026\u2026\u00d7an\uff0c\u4f9d\u636e\u4e58\u6cd5\u7ed3\u5408\u5f8b\uff0c\u4e0d\u6539\u53d8\u5176\u987a\u5e8f\uff0c\u53ea\u7528\u62ec\u53f7\u8868\u793a\u6210\u5bf9\u7684\u4e58\u79ef\uff0c\u8bd5\u95ee\u6709\u51e0\u79cd\u62ec\u53f7\u5316\u7684\u65b9\u6848\uff1f(h(n)\u79cd) [3]","title":"\u62ec\u53f7\u5316"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_3","text":"\u4e00\u4e2a\u6808(\u65e0\u7a77\u5927)\u7684 \u8fdb\u6808 \u5e8f\u5217\u4e3a1\uff0c2\uff0c3\uff0c\u2026\uff0cn\uff0c\u6709\u591a\u5c11\u4e2a\u4e0d\u540c\u7684 \u51fa\u6808 \u5e8f\u5217?","title":"\u51fa\u6808\u6b21\u5e8f"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_4","text":"\u9996\u5148\uff0c\u6211\u4eec\u8bbe f\uff08n\uff09=\u5e8f\u5217\u4e2a\u6570\u4e3an\u7684\u51fa\u6808\u5e8f\u5217\u79cd\u6570 \u3002\uff08\u6211\u4eec\u5047\u5b9a\uff0c\u6700\u540e\u51fa\u6808\u7684\u5143\u7d20\u4e3ak\uff08\u663e\u7136k\u7684\u53d6\u503c\u8303\u56f4\u662f 1-n \uff09\uff0c\u663e\u7136\uff0c k \u53d6\u4e0d\u540c\u503c\u65f6\u7684\u60c5\u51b5\u662f**\u76f8\u4e92\u72ec\u7acb**\u7684\uff0c\u4e5f\u5c31\u662f\u6c42\u51fa\u6bcf\u79cd k \u6700\u540e\u51fa\u6808\u7684\u60c5\u51b5\u6570\u540e\u53ef\u7528**\u52a0\u6cd5\u539f\u5219**\uff0c\u7531\u4e8e k \u6700\u540e\u51fa\u6808\uff0c\u56e0\u6b64\uff0c\u5728 k \u5165\u6808\u4e4b\u524d\uff0c\u6bd4 k \u5c0f\u7684\u503c\u5747\u51fa\u6808\uff08\u56e0\u4e3a\u6bd4 k \u5c0f\u7684\u6570\u80af\u5b9a\u662f\u6bd4 k \u5148\u5165\u6808\u7684\uff09\uff0c\u6b64\u5904\u60c5\u51b5\u6709 f(k-1) \u79cd\uff0c\u800c\u4e4b\u540e\u6bd4 k \u5927\u7684\u503c\u5165\u6808\uff0c\u4e14\u90fd\u5728 k \u4e4b\u524d\u51fa\u6808\uff0c\u56e0\u6b64\u6709 f(n-k) \u79cd\u65b9\u5f0f\uff0c\u7531\u4e8e\u6bd4 k \u5c0f\u548c\u6bd4 k \u5927\u7684\u503c\u5165\u6808\u51fa\u6808\u60c5\u51b5\u662f**\u76f8\u4e92\u72ec\u7acb**\u7684\uff0c\u6b64\u5904\u53ef\u7528**\u4e58\u6cd5\u539f\u5219**\uff0c f(n-k)*f(k-1) \u79cd\uff0c\u6c42\u548c\u4fbf\u662fCatalan\u9012\u5f52\u5f0f\u3002ps.author.\u9676\u767e\u767e\uff09 SUMMARY : k \u7684\u53d6\u503c\u8303\u56f4\u662f 1-n \uff0c\u5373\u5171\u6709 n \u79cd\u53ef\u80fd\u6027\uff1b\u90a3\u6bcf\u79cd\u53ef\u80fd\u6027\u4e2d\uff0c\u6709\u591a\u5c11\u79cd\u7ec4\u5408\u60c5\u51b5\u5462\uff1f\u662f f(n-k)*f(k-1) \u56e0\u4e3a\u9700\u8981\u5c06\u524d k-1 \u4e2a\u5143\u7d20\u7684\u51fa\u6808\u60c5\u51b5\u548c\u540e n-k \u4e2a\u5143\u7d20\u7684\u51fa\u6808\u60c5\u51b5\u7ec4\u5408\u8d77\u6765\u624d\u80fd\u591f\u5f97\u5230\u8fd9\u4e2an\u4e2a\u5143\u7d20\u7684\u6700\u7ec8\u7684\u51fa\u6808\u60c5\u51b5\uff0c f(n-k) \u4e2d\u7684\u6bcf\u4e00\u79cd\u60c5\u51b5\u90fd\u53ef\u4ee5\u548c f(k-1) \u4e2d\u7684\u6bcf\u4e00\u79cd\u60c5\u51b5\u8fdb\u884c\u7ec4\u5408\uff0c\u6bcf\u4e00\u79cd\u7ec4\u5408\u90fd\u662f\u4e00\u79cd\u5408\u7406\u7684\uff0c\u6240\u4ee5\u9700\u8981\u4f7f\u7528**\u4e58\u6cd5**\uff0c\u800c\u4e0d\u662f\u4f7f\u7528**\u52a0\u6cd5**\uff1b\u6240\u4ee5\u6700\u7ec8\u7684\u516c\u5f0f\u5c31\u662f: $ C_{0}=1\\quad {\\text{and}}\\quad C_{n+1}=\\sum {i=0}^{n}C {i}\\,C_{n-i}\\quad {\\text{for }}n\\geq 0, $ \u6b64\u65f6\uff0c\u6211\u4eec\u82e5\u628a k \u89c6\u4e3a\u786e\u5b9a\u4e00\u4e2a\u5e8f\u6570\uff0c\u90a3\u4e48\u6839\u636e \u4e58\u6cd5\u539f\u7406 \uff0c f\uff08n\uff09 \u7684\u95ee\u9898\u5c31\u7b49\u4ef7\u4e8e\u2014\u2014\u5e8f\u5217\u4e2a\u6570\u4e3a k-1 \u7684\u51fa\u6808\u5e8f\u5217\u79cd\u6570\u4e58\u4ee5\u5e8f\u5217\u4e2a\u6570\u4e3a n - k \u7684\u51fa\u6808\u5e8f\u5217\u79cd\u6570\uff0c\u5373\u9009\u62e9 k \u8fd9\u4e2a\u5e8f\u6570\u7684 f\uff08n\uff09=f\uff08k-1\uff09\u00d7f\uff08n-k\uff09 \u3002\u800ck\u53ef\u4ee5\u90091\u5230n\uff0c\u6240\u4ee5\u518d\u6839\u636e \u52a0\u6cd5\u539f\u7406 \uff0c\u5c06 k \u53d6\u4e0d\u540c\u503c\u7684\u5e8f\u5217\u79cd\u6570\u76f8\u52a0\uff0c\u5f97\u5230\u7684\u603b\u5e8f\u5217\u79cd\u6570\u4e3a\uff1a f\uff08n\uff09=f\uff080\uff09f\uff08n-1\uff09+f\uff081\uff09f\uff08n-2\uff09+\u2026\u2026+f\uff08n-1\uff09f\uff080\uff09 \u3002 \u770b\u5230\u6b64\u5904\uff0c\u518d\u770b\u770b\u5361\u7279\u5170\u6570\u7684\u9012\u63a8\u5f0f\uff0c\u7b54\u6848\u4e0d\u8a00\u800c\u55bb\uff0c\u5373\u4e3a f\uff08n\uff09=h\uff08n\uff09= C\uff082n,n\uff09/\uff08n+1\uff09= c\uff082n,n\uff09-c\uff082n,n-1\uff09\uff08n=0\uff0c1\uff0c2\uff0c\u2026\u2026\uff09 \u3002 \u6700\u540e\uff0c\u4ee4 f\uff080\uff09=1\uff0cf\uff081\uff09=1 \u3002 SUMMARY : \u4e0a\u8ff0\u9012\u5f52\u89e3\u91ca\u975e\u5e38\u597d\uff1b","title":"\u5e38\u89c4\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_5","text":"\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u6570\u6765\u8bf4\uff0c\u5fc5\u987b\u8fdb\u6808\u4e00\u6b21\u3001\u51fa\u6808\u4e00\u6b21\u3002\u6211\u4eec\u628a \u8fdb\u6808 \u8bbe\u4e3a\u72b6\u6001\u20181\u2019\uff0c\u51fa\u6808\u8bbe\u4e3a\u72b6\u6001\u20180\u2019\u3002n\u4e2a\u6570\u7684\u6240\u6709\u72b6\u6001\u5bf9\u5e94n\u4e2a1\u548cn\u4e2a0\u7ec4\u6210\u76842n\u4f4d \u4e8c\u8fdb\u5236\u6570 \u3002\u7531\u4e8e\u7b49\u5f85\u5165\u6808\u7684\u64cd\u4f5c\u6570\u6309\u71671\u2025n\u7684\u987a\u5e8f\u6392\u5217\u3001\u5165\u6808\u7684\u64cd\u4f5c\u6570 b \u5927\u4e8e\u7b49\u4e8e \u51fa\u6808 \u7684\u64cd\u4f5c\u6570 a ( a\u2264b )\uff08 a \u8981\u51fa\u6808\uff0c\u8bf4\u660e a \u5df2\u7ecf\u5165\u8fc7\u6808\u7684\uff09\uff0c\u56e0\u6b64\u8f93\u51fa\u5e8f\u5217\u7684\u603b\u6570\u76ee=\u7531\u5de6\u800c\u53f3\u626b\u63cf\u7531 n \u4e2a1\u548c n \u4e2a0\u7ec4\u6210\u7684 2n \u4f4d\u4e8c\u8fdb\u5236\u6570\uff0c1\uff08\u8868\u793a\u8fdb\u6808\uff09\u7684\u7d2f\u8ba1\u6570\u4e0d\u5c0f\u4e8e0\uff08\u8868\u793a\u51fa\u6808\uff09\uff08\u8868\u793a\u51fa\u6808\uff09\u7684\u7d2f\u8ba1\u6570\u7684\u65b9\u6848\u79cd\u6570\u3002 SUMMARY : \u5bf9\u4e8e\u672c\u95ee\u9898\uff0c\u9650\u5236\u6761\u4ef6\u662f\uff1a 1 \uff08\u8868\u793a\u8fdb\u6808\uff09\u7684\u7d2f\u8ba1\u6570\u4e0d\u5c0f\u4e8e 0 \uff08\u8868\u793a\u51fa\u6808\uff09\uff08\u8868\u793a\u51fa\u6808\uff09\u7684\u7d2f\u8ba1\u6570 SUMMARY : \u8fd9\u662f\u8fd9\u7c7b\u95ee\u9898\u7684\u5e38\u89c4\u601d\u8003\u65b9\u5f0f\uff0c\u5b83\u5b9e\u9645\u662f bijection \uff1b \u5728 2n \u4f4d\u4e8c\u8fdb\u5236\u6570\u4e2d\u586b\u5165n\u4e2a1\u7684\u65b9\u6848\u6570\u4e3a c(2n,n) ,\u4e0d\u586b1\u7684\u5176\u4f59n\u4f4d\u81ea\u52a8\u586b0\u3002\u4ece\u4e2d\u51cf\u53bb\u4e0d\u7b26\u5408\u8981\u6c42\uff08\u7531\u5de6\u800c\u53f3\u626b\u63cf\uff0c0\u7684\u7d2f\u8ba1\u6570\u5927\u4e8e1\u7684\u7d2f\u8ba1\u6570\uff09\u7684\u65b9\u6848\u6570\u5373\u4e3a\u6240\u6c42\u3002 \u4e0d\u7b26\u5408\u8981\u6c42\u7684\u6570\u7684\u7279\u5f81\u662f\u7531\u5de6\u800c\u53f3\u626b\u63cf\u65f6\uff0c\u5fc5\u7136\u5728\u67d0\u4e00\u5947\u6570\u4f4d 2m+1 \u4f4d\u4e0a\u9996\u5148\u51fa\u73b0 m+1 \u4e2a0\u7684\u7d2f\u8ba1\u6570\u548c m \u4e2a1\u7684\u7d2f\u8ba1\u6570\uff0c\u6b64\u540e\u7684 2(n-m)-1 \u4f4d\u4e0a\u6709 n-m \u4e2a 1\u548c n-m-1 \u4e2a0\u3002\u5982\u82e5\u628a\u540e\u9762\u8fd9 2(n-m)-1 \u4f4d\u4e0a\u76840\u548c1\u4e92\u6362\uff0c\u4f7f\u4e4b\u6210\u4e3a n-m \u4e2a0\u548c n-m-1 \u4e2a1\uff0c\u7ed3\u679c\u5f971\u4e2a\u7531 n+1 \u4e2a0\u548c n-1 \u4e2a1\u7ec4\u6210\u7684 2n \u4f4d\u6570\uff0c\u5373\u4e00\u4e2a\u4e0d\u5408\u8981\u6c42\u7684\u6570\u5bf9\u5e94\u4e8e\u4e00\u4e2a\u7531 n+1 \u4e2a0\u548c n-1 \u4e2a1\u7ec4\u6210\u7684\u6392\u5217\u3002 \u53cd\u8fc7\u6765\uff0c\u4efb\u4f55\u4e00\u4e2a\u7531n+1\u4e2a0\u548cn-1\u4e2a1\u7ec4\u6210\u76842n\u4f4d \u4e8c\u8fdb\u5236\u6570 \uff0c\u7531\u4e8e0\u7684\u4e2a\u6570\u591a2\u4e2a\uff0c2n\u4e3a \u5076\u6570 \uff0c\u6545\u5fc5\u5728\u67d0\u4e00\u4e2a\u5947\u6570\u4f4d\u4e0a\u51fa\u73b00\u7684\u7d2f\u8ba1\u6570\u8d85\u8fc71\u7684\u7d2f\u8ba1\u6570\u3002\u540c\u6837\u5728\u540e\u9762\u90e8\u52060\u548c1\u4e92\u6362\uff0c\u4f7f\u4e4b\u6210\u4e3a\u7531n\u4e2a0\u548cn\u4e2a1\u7ec4\u6210\u76842n\u4f4d\u6570\uff0c\u5373n+1\u4e2a0\u548cn-1\u4e2a1\u7ec4\u6210\u76842n\u4f4d\u6570\u5fc5\u5bf9\u5e94\u4e00\u4e2a\u4e0d\u7b26\u5408\u8981\u6c42\u7684\u6570\u3002 \u56e0\u800c\u4e0d\u5408\u8981\u6c42\u76842n\u4f4d\u6570\u4e0en+1\u4e2a0\uff0cn\uff0d1\u4e2a1\u7ec4\u6210\u7684\u6392\u5217\u4e00\u4e00\u5bf9\u5e94\u3002 \u663e\u7136\uff0c\u4e0d\u7b26\u5408\u8981\u6c42\u7684\u65b9\u6848\u6570\u4e3ac(2n,n+1)\u3002\u7531\u6b64\u5f97\u51fa \u8f93\u51fa\u5e8f\u5217\u7684\u603b\u6570\u76ee=c(2n,n)-c(2n,n-1)=c(2n,n)/(n+1)=h(n) \u3002 SUMMARY : \u4e0a\u8ff0\u5bf9\u6b64\u95ee\u9898\u7684\u89e3\u91ca\u662f\u4e0d\u597d\u7684\uff0c\u53c2\u8003\u5982\u4e0b\u6587\u7ae0\uff1a \u6298\u73b0\u6cd5\u2014\u2014\u5361\u7279\u5170\u6570\u8bc1\u660e","title":"\u975e\u5e38\u89c4\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_6","text":"","title":""},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_7","text":"\u67092n\u4e2a\u4eba\u6392\u6210\u4e00\u884c\u8fdb\u5165\u5267\u573a\u3002\u5165\u573a\u8d395\u5143\u3002\u5176\u4e2d\u53ea\u6709n\u4e2a\u4eba\u6709\u4e00\u5f205\u5143\u949e\u7968\uff0c\u53e6\u5916n\u4eba\u53ea\u670910\u5143\u949e\u7968\uff0c\u5267\u9662\u65e0\u5176\u5b83\u949e\u7968\uff0c\u95ee\u6709\u591a\u5c11\u79cd\u65b9\u6cd5\u4f7f\u5f97\u53ea\u8981\u670910\u5143\u7684\u4eba\u4e70\u7968\uff0c\u552e\u7968\u5904\u5c31\u67095\u5143\u7684\u949e\u7968\u627e\u96f6\uff1f(\u5c06\u63015\u5143\u8005\u5230\u8fbe\u89c6\u4f5c\u5c065\u5143\u5165\u6808\uff0c\u630110\u5143\u8005\u5230\u8fbe\u89c6\u4f5c\u4f7f\u6808\u4e2d\u67d05\u5143\u51fa\u6808) SUMMARY :","title":"\u4e70\u7968\u627e\u96f6"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_8","text":"\u5728\u4e00\u4e2a \u51f8\u591a\u8fb9\u5f62 \u4e2d\uff0c\u901a\u8fc7\u82e5\u5e72\u6761\u4e92\u4e0d\u76f8\u4ea4\u7684\u5bf9\u89d2\u7ebf\uff0c\u628a\u8fd9\u4e2a\u591a\u8fb9\u5f62\u5212\u5206\u6210\u4e86\u82e5\u5e72\u4e2a\u4e09\u89d2\u5f62\u3002\u4efb\u52a1\u662f\u952e\u76d8\u4e0a\u8f93\u5165\u51f8\u591a\u8fb9\u5f62\u7684\u8fb9\u6570n\uff0c\u6c42\u4e0d\u540c\u5212\u5206\u7684\u65b9\u6848\u6570f\uff08n\uff09\u3002\u6bd4\u5982\u5f53n=6\u65f6\uff0cf\uff086\uff09=14\u3002","title":"\u51f8\u591a\u8fb9\u5f62\u4e09\u89d2\u5212\u5206"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_9","text":"\u5982\u679c\u7eaf\u7cb9\u4ecef\uff084\uff09=2\uff0cf\uff085\uff09=5\uff0cf\uff086\uff09=14\uff0c\u2026\u2026\uff0cf\uff08n\uff09=n\u6162\u6162\u53bb\u5f52\u7eb3\uff0c\u6050\u6015\u5f88\u96be\u627e\u5230\u95ee\u9898\u7684\u9012\u63a8\u5f0f\uff0c\u6211\u4eec\u5fc5\u987b\u4ece\u4e00\u822c\u60c5\u51b5\u51fa\u53d1\u53bb\u627e\u89c4\u5f8b\u3002 \u56e0\u4e3a\u51f8\u591a\u8fb9\u5f62\u7684\u4efb\u610f\u4e00\u6761\u8fb9\u5fc5\u5b9a\u5c5e\u4e8e\u67d0\u4e00\u4e2a\u4e09\u89d2\u5f62\uff0c\u6240\u4ee5\u6211\u4eec\u4ee5\u67d0\u4e00\u6761\u8fb9\u4e3a\u57fa\u51c6\uff0c\u4ee5\u8fd9\u6761\u8fb9\u7684\u4e24\u4e2a\u9876\u70b9\u4e3a\u8d77\u70b9P1\u548c\u7ec8\u70b9Pn\uff08P\u5373Point\uff09\uff0c\u5c06\u8be5\u51f8\u591a\u8fb9\u5f62\u7684\u9876\u70b9\u4f9d\u5e8f\u6807\u8bb0\u4e3aP1\u3001P2\u3001\u2026\u2026\u3001Pn\uff0c\u518d\u5728\u8be5\u51f8\u591a\u8fb9\u5f62\u4e2d\u627e\u4efb\u610f\u4e00\u4e2a\u4e0d\u5c5e\u4e8e\u8fd9\u4e24\u4e2a\u70b9\u7684\u9876\u70b9Pk\uff082<=k<=n-1\uff09\uff0c\u6765\u6784\u6210\u4e00\u4e2a\u4e09\u89d2\u5f62\uff0c\u7528\u8fd9\u4e2a\u4e09\u89d2\u5f62\u628a\u4e00\u4e2a\u51f8\u591a\u8fb9\u5f62\u5212\u5206\u6210\u4e24\u4e2a\u51f8\u591a\u8fb9\u5f62\uff0c\u5176\u4e2d\u4e00\u4e2a\u51f8\u591a\u8fb9\u5f62\uff0c\u662f\u7531P1\uff0cP2\uff0c\u2026\u2026\uff0cPk\u6784\u6210\u7684\u51f8k\u8fb9\u5f62\uff08\u9876\u70b9\u6570\u5373\u662f\u8fb9\u6570\uff09\uff0c\u53e6\u4e00\u4e2a\u51f8\u591a\u8fb9\u5f62\uff0c\u662f\u7531Pk\uff0cPk+1\uff0c\u2026\u2026\uff0cPn\u6784\u6210\u7684\u51f8n-k+1\u8fb9\u5f62\u3002 \u6b64\u65f6\uff0c\u6211\u4eec\u82e5\u628aPk\u89c6\u4e3a\u786e\u5b9a\u4e00\u70b9\uff0c\u90a3\u4e48\u6839\u636e \u4e58\u6cd5\u539f\u7406 \uff08\u4e24\u4e2a\u5b50\u95ee\u9898\u662f\u5f7c\u6b64\u72ec\u7acb\u7684\uff09\uff0cf\uff08n\uff09\u7684\u95ee\u9898\u5c31\u7b49\u4ef7\u4e8e\u2014\u2014\u51f8k\u591a\u8fb9\u5f62\u7684\u5212\u5206\u65b9\u6848\u6570\u4e58\u4ee5\u51f8n-k+1\u591a\u8fb9\u5f62\u7684\u5212\u5206\u65b9\u6848\u6570\uff0c\u5373\u9009\u62e9Pk\u8fd9\u4e2a\u9876\u70b9\u7684f\uff08n\uff09=f\uff08k\uff09\u00d7f\uff08n-k+1\uff09\u3002\u800ck\u53ef\u4ee5\u90092\u5230n-1\uff0c\u6240\u4ee5\u518d\u6839\u636e\u52a0\u6cd5\u539f\u7406\uff0c\u5c06k\u53d6\u4e0d\u540c\u503c\u7684\u5212\u5206\u65b9\u6848\u76f8\u52a0\uff0c\u5f97\u5230\u7684\u603b\u65b9\u6848\u6570\u4e3a\uff1af\uff08n\uff09=f\uff082\uff09f\uff08n-2+1\uff09+f\uff083\uff09f\uff08n-3+1\uff09+\u2026\u2026+f\uff08n-1\uff09f\uff082\uff09\u3002\u770b\u5230\u6b64\u5904\uff0c\u518d\u770b\u770b\u5361\u7279\u5170\u6570\u7684\u9012\u63a8\u5f0f\uff0c\u7b54\u6848\u4e0d\u8a00\u800c\u55bb\uff0c\u5373\u4e3af\uff08n\uff09=h\uff08n-2\uff09 \uff08n=2\uff0c3\uff0c4\uff0c\u2026\u2026\uff09\u3002 SUMMARY : \u4e0a\u8ff0\u5206\u6790\u65b9\u6848\u662f\u975e\u5e38\u597d\u7684\uff0c\u5b83\u548c\u300a\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790 \u7b2c\u56db\u7248 \u738b\u6653\u4e1c\u300b\u4e2d\u5206\u6790\u77e9\u9635\u8fde\u4e58\u95ee\u9898\u7684\u601d\u8def\u662f\u4e00\u6a21\u4e00\u6837\u7684\uff0c\u7531\u6b64\u4e5f\u53ef\u4ee5\u770b\u51fa**\u51f8\u591a\u8fb9\u5f62\u4e09\u89d2\u5212\u5206\u95ee\u9898**\u548c**\u77e9\u9635\u8fde\u4e58\u95ee\u9898**\u672c\u8d28\u4e0a\u662f\u7c7b\u4f3c\u7684\uff1b SUMMARY : \u5728\u4f7f\u7528\u5206\u800c\u6cbb\u4e4b\u601d\u60f3\u6765\u89e3\u91ca\u8bf8\u5982**\u51fa\u6808\u6b21\u5e8f\u95ee\u9898**\u3001**\u51f8\u591a\u8fb9\u5f62\u4e09\u89d2\u5212\u5206\u95ee\u9898**\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u8fd9\u4e9b\u5b83\u4eec\u4e4b\u95f4\u7684\u4e00\u4e2a\u5171\u6027\u662f\u5b50\u95ee\u9898\u4e4b\u95f4\u662f**\u5f7c\u6b64\u72ec\u7acb**\u7684\uff0c\u5728**\u51f8\u591a\u8fb9\u5f62\u4e09\u89d2\u5212\u5206\u95ee\u9898**\u4e2d\uff0c\u8fd9\u610f\u5473\u4e2d\u4e24\u4e2a\u5b50\u591a\u8fb9\u5f62\u4e4b\u95f4\u662f\u4e0d\u53ef\u80fd\u5b58\u5728\u8282\u70b9\u4e4b\u95f4\u7684\u76f8\u4e92\u8fde\u63a5\u7684\uff0c\u5728**\u51fa\u6808\u6b21\u5e8f\u95ee\u9898**\u4e2d\u610f\u5473\u7740\u524dk-1\u4e2a\u5143\u7d20\u5df2\u7ecf\u5b8c\u5168\u51fa\u6808\u4e86\uff0c\u6240\u4ee5\u5b83\u538b\u6839\u5c31\u4e0d\u4f1a\u5f71\u54cd\u7b2ck\u4e2a\u5143\u7d20\uff08\u8fd9\u4e2a\u6bd4\u8f83\u597d\u7406\u89e3\uff09\uff1b \u6700\u540e\uff0c\u4ee4f\uff082\uff09=1\uff0cf\uff083\uff09=1\u3002 \u6b64\u5904f\uff082\uff09=1\u548cf\uff083\uff09=1\u7684\u5177\u4f53\u7f18\u7531\u987b\u53c2\u8003\u8be6\u5c3d\u7684\u201c\u5361\u7279\u5170\u6570\u201d\uff0c\u4e5f\u8bb8\u53ef\u4ece \u51f8\u56db\u8fb9\u5f62 f\uff084\uff09=f\uff082\uff09f\uff083\uff09+ f\uff083\uff09f\uff082\uff09=2\u00d7f\uff082\uff09f\uff083\uff09\u5012\u63a8\uff0c\u56db\u8fb9\u5f62\u7684\u5212\u5206\u65b9\u6848\u4e0d\u7528\u89c4\u5f8b\u63a8\u5bfc\u90fd\u53ef\u4ee5\u77e5\u9053\u662f2\uff0c\u90a3\u4e482\u00d7f\uff082\uff09f\uff083\uff09=2\uff0c\u5219f\uff082\uff09f\uff083\uff09=1\uff0c\u53c8f\uff082\uff09\u548cf\uff083\uff09\u82e5\u5b58\u5728\u7684\u8bdd\u4e00\u5b9a\u662f\u6574\u6570\uff0c\u5219f\uff082\uff09=1\uff0cf\uff083\uff09=1\u3002\uff08\u56e0\u4e3a\u6211\u6ca1\u7814\u7a76\u8fc7\u5361\u7279\u5170\u6570\u7684\u7531\u6765\uff0c\u6b64\u5904\u4ec5\u4f5c\u5218\u629f\u7fbd\u7684\u81c6\u6d4b\uff09\u3002","title":"\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_10","text":"\u4e00\u4f4d\u5927\u57ce\u5e02\u7684\u5f8b\u5e08\u5728\u5979\u4f4f\u6240\u4ee5\u5317n\u4e2a\u8857\u533a\u548c\u4ee5\u4e1cn\u4e2a\u8857\u533a\u5904\u5de5\u4f5c\u3002\u6bcf\u5929\u5979\u8d702n\u4e2a\u8857\u533a\u53bb\u4e0a\u73ed\u3002\u5982\u679c\u5979\u4ece\u4e0d\u7a7f\u8d8a\uff08\u4f46\u53ef\u4ee5\u78b0\u5230\uff09\u4ece\u5bb6\u5230\u529e\u516c\u5ba4\u7684\u5bf9\u89d2\u7ebf\uff0c\u90a3\u4e48\u6709\u591a\u5c11\u6761\u53ef\u80fd\u7684\u9053\u8def\uff1f","title":"\u7c7b\u4f3c\u95ee\u9898"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#_11","text":"\u7ed9\u5b9aN\u4e2a \u8282\u70b9 \uff0c\u80fd\u6784\u6210\u591a\u5c11\u79cd\u4e0d\u540c\u7684 \u4e8c\u53c9\u641c\u7d22\u6811 \uff1f \uff08\u80fd\u6784\u6210h\uff08N\uff09\u4e2a\uff09 \uff08\u8fd9\u4e2a\u516c\u5f0f\u7684\u4e0b\u6807\u662f\u4eceh(0)=1\u5f00\u59cb\u7684\uff09","title":"\u7ed9\u5b9a\u8282\u70b9\u7ec4\u6210\u4e8c\u53c9\u641c\u7d22\u6811"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/baike-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/#n","text":"\u7ed9\u5b9an\u5bf9\u62ec\u53f7\uff0c\u6c42\u62ec\u53f7\u6b63\u786e\u914d\u5bf9\u7684\u5b57\u7b26\u4e32\u6570\uff0c\u4f8b\u5982\uff1a 0\u5bf9\u62ec\u53f7\uff1a[\u7a7a\u5e8f\u5217] 1\u79cd\u53ef\u80fd 1\u5bf9\u62ec\u53f7\uff1a() 1\u79cd\u53ef\u80fd 2\u5bf9\u62ec\u53f7\uff1a()() (()) 2\u79cd\u53ef\u80fd 3\u5bf9\u62ec\u53f7\uff1a((())) ()(()) ()()() (())() (()()) 5\u79cd\u53ef\u80fd \u90a3\u4e48\u95ee\u9898\u6765\u4e86\uff0cn\u5bf9\u62ec\u53f7\u6709\u591a\u5c11\u79cd\u6b63\u786e\u914d\u5bf9\u7684\u53ef\u80fd\u5462\uff1f \u8003\u8651n\u5bf9\u62ec\u53f7\u65f6\u7684\u4efb\u610f\u4e00\u79cd\u914d\u5bf9\u65b9\u6848\uff0c\u6700\u540e\u4e00\u4e2a\u53f3\u62ec\u53f7\u6709\u552f\u4e00\u7684\u4e0e\u4e4b\u5339\u914d\u7684\u5de6\u62ec\u53f7\uff0c\u4e8e\u662f\u6709\u552f\u4e00\u7684\u8868\u793aA(B)\uff0c\u5176\u4e2dA\u548cB\u4e5f\u662f\u5408\u6cd5\u7684\u62ec\u53f7\u5339\u914d\u5e8f\u5217 \u5047\u8bbeS(n)\u4e3an\u5bf9\u62ec\u53f7\u7684\u6b63\u786e\u914d\u5bf9\u6570\u76ee\uff0c\u90a3\u4e48\u6709\u9012\u63a8\u5173\u7cfbS(n)=S(0)S(n-1)+S(1)S(n-2) +...+S(n-1)S(0)\uff0c\u663e\u7136S(n)\u662f\u5361\u7279\u5170\u6570\u3002","title":"n\u5bf9\u62ec\u53f7\u6b63\u786e\u5339\u914d\u6570\u76ee"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/","text":"\u5361\u7279\u5170\u6570\u7684\u8bc1\u660e \u6298\u73b0\u6cd5\u2014\u2014\u5361\u7279\u5170\u6570\u8bc1\u660e\uff1a http://blog.sina.com.cn/s/blog_6917f47301010cno.html The On-Line Encyclopedia of Integer Sequences\u00ae (OEIS\u00ae)\uff1a http://oeis.org/ Catalan Numbers https://brilliant.org/wiki/catalan-numbers/ \u5e94\u7528 \u51fa\u6808\u987a\u5e8f \u4e0e \u5361\u7279\u5170\u6570(Catalan)\u7684\u5173\u7cfb\uff1a https://www.cnblogs.com/hapjin/p/5758083.html https://blog.csdn.net/qq1169091731/article/details/51284435 https://en.wikipedia.org/wiki/Stack-sortable_permutation https://www.geeksforgeeks.org/check-if-the-given-push-and-pop-sequences-of-stack-is-valid-or-not/ \u5361\u7279\u5170\u6570\uff08Catalan number\uff09\uff1a https://zhuanlan.zhihu.com/p/31317307 https://zhuanlan.zhihu.com/p/31526354 https://zhuanlan.zhihu.com/p/31585260 \u4ece\u300a\u7f16\u7a0b\u4e4b\u7f8e\u300b\u4e70\u7968\u627e\u96f6\u95ee\u9898\u8bf4\u8d77\uff0c\u5a13\u5a13\u9053\u6765\u5361\u7279\u5170\u6570\u2014\u2014\u517c\u722c\u5751\u6307\u5357 https://www.cnblogs.com/wuyuegb2312/p/3016878.html Catalan Numbers http://www-math.mit.edu/~rstan/transparencies/china.pdf Applications of Catalan Numbers https://www.geeksforgeeks.org/applications-of-catalan-numbers/ Catalan Numbers http://www.geometer.org/mathcircles/catalan.pdf catalan\u7684\u7a0b\u5e8f\u5b9e\u73b0 https://cp-algorithms.com/combinatorics/catalan-numbers.html https://rosettacode.org/wiki/Catalan_numbers#C.2B.2B","title":"\u5361\u7279\u5170\u6570\u7684\u8bc1\u660e"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#_1","text":"","title":"\u5361\u7279\u5170\u6570\u7684\u8bc1\u660e"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#_2","text":"http://blog.sina.com.cn/s/blog_6917f47301010cno.html The On-Line Encyclopedia of Integer Sequences\u00ae (OEIS\u00ae)\uff1a http://oeis.org/","title":"\u6298\u73b0\u6cd5\u2014\u2014\u5361\u7279\u5170\u6570\u8bc1\u660e\uff1a"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#catalan#numbers","text":"https://brilliant.org/wiki/catalan-numbers/","title":"Catalan Numbers"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#_3","text":"","title":"\u5e94\u7528"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#catalan","text":"https://www.cnblogs.com/hapjin/p/5758083.html https://blog.csdn.net/qq1169091731/article/details/51284435 https://en.wikipedia.org/wiki/Stack-sortable_permutation https://www.geeksforgeeks.org/check-if-the-given-push-and-pop-sequences-of-stack-is-valid-or-not/","title":"\u51fa\u6808\u987a\u5e8f \u4e0e \u5361\u7279\u5170\u6570(Catalan)\u7684\u5173\u7cfb\uff1a"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#catalan#number","text":"https://zhuanlan.zhihu.com/p/31317307 https://zhuanlan.zhihu.com/p/31526354 https://zhuanlan.zhihu.com/p/31585260","title":"\u5361\u7279\u5170\u6570\uff08Catalan number\uff09\uff1a"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#_4","text":"https://www.cnblogs.com/wuyuegb2312/p/3016878.html","title":"\u4ece\u300a\u7f16\u7a0b\u4e4b\u7f8e\u300b\u4e70\u7968\u627e\u96f6\u95ee\u9898\u8bf4\u8d77\uff0c\u5a13\u5a13\u9053\u6765\u5361\u7279\u5170\u6570\u2014\u2014\u517c\u722c\u5751\u6307\u5357"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#catalan#numbers_1","text":"http://www-math.mit.edu/~rstan/transparencies/china.pdf","title":"Catalan Numbers"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#applications#of#catalan#numbers","text":"https://www.geeksforgeeks.org/applications-of-catalan-numbers/","title":"Applications of Catalan Numbers"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#catalan#numbers_2","text":"http://www.geometer.org/mathcircles/catalan.pdf","title":"Catalan Numbers"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-reading%20list/#catalan_1","text":"https://cp-algorithms.com/combinatorics/catalan-numbers.html https://rosettacode.org/wiki/Catalan_numbers#C.2B.2B","title":"catalan\u7684\u7a0b\u5e8f\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-%E6%8A%98%E7%8E%B0%E6%B3%95-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0%E7%9A%84%E8%AF%81%E6%98%8E/","text":"sina \u6298\u73b0\u6cd5\u2014\u2014\u5361\u7279\u5170\u6570\u8bc1\u660e 1.\u996d\u540e\uff0c\u59d0\u59d0\u6d17\u7897\uff0c\u59b9\u59b9\u628a\u59d0\u59d0\u6d17\u8fc7\u7684\u7897**\u4e00\u4e2a\u4e00\u4e2a\u5730**\u653e\u8fdb\u7897\u6a71\u645e\u6210\u4e00\u645e\u3002\u4e00\u5171\u6709n\u4e2a\u4e0d\u540c\u7684\u7897\uff0c\u6d17\u524d\u4e5f\u662f\u645e\u6210\u4e00\u645e\u7684\uff0c\u4e5f\u8bb8\u56e0\u4e3a\u5c0f\u59b9\u8d2a\u73a9\u800c\u4f7f\u7897\u62ff\u8fdb\u7897\u6a71\u4e0d\u53ca\u65f6\uff0c\u59d0\u59d0\u5219\u628a\u6d17\u8fc7\u7684\u7897\u645e\u5728\u65c1\u8fb9\uff0c\u95ee\uff1a\u5c0f\u59b9\u645e\u8d77\u7684\u7897\u6709\u591a\u5c11\u79cd\u53ef\u80fd\u7684\u65b9\u5f0f\uff1f 2.\u7ed9\u5b9an\u4e2a\u6570\uff0c\u6709\u591a\u5c11\u79cd\u51fa\u6808\u5e8f\u5217\uff1f 3.\u4e00\u4e2a\u6709*n*\u4e2a1\u548c*n*\u4e2a-1\u7ec4\u6210\u7684\u5b57\u4e32\uff0c\u4e14\u524dk\u4e2a\u6570\u7684\u548c\u5747\u4e0d\u5c0f\u4e8e0\uff0c\u90a3\u8fd9\u79cd\u5b57\u4e32\u7684\u603b\u6570\u4e3a\u591a\u5c11\uff1f SUMMARY : \u4e0a\u8ff0\u95ee\u9898\u6700\u7ec8\u90fd\u53ef\u4ee5\u901a\u8fc7 bijection \u8f6c\u6362\u4e3a\u540c\u4e00\u7c7b\u95ee\u9898\uff1b \u8fd9\u4e09\u4e2a\u95ee\u9898\u5177\u6709\u76f8\u540c\u7684\u7ed3\u6784\uff0c\u4e09\u4e2a\u95ee\u9898\u662f\u53ef\u4ee5\u4e92\u76f8\u8f6c\u5316\u3002\u5c06\u59d0\u59d0\u653e\u7897\u770b\u505a\u5165\u6808\u64cd\u4f5c\uff0c\u5c06\u59b9\u59b9\u653e\u7897\u770b\u505a\u51fa\u6808\u64cd\u4f5c\u3002\u5219\u95ee\u9898\u4e00\u53d8\u4e3a\u95ee\u9898\u4e8c\u3002\u5c06\u5165\u6808\u64cd\u4f5c\u8bb0\u4e3a1\uff0c\u51fa\u6808\u8bb0\u4e3a-1\uff0c\u95ee\u98982\u53d8\u4e3a\u95ee\u98983\u3002 \u95ee\u9898\u7684\u7b54\u6848\u662f\u4e00\u4e2a\u8457\u540d\u7684\u6570\u5217\uff0c \u5361\u7279\u5170\u6570 \u3002\u8be5\u95ee\u9898\u7684\u4ee3\u6570\u89e3\u6cd5\u6bd4\u8f83\u62bd\u8c61\uff0c\u800c\u8fd0\u7528\u5230\u51e0\u4f55\u4e0a\uff0c\u7528\u56fe\u7247\u6765\u63cf\u8ff0\uff0c\u5374\u6709\u8ba9\u4eba\u604d\u7136\u5927\u609f\u7684\u611f\u89c9\u3002 \u4e8b\u5b9e\u4e0a\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u95ee\u9898\u662f\uff0c\u4efb\u610f\u4e24\u79cd\u64cd\u4f5c\uff0c\u8981\u6c42\u6bcf\u79cd\u64cd\u4f5c\u7684\u603b\u6b21\u6570\u4e00\u6837\uff0c\u4e14\u8fdb\u884c\u7b2ck\u6b21**\u64cd\u4f5c2**\u524d\u5fc5\u987b\u5148\u8fdb\u884c\u81f3\u5c11k\u6b21**\u64cd\u4f5c1**\u3002\u6211\u4eec\u5047\u8bbe\u4e00\u4e2a\u4eba\u5728\u539f\u70b9\uff0c \u64cd\u4f5c1**\u662f\u6b64\u4eba\u6cbf\u53f3\u4e0a\u89d245\u00b0\u8d70\u4e00\u4e2a\u5355\u4f4d\uff08\u4e00\u4e2a\u5355\u4f4d\u8bbe\u4e3a\u6839\u53f72\uff0c\u8fd9\u6837\u4ed6\u7b2c\u4e00\u6b21\u8fdb\u884c\u64cd\u4f5c1\u5c31\u521a\u597d\u8d70\u5230\uff081,1\uff09\u70b9\uff09\uff0c**\u64cd\u4f5c2**\u662f\u6b64\u4eba\u6cbf\u53f3\u4e0b\u89d245\u00b0\u8d70\u4e00\u4e2a\u5355\u4f4d\u3002\u7b2ck\u6b21**\u64cd\u4f5c2**\u524d\u5fc5\u987b\u5148\u8fdb\u884c\u81f3\u5c11k\u6b21**\u64cd\u4f5c1 \uff0c\u5c31\u662f\u8bf4\u660e\u6240\u8d70\u51fa\u6765\u7684\u6298\u7ebf\u4e0d\u80fd\u8de8\u8d8ax\u8f74\u8d70\u5230 y=-1 \u8fd9\u6761\u7ebf\u4e0a\uff01\u5728\u8fdb\u884cn\u6b21\u64cd\u4f5c1\u548cn\u6b64\u64cd\u4f5c2\u540e\uff0c\u6b64\u4eba\u5fc5\u5c06\u5230\u5230\u8fbe \uff082n\uff0c0\uff09 \uff01\u82e5\u65e0\u8de8\u8d8ax\u8f74\u7684\u9650\u5236\uff0c\u6298\u7ebf\u7684\u79cd\u6570\u5c06\u4e3a C\uff082n\uff0cn\uff09 \uff0c\u5373\u57282n\u6b21\u64cd\u4f5c\u4e2d\u9009\u51fan\u6b21\u4f5c\u4e3a\u64cd\u4f5c1\u7684\u65b9\u6cd5\u6570\u3002 \u73b0\u5728\u53ea\u8981\u51cf\u53bb\u8de8\u8d8a\u4e86x\u8f74\u7684\u60c5\u51b5\u6570\u3002\u5bf9\u4e8e\u4efb\u610f\u8de8\u8d8ax\u8f74\u7684\u60c5\u51b5\uff0c\u5fc5\u6709\u5c06\u4e0e y=-1 \u76f8\u4ea4\u3002\u627e\u51fa\u7b2c\u4e00\u4e2a\u4e0ey=-1\u76f8\u4ea4\u7684\u70b9 k \uff0c\u5c06 k \u70b9\u4ee5\u53f3\u7684\u6298\u7ebf\u6839\u636ey=-1\u5bf9\u79f0\uff08\u5373\u64cd\u4f5c1\u4e0e\u64cd\u4f5c2\u4e92\u6362\u4e86\uff09\u3002\u53ef\u4ee5\u53d1\u73b0\u7ec8\u70b9\u6700\u7ec8\u90fd\u4f1a\u4ece \uff082n\uff0c0\uff09 \u5bf9\u79f0\u5230 \uff082n\uff0c-2\uff09 \u3002\u7531\u4e8e\u5bf9\u79f0\u603b\u662f\u80fd\u8fdb\u884c\u7684\uff0c\u4e14\u662f\u53ef\u9006\u7684\u3002\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u6240\u6709\u8de8\u8d8a\u4e86x\u8f74\u7684\u6298\u7ebf\u603b\u6570\u662f\u4e0e\u4ece\uff080,0\uff09\u5230\uff082n,-2\uff09\u7684\u6298\u7ebf\u603b\u6570\u3002\u800c\u540e\u8005\u7684**\u64cd\u4f5c2**\u6bd4**\u64cd\u4f5c1**\u8981\u591a0-\uff08-2\uff09=2\u6b21\u3002\u5373\u64cd\u4f5c1\u4e3an-1,\u64cd\u4f5c2\u4e3an+1\u3002\u603b\u6570\u4e3a C\uff082n\uff0cn-1\uff09 \u3002 SUMMARY : \u8981\u5145\u5206\u7406\u89e3\u4e0a\u8ff0\u8bc1\u660e\u65b9\u6cd5\u4e2d\u4e3a\u4ec0\u4e48\u8981\u6c42\uff1a \u6240\u8d70\u51fa\u6765\u7684\u6298\u7ebf\u4e0d\u80fd\u8de8\u8d8ax\u8f74\u8d70\u5230 y=-1 \u8fd9\u6761\u7ebf\u4e0a \uff0c\u9700\u8981\u7ed3\u5408\u5b9e\u9645\u95ee\u9898\u6765\u8fdb\u884c\u8003\u8651\uff1b\u9996\u5148\u660e\u786e\u7684\u662f\uff0c \u8981\u6c42\u6240\u8d70\u51fa\u6765\u7684\u6298\u7ebf\u4e0d\u80fd\u8de8\u8d8ax\u8f74\u8d70\u5230 y=-1 \u8fd9\u6761\u7ebf\u4e0a**\u5176\u5b9e\u662f\u8981\u6c42\uff1a\u7b2ck\u6b21**\u64cd\u4f5c2**\u524d\u5fc5\u987b\u5148\u8fdb\u884c\u81f3\u5c11k\u6b21**\u64cd\u4f5c1 \uff08\u5373\u5982\u4f55\u65f6\u523b\uff0c**\u64cd\u4f5c1**\u6267\u884c\u7684\u6b21\u6570\u90fd\u5927\u4e8e\u7b49\u4e8e**\u64cd\u4f5c2**\u6267\u884c\u7684\u6b21\u6570\uff09\uff1b\u8fd9\u4e2a\u9650\u5236\u662f\u6e90\u4e8e\u5b9e\u9645\u95ee\u9898\uff0c\u6bd4\u5982\uff1a \u95ee\u9898\uff1a\u7ed9\u5b9an\u4e2a\u6570\uff0c\u6709\u591a\u5c11\u79cd\u51fa\u6808\u5e8f\u5217\uff1f\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u4f1a\u5c06**\u5165\u6808**\u62bd\u8c61\u4e3a**\u64cd\u4f5c1**\uff0c\u5c06**\u51fa\u6808**\u62bd\u8c61\u4e3a**\u64cd\u4f5c2**\uff0c\u663e\u7136\uff0c\u53ea\u6709\u6808\u4e2d\u6709\u5143\u7d20\uff0c\u624d\u53ef\u80fd\u51fa\u6808\uff0c\u5982\u679c\u6808\u4e2d\u6ca1\u6709\u5143\u7d20\uff0c\u662f\u4e0d\u53ef\u80fd\u51fa\u6808\u7684\uff0c\u6240\u4ee5\u5728\u8be5\u95ee\u9898\u4e2d\u7684\u4e00\u4e2a\u660e\u663e\u7684\u9650\u5236\u662f\uff1a\u5982\u4f55\u65f6\u523b\uff0c**\u64cd\u4f5c1**\u6267\u884c\u7684\u6b21\u6570\u90fd\u5927\u4e8e\u7b49\u4e8e**\u64cd\u4f5c2**\u6267\u884c\u7684\u6b21\u6570 \u95ee\u9898\uff1a\u62ec\u53f7\u5339\u914d\u95ee\u9898\uff1f\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u4f1a\u5c06\u2018\uff08\u2019\u62bd\u8c61\u4e3a**\u64cd\u4f5c1**\uff0c\u5c06\u2018\uff09\u2019\u62bd\u8c61\u4e3a**\u64cd\u4f5c2**\uff0c\u663e\u7136\uff0c\u53ea\u6709\u6709\u4e86\u2018\uff08\u2019\uff0c\u624d\u80fd\u591f\u8003\u8651\u5339\u914d\u2018\uff09\u2019\uff0c\u6240\u4ee5\u5728\u8be5\u95ee\u9898\u4e2d\u7684\u4e00\u4e2a\u660e\u663e\u7684\u9650\u5236\u662f\uff1a\u5982\u4f55\u65f6\u523b\uff0c**\u64cd\u4f5c1**\u6267\u884c\u7684\u6b21\u6570\u90fd\u5927\u4e8e\u7b49\u4e8e**\u64cd\u4f5c2**\u6267\u884c\u7684\u6b21\u6570\uff1b \u7c7b\u4f3c\u7684\u95ee\u9898\u90fd\u9700\u8981\u6ee1\u8db3\u4e0a\u8ff0\u7684\u8fd9\u79cd\u9650\u5236\uff0c\u8fd9\u79cd\u9650\u5236\u6700\u7ec8\u5728\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u6765\u5c31\u662f\uff1a \u6240\u8d70\u51fa\u6765\u7684\u6298\u7ebf\u4e0d\u80fd\u8de8\u8d8ax\u8f74\u8d70\u5230 y=-1 \u8fd9\u6761\u7ebf\u4e0a \u3002 \u3010\u8bc1\u660e\u3011\u5361\u7279\u5170\u6570\uff08\u6298\u7ebf\u6cd5\uff09","title":"sina [\u6298\u73b0\u6cd5\u2014\u2014\u5361\u7279\u5170\u6570\u8bc1\u660e](http://blog.sina.com.cn/s/blog_6917f47301010cno.html)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-%E6%8A%98%E7%8E%B0%E6%B3%95-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0%E7%9A%84%E8%AF%81%E6%98%8E/#sina","text":"1.\u996d\u540e\uff0c\u59d0\u59d0\u6d17\u7897\uff0c\u59b9\u59b9\u628a\u59d0\u59d0\u6d17\u8fc7\u7684\u7897**\u4e00\u4e2a\u4e00\u4e2a\u5730**\u653e\u8fdb\u7897\u6a71\u645e\u6210\u4e00\u645e\u3002\u4e00\u5171\u6709n\u4e2a\u4e0d\u540c\u7684\u7897\uff0c\u6d17\u524d\u4e5f\u662f\u645e\u6210\u4e00\u645e\u7684\uff0c\u4e5f\u8bb8\u56e0\u4e3a\u5c0f\u59b9\u8d2a\u73a9\u800c\u4f7f\u7897\u62ff\u8fdb\u7897\u6a71\u4e0d\u53ca\u65f6\uff0c\u59d0\u59d0\u5219\u628a\u6d17\u8fc7\u7684\u7897\u645e\u5728\u65c1\u8fb9\uff0c\u95ee\uff1a\u5c0f\u59b9\u645e\u8d77\u7684\u7897\u6709\u591a\u5c11\u79cd\u53ef\u80fd\u7684\u65b9\u5f0f\uff1f 2.\u7ed9\u5b9an\u4e2a\u6570\uff0c\u6709\u591a\u5c11\u79cd\u51fa\u6808\u5e8f\u5217\uff1f 3.\u4e00\u4e2a\u6709*n*\u4e2a1\u548c*n*\u4e2a-1\u7ec4\u6210\u7684\u5b57\u4e32\uff0c\u4e14\u524dk\u4e2a\u6570\u7684\u548c\u5747\u4e0d\u5c0f\u4e8e0\uff0c\u90a3\u8fd9\u79cd\u5b57\u4e32\u7684\u603b\u6570\u4e3a\u591a\u5c11\uff1f SUMMARY : \u4e0a\u8ff0\u95ee\u9898\u6700\u7ec8\u90fd\u53ef\u4ee5\u901a\u8fc7 bijection \u8f6c\u6362\u4e3a\u540c\u4e00\u7c7b\u95ee\u9898\uff1b \u8fd9\u4e09\u4e2a\u95ee\u9898\u5177\u6709\u76f8\u540c\u7684\u7ed3\u6784\uff0c\u4e09\u4e2a\u95ee\u9898\u662f\u53ef\u4ee5\u4e92\u76f8\u8f6c\u5316\u3002\u5c06\u59d0\u59d0\u653e\u7897\u770b\u505a\u5165\u6808\u64cd\u4f5c\uff0c\u5c06\u59b9\u59b9\u653e\u7897\u770b\u505a\u51fa\u6808\u64cd\u4f5c\u3002\u5219\u95ee\u9898\u4e00\u53d8\u4e3a\u95ee\u9898\u4e8c\u3002\u5c06\u5165\u6808\u64cd\u4f5c\u8bb0\u4e3a1\uff0c\u51fa\u6808\u8bb0\u4e3a-1\uff0c\u95ee\u98982\u53d8\u4e3a\u95ee\u98983\u3002 \u95ee\u9898\u7684\u7b54\u6848\u662f\u4e00\u4e2a\u8457\u540d\u7684\u6570\u5217\uff0c \u5361\u7279\u5170\u6570 \u3002\u8be5\u95ee\u9898\u7684\u4ee3\u6570\u89e3\u6cd5\u6bd4\u8f83\u62bd\u8c61\uff0c\u800c\u8fd0\u7528\u5230\u51e0\u4f55\u4e0a\uff0c\u7528\u56fe\u7247\u6765\u63cf\u8ff0\uff0c\u5374\u6709\u8ba9\u4eba\u604d\u7136\u5927\u609f\u7684\u611f\u89c9\u3002 \u4e8b\u5b9e\u4e0a\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u95ee\u9898\u662f\uff0c\u4efb\u610f\u4e24\u79cd\u64cd\u4f5c\uff0c\u8981\u6c42\u6bcf\u79cd\u64cd\u4f5c\u7684\u603b\u6b21\u6570\u4e00\u6837\uff0c\u4e14\u8fdb\u884c\u7b2ck\u6b21**\u64cd\u4f5c2**\u524d\u5fc5\u987b\u5148\u8fdb\u884c\u81f3\u5c11k\u6b21**\u64cd\u4f5c1**\u3002\u6211\u4eec\u5047\u8bbe\u4e00\u4e2a\u4eba\u5728\u539f\u70b9\uff0c \u64cd\u4f5c1**\u662f\u6b64\u4eba\u6cbf\u53f3\u4e0a\u89d245\u00b0\u8d70\u4e00\u4e2a\u5355\u4f4d\uff08\u4e00\u4e2a\u5355\u4f4d\u8bbe\u4e3a\u6839\u53f72\uff0c\u8fd9\u6837\u4ed6\u7b2c\u4e00\u6b21\u8fdb\u884c\u64cd\u4f5c1\u5c31\u521a\u597d\u8d70\u5230\uff081,1\uff09\u70b9\uff09\uff0c**\u64cd\u4f5c2**\u662f\u6b64\u4eba\u6cbf\u53f3\u4e0b\u89d245\u00b0\u8d70\u4e00\u4e2a\u5355\u4f4d\u3002\u7b2ck\u6b21**\u64cd\u4f5c2**\u524d\u5fc5\u987b\u5148\u8fdb\u884c\u81f3\u5c11k\u6b21**\u64cd\u4f5c1 \uff0c\u5c31\u662f\u8bf4\u660e\u6240\u8d70\u51fa\u6765\u7684\u6298\u7ebf\u4e0d\u80fd\u8de8\u8d8ax\u8f74\u8d70\u5230 y=-1 \u8fd9\u6761\u7ebf\u4e0a\uff01\u5728\u8fdb\u884cn\u6b21\u64cd\u4f5c1\u548cn\u6b64\u64cd\u4f5c2\u540e\uff0c\u6b64\u4eba\u5fc5\u5c06\u5230\u5230\u8fbe \uff082n\uff0c0\uff09 \uff01\u82e5\u65e0\u8de8\u8d8ax\u8f74\u7684\u9650\u5236\uff0c\u6298\u7ebf\u7684\u79cd\u6570\u5c06\u4e3a C\uff082n\uff0cn\uff09 \uff0c\u5373\u57282n\u6b21\u64cd\u4f5c\u4e2d\u9009\u51fan\u6b21\u4f5c\u4e3a\u64cd\u4f5c1\u7684\u65b9\u6cd5\u6570\u3002 \u73b0\u5728\u53ea\u8981\u51cf\u53bb\u8de8\u8d8a\u4e86x\u8f74\u7684\u60c5\u51b5\u6570\u3002\u5bf9\u4e8e\u4efb\u610f\u8de8\u8d8ax\u8f74\u7684\u60c5\u51b5\uff0c\u5fc5\u6709\u5c06\u4e0e y=-1 \u76f8\u4ea4\u3002\u627e\u51fa\u7b2c\u4e00\u4e2a\u4e0ey=-1\u76f8\u4ea4\u7684\u70b9 k \uff0c\u5c06 k \u70b9\u4ee5\u53f3\u7684\u6298\u7ebf\u6839\u636ey=-1\u5bf9\u79f0\uff08\u5373\u64cd\u4f5c1\u4e0e\u64cd\u4f5c2\u4e92\u6362\u4e86\uff09\u3002\u53ef\u4ee5\u53d1\u73b0\u7ec8\u70b9\u6700\u7ec8\u90fd\u4f1a\u4ece \uff082n\uff0c0\uff09 \u5bf9\u79f0\u5230 \uff082n\uff0c-2\uff09 \u3002\u7531\u4e8e\u5bf9\u79f0\u603b\u662f\u80fd\u8fdb\u884c\u7684\uff0c\u4e14\u662f\u53ef\u9006\u7684\u3002\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u6240\u6709\u8de8\u8d8a\u4e86x\u8f74\u7684\u6298\u7ebf\u603b\u6570\u662f\u4e0e\u4ece\uff080,0\uff09\u5230\uff082n,-2\uff09\u7684\u6298\u7ebf\u603b\u6570\u3002\u800c\u540e\u8005\u7684**\u64cd\u4f5c2**\u6bd4**\u64cd\u4f5c1**\u8981\u591a0-\uff08-2\uff09=2\u6b21\u3002\u5373\u64cd\u4f5c1\u4e3an-1,\u64cd\u4f5c2\u4e3an+1\u3002\u603b\u6570\u4e3a C\uff082n\uff0cn-1\uff09 \u3002 SUMMARY : \u8981\u5145\u5206\u7406\u89e3\u4e0a\u8ff0\u8bc1\u660e\u65b9\u6cd5\u4e2d\u4e3a\u4ec0\u4e48\u8981\u6c42\uff1a \u6240\u8d70\u51fa\u6765\u7684\u6298\u7ebf\u4e0d\u80fd\u8de8\u8d8ax\u8f74\u8d70\u5230 y=-1 \u8fd9\u6761\u7ebf\u4e0a \uff0c\u9700\u8981\u7ed3\u5408\u5b9e\u9645\u95ee\u9898\u6765\u8fdb\u884c\u8003\u8651\uff1b\u9996\u5148\u660e\u786e\u7684\u662f\uff0c \u8981\u6c42\u6240\u8d70\u51fa\u6765\u7684\u6298\u7ebf\u4e0d\u80fd\u8de8\u8d8ax\u8f74\u8d70\u5230 y=-1 \u8fd9\u6761\u7ebf\u4e0a**\u5176\u5b9e\u662f\u8981\u6c42\uff1a\u7b2ck\u6b21**\u64cd\u4f5c2**\u524d\u5fc5\u987b\u5148\u8fdb\u884c\u81f3\u5c11k\u6b21**\u64cd\u4f5c1 \uff08\u5373\u5982\u4f55\u65f6\u523b\uff0c**\u64cd\u4f5c1**\u6267\u884c\u7684\u6b21\u6570\u90fd\u5927\u4e8e\u7b49\u4e8e**\u64cd\u4f5c2**\u6267\u884c\u7684\u6b21\u6570\uff09\uff1b\u8fd9\u4e2a\u9650\u5236\u662f\u6e90\u4e8e\u5b9e\u9645\u95ee\u9898\uff0c\u6bd4\u5982\uff1a \u95ee\u9898\uff1a\u7ed9\u5b9an\u4e2a\u6570\uff0c\u6709\u591a\u5c11\u79cd\u51fa\u6808\u5e8f\u5217\uff1f\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u4f1a\u5c06**\u5165\u6808**\u62bd\u8c61\u4e3a**\u64cd\u4f5c1**\uff0c\u5c06**\u51fa\u6808**\u62bd\u8c61\u4e3a**\u64cd\u4f5c2**\uff0c\u663e\u7136\uff0c\u53ea\u6709\u6808\u4e2d\u6709\u5143\u7d20\uff0c\u624d\u53ef\u80fd\u51fa\u6808\uff0c\u5982\u679c\u6808\u4e2d\u6ca1\u6709\u5143\u7d20\uff0c\u662f\u4e0d\u53ef\u80fd\u51fa\u6808\u7684\uff0c\u6240\u4ee5\u5728\u8be5\u95ee\u9898\u4e2d\u7684\u4e00\u4e2a\u660e\u663e\u7684\u9650\u5236\u662f\uff1a\u5982\u4f55\u65f6\u523b\uff0c**\u64cd\u4f5c1**\u6267\u884c\u7684\u6b21\u6570\u90fd\u5927\u4e8e\u7b49\u4e8e**\u64cd\u4f5c2**\u6267\u884c\u7684\u6b21\u6570 \u95ee\u9898\uff1a\u62ec\u53f7\u5339\u914d\u95ee\u9898\uff1f\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u4f1a\u5c06\u2018\uff08\u2019\u62bd\u8c61\u4e3a**\u64cd\u4f5c1**\uff0c\u5c06\u2018\uff09\u2019\u62bd\u8c61\u4e3a**\u64cd\u4f5c2**\uff0c\u663e\u7136\uff0c\u53ea\u6709\u6709\u4e86\u2018\uff08\u2019\uff0c\u624d\u80fd\u591f\u8003\u8651\u5339\u914d\u2018\uff09\u2019\uff0c\u6240\u4ee5\u5728\u8be5\u95ee\u9898\u4e2d\u7684\u4e00\u4e2a\u660e\u663e\u7684\u9650\u5236\u662f\uff1a\u5982\u4f55\u65f6\u523b\uff0c**\u64cd\u4f5c1**\u6267\u884c\u7684\u6b21\u6570\u90fd\u5927\u4e8e\u7b49\u4e8e**\u64cd\u4f5c2**\u6267\u884c\u7684\u6b21\u6570\uff1b \u7c7b\u4f3c\u7684\u95ee\u9898\u90fd\u9700\u8981\u6ee1\u8db3\u4e0a\u8ff0\u7684\u8fd9\u79cd\u9650\u5236\uff0c\u8fd9\u79cd\u9650\u5236\u6700\u7ec8\u5728\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u6765\u5c31\u662f\uff1a \u6240\u8d70\u51fa\u6765\u7684\u6298\u7ebf\u4e0d\u80fd\u8de8\u8d8ax\u8f74\u8d70\u5230 y=-1 \u8fd9\u6761\u7ebf\u4e0a \u3002","title":"sina \u6298\u73b0\u6cd5\u2014\u2014\u5361\u7279\u5170\u6570\u8bc1\u660e"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/catalan-%E6%8A%98%E7%8E%B0%E6%B3%95-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0%E7%9A%84%E8%AF%81%E6%98%8E/#_1","text":"","title":"\u3010\u8bc1\u660e\u3011\u5361\u7279\u5170\u6570\uff08\u6298\u7ebf\u6cd5\uff09"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/","text":"yufeizhao Bijections In this lecture, we will look at using bijections to solve combinatorics problems . Given two sets A and B, a bijection (also called bijective correspondence ) is a map f : A \u2192 B that is both injective \uff08\u5355\u5c04\uff09 and surjective \uff08\u6ee1\u5c04\uff09, meaning that no two elements of A get mapped onto the same element in B, and every element of B is the image of some element of A . This gives us a way of pairing up every element from A with some element of B. In particular, when the sets are finite, the existence of a bijection implies that |A | = |B|. This explanation of a bijection may seem a little abstract. Let us take a look at some examples of how bijections can be used Problem 1. Q: Determine the number of walks from (0, 0) to (m, n) allowing only unit steps up or to the right A The idea of using bijection is that we want to tranform the problem into something that we know better how to count. In this case, we can encode the path as a sequence letters U and R, correponding to whether the step taken was a unit up step (U) or a unit right step (R). The path displayed above, for instance, is encoded as RRURRUURRURRUUURRU Note that the resulting sequence of letters always has m copies of R and n copies of U, since it takes in total of m right unit steps and n up unit steps to move from (0, 0) to (m, n). Also, you can check that for any sequence consisting of m copies of R and n copies of U, we can construct a corresponding path encoded by this sequence. Thus we have constructed a bijection between the following two sets: the set of walks from (0, 0) to (m, n) using only unit up or right steps \u200b /\\ \u200b | \u200b \\/ the set of sequences consisting of m copies of R and n copies of U. We know how to count the latter set. It has exactly \\tbinom {m+n} {m} \\tbinom {m+n} {m} elements, which can be thought of as choosing m spots to place the R\u2019s in a sequence of length m+n. Therefore, the number of lattice walks from (0, 0) to (m, n) must be \\tbinom {m+n} {n} \\tbinom {m+n} {n} as wel Next, let us explore the Catalan numbers , which is a sequence is that comes up in many counting problems, which provide a rich source for bijections. Problem 5 Q Let n n be a positive integer. Determine the number of lattice paths from (0, 0) to (n, n) using only unit up and right steps, such that the path stays in the region x \u2265 y. A We saw previously that the total number of lattice paths from (0, 0) to (n, n) without the x \u2265 y restriction is equal is \\tbinom {2 \\times n} {n} \\tbinom {2 \\times n} {n} . Let us count the number of paths that goes into the x < y region. Call these paths bad path Suppose that P is a bad path. Since P goes into the region x < y, it must hit the line y = x + 1 at some point. Let X be the first point on the path P that lies on the line y = x + 1. Now, reflect the portion of path P up to X about the line y = x + 1, keeping the latter portion of P the same. This gives us a new path P 0 . Problem 6 Q Show that the n-th Catalan number counts the number of expressions containing n pairs of parentheses which are correctly matched. E.g., for n = 3, ((())) (()()) (())() ()(()) ()()() We could solve this problem by counting it using techniques similar to the one used to count paths above. A much quicker solution is to find a bijection between these parentheses expressions and the lattice paths counted in the previous problem. Indeed, note that by interpretating each ( as a unit right step and each ) as a unit up step, we obtain the desired bijection . The condition that the parentheses expression is correctedly matched corresponds exactly to the condition that the lattice path do not go into the x < y region (why?). This bijection shows that the number of expressions of n pairs of parentheses which are correctly matched is also equal to n-th Catalan number, as desired. The previous bijection was rather simple. Let us look at a more involved Catalan number bijection. A plane tree \uff08\u5e73\u9762\u6811\uff09 is an object with the following structure. We start with a root vertex (drawn at the top), and then with each node we attach a number of new vertices (possibly none), where the order of the attached vertices matters. For instance, there are exactly 5 plane trees with 4 vertices: Problem 7 Q Show that the n-th Catalan number counts the number of plane trees with n + 1 vertices. A We will produce a bijection between plane trees and the parentheses expressions considered in the previous problem. We first describe an algorithm to turn a plane tree into a parentheses expression . Given a plane tree \uff08\u5e73\u9762\u6811\uff09, starting from the top vertex , let us perform a depth-first search walk, meaning that we go as further down as possible until we hit a dead-end, and then backtrack to a branch point, where we then explore a new branch. We will always explore the branches of a vertex in order from left to right. For instance, starting with the plane \u56fe\u7565 we obtain the following walk, where the steps are labeled in order. \u56fe\u7565 Now we record the sequence of steps we took, writing down a ( each time we stepped downward along an edge, and a ) every time we stepped upward along an edge. For example, the above walk corresponds to A plane tree with n + 1 vertices always produces a correctly matched expression of n pairs of parentheses correctly matched (why is it correctly matched?). Conversely, given an expression of n pairs of correctly matched parentheses, it is possible to reverse this construction to produce a plane tree that corresponds to it. You should first convince yourself that this is the case why writing down a few parentheses expressions and then figure out what the corresponding tree is. Then, you should write down a description of this bijection. SUMMARY : n + 1\u4e2a\u8282\u70b9\u5171\u6709n\u6761\u8fb9\uff0c\u5728\u904d\u5386\u8fc7\u7a0b\u4e2d\uff0c\u80af\u5b9a\u4f1a\u6cbf\u7740\u6bcf\u6761\u8fb9\u4e0b\u4e00\u6b21\u7136\u540e\u4e0a\u4e00\u6b21\uff0c\u6240\u4ee5\u8fd9\u5c31\u5bf9\u5e94\u4e86\u4e00\u5bf9\u62ec\u53f7\u4e86\uff1b","title":"yufeizhao [Bijections](http://yufeizhao.com/olympiad/bijections.pdf)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#yufeizhao#bijections","text":"In this lecture, we will look at using bijections to solve combinatorics problems . Given two sets A and B, a bijection (also called bijective correspondence ) is a map f : A \u2192 B that is both injective \uff08\u5355\u5c04\uff09 and surjective \uff08\u6ee1\u5c04\uff09, meaning that no two elements of A get mapped onto the same element in B, and every element of B is the image of some element of A . This gives us a way of pairing up every element from A with some element of B. In particular, when the sets are finite, the existence of a bijection implies that |A | = |B|. This explanation of a bijection may seem a little abstract. Let us take a look at some examples of how bijections can be used","title":"yufeizhao Bijections"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#problem#1","text":"","title":"Problem 1."},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#q","text":"Determine the number of walks from (0, 0) to (m, n) allowing only unit steps up or to the right","title":"Q:"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#a","text":"The idea of using bijection is that we want to tranform the problem into something that we know better how to count. In this case, we can encode the path as a sequence letters U and R, correponding to whether the step taken was a unit up step (U) or a unit right step (R). The path displayed above, for instance, is encoded as RRURRUURRURRUUURRU Note that the resulting sequence of letters always has m copies of R and n copies of U, since it takes in total of m right unit steps and n up unit steps to move from (0, 0) to (m, n). Also, you can check that for any sequence consisting of m copies of R and n copies of U, we can construct a corresponding path encoded by this sequence. Thus we have constructed a bijection between the following two sets: the set of walks from (0, 0) to (m, n) using only unit up or right steps \u200b /\\ \u200b | \u200b \\/ the set of sequences consisting of m copies of R and n copies of U. We know how to count the latter set. It has exactly \\tbinom {m+n} {m} \\tbinom {m+n} {m} elements, which can be thought of as choosing m spots to place the R\u2019s in a sequence of length m+n. Therefore, the number of lattice walks from (0, 0) to (m, n) must be \\tbinom {m+n} {n} \\tbinom {m+n} {n} as wel Next, let us explore the Catalan numbers , which is a sequence is that comes up in many counting problems, which provide a rich source for bijections.","title":"A"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#problem#5","text":"","title":"Problem 5"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#q_1","text":"Let n n be a positive integer. Determine the number of lattice paths from (0, 0) to (n, n) using only unit up and right steps, such that the path stays in the region x \u2265 y.","title":"Q"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#a_1","text":"We saw previously that the total number of lattice paths from (0, 0) to (n, n) without the x \u2265 y restriction is equal is \\tbinom {2 \\times n} {n} \\tbinom {2 \\times n} {n} . Let us count the number of paths that goes into the x < y region. Call these paths bad path Suppose that P is a bad path. Since P goes into the region x < y, it must hit the line y = x + 1 at some point. Let X be the first point on the path P that lies on the line y = x + 1. Now, reflect the portion of path P up to X about the line y = x + 1, keeping the latter portion of P the same. This gives us a new path P 0 .","title":"A"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#problem#6","text":"","title":"Problem 6"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#q_2","text":"Show that the n-th Catalan number counts the number of expressions containing n pairs of parentheses which are correctly matched. E.g., for n = 3, ((())) (()()) (())() ()(()) ()()() We could solve this problem by counting it using techniques similar to the one used to count paths above. A much quicker solution is to find a bijection between these parentheses expressions and the lattice paths counted in the previous problem. Indeed, note that by interpretating each ( as a unit right step and each ) as a unit up step, we obtain the desired bijection . The condition that the parentheses expression is correctedly matched corresponds exactly to the condition that the lattice path do not go into the x < y region (why?). This bijection shows that the number of expressions of n pairs of parentheses which are correctly matched is also equal to n-th Catalan number, as desired. The previous bijection was rather simple. Let us look at a more involved Catalan number bijection. A plane tree \uff08\u5e73\u9762\u6811\uff09 is an object with the following structure. We start with a root vertex (drawn at the top), and then with each node we attach a number of new vertices (possibly none), where the order of the attached vertices matters. For instance, there are exactly 5 plane trees with 4 vertices:","title":"Q"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#problem#7","text":"","title":"Problem 7"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#q_3","text":"Show that the n-th Catalan number counts the number of plane trees with n + 1 vertices.","title":"Q"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Combinatorial-optimization/Combinatorics/Enumerative-combinatorics/Catalan-number/yufeizhao-Bijections/#a_2","text":"We will produce a bijection between plane trees and the parentheses expressions considered in the previous problem. We first describe an algorithm to turn a plane tree into a parentheses expression . Given a plane tree \uff08\u5e73\u9762\u6811\uff09, starting from the top vertex , let us perform a depth-first search walk, meaning that we go as further down as possible until we hit a dead-end, and then backtrack to a branch point, where we then explore a new branch. We will always explore the branches of a vertex in order from left to right. For instance, starting with the plane \u56fe\u7565 we obtain the following walk, where the steps are labeled in order. \u56fe\u7565 Now we record the sequence of steps we took, writing down a ( each time we stepped downward along an edge, and a ) every time we stepped upward along an edge. For example, the above walk corresponds to A plane tree with n + 1 vertices always produces a correctly matched expression of n pairs of parentheses correctly matched (why is it correctly matched?). Conversely, given an expression of n pairs of correctly matched parentheses, it is possible to reverse this construction to produce a plane tree that corresponds to it. You should first convince yourself that this is the case why writing down a few parentheses expressions and then figure out what the corresponding tree is. Then, you should write down a description of this bijection. SUMMARY : n + 1\u4e2a\u8282\u70b9\u5171\u6709n\u6761\u8fb9\uff0c\u5728\u904d\u5386\u8fc7\u7a0b\u4e2d\uff0c\u80af\u5b9a\u4f1a\u6cbf\u7740\u6bcf\u6761\u8fb9\u4e0b\u4e00\u6b21\u7136\u540e\u4e0a\u4e00\u6b21\uff0c\u6240\u4ee5\u8fd9\u5c31\u5bf9\u5e94\u4e86\u4e00\u5bf9\u62ec\u53f7\u4e86\uff1b","title":"A"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Mathematical-optimization/Hill-climbing/","text":"","title":"Hill-climbing"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Mathematical-optimization/Iterative-method/","text":"Iterative method In computational mathematics , an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the n -th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic -based iterative methods are also common. In contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors , direct methods would deliver an exact solution (like solving a linear system of equations $ A\\mathbf {x} =\\mathbf {b} $ by Gaussian elimination ). Iterative methods are often the only choice for nonlinear equations . However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.[ 1]","title":"Iterative-method"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Mathematical-optimization/Iterative-method/#iterative#method","text":"In computational mathematics , an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the n -th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic -based iterative methods are also common. In contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors , direct methods would deliver an exact solution (like solving a linear system of equations $ A\\mathbf {x} =\\mathbf {b} $ by Gaussian elimination ). Iterative methods are often the only choice for nonlinear equations . However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.[ 1]","title":"Iterative method"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Mathematical-optimization/Loss-function-or-Objective-function/","text":"Loss function Loss function","title":"Loss-function-or-Objective-function"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Mathematical-optimization/Loss-function-or-Objective-function/#loss#function","text":"","title":"Loss function"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Mathematical-optimization/iterative-optimization-algorithm/","text":"\u524d\u8a00 \u5728\u9605\u8bfb Neural Networks Tutorial \u2013 A Pathway to Deep Learning \u7684 4.1 A simple example in code \u5c0f\u8282\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u7684\u4f8b\u5b50\uff1a x_old = 0 # The value does not matter as long as abs(x_new - x_old) > precision x_new = 6 # The algorithm starts at x=6 gamma = 0.01 # step size precision = 0.00001 def df ( x ): y = 4 * x ** 3 - 9 * x ** 2 return y while abs ( x_new - x_old ) > precision : x_old = x_new x_new += - gamma * df ( x_old ) print ( \"The local minimum occurs at %f \" % x_new ) \u8fd9\u662f\u5178\u578b\u7684\u901a\u8fc7\u4e0d\u65ad\u8fdb\u884c\u8fed\u4ee3\uff0c\u4ece\u800c\u8ba1\u7b97\u5f97\u5230\u6700\u503c\u7684\u65b9\u6cd5\uff1b\u5728algorithm\u9886\u57df\u8fd9\u53eb\u4ec0\u4e48\u5462\uff1f\u4ee5\u6211\u7684\u76f4\u89c9\u6765\u770b\uff0c\u6211\u6682\u65f6\u53eb\u505a\u5b83\uff1aiterative optimization algorithm\uff1b\u5b83\u6709\u4e9b\u8d2a\u5fc3\u7b97\u6cd5\u7684\u610f\u5473\uff1b","title":"Iterative-optimization-algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/Mathematical-optimization/iterative-optimization-algorithm/#_1","text":"\u5728\u9605\u8bfb Neural Networks Tutorial \u2013 A Pathway to Deep Learning \u7684 4.1 A simple example in code \u5c0f\u8282\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u7684\u4f8b\u5b50\uff1a x_old = 0 # The value does not matter as long as abs(x_new - x_old) > precision x_new = 6 # The algorithm starts at x=6 gamma = 0.01 # step size precision = 0.00001 def df ( x ): y = 4 * x ** 3 - 9 * x ** 2 return y while abs ( x_new - x_old ) > precision : x_old = x_new x_new += - gamma * df ( x_old ) print ( \"The local minimum occurs at %f \" % x_new ) \u8fd9\u662f\u5178\u578b\u7684\u901a\u8fc7\u4e0d\u65ad\u8fdb\u884c\u8fed\u4ee3\uff0c\u4ece\u800c\u8ba1\u7b97\u5f97\u5230\u6700\u503c\u7684\u65b9\u6cd5\uff1b\u5728algorithm\u9886\u57df\u8fd9\u53eb\u4ec0\u4e48\u5462\uff1f\u4ee5\u6211\u7684\u76f4\u89c9\u6765\u770b\uff0c\u6211\u6682\u65f6\u53eb\u505a\u5b83\uff1aiterative optimization algorithm\uff1b\u5b83\u6709\u4e9b\u8d2a\u5fc3\u7b97\u6cd5\u7684\u610f\u5473\uff1b","title":"\u524d\u8a00"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/TODO-Linear-programming/wikipedia-Integer-programming/","text":"Integer programming","title":"wikipedia-Integer-programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/TODO-Linear-programming/wikipedia-Integer-programming/#integer#programming","text":"","title":"Integer programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/TODO-Linear-programming/wikipedia-Linear-programming/","text":"Linear programming Linear programming ( LP , also called linear optimization ) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships . Linear programming is a special case of mathematical programming (also known as mathematical optimization ). More formally, linear programming is a technique for the optimization of a linear objective function , subject to linear equality and linear inequality constraints . Its feasible region is a convex polytope , which is a set defined as the intersection of finitely many half spaces , each of which is defined by a linear inequality. Its objective function is a real -valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists. Linear programs are problems that can be expressed in canonical form as where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and $ (\\cdot )^{\\mathrm {T} } $ is the matrix transpose . The expression to be maximized or minimized is called the objective function ( c**T**x in this case). The inequalities A***x* \u2264 b and x \u2265 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector. Linear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics , and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning , routing , scheduling , assignment , and design.","title":"wikipedia-Linear-programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Optimization/TODO-Linear-programming/wikipedia-Linear-programming/#linear#programming","text":"Linear programming ( LP , also called linear optimization ) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships . Linear programming is a special case of mathematical programming (also known as mathematical optimization ). More formally, linear programming is a technique for the optimization of a linear objective function , subject to linear equality and linear inequality constraints . Its feasible region is a convex polytope , which is a set defined as the intersection of finitely many half spaces , each of which is defined by a linear inequality. Its objective function is a real -valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists. Linear programs are problems that can be expressed in canonical form as where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and $ (\\cdot )^{\\mathrm {T} } $ is the matrix transpose . The expression to be maximized or minimized is called the objective function ( c**T**x in this case). The inequalities A***x* \u2264 b and x \u2265 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector. Linear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics , and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning , routing , scheduling , assignment , and design.","title":"Linear programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/","text":"Search algorithm \u5173\u4e8esearch algorithm\uff0c\u7ef4\u57fa\u767e\u79d1 Search algorithm \u603b\u7ed3\u5730\u4e0d\u9519\uff0c\u672c\u6587\u4ee5\u5b83\u4f5c\u4e3a\u5165\u95e8\uff0c\u7136\u540e\u5bf9search algorithm\u8fdb\u884c\u603b\u7ed3\uff0c\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u9700\u8981\u5173\u6ce8\u7684\u6709\uff1a \u539f\u7406 \u5b9e\u73b0\u6280\u5de7 \u7ef4\u57fa\u767e\u79d1 Search algorithm NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5bf9search algorithm\u7684\u63cf\u8ff0\u662f\u975e\u5e38\u597d\u7684\uff0c\u5c24\u5176\u662f\u5bf9\u95ee\u9898\u7684\u5206\u7c7b In computer science , a search algorithm is any algorithm which solves the search problem , namely, to retrieve information stored within some data structure, or calculated in the search space \uff08\u53ef\u884c\u57df\u3001\u89e3\u7a7a\u95f4\uff09 of a problem domain , either with discrete or continuous values . Specific applications of search algorithms include: Problems in combinatorial optimization , such as: The vehicle routing problem , a form of shortest path problem NOTE: \u6c7d\u8f66\u8def\u5f84\u5b89\u6392\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u79cd\u201c\u6700\u77ed\u8def\u5f84\u95ee\u9898\u201d The knapsack problem : Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. The nurse scheduling problem Problems in constraint satisfaction , such as: The map coloring problem Filling in a sudoku or crossword puzzle In game theory and especially combinatorial game theory , choosing the best move to make next (such as with the minmax algorithm) Finding a combination or password from the whole set of possibilities Factoring an integer (an important problem in cryptography ) NOTE: \u5206\u89e3\u4e00\u4e2ainteger Optimizing an industrial process, such as a chemical reaction , by changing the parameters of the process (like temperature, pressure, and pH) Retrieving a record from a database Finding the maximum or minimum value in a list or array Checking to see if a given value is present in a set of values The classic search problems described above and web search are both problems in information retrieval , but are generally studied as separate subfields and are solved and evaluated differently. are generally focused on filtering and that find documents most relevant to human queries. Classic search algorithms are typically evaluated on how fast they can find a solution, and whether or not that solution is guaranteed to be optimal. Though information retrieval algorithms must be fast, the quality of ranking is more important, as is whether or not good results have been left out and bad results included. NOTE: search\u662f\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u6982\u5ff5 The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Some database structures are specially constructed to make search algorithms faster or more efficient, such as a search tree , hash map , or a database index . [ 1] [ 2] -2) NOTE: \u9009\u62e9\u5408\u9002\u7684data structure\u7ec4\u7ec7\u6570\u636e\u6765\u4f7fsearch\u7684time\u6700\u4f18 Search algorithms can be classified based on their mechanism of searching. Linear search algorithms check every record for the one associated with a target key in a linear fashion.[ 3] -3) Binary, or half interval searches , repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order.[ 4] -4) Digital search algorithms work based on the properties of digits in data structures that use numerical keys.[ 5] Finally, hashing directly maps keys to records based on a hash function .[ 6] Searches outside a linear search require that the data be sorted in some way. NOTE: \u201c Search algorithms can be classified based on their mechanism of searching\u201d\uff0c\u6211\u4eec\u5c06\u201cmechanism of searching\u201d\u79f0\u4e3a\u201cmethod of search\u201d\uff0c\u540e\u9762\u6211\u4eec\u5c06\u770b\u5230\u201cmethod of search\u201d\u662f\u975e\u5e38\u4e4b\u591a\u7684\uff0c\u7edd\u4e0d\u5c06\u5c31\u4e0a\u8ff0\u5217\u4e3e\u7684\u51e0\u79cd Algorithms are often evaluated by their computational complexity , or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O (log n ), or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space . NOTE: \u6700\u540e\u4e00\u53e5\u63d0\u53ca\u7684search space\u7684\u6982\u5ff5\u975e\u5e38\u91cd\u8981\u3002 Classes NOTE : Search algorithm \u7684\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\u5c31\u662f\u786e\u5b9a search space \uff0c\u672c\u6587\u4e2d search space \u6240\u94fe\u63a5\u7684\u6587\u7ae0\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u53ef\u884c\u57df\uff0c\u53ef\u884c\u57df\u7684\u542b\u4e49\u53ef\u80fd\u6709\u4e9b\u9650\u5236\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u7528\uff1b**search space**\u53ef\u80fd\u662fvirtual spaces\uff08\u5982backtracing\u5728\u89e3\u7a7a\u95f4\u4e2d\u641c\u7d22\uff09\u4e5f\u53ef\u80fd\u662f\u7684\u786e\u5b58\u5728\u4e00\u4e2adata structure\uff08\u5982\u4e8c\u5206\u641c\u7d22\u5728\u4e00\u4e2asorted array\u4e2d\u8fdb\u884c\u641c\u7d22\uff09\uff1b\u6b63\u5982\u4e0b\u9762\u4f1a\u8fdb\u884c\u5206\u7c7b\uff1a For virtual search spaces For sub-structures of a given structure Search for the maximum of a function For virtual search spaces Algorithms for searching virtual spaces are used in the constraint satisfaction problem , where the goal is to find a set of value assignments to certain variables that will satisfy specific mathematical equations and inequations / equalities. They are also used when the goal is to find a variable assignment that will maximize or minimize a certain function of those variables. Algorithms for these problems include the basic brute-force search (also called \"na\u00efve\" or \"uninformed\" search), and a variety of heuristics that try to exploit partial knowledge about the structure of this space, such as linear relaxation, constraint generation, and constraint propagation . NOTE: blind search VS heuristic search An important subclass are the local search methods, that view the elements of the search space as the vertices \uff08\u70b9\uff09 of a graph, with edges defined by a set of heuristics applicable to the case; and scan the space by moving from item to item along the edges, for example according to the steepest descent or best-first criterion, or in a stochastic search . This category includes a great variety of general metaheuristic methods, such as simulated annealing , tabu search , A-teams, and genetic programming , that combine arbitrary heuristics in specific ways. This class also includes various tree search algorithms , that view the elements as vertices of a tree , and traverse that tree in some special order. Examples of the latter include the exhaustive methods such as depth-first search and breadth-first search , as well as various heuristic-based search tree pruning methods such as backtracking and branch and bound . Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these tree-search methods are guaranteed to find the exact or optimal solution, if given enough time. This is called \" completeness \". NOTE: graph search\uff08tree search\u4e5f\u5305\u62ec\u5728\u5176\u4e2d\uff0c\u56e0\u4e3atree\u662f\u4e00\u79cd\u7279\u6b8a\u7684graph\uff09\uff0c\u540e\u9762\u4f1a\u8fdb\u884c\u4e13\u95e8\u7684\u4ecb\u7ecd\u3002graph search\u662f\u975e\u5e38\u91cd\u8981\uff0c\u56e0\u4e3avirtual search space\u4e5f\u53ef\u4ee5\u8f6c\u6362\u4e3agraph search\u3002 Another important sub-class consists of algorithms for exploring the game tree of multiple-player games, such as chess or backgammon \uff08\u53cc\u9646\u68cb\uff09, whose nodes consist of all possible game situations that could result from the current situation. The goal in these problems is to find the move that provides the best chance of a win, taking into account all possible moves of the opponent(s). Similar problems occur when humans or machines have to make successive decisions whose outcomes are not entirely under one's control, such as in robot guidance or in marketing , financial , or military strategy planning. This kind of problem \u2014 combinatorial search \uff08\u7ec4\u5408\u641c\u7d22\uff09 \u2014 has been extensively studied in the context of artificial intelligence . Examples of algorithms for this class are the minimax algorithm , alpha\u2013beta pruning , * Informational search [ 7] and the A* algorithm . For sub-structures of a given structure The name \"combinatorial search\"\uff08\u7ec4\u5408\u641c\u7d22\uff09 is generally used for algorithms that look for a specific sub-structure of a given discrete structure , such as a graph, a string , a finite group , and so on. The term combinatorial optimization is typically used when the goal is to find a sub-structure with a maximum (or minimum) value of some parameter. (Since the sub-structure is usually represented in the computer by a set of integer variables with constraints, these problems can be viewed as special cases of constraint satisfaction or discrete optimization; but they are usually formulated and solved in a more abstract setting where the internal representation is not explicitly mentioned.) NOTE: \u8fd9\u8ba9\u6211\u60f3\u8d77\u6765 Optimal substructure An important and extensively studied subclass are the graph algorithms , in particular graph traversal algorithms, for finding specific sub-structures in a given graph \u2014 such as subgraphs , paths , circuits, and so on. Examples include Dijkstra's algorithm , Kruskal's algorithm , the nearest neighbour algorithm , and Prim's algorithm . Another important subclass of this category are the string searching algorithms , that search for patterns within strings. Two famous examples are the Boyer\u2013Moore and Knuth\u2013Morris\u2013Pratt algorithms , and several algorithms based on the suffix tree data structure. Search for the maximum of a function In 1953, American statistician Jack Kiefer devised Fibonacci search which can be used to find the maximum of a unimodal\uff08\u5355\u5cf0\uff09 function and has many other applications in computer science. For quantum computers There are also search methods designed for quantum computers , like Grover's algorithm , that are theoretically faster than linear or brute-force search even without the help of data structures or heuristics. See also Backward induction Content-addressable memory hardware Dual-phase evolution Linear search problem No free lunch in search and optimization Recommender system , also use statistical methods to rank results in very large data sets Search engine (computing) Search game Selection algorithm Solver Sorting algorithm \u2013 An algorithm that arranges lists in order, necessary for executing certain search algorithms Web search engine \u2013 Software system that is designed to search for information on the World Wide Web Categories: Category:Search algorithms Guide \u901a\u8fc7\u7ef4\u57fa\u767e\u79d1 Search algorithm \uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff1a\u201csearch\u201d\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u4e3b\u9898\uff0c\u6709\u65f6\u5019\u6709\u8fd9\u6837\u7684\u611f\u89c9\uff1acomputer science\u89e3\u51b3\u5f88\u591a\u95ee\u9898\u7684\u89e3\u6cd5\u6700\u7ec8\u90fd\u53ef\u4ee5\u5f52\u5165search\u7684\u8303\u8f74\u3002 \u6b63\u56e0\u4e3asearch algorithm\u7684\u65e0\u5904\u4e0d\u5728\uff0c\u6240\u4ee5\u5bf9\u5b83\u7684\u7814\u7a76\u662f\u975e\u5e38\u6709\u5fc5\u8981\u7684\u3002 Search space \u4e3a\u4e86\u66f4\u597d\u5730\u63cf\u8ff0\u3001\u603b\u7ed3search algorithm\uff0c\u6211\u4eec\u9996\u5148\u7ad9\u5728\u4e00\u4e2a\u66f4\u9ad8\u7684\u5c42\u6b21\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u4f7f\u7528\u62bd\u8c61\u800c\u4e0d\u662f\u5177\u4f53\u3002\u6211\u4eec\u9996\u5148\u660e\u786esearch\u95ee\u9898\u7684object\uff1asearch space\u3002 Search space\u7684\u5206\u7c7b \u6709\u7740\u591a\u79cd\u5206\u7c7b\u6807\u51c6\uff1a Classification criteria Examples virtual \u53c2\u89c1 \u7ef4\u57fa\u767e\u79d1 Search algorithm # For virtual search spaces \u7ae0\u8282 physical/concrete - tree - graph \u6309\u7167\u201c\u7ed3\u6784\u5316\u601d\u7ef4\u201d\uff0c\u53ef\u77e5\uff1a\u672c\u8d28\u4e0a\u90fd\u53ef\u4ee5\u4f7f\u7528relation\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u672c\u8d28\u4e0a\u90fd\u662fdiscrete structure\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528\u76f8\u540c\u7684algorithm\u6765\u89e3\u51b3\u5b83\u4eec\u7684\u95ee\u9898\u3002 virtual search space\u548cconcrete search space\u9075\u5faa\u76f8\u540c\u7684relation\uff0c\u56e0\u6b64\uff1a \u5b83\u4eec\u5448\u73b0\u76f8\u540c\u7684structure \u53ef\u4ee5\u4f7f\u7528\u76f8\u540c\u7684algorithm\u6765\u5b9e\u73b0\u5b83\u4eec\u7684computation \u7531\u4e8e\u4eba\u4eec\u7684\u601d\u7ef4\u5f80\u5f80\u662f\u6613\u4e8e\u63a5\u53d7\u5177\u4f53\uff0c\u6240\u4ee5\u5bf9\u4e8evirtual space\u8fd9\u79cd\u6bd4\u8f83\u62bd\u8c61\u7684\uff0c\u6211\u4eec\u5f80\u5f80\u4f7f\u7528\u5177\u4f53\u4e8b\u7269\u6765\u8fdb\u884c\u523b\u753b\uff0c\u6bd4\u5982\u4f7f\u7528search tree\u6765\u63cf\u8ff0\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\uff0c\u56e0\u6b64\uff0c\u540e\u9762\u5728\u63cf\u8ff0\u5404\u79cdsearch strategy\u7684\u65f6\u5019\uff0c\u6211\u4eec\u662f\u57fa\u4e8econcrete structure\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u8fd9\u6837\u4fbf\u4e8e\u7406\u89e3\uff0c\u6240\u4ee5\u8bb2\u8fd9\u90e8\u5206\u5185\u5bb9\u90fd\u653e\u5230\u4e86 Relation-structure-computation\\Structure \u7ae0\u8282\u4e2d\u3002 Search in virtual space virtual space\u7684\u641c\u7d22\u548cconcrete structure\u7684\u641c\u7d22\u7684\u8fc7\u7a0b\u3001\u5b9e\u73b0\u65b9\u5f0f\u975e\u5e38\u7c7b\u4f3c\u3002 virtual space\u5305\u62ec\uff1a \u89e3\u7a7a\u95f4 \u72b6\u6001\u7a7a\u95f4 \u4ee5**\u7ed3\u6784\u5316\u601d\u7ef4**\u6765\u770b\uff0cvirtual space\u4e5f\u6709\u7740**\u7ed3\u6784**\uff0c\u6700\u6700\u5178\u578b\u7684\u4f8b\u5b50\u662feight-queen puzzle\uff0c\u5b83\u53ef\u4ee5\u4f7f\u7528**nesting\u5173\u7cfb**\u6765\u89e3\u91ca\uff1a\u7b2c\u4e00\u6b21\u5305\u542bn\u4e2a\u9009\u62e9\uff0c\u7b2c\u4e8c\u6b21\u5305\u542bn-1\u4e2a\u9009\u62e9\uff0c\u6240\u4ee5\u5b83\u6700\u7ec8\u5448\u73b0\u6811\u5f62\uff0c\u6240\u4ee5\u53ef\u4ee5\u4f7f\u7528**Relation-based algorithm model**\u3002 \u53c2\u89c1\uff1a \u7ed3\u6784\u5316\u601d\u7ef4\uff1a Relation-structure-computation\\index.md Relation-based algorithm model\uff1a Relation-structure-computation\\Computation\\Relation-based-algorithm-model.md nesting\u5173\u7cfb\uff1a Relation-structure-computation\\Model\\Nesting-relation-model Graph and tree search algorithms \u8fd9\u90e8\u5206\u5185\u5bb9 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Search-algorithm \u7ae0\u8282\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 Search purpose \u641c\u7d22\u76ee\u6807\uff0c\u662f\u5728search space\u4e2d\u641c\u7d22\u6240\u6709\u7684solution\uff0c\u8fd8\u662f\u641c\u7d22best solution\u3002 Search strategy \u5728 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Search-algorithm\\Methods \u4e2d\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 Optimization algorithm and search algorithm \u4f7f\u7528\u5404\u79cd\u5404\u6837\u7684 Optimization algorithm \u6765\u5b9e\u73b0 Optimization \u95ee\u9898\uff0c\u5176\u5b9e\u5176\u8fc7\u7a0b\u4e5f\u53ef\u4ee5\u770b\u505a\u662f Search algorithm \uff1asearch for best solution\uff0c\u5373\u5bfb\u627e\u6700\u4f18\u89e3\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5f80\u5f80\u91c7\u7528\u7684\u7b56\u7565\u662f\uff1a\u4e0d\u65ad\u5730\u5411\u6700\u4f18\u89e3\u903c\u8fd1\uff0c\u5173\u4e8e\u8fd9\u4e2a\u89c2\u70b9\uff0c\u5728 Relation-structure-computation\\Computation\\index.md#Iterative method \u7ae0\u8282\u4e2d\u4e5f\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 \u6bd4\u5982 Gradient descent \u5c31\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff1a\u8be5\u7b97\u6cd5\u7684\u8fc7\u7a0b\u53ef\u4ee5\u770b\u505a\u662f\u4e0d\u65ad\u5730\u5411\u76ee\u6807\u903c\u8fd1\u7684\u8fc7\u7a0b\uff0c\u4e0b\u9762\u662f\u8bf4\u660e\u6b64\u89c2\u70b9\u7684\u7d20\u6750\uff1a \u5982\u4f55\u76f4\u89c2\u5730\u89e3\u91ca backpropagation \u7b97\u6cd5\uff1f - Anonymous\u7684\u56de\u7b54 - \u77e5\u4e4e \u4e2d\u6709\u8fd9\u6837\u7684\u8bf4\u660e\uff1a \u68af\u5ea6\u4e0b\u964d\u6cd5\u9700\u8981\u7ed9\u5b9a\u4e00\u4e2a\u521d\u59cb\u70b9\uff0c\u5e76\u6c42\u51fa\u8be5\u70b9\u7684\u68af\u5ea6\u5411\u91cf\uff0c\u7136\u540e\u4ee5\u8d1f\u68af\u5ea6\u65b9\u5411\u4e3a\u641c\u7d22\u65b9\u5411\uff0c\u4ee5\u4e00\u5b9a\u7684\u6b65\u957f\u8fdb\u884c\u641c\u7d22\uff0c\u4ece\u800c\u786e\u5b9a\u4e0b\u4e00\u4e2a\u8fed\u4ee3\u70b9\uff0c\u518d\u8ba1\u7b97\u8be5\u65b0\u7684\u68af\u5ea6\u65b9\u5411\uff0c\u5982\u6b64\u91cd\u590d\u76f4\u5230cost\u6536\u655b\u3002","title":"Search-algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#search#algorithm","text":"\u5173\u4e8esearch algorithm\uff0c\u7ef4\u57fa\u767e\u79d1 Search algorithm \u603b\u7ed3\u5730\u4e0d\u9519\uff0c\u672c\u6587\u4ee5\u5b83\u4f5c\u4e3a\u5165\u95e8\uff0c\u7136\u540e\u5bf9search algorithm\u8fdb\u884c\u603b\u7ed3\uff0c\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u9700\u8981\u5173\u6ce8\u7684\u6709\uff1a \u539f\u7406 \u5b9e\u73b0\u6280\u5de7","title":"Search algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#search#algorithm_1","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5bf9search algorithm\u7684\u63cf\u8ff0\u662f\u975e\u5e38\u597d\u7684\uff0c\u5c24\u5176\u662f\u5bf9\u95ee\u9898\u7684\u5206\u7c7b In computer science , a search algorithm is any algorithm which solves the search problem , namely, to retrieve information stored within some data structure, or calculated in the search space \uff08\u53ef\u884c\u57df\u3001\u89e3\u7a7a\u95f4\uff09 of a problem domain , either with discrete or continuous values . Specific applications of search algorithms include: Problems in combinatorial optimization , such as: The vehicle routing problem , a form of shortest path problem NOTE: \u6c7d\u8f66\u8def\u5f84\u5b89\u6392\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u79cd\u201c\u6700\u77ed\u8def\u5f84\u95ee\u9898\u201d The knapsack problem : Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. The nurse scheduling problem Problems in constraint satisfaction , such as: The map coloring problem Filling in a sudoku or crossword puzzle In game theory and especially combinatorial game theory , choosing the best move to make next (such as with the minmax algorithm) Finding a combination or password from the whole set of possibilities Factoring an integer (an important problem in cryptography ) NOTE: \u5206\u89e3\u4e00\u4e2ainteger Optimizing an industrial process, such as a chemical reaction , by changing the parameters of the process (like temperature, pressure, and pH) Retrieving a record from a database Finding the maximum or minimum value in a list or array Checking to see if a given value is present in a set of values The classic search problems described above and web search are both problems in information retrieval , but are generally studied as separate subfields and are solved and evaluated differently. are generally focused on filtering and that find documents most relevant to human queries. Classic search algorithms are typically evaluated on how fast they can find a solution, and whether or not that solution is guaranteed to be optimal. Though information retrieval algorithms must be fast, the quality of ranking is more important, as is whether or not good results have been left out and bad results included. NOTE: search\u662f\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u6982\u5ff5 The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Some database structures are specially constructed to make search algorithms faster or more efficient, such as a search tree , hash map , or a database index . [ 1] [ 2] -2) NOTE: \u9009\u62e9\u5408\u9002\u7684data structure\u7ec4\u7ec7\u6570\u636e\u6765\u4f7fsearch\u7684time\u6700\u4f18 Search algorithms can be classified based on their mechanism of searching. Linear search algorithms check every record for the one associated with a target key in a linear fashion.[ 3] -3) Binary, or half interval searches , repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order.[ 4] -4) Digital search algorithms work based on the properties of digits in data structures that use numerical keys.[ 5] Finally, hashing directly maps keys to records based on a hash function .[ 6] Searches outside a linear search require that the data be sorted in some way. NOTE: \u201c Search algorithms can be classified based on their mechanism of searching\u201d\uff0c\u6211\u4eec\u5c06\u201cmechanism of searching\u201d\u79f0\u4e3a\u201cmethod of search\u201d\uff0c\u540e\u9762\u6211\u4eec\u5c06\u770b\u5230\u201cmethod of search\u201d\u662f\u975e\u5e38\u4e4b\u591a\u7684\uff0c\u7edd\u4e0d\u5c06\u5c31\u4e0a\u8ff0\u5217\u4e3e\u7684\u51e0\u79cd Algorithms are often evaluated by their computational complexity , or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O (log n ), or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space . NOTE: \u6700\u540e\u4e00\u53e5\u63d0\u53ca\u7684search space\u7684\u6982\u5ff5\u975e\u5e38\u91cd\u8981\u3002","title":"\u7ef4\u57fa\u767e\u79d1 Search algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#classes","text":"NOTE : Search algorithm \u7684\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\u5c31\u662f\u786e\u5b9a search space \uff0c\u672c\u6587\u4e2d search space \u6240\u94fe\u63a5\u7684\u6587\u7ae0\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u53ef\u884c\u57df\uff0c\u53ef\u884c\u57df\u7684\u542b\u4e49\u53ef\u80fd\u6709\u4e9b\u9650\u5236\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u7528\uff1b**search space**\u53ef\u80fd\u662fvirtual spaces\uff08\u5982backtracing\u5728\u89e3\u7a7a\u95f4\u4e2d\u641c\u7d22\uff09\u4e5f\u53ef\u80fd\u662f\u7684\u786e\u5b58\u5728\u4e00\u4e2adata structure\uff08\u5982\u4e8c\u5206\u641c\u7d22\u5728\u4e00\u4e2asorted array\u4e2d\u8fdb\u884c\u641c\u7d22\uff09\uff1b\u6b63\u5982\u4e0b\u9762\u4f1a\u8fdb\u884c\u5206\u7c7b\uff1a For virtual search spaces For sub-structures of a given structure Search for the maximum of a function","title":"Classes"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#for#virtual#search#spaces","text":"Algorithms for searching virtual spaces are used in the constraint satisfaction problem , where the goal is to find a set of value assignments to certain variables that will satisfy specific mathematical equations and inequations / equalities. They are also used when the goal is to find a variable assignment that will maximize or minimize a certain function of those variables. Algorithms for these problems include the basic brute-force search (also called \"na\u00efve\" or \"uninformed\" search), and a variety of heuristics that try to exploit partial knowledge about the structure of this space, such as linear relaxation, constraint generation, and constraint propagation . NOTE: blind search VS heuristic search An important subclass are the local search methods, that view the elements of the search space as the vertices \uff08\u70b9\uff09 of a graph, with edges defined by a set of heuristics applicable to the case; and scan the space by moving from item to item along the edges, for example according to the steepest descent or best-first criterion, or in a stochastic search . This category includes a great variety of general metaheuristic methods, such as simulated annealing , tabu search , A-teams, and genetic programming , that combine arbitrary heuristics in specific ways. This class also includes various tree search algorithms , that view the elements as vertices of a tree , and traverse that tree in some special order. Examples of the latter include the exhaustive methods such as depth-first search and breadth-first search , as well as various heuristic-based search tree pruning methods such as backtracking and branch and bound . Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these tree-search methods are guaranteed to find the exact or optimal solution, if given enough time. This is called \" completeness \". NOTE: graph search\uff08tree search\u4e5f\u5305\u62ec\u5728\u5176\u4e2d\uff0c\u56e0\u4e3atree\u662f\u4e00\u79cd\u7279\u6b8a\u7684graph\uff09\uff0c\u540e\u9762\u4f1a\u8fdb\u884c\u4e13\u95e8\u7684\u4ecb\u7ecd\u3002graph search\u662f\u975e\u5e38\u91cd\u8981\uff0c\u56e0\u4e3avirtual search space\u4e5f\u53ef\u4ee5\u8f6c\u6362\u4e3agraph search\u3002 Another important sub-class consists of algorithms for exploring the game tree of multiple-player games, such as chess or backgammon \uff08\u53cc\u9646\u68cb\uff09, whose nodes consist of all possible game situations that could result from the current situation. The goal in these problems is to find the move that provides the best chance of a win, taking into account all possible moves of the opponent(s). Similar problems occur when humans or machines have to make successive decisions whose outcomes are not entirely under one's control, such as in robot guidance or in marketing , financial , or military strategy planning. This kind of problem \u2014 combinatorial search \uff08\u7ec4\u5408\u641c\u7d22\uff09 \u2014 has been extensively studied in the context of artificial intelligence . Examples of algorithms for this class are the minimax algorithm , alpha\u2013beta pruning , * Informational search [ 7] and the A* algorithm .","title":"For virtual search spaces"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#for#sub-structures#of#a#given#structure","text":"The name \"combinatorial search\"\uff08\u7ec4\u5408\u641c\u7d22\uff09 is generally used for algorithms that look for a specific sub-structure of a given discrete structure , such as a graph, a string , a finite group , and so on. The term combinatorial optimization is typically used when the goal is to find a sub-structure with a maximum (or minimum) value of some parameter. (Since the sub-structure is usually represented in the computer by a set of integer variables with constraints, these problems can be viewed as special cases of constraint satisfaction or discrete optimization; but they are usually formulated and solved in a more abstract setting where the internal representation is not explicitly mentioned.) NOTE: \u8fd9\u8ba9\u6211\u60f3\u8d77\u6765 Optimal substructure An important and extensively studied subclass are the graph algorithms , in particular graph traversal algorithms, for finding specific sub-structures in a given graph \u2014 such as subgraphs , paths , circuits, and so on. Examples include Dijkstra's algorithm , Kruskal's algorithm , the nearest neighbour algorithm , and Prim's algorithm . Another important subclass of this category are the string searching algorithms , that search for patterns within strings. Two famous examples are the Boyer\u2013Moore and Knuth\u2013Morris\u2013Pratt algorithms , and several algorithms based on the suffix tree data structure.","title":"For sub-structures of a given structure"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#search#for#the#maximum#of#a#function","text":"In 1953, American statistician Jack Kiefer devised Fibonacci search which can be used to find the maximum of a unimodal\uff08\u5355\u5cf0\uff09 function and has many other applications in computer science.","title":"Search for the maximum of a function"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#for#quantum#computers","text":"There are also search methods designed for quantum computers , like Grover's algorithm , that are theoretically faster than linear or brute-force search even without the help of data structures or heuristics.","title":"For quantum computers"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#see#also","text":"Backward induction Content-addressable memory hardware Dual-phase evolution Linear search problem No free lunch in search and optimization Recommender system , also use statistical methods to rank results in very large data sets Search engine (computing) Search game Selection algorithm Solver Sorting algorithm \u2013 An algorithm that arranges lists in order, necessary for executing certain search algorithms Web search engine \u2013 Software system that is designed to search for information on the World Wide Web Categories: Category:Search algorithms","title":"See also"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#guide","text":"\u901a\u8fc7\u7ef4\u57fa\u767e\u79d1 Search algorithm \uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff1a\u201csearch\u201d\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u4e3b\u9898\uff0c\u6709\u65f6\u5019\u6709\u8fd9\u6837\u7684\u611f\u89c9\uff1acomputer science\u89e3\u51b3\u5f88\u591a\u95ee\u9898\u7684\u89e3\u6cd5\u6700\u7ec8\u90fd\u53ef\u4ee5\u5f52\u5165search\u7684\u8303\u8f74\u3002 \u6b63\u56e0\u4e3asearch algorithm\u7684\u65e0\u5904\u4e0d\u5728\uff0c\u6240\u4ee5\u5bf9\u5b83\u7684\u7814\u7a76\u662f\u975e\u5e38\u6709\u5fc5\u8981\u7684\u3002","title":"Guide"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#search#space","text":"\u4e3a\u4e86\u66f4\u597d\u5730\u63cf\u8ff0\u3001\u603b\u7ed3search algorithm\uff0c\u6211\u4eec\u9996\u5148\u7ad9\u5728\u4e00\u4e2a\u66f4\u9ad8\u7684\u5c42\u6b21\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u4f7f\u7528\u62bd\u8c61\u800c\u4e0d\u662f\u5177\u4f53\u3002\u6211\u4eec\u9996\u5148\u660e\u786esearch\u95ee\u9898\u7684object\uff1asearch space\u3002","title":"Search space"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#search#space_1","text":"\u6709\u7740\u591a\u79cd\u5206\u7c7b\u6807\u51c6\uff1a Classification criteria Examples virtual \u53c2\u89c1 \u7ef4\u57fa\u767e\u79d1 Search algorithm # For virtual search spaces \u7ae0\u8282 physical/concrete - tree - graph \u6309\u7167\u201c\u7ed3\u6784\u5316\u601d\u7ef4\u201d\uff0c\u53ef\u77e5\uff1a\u672c\u8d28\u4e0a\u90fd\u53ef\u4ee5\u4f7f\u7528relation\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u672c\u8d28\u4e0a\u90fd\u662fdiscrete structure\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528\u76f8\u540c\u7684algorithm\u6765\u89e3\u51b3\u5b83\u4eec\u7684\u95ee\u9898\u3002 virtual search space\u548cconcrete search space\u9075\u5faa\u76f8\u540c\u7684relation\uff0c\u56e0\u6b64\uff1a \u5b83\u4eec\u5448\u73b0\u76f8\u540c\u7684structure \u53ef\u4ee5\u4f7f\u7528\u76f8\u540c\u7684algorithm\u6765\u5b9e\u73b0\u5b83\u4eec\u7684computation \u7531\u4e8e\u4eba\u4eec\u7684\u601d\u7ef4\u5f80\u5f80\u662f\u6613\u4e8e\u63a5\u53d7\u5177\u4f53\uff0c\u6240\u4ee5\u5bf9\u4e8evirtual space\u8fd9\u79cd\u6bd4\u8f83\u62bd\u8c61\u7684\uff0c\u6211\u4eec\u5f80\u5f80\u4f7f\u7528\u5177\u4f53\u4e8b\u7269\u6765\u8fdb\u884c\u523b\u753b\uff0c\u6bd4\u5982\u4f7f\u7528search tree\u6765\u63cf\u8ff0\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\uff0c\u56e0\u6b64\uff0c\u540e\u9762\u5728\u63cf\u8ff0\u5404\u79cdsearch strategy\u7684\u65f6\u5019\uff0c\u6211\u4eec\u662f\u57fa\u4e8econcrete structure\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u8fd9\u6837\u4fbf\u4e8e\u7406\u89e3\uff0c\u6240\u4ee5\u8bb2\u8fd9\u90e8\u5206\u5185\u5bb9\u90fd\u653e\u5230\u4e86 Relation-structure-computation\\Structure \u7ae0\u8282\u4e2d\u3002","title":"Search space\u7684\u5206\u7c7b"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#search#in#virtual#space","text":"virtual space\u7684\u641c\u7d22\u548cconcrete structure\u7684\u641c\u7d22\u7684\u8fc7\u7a0b\u3001\u5b9e\u73b0\u65b9\u5f0f\u975e\u5e38\u7c7b\u4f3c\u3002 virtual space\u5305\u62ec\uff1a \u89e3\u7a7a\u95f4 \u72b6\u6001\u7a7a\u95f4 \u4ee5**\u7ed3\u6784\u5316\u601d\u7ef4**\u6765\u770b\uff0cvirtual space\u4e5f\u6709\u7740**\u7ed3\u6784**\uff0c\u6700\u6700\u5178\u578b\u7684\u4f8b\u5b50\u662feight-queen puzzle\uff0c\u5b83\u53ef\u4ee5\u4f7f\u7528**nesting\u5173\u7cfb**\u6765\u89e3\u91ca\uff1a\u7b2c\u4e00\u6b21\u5305\u542bn\u4e2a\u9009\u62e9\uff0c\u7b2c\u4e8c\u6b21\u5305\u542bn-1\u4e2a\u9009\u62e9\uff0c\u6240\u4ee5\u5b83\u6700\u7ec8\u5448\u73b0\u6811\u5f62\uff0c\u6240\u4ee5\u53ef\u4ee5\u4f7f\u7528**Relation-based algorithm model**\u3002 \u53c2\u89c1\uff1a \u7ed3\u6784\u5316\u601d\u7ef4\uff1a Relation-structure-computation\\index.md Relation-based algorithm model\uff1a Relation-structure-computation\\Computation\\Relation-based-algorithm-model.md nesting\u5173\u7cfb\uff1a Relation-structure-computation\\Model\\Nesting-relation-model","title":"Search in virtual space"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#graph#and#tree#search#algorithms","text":"\u8fd9\u90e8\u5206\u5185\u5bb9 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Search-algorithm \u7ae0\u8282\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002","title":"Graph and tree search algorithms"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#search#purpose","text":"\u641c\u7d22\u76ee\u6807\uff0c\u662f\u5728search space\u4e2d\u641c\u7d22\u6240\u6709\u7684solution\uff0c\u8fd8\u662f\u641c\u7d22best solution\u3002","title":"Search purpose"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#search#strategy","text":"\u5728 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Search-algorithm\\Methods \u4e2d\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002","title":"Search strategy"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Search-algorithm/#optimization#algorithm#and#search#algorithm","text":"\u4f7f\u7528\u5404\u79cd\u5404\u6837\u7684 Optimization algorithm \u6765\u5b9e\u73b0 Optimization \u95ee\u9898\uff0c\u5176\u5b9e\u5176\u8fc7\u7a0b\u4e5f\u53ef\u4ee5\u770b\u505a\u662f Search algorithm \uff1asearch for best solution\uff0c\u5373\u5bfb\u627e\u6700\u4f18\u89e3\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5f80\u5f80\u91c7\u7528\u7684\u7b56\u7565\u662f\uff1a\u4e0d\u65ad\u5730\u5411\u6700\u4f18\u89e3\u903c\u8fd1\uff0c\u5173\u4e8e\u8fd9\u4e2a\u89c2\u70b9\uff0c\u5728 Relation-structure-computation\\Computation\\index.md#Iterative method \u7ae0\u8282\u4e2d\u4e5f\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 \u6bd4\u5982 Gradient descent \u5c31\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff1a\u8be5\u7b97\u6cd5\u7684\u8fc7\u7a0b\u53ef\u4ee5\u770b\u505a\u662f\u4e0d\u65ad\u5730\u5411\u76ee\u6807\u903c\u8fd1\u7684\u8fc7\u7a0b\uff0c\u4e0b\u9762\u662f\u8bf4\u660e\u6b64\u89c2\u70b9\u7684\u7d20\u6750\uff1a \u5982\u4f55\u76f4\u89c2\u5730\u89e3\u91ca backpropagation \u7b97\u6cd5\uff1f - Anonymous\u7684\u56de\u7b54 - \u77e5\u4e4e \u4e2d\u6709\u8fd9\u6837\u7684\u8bf4\u660e\uff1a \u68af\u5ea6\u4e0b\u964d\u6cd5\u9700\u8981\u7ed9\u5b9a\u4e00\u4e2a\u521d\u59cb\u70b9\uff0c\u5e76\u6c42\u51fa\u8be5\u70b9\u7684\u68af\u5ea6\u5411\u91cf\uff0c\u7136\u540e\u4ee5\u8d1f\u68af\u5ea6\u65b9\u5411\u4e3a\u641c\u7d22\u65b9\u5411\uff0c\u4ee5\u4e00\u5b9a\u7684\u6b65\u957f\u8fdb\u884c\u641c\u7d22\uff0c\u4ece\u800c\u786e\u5b9a\u4e0b\u4e00\u4e2a\u8fed\u4ee3\u70b9\uff0c\u518d\u8ba1\u7b97\u8be5\u65b0\u7684\u68af\u5ea6\u65b9\u5411\uff0c\u5982\u6b64\u91cd\u590d\u76f4\u5230cost\u6536\u655b\u3002","title":"Optimization algorithm and search algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0\u5b9e\u73b0search algorithm\u7684\u5e38\u89c1Paradigm\uff1a algorithm paradigm search strategy Backtracking - Depth-first search Branch-and-Bound - Breadth-first search - Best-first search","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u5b9e\u73b0search algorithm\u7684\u5e38\u89c1Paradigm\uff1a algorithm paradigm search strategy Backtracking - Depth-first search Branch-and-Bound - Breadth-first search - Best-first search","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Compared-with-brute-force/","text":"Compared with brute-force search \u901a\u8fc7\u4e0ebrute-force search\u6765\u8fdb\u884c\u5bf9\u6bd4\u80fd\u591f\u66f4\u52a0\u6df1\u5165\u5730\u7406\u89e3backtracing\u3001Branch-and-bound\u7684\u601d\u60f3\uff1a \u603b\u7684\u6765\u8bf4\uff0c\u65e0\u8bba\u662fbacktracing\u8fd8\u662fbranch-and-bound\uff0c\u90fd\u662f\u5145\u5206\u8fd0\u7528\u5df2\u77e5\u4fe1\u606f\u6765\u8fdb\u884c\u526a\u679d\u3001\u8fdb\u884c\u4f18\u5316\u3001\u52a0\u901f\u641c\u7d22\uff0c\u8fdb\u800c\u907f\u514d\u50cf\u66b4\u529b\u641c\u7d22\u90a3\u6837\u641c\u7d22\u5b8c\u6574\u7684\u89e3\u7a7a\u95f4\u3002 \u5982\u4f55\u907f\u514d\u56de\u6eaf \u5982\u4f55\u907f\u514d\u56de\u6eaf\uff1f\u4e0b\u9762\u7ed3\u5408\u4e86\u4f8b\u5b50\u8fdb\u884c\u4e86\u8bf4\u660e\uff1a KMP\u7b97\u6cd5: https://www.cnblogs.com/dusf/p/kmp.html","title":"Compared-with-brute-force"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Compared-with-brute-force/#compared#with#brute-force#search","text":"\u901a\u8fc7\u4e0ebrute-force search\u6765\u8fdb\u884c\u5bf9\u6bd4\u80fd\u591f\u66f4\u52a0\u6df1\u5165\u5730\u7406\u89e3backtracing\u3001Branch-and-bound\u7684\u601d\u60f3\uff1a \u603b\u7684\u6765\u8bf4\uff0c\u65e0\u8bba\u662fbacktracing\u8fd8\u662fbranch-and-bound\uff0c\u90fd\u662f\u5145\u5206\u8fd0\u7528\u5df2\u77e5\u4fe1\u606f\u6765\u8fdb\u884c\u526a\u679d\u3001\u8fdb\u884c\u4f18\u5316\u3001\u52a0\u901f\u641c\u7d22\uff0c\u8fdb\u800c\u907f\u514d\u50cf\u66b4\u529b\u641c\u7d22\u90a3\u6837\u641c\u7d22\u5b8c\u6574\u7684\u89e3\u7a7a\u95f4\u3002","title":"Compared with brute-force search"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Compared-with-brute-force/#_1","text":"\u5982\u4f55\u907f\u514d\u56de\u6eaf\uff1f\u4e0b\u9762\u7ed3\u5408\u4e86\u4f8b\u5b50\u8fdb\u884c\u4e86\u8bf4\u660e\uff1a KMP\u7b97\u6cd5: https://www.cnblogs.com/dusf/p/kmp.html","title":"\u5982\u4f55\u907f\u514d\u56de\u6eaf"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/","text":"Reading list https://www.geeksforgeeks.org/top-20-backtracking-algorithm-interview-questions/ Pattern Searching \u5728\u5b57\u7b26\u4e32\uff08\u4e5f\u53eb\u4e3b\u4e32\uff09\u4e2d\u7684\u5b9a\u4f4d\u6a21\u5f0f\uff08pattern\uff09\u95ee\u9898\u53ef\u4ee5\u4f7f\u7528\u56de\u6eaf\u6cd5\u8fdb\u884c\u89e3\u51b3\uff0c\u4f46\u662f\u8fd9\u79cd\u89e3\u6cd5\u662fnaive\u7684\u3002\u4f18\u5316\u65b9\u6cd5\u662fKMP\u7b97\u6cd5\uff0c\u5728\u4e0b\u9762\u4e24\u7bc7\u6587\u7ae0\u4e2d\u5bf9\u4e24\u79cd\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff1a - \u8be6\u89e3KMP\u7b97\u6cd5 eight queens puzzle crosswords verbal arithmetic Sudoku parsing \u53c2\u89c1 Compilers: Principles, Techniques, and Tools \u76844.4 Top-Down Parsing\uff0c\u5176\u4e2d\u4ecb\u7ecd\u4e86\u4f7f\u7528backtrack\u6765\u5b9e\u73b0parsing\u3002 \u5728GitHub\u4e2d\uff0c\u53ef\u4ee5\u68c0\u7d22\u975e\u5e38\u591a\u7684\u8fd9\u79cd\u9879\u76ee\uff1a https://github.com/search?utf8=%E2%9C%93&q=backtrack+parse&type=","title":"Reading-list"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/#reading#list","text":"https://www.geeksforgeeks.org/top-20-backtracking-algorithm-interview-questions/","title":"Reading list"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/#pattern#searching","text":"\u5728\u5b57\u7b26\u4e32\uff08\u4e5f\u53eb\u4e3b\u4e32\uff09\u4e2d\u7684\u5b9a\u4f4d\u6a21\u5f0f\uff08pattern\uff09\u95ee\u9898\u53ef\u4ee5\u4f7f\u7528\u56de\u6eaf\u6cd5\u8fdb\u884c\u89e3\u51b3\uff0c\u4f46\u662f\u8fd9\u79cd\u89e3\u6cd5\u662fnaive\u7684\u3002\u4f18\u5316\u65b9\u6cd5\u662fKMP\u7b97\u6cd5\uff0c\u5728\u4e0b\u9762\u4e24\u7bc7\u6587\u7ae0\u4e2d\u5bf9\u4e24\u79cd\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff1a","title":"Pattern Searching"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/#-#kmp","text":"","title":"- \u8be6\u89e3KMP\u7b97\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/#eight#queens#puzzle","text":"","title":"eight queens puzzle"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/#crosswords","text":"","title":"crosswords"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/#verbal#arithmetic","text":"","title":"verbal arithmetic"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/#sudoku","text":"","title":"Sudoku"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Reading-list/#parsing","text":"\u53c2\u89c1 Compilers: Principles, Techniques, and Tools \u76844.4 Top-Down Parsing\uff0c\u5176\u4e2d\u4ecb\u7ecd\u4e86\u4f7f\u7528backtrack\u6765\u5b9e\u73b0parsing\u3002 \u5728GitHub\u4e2d\uff0c\u53ef\u4ee5\u68c0\u7d22\u975e\u5e38\u591a\u7684\u8fd9\u79cd\u9879\u76ee\uff1a https://github.com/search?utf8=%E2%9C%93&q=backtrack+parse&type=","title":"parsing"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/","text":"Backtrack \u4e66\u5199\u601d\u8def\uff1a \u4ecesearch algorithm\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0Backtrack\uff1a\u5bf9\u63a5\u7a7a\u95f4\u8fdb\u884cdeep-first search \u4ecerelation-based algorithmd\u7684\u89d2\u5ea6 TODO: Google: is backtracking algorithm a kind of search \u89e3 \u4e0e \u89e3\u7a7a\u95f4 \u4ece\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\u6765\u8fdb\u884c\u63cf\u8ff0\uff1a \u9010\u6b65\uff08one-by-one\uff09\u6784\u9020\u51fa\u5b8c\u6574\u7684\u89e3\uff0c\u5b8c\u6574\u89e3\u7684\u957f\u5ea6 n n \u4f7f\u7528**nesting\u5173\u7cfb**\u6765\u8fdb\u884c\u63cf\u8ff0\uff1a\u7b2c 1 1 \u6b65\u5305\u542b x_1 x_1 \u4e2a\u9009\u62e9\u3001\u7b2c 2 2 \u6b65\u5305\u542b x_2 x_2 \u4e2a\u9009\u62e9, ..., \u7b2c n n \u6b65\u5305\u542b x_n x_n \u4e2a\u9009\u62e9\uff0c\u56e0\u6b64\uff0c\u6574\u4e2a**\u89e3\u7a7a\u95f4**\u7684$size = x_1 * x_2 * \\dots * x_n $\uff0c\u540e\u9762\u6240\u6709\u7684\u4f8b\u5b50\uff0c\u90fd\u4f1a\u4f7f\u7528\u8fd9\u4e2a\u5173\u7cfb\u6765\u8fdb\u884c\u63cf\u8ff0 nesting\u5173\u7cfb -> \u89e3\u7a7a\u95f4\u5448\u73b0\u51fatree structure \u5982\u679c\u6bcf\u4e00\u6b65\u7684\u9009\u62e9\u7684\u4e2a\u6570\u76f8\u540c\uff0c\u5219\u5bf9\u5e94\u7684\u662f**\u7ec4\u5408**\uff0c\u5bf9\u5e94\u7684\u89e3\u7a7a\u95f4\u662f**\u7ec4\u5408\u6811**\uff1b\u5982\u679c\u6bcf\u4e00\u6b65\u7684\u9009\u62e9\u4e2a\u6570\u9010\u4e2a\u9012\u51cf\uff0c\u5219\u5bf9\u5e94\u7684\u662f**\u6392\u5217**\uff0c\u5bf9\u5e94\u7684\u89e3\u7a7a\u95f4\u662f**\u6392\u5217\u6811**\uff0c\u5177\u4f53\u53c2\u89c1\u300a\u7ec4\u5408\u6811 \u4e0e \u6392\u5217\u6811\u300b\u7ae0\u8282 \u89e3\u7a7a\u95f4\u662f\u4e00\u79cdvirtual space\uff0c\u89e3\u7a7a\u95f4\u548cstate space\u7c7b\u4f3c \u95ee\u9898\u7684\u89e3 X X \uff1a\u5bf9\u5e94\u7684\u662f\u89e3\u7a7a\u95f4\u6811\u7684\u4e00\u6761**\u8def\u5f84**\uff0c\u4e00\u822c\u53ef\u4ee5\u4f7f\u7528 \u52a8\u6001\u6570\u7ec4 \u6765\u5b9e\u73b0 n n \u51b3\u5b9a\u4e86**\u89e3\u7a7a\u95f4\u6811**\u7684\u6df1\u5ea6 \u89e3\u7a7a\u95f4\u7684size\u51b3\u5b9a\u4e86\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6 \u95ee\u9898\u7684\u89e3 X X \u5f71\u54cd\u4e86\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6 \u4e0a\u9762\u8fd9\u79cd\u63cf\u8ff0\u601d\u60f3\u662f\u7b26\u5408 \u7ed3\u6784\u5316\u601d\u7ef4\u7684 \u63a8\u5e7f\uff1a \u5bf9\u4e8e\u89e3\u7a7a\u95f4\u6ee1\u8db3nesting\u5173\u7cfb\u7684\u7684\u95ee\u9898\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528Backtrack\u6765\u8fdb\u884c\u6c42\u89e3\u3002 Backtrack: a kind of systematic search algorithm \u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662f\uff1a\u56de\u6eaf\u6cd5\uff0c\u4e00\u79cd\u7cfb\u7edf\u6027\u5730\u641c\u7d22\u7b97\u6cd5\u3002 \u5bf9\u89e3\u7a7a\u95f4\u6811\u8fdb\u884c\u7cfb\u7edf\u6027\u5730\u641c\u7d22 \u7cfb\u7edf\u6027\u7684\u542b\u4e49\u662f\uff1a\u6bcf\u79cd\u53ef\u80fd\u7684\u8def\u5f84\u90fd\u4f1a\u53bb\u5c1d\u8bd5\uff0c\u56e0\u4e3a\u6bcf\u4e00\u6761\u8def\u5f84\u90fd\u53ef\u80fd\u662f\u4e00\u4e2a**\u53ef\u80fd\u89e3**\u3002 \u4e00\u822c\u53ef\u4ee5\u4f7f\u7528combination\u3001permutation\u6765\u63cf\u8ff0\u4e00\u4e2a\u95ee\u9898\u7684\u6240\u6709\u53ef\u80fd\u89e3\uff0c\u5373\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\uff0c\u8fd9\u5c31\u5bf9\u5e94\u4e86combination tree\u3001permutation tree\u3002 \u90a3\u5982\u4f55\u5b9e\u73b0\u5462\uff1f deep-first order\uff08\u6df1\u5ea6\u4f18\u5148\uff09 \u8fd9\u79cd\u7b56\u7565\u7684\u4f18\u52bf\u6709\uff1a \u5c3d\u53ef\u80fd\u5730\u641c\u7d22\u5230\u4e00\u4e2a\u5b8c\u6574\u7684\u89e3 \u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1a\u53ca\u65f6\u53d1\u73b0\u65e0\u6548\u89e3\uff0c\u8fdb\u884cprune\uff08\u526a\u679d\uff09 prune wikipedia Backtracking \u5173\u4e8e\u8fd9\u4e00\u70b9\u63cf\u8ff0\u975e\u5e38\u597d\uff1a \u8fd0\u7528backtracking\u89e3\u51b3\u95ee\u9898\u7684\u5173\u952e\u662f\uff1a\u80fd\u5426\u9ad8\u6548\u3001\u53ca\u65f6\u5730prune\uff0c\u5982\u679c\u65e0\u6cd5\u8fbe\u5230\uff0c\u5219\u5b83\u65e0\u5f02\u4e8ebrute-force search\u3002\u53ca\u65f6\u8fdb\u884cprune\uff0c\u8fdb\u884c\u56de\u6eaf\uff0c\u5c1d\u8bd5\u4e0b\u4e00\u4e2a\u89e3\uff0c\u53ef\u4ee5\u5927\u5927\u52a0\u901f\u641c\u7d22\u3002 \u56de\u6eaf \u5c1d\u8bd5\u6240\u6709\u7684\u53ef\u80fd\u6027 One-by-one \u5178\u578b\u7684one-by-one computation\uff0c\u9010\u6b65\u8ba1\u7b97\u5f97\u5230\u5b8c\u6574\u89e3\uff0c\u5b8c\u6574\u89e3\u7684\u957f\u5ea6 N \u662f\u53ef\u4ee5\u63d0\u524d\u786e\u5b9a\u7684\uff0c\u7528 t \u6765\u63a7\u5236\u8ba1\u7b97\u6b65\u9aa4\uff0c\u5f53 t==N \u65f6\uff0c\u5219\u8868\u793a\u5df2\u7ecf\u8ba1\u7b97\u5f97\u5230\u4e86\u5b8c\u6574\u89e3\u3002 N \u5bf9\u5e94\u7684\u662f\u89e3\u7a7a\u95f4\u6811\u7684\u6df1\u5ea6 t \u5bf9\u5e94\u7684\u662f\u5f53\u524d\u6269\u5c55\u8282\u70b9\u7684\u6811\u6df1\u5ea6 \u641c\u7d22\u987a\u5e8f \u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff0c\u5bf9\u5e94\u7684\u662f\u6811\u7684\u5148\u5e8f\u904d\u5386 \u5728n-queue\u95ee\u9898\u4e2d\uff0c\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u601d\u8003 \u5b9e\u73b0 \u9012\u5f52\u56de\u6eaf void BackTrack ( int t ) { // \u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u89e3 if ( t > n ) { Output ( x ); } // \u89e3\u4e0d\u5b8c\u6574 else { for ( int i = f ( n , t ); i < g ( n , t ); i ++ ) { x [ t ] = h ( i ); if ( Constraint ( t ) && Bound ( t )) { BackTrack ( t + 1 ); } // \u53ca\u65f6\u526a\u679d } } } /* 1.t\u8868\u793a\u9012\u5f52\u6df1\u5ea6 2.x[]\u7528\u6765\u8bb0\u5f55\u53ef\u884c\u89e3 3.f(n, t)\u548c g(n, t)\u5206\u522b\u8868\u793a\u5f53\u524d\u5f00\u5c55\u7ed3\u70b9\u5904\uff0c\u672a\u641c\u7d22\u8fc7\u7684\u5b50\u6811\u7684\u8d77\u59cb\u7f16\u53f7\u548c\u7ec8\u6b62\u7f16\u53f7 4.h\uff08i\uff09\u8868\u793a\u5728\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u5904\uff0cx[t]\u7684\u7b2ci\u53ef\u9009\u503c */ \u8fed\u4ee3\u56de\u6eaf void IterativeBacktrack ( void ) { int t = 1 ; while ( t > 0 ) { if ( f ( n , t ) <= g ( n , t )) { for ( int i = f ( n , t ), i < g ( n , t ), i ++ ) { x [ t ] = h ( i ); if ( Constraint ( t ) && Bound ( t )) { // \u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u89e3 if ( Solution ( t )) { Output ( x ); //\u6b64\u65f6\u5df2\u7ecf\u5f97\u5230\u4e86\u5b8c\u6574\u89e3\uff0c\u5219\u53ef\u4ee5\u8fdb\u884c\u8f93\u51fa\u4e86 } // \u89e3\u4e0d\u5b8c\u6574 else { t ++ ; //\u8fd8\u6ca1\u6709\u6c42\u5f97\u5b8c\u6574\u89e3\uff0c\u5219\u5f80\u4e0b\u4e00\u5c42\u8fed\u4ee3\uff0c\u8fd9\u662f\u6df1\u5ea6\u4f18\u5148\u7684\u904d\u5386\u7b97\u6cd5 } } //\u6b64\u5904\u4f1a\u526a\u53bb\u76f8\u5e94\u7684\u5b50\u6811 } } { else t -- ; //\u8fdb\u884c\u56de\u6eaf } } } /* 1.t\u8868\u793a\u9012\u5f52\u6df1\u5ea6 2.x[]\u7528\u6765\u8bb0\u5f55\u53ef\u884c\u89e3 3.f(n, t)\u548c g(n, t)\u5206\u522b\u8868\u793a\u5f53\u524d\u5f00\u5c55\u7ed3\u70b9\u5904\uff0c\u672a\u641c\u7d22\u8fc7\u7684\u5b50\u6811\u7684\u8d77\u59cb\u7f16\u53f7\u548c\u7ec8\u6b62\u7f16\u53f7,\u6bd4\u5982m\u7740\u8272\u95ee\u9898\u4e2d\u4e3a\u56fe\u7684\u989c\u8272\u603b\u6570 4.h\uff08i\uff09\u8868\u793a\u5728\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u5904\uff0cx[t]\u7684\u7b2ci\u53ef\u9009\u503c\uff0c\u6bd4\u5982m\u7740\u8272\u4e2d\u7684\u5404\u79cd\u989c\u8272 5.Solution(t)\u5224\u65ad\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u662f\u5426\u5df2\u7ecf\u5f97\u5230\u95ee\u9898\u7684\u53ef\u884c\u89e3\uff0c\u5982\u679c\u5f97\u5230\u4e86\u5b8c\u6574\u89e3\uff0c\u5219\u7531Output(x)\u8f93\u51fa\u5b8c\u6574\u89e3\uff0c\u5426\u5219\u5728\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u5904\u5f97\u5230\u7684\u53ea\u662f\u90e8\u5206\u89e3\uff0c\u9700\u8981\u7ee7\u7eed\u5411\u7eb5\u6df1\u65b9\u5411\u7ee7\u7eed\u641c\u7d22 */ \u9012\u5f52\u56de\u6eaf VS \u8fed\u4ee3\u56de\u6eaf \u9012\u5f52\u56de\u6eaf\u5229\u7528call stack\uff0c\u5f53 Constrain(t) && Bound(t) \u4e0d\u6ee1\u8db3\u7684\u65f6\u5019\uff0c\u5b83\u4e0d\u5f80\u4e0b\u4e00\u5c42\u9012\u5f52\uff08 t=t+1 \uff09\uff0c\u51fd\u6570\u76f4\u63a5\u8fd4\u56de\uff08\u526a\u679d\uff09\uff0c\u5219call stack\u4f1a\u5f39\u51fa\u5f53\u524dframe\uff0c\u56de\u5230\u4e0a\u4e00\u5c42( t=t-1 )\uff0c\u5373**\u56de\u6eaf**\uff0c\u7136\u540e\u63a5\u7740\u5c1d\u8bd5\u5176\u4ed6\u7684\u53ef\u9009\u503c\u3002 \u8fed\u4ee3\u56de\u6eaf\uff0c\u9700\u8981\u7531programmer\u6765\u7ef4\u62a4\u5728\u5404\u5c42\u6b21\u7684\u641c\u7d22\uff0c\u76f4\u89c2\u6765\u8bf4\uff0c\u662f\u7ef4\u62a4 t \u503c\uff1a \u5f53\u8fdb\u5165\u4e0b\u4e00\u5c42\u65f6\uff1a t++ \u5f53**\u56de\u6eaf**\uff0c\u5373\u8fd4\u56de\u4e0a\u4e00\u5c42\u65f6\uff1a t-- \u5b50\u96c6\u6811 \u4e0e \u6392\u5217\u6811 \u6392\u5217\u6811\u7684\u56de\u6eaf\u6cd5\u7b97\u6cd5\u6846\u67b6 \u7528\u56de\u6eaf\u6cd5\u641c\u7d22\u6392\u5217\u6811\u7684\u7b97\u6cd5\u6846\u67b6\u53ef\u63cf\u8ff0\u5982\u4e0b\uff1a void Backtrack ( int t ) { if ( t > n ) { Output ( x ); } else { for ( int i = t ; i <= n ; i ++ ) { Swap ( x [ t ], x [ i ]); if ( Constrain ( t ) && Bound ( t ) ) { Backtrack ( t + 1 ); } Swap ( x [ t ], x [ i ]); } } } \u7ec4\u5408\u6811 \u4e0e \u6392\u5217\u6811 \u5728\u4e0b\u9762\u7ae0\u8282\u4e2d\u5bf9\u7ec4\u5408\u6811\u3001\u6392\u5217\u6811\u8fdb\u884c\u4e86\u63cf\u8ff0\uff1a Relation-structure-computation\\Structured-thinking-model.md \u4e2d\u5bf9**\u7ec4\u5408\u6811**\u3001 \u6392\u5217\u6811 \u300a\u89e3 \u4e0e \u89e3\u7a7a\u95f4\u300b \u539f\u6587\u4e2d\uff0c\u5173\u4e8e\u5b50\u96c6\u6811\u3001\u6392\u5217\u6811\u7684\u547d\u540d\uff0c\u5176\u5b9e\u662f\u6839\u636e**\u89e3\u7a7a\u95f4**\u7684\u6784\u6210\u6765\u547d\u540d\u7684\u3002 \u4e0a\u8ff0**\u5b50\u96c6\u6811**\uff0c\u5176\u5b9e\u5c5e\u4e8e**\u7ec4\u5408\u6811**\uff0c\u5e76\u4e14\u7ed3\u5408\u540e\u9762\u7684\u5f88\u591a\u4f8b\u5b50\u6765\u8bf4\uff1a\u76f8\u6bd4\u5b50\u96c6\u6811\uff0c\u7ec4\u5408\u6811\u662f\u66f4\u52a0\u80fd\u591f\u4f53\u73b0\u89e3\u7a7a\u95f4\u7684\u6784\u6210\u7684\u3002 \u5b9e\u73b0\u5bf9\u6bd4: expand \u4f8b\u5b50 \u7ec4\u5408\u6811 for(int i = 1; i <= k; i++) \u5bf9 8\u7687\u540e\u95ee\u9898\uff0c\u4e0a\u8ff0 k \u7b49\u4e8e8 \u5bf9\u88c5\u7f6e\u95ee\u9898\uff0c\u4e0a\u8ff0 k \u7b49\u4e8e2 \u6392\u5217\u6811 for(int i = t; i <= n; i++)","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#backtrack","text":"\u4e66\u5199\u601d\u8def\uff1a \u4ecesearch algorithm\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0Backtrack\uff1a\u5bf9\u63a5\u7a7a\u95f4\u8fdb\u884cdeep-first search \u4ecerelation-based algorithmd\u7684\u89d2\u5ea6 TODO: Google: is backtracking algorithm a kind of search","title":"Backtrack"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_1","text":"\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\u6765\u8fdb\u884c\u63cf\u8ff0\uff1a \u9010\u6b65\uff08one-by-one\uff09\u6784\u9020\u51fa\u5b8c\u6574\u7684\u89e3\uff0c\u5b8c\u6574\u89e3\u7684\u957f\u5ea6 n n \u4f7f\u7528**nesting\u5173\u7cfb**\u6765\u8fdb\u884c\u63cf\u8ff0\uff1a\u7b2c 1 1 \u6b65\u5305\u542b x_1 x_1 \u4e2a\u9009\u62e9\u3001\u7b2c 2 2 \u6b65\u5305\u542b x_2 x_2 \u4e2a\u9009\u62e9, ..., \u7b2c n n \u6b65\u5305\u542b x_n x_n \u4e2a\u9009\u62e9\uff0c\u56e0\u6b64\uff0c\u6574\u4e2a**\u89e3\u7a7a\u95f4**\u7684$size = x_1 * x_2 * \\dots * x_n $\uff0c\u540e\u9762\u6240\u6709\u7684\u4f8b\u5b50\uff0c\u90fd\u4f1a\u4f7f\u7528\u8fd9\u4e2a\u5173\u7cfb\u6765\u8fdb\u884c\u63cf\u8ff0 nesting\u5173\u7cfb -> \u89e3\u7a7a\u95f4\u5448\u73b0\u51fatree structure \u5982\u679c\u6bcf\u4e00\u6b65\u7684\u9009\u62e9\u7684\u4e2a\u6570\u76f8\u540c\uff0c\u5219\u5bf9\u5e94\u7684\u662f**\u7ec4\u5408**\uff0c\u5bf9\u5e94\u7684\u89e3\u7a7a\u95f4\u662f**\u7ec4\u5408\u6811**\uff1b\u5982\u679c\u6bcf\u4e00\u6b65\u7684\u9009\u62e9\u4e2a\u6570\u9010\u4e2a\u9012\u51cf\uff0c\u5219\u5bf9\u5e94\u7684\u662f**\u6392\u5217**\uff0c\u5bf9\u5e94\u7684\u89e3\u7a7a\u95f4\u662f**\u6392\u5217\u6811**\uff0c\u5177\u4f53\u53c2\u89c1\u300a\u7ec4\u5408\u6811 \u4e0e \u6392\u5217\u6811\u300b\u7ae0\u8282 \u89e3\u7a7a\u95f4\u662f\u4e00\u79cdvirtual space\uff0c\u89e3\u7a7a\u95f4\u548cstate space\u7c7b\u4f3c \u95ee\u9898\u7684\u89e3 X X \uff1a\u5bf9\u5e94\u7684\u662f\u89e3\u7a7a\u95f4\u6811\u7684\u4e00\u6761**\u8def\u5f84**\uff0c\u4e00\u822c\u53ef\u4ee5\u4f7f\u7528 \u52a8\u6001\u6570\u7ec4 \u6765\u5b9e\u73b0 n n \u51b3\u5b9a\u4e86**\u89e3\u7a7a\u95f4\u6811**\u7684\u6df1\u5ea6 \u89e3\u7a7a\u95f4\u7684size\u51b3\u5b9a\u4e86\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6 \u95ee\u9898\u7684\u89e3 X X \u5f71\u54cd\u4e86\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6 \u4e0a\u9762\u8fd9\u79cd\u63cf\u8ff0\u601d\u60f3\u662f\u7b26\u5408 \u7ed3\u6784\u5316\u601d\u7ef4\u7684 \u63a8\u5e7f\uff1a \u5bf9\u4e8e\u89e3\u7a7a\u95f4\u6ee1\u8db3nesting\u5173\u7cfb\u7684\u7684\u95ee\u9898\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528Backtrack\u6765\u8fdb\u884c\u6c42\u89e3\u3002","title":"\u89e3 \u4e0e \u89e3\u7a7a\u95f4"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#backtrack#a#kind#of#systematic#search#algorithm","text":"\u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662f\uff1a\u56de\u6eaf\u6cd5\uff0c\u4e00\u79cd\u7cfb\u7edf\u6027\u5730\u641c\u7d22\u7b97\u6cd5\u3002","title":"Backtrack: a kind of systematic search algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_2","text":"\u7cfb\u7edf\u6027\u7684\u542b\u4e49\u662f\uff1a\u6bcf\u79cd\u53ef\u80fd\u7684\u8def\u5f84\u90fd\u4f1a\u53bb\u5c1d\u8bd5\uff0c\u56e0\u4e3a\u6bcf\u4e00\u6761\u8def\u5f84\u90fd\u53ef\u80fd\u662f\u4e00\u4e2a**\u53ef\u80fd\u89e3**\u3002 \u4e00\u822c\u53ef\u4ee5\u4f7f\u7528combination\u3001permutation\u6765\u63cf\u8ff0\u4e00\u4e2a\u95ee\u9898\u7684\u6240\u6709\u53ef\u80fd\u89e3\uff0c\u5373\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\uff0c\u8fd9\u5c31\u5bf9\u5e94\u4e86combination tree\u3001permutation tree\u3002 \u90a3\u5982\u4f55\u5b9e\u73b0\u5462\uff1f deep-first order\uff08\u6df1\u5ea6\u4f18\u5148\uff09 \u8fd9\u79cd\u7b56\u7565\u7684\u4f18\u52bf\u6709\uff1a \u5c3d\u53ef\u80fd\u5730\u641c\u7d22\u5230\u4e00\u4e2a\u5b8c\u6574\u7684\u89e3 \u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff1a\u53ca\u65f6\u53d1\u73b0\u65e0\u6548\u89e3\uff0c\u8fdb\u884cprune\uff08\u526a\u679d\uff09 prune wikipedia Backtracking \u5173\u4e8e\u8fd9\u4e00\u70b9\u63cf\u8ff0\u975e\u5e38\u597d\uff1a \u8fd0\u7528backtracking\u89e3\u51b3\u95ee\u9898\u7684\u5173\u952e\u662f\uff1a\u80fd\u5426\u9ad8\u6548\u3001\u53ca\u65f6\u5730prune\uff0c\u5982\u679c\u65e0\u6cd5\u8fbe\u5230\uff0c\u5219\u5b83\u65e0\u5f02\u4e8ebrute-force search\u3002\u53ca\u65f6\u8fdb\u884cprune\uff0c\u8fdb\u884c\u56de\u6eaf\uff0c\u5c1d\u8bd5\u4e0b\u4e00\u4e2a\u89e3\uff0c\u53ef\u4ee5\u5927\u5927\u52a0\u901f\u641c\u7d22\u3002 \u56de\u6eaf \u5c1d\u8bd5\u6240\u6709\u7684\u53ef\u80fd\u6027","title":"\u5bf9\u89e3\u7a7a\u95f4\u6811\u8fdb\u884c\u7cfb\u7edf\u6027\u5730\u641c\u7d22"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#one-by-one","text":"\u5178\u578b\u7684one-by-one computation\uff0c\u9010\u6b65\u8ba1\u7b97\u5f97\u5230\u5b8c\u6574\u89e3\uff0c\u5b8c\u6574\u89e3\u7684\u957f\u5ea6 N \u662f\u53ef\u4ee5\u63d0\u524d\u786e\u5b9a\u7684\uff0c\u7528 t \u6765\u63a7\u5236\u8ba1\u7b97\u6b65\u9aa4\uff0c\u5f53 t==N \u65f6\uff0c\u5219\u8868\u793a\u5df2\u7ecf\u8ba1\u7b97\u5f97\u5230\u4e86\u5b8c\u6574\u89e3\u3002 N \u5bf9\u5e94\u7684\u662f\u89e3\u7a7a\u95f4\u6811\u7684\u6df1\u5ea6 t \u5bf9\u5e94\u7684\u662f\u5f53\u524d\u6269\u5c55\u8282\u70b9\u7684\u6811\u6df1\u5ea6","title":"One-by-one"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_3","text":"\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff0c\u5bf9\u5e94\u7684\u662f\u6811\u7684\u5148\u5e8f\u904d\u5386 \u5728n-queue\u95ee\u9898\u4e2d\uff0c\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u601d\u8003","title":"\u641c\u7d22\u987a\u5e8f"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_4","text":"","title":"\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_5","text":"void BackTrack ( int t ) { // \u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u89e3 if ( t > n ) { Output ( x ); } // \u89e3\u4e0d\u5b8c\u6574 else { for ( int i = f ( n , t ); i < g ( n , t ); i ++ ) { x [ t ] = h ( i ); if ( Constraint ( t ) && Bound ( t )) { BackTrack ( t + 1 ); } // \u53ca\u65f6\u526a\u679d } } } /* 1.t\u8868\u793a\u9012\u5f52\u6df1\u5ea6 2.x[]\u7528\u6765\u8bb0\u5f55\u53ef\u884c\u89e3 3.f(n, t)\u548c g(n, t)\u5206\u522b\u8868\u793a\u5f53\u524d\u5f00\u5c55\u7ed3\u70b9\u5904\uff0c\u672a\u641c\u7d22\u8fc7\u7684\u5b50\u6811\u7684\u8d77\u59cb\u7f16\u53f7\u548c\u7ec8\u6b62\u7f16\u53f7 4.h\uff08i\uff09\u8868\u793a\u5728\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u5904\uff0cx[t]\u7684\u7b2ci\u53ef\u9009\u503c */","title":"\u9012\u5f52\u56de\u6eaf"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_6","text":"void IterativeBacktrack ( void ) { int t = 1 ; while ( t > 0 ) { if ( f ( n , t ) <= g ( n , t )) { for ( int i = f ( n , t ), i < g ( n , t ), i ++ ) { x [ t ] = h ( i ); if ( Constraint ( t ) && Bound ( t )) { // \u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u89e3 if ( Solution ( t )) { Output ( x ); //\u6b64\u65f6\u5df2\u7ecf\u5f97\u5230\u4e86\u5b8c\u6574\u89e3\uff0c\u5219\u53ef\u4ee5\u8fdb\u884c\u8f93\u51fa\u4e86 } // \u89e3\u4e0d\u5b8c\u6574 else { t ++ ; //\u8fd8\u6ca1\u6709\u6c42\u5f97\u5b8c\u6574\u89e3\uff0c\u5219\u5f80\u4e0b\u4e00\u5c42\u8fed\u4ee3\uff0c\u8fd9\u662f\u6df1\u5ea6\u4f18\u5148\u7684\u904d\u5386\u7b97\u6cd5 } } //\u6b64\u5904\u4f1a\u526a\u53bb\u76f8\u5e94\u7684\u5b50\u6811 } } { else t -- ; //\u8fdb\u884c\u56de\u6eaf } } } /* 1.t\u8868\u793a\u9012\u5f52\u6df1\u5ea6 2.x[]\u7528\u6765\u8bb0\u5f55\u53ef\u884c\u89e3 3.f(n, t)\u548c g(n, t)\u5206\u522b\u8868\u793a\u5f53\u524d\u5f00\u5c55\u7ed3\u70b9\u5904\uff0c\u672a\u641c\u7d22\u8fc7\u7684\u5b50\u6811\u7684\u8d77\u59cb\u7f16\u53f7\u548c\u7ec8\u6b62\u7f16\u53f7,\u6bd4\u5982m\u7740\u8272\u95ee\u9898\u4e2d\u4e3a\u56fe\u7684\u989c\u8272\u603b\u6570 4.h\uff08i\uff09\u8868\u793a\u5728\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u5904\uff0cx[t]\u7684\u7b2ci\u53ef\u9009\u503c\uff0c\u6bd4\u5982m\u7740\u8272\u4e2d\u7684\u5404\u79cd\u989c\u8272 5.Solution(t)\u5224\u65ad\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u662f\u5426\u5df2\u7ecf\u5f97\u5230\u95ee\u9898\u7684\u53ef\u884c\u89e3\uff0c\u5982\u679c\u5f97\u5230\u4e86\u5b8c\u6574\u89e3\uff0c\u5219\u7531Output(x)\u8f93\u51fa\u5b8c\u6574\u89e3\uff0c\u5426\u5219\u5728\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u5904\u5f97\u5230\u7684\u53ea\u662f\u90e8\u5206\u89e3\uff0c\u9700\u8981\u7ee7\u7eed\u5411\u7eb5\u6df1\u65b9\u5411\u7ee7\u7eed\u641c\u7d22 */","title":"\u8fed\u4ee3\u56de\u6eaf"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#vs","text":"\u9012\u5f52\u56de\u6eaf\u5229\u7528call stack\uff0c\u5f53 Constrain(t) && Bound(t) \u4e0d\u6ee1\u8db3\u7684\u65f6\u5019\uff0c\u5b83\u4e0d\u5f80\u4e0b\u4e00\u5c42\u9012\u5f52\uff08 t=t+1 \uff09\uff0c\u51fd\u6570\u76f4\u63a5\u8fd4\u56de\uff08\u526a\u679d\uff09\uff0c\u5219call stack\u4f1a\u5f39\u51fa\u5f53\u524dframe\uff0c\u56de\u5230\u4e0a\u4e00\u5c42( t=t-1 )\uff0c\u5373**\u56de\u6eaf**\uff0c\u7136\u540e\u63a5\u7740\u5c1d\u8bd5\u5176\u4ed6\u7684\u53ef\u9009\u503c\u3002 \u8fed\u4ee3\u56de\u6eaf\uff0c\u9700\u8981\u7531programmer\u6765\u7ef4\u62a4\u5728\u5404\u5c42\u6b21\u7684\u641c\u7d22\uff0c\u76f4\u89c2\u6765\u8bf4\uff0c\u662f\u7ef4\u62a4 t \u503c\uff1a \u5f53\u8fdb\u5165\u4e0b\u4e00\u5c42\u65f6\uff1a t++ \u5f53**\u56de\u6eaf**\uff0c\u5373\u8fd4\u56de\u4e0a\u4e00\u5c42\u65f6\uff1a t--","title":"\u9012\u5f52\u56de\u6eaf VS \u8fed\u4ee3\u56de\u6eaf"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_7","text":"","title":"\u5b50\u96c6\u6811 \u4e0e \u6392\u5217\u6811"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_8","text":"\u7528\u56de\u6eaf\u6cd5\u641c\u7d22\u6392\u5217\u6811\u7684\u7b97\u6cd5\u6846\u67b6\u53ef\u63cf\u8ff0\u5982\u4e0b\uff1a void Backtrack ( int t ) { if ( t > n ) { Output ( x ); } else { for ( int i = t ; i <= n ; i ++ ) { Swap ( x [ t ], x [ i ]); if ( Constrain ( t ) && Bound ( t ) ) { Backtrack ( t + 1 ); } Swap ( x [ t ], x [ i ]); } } }","title":"\u6392\u5217\u6811\u7684\u56de\u6eaf\u6cd5\u7b97\u6cd5\u6846\u67b6"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/Backtrack/#_9","text":"\u5728\u4e0b\u9762\u7ae0\u8282\u4e2d\u5bf9\u7ec4\u5408\u6811\u3001\u6392\u5217\u6811\u8fdb\u884c\u4e86\u63cf\u8ff0\uff1a Relation-structure-computation\\Structured-thinking-model.md \u4e2d\u5bf9**\u7ec4\u5408\u6811**\u3001 \u6392\u5217\u6811 \u300a\u89e3 \u4e0e \u89e3\u7a7a\u95f4\u300b \u539f\u6587\u4e2d\uff0c\u5173\u4e8e\u5b50\u96c6\u6811\u3001\u6392\u5217\u6811\u7684\u547d\u540d\uff0c\u5176\u5b9e\u662f\u6839\u636e**\u89e3\u7a7a\u95f4**\u7684\u6784\u6210\u6765\u547d\u540d\u7684\u3002 \u4e0a\u8ff0**\u5b50\u96c6\u6811**\uff0c\u5176\u5b9e\u5c5e\u4e8e**\u7ec4\u5408\u6811**\uff0c\u5e76\u4e14\u7ed3\u5408\u540e\u9762\u7684\u5f88\u591a\u4f8b\u5b50\u6765\u8bf4\uff1a\u76f8\u6bd4\u5b50\u96c6\u6811\uff0c\u7ec4\u5408\u6811\u662f\u66f4\u52a0\u80fd\u591f\u4f53\u73b0\u89e3\u7a7a\u95f4\u7684\u6784\u6210\u7684\u3002 \u5b9e\u73b0\u5bf9\u6bd4: expand \u4f8b\u5b50 \u7ec4\u5408\u6811 for(int i = 1; i <= k; i++) \u5bf9 8\u7687\u540e\u95ee\u9898\uff0c\u4e0a\u8ff0 k \u7b49\u4e8e8 \u5bf9\u88c5\u7f6e\u95ee\u9898\uff0c\u4e0a\u8ff0 k \u7b49\u4e8e2 \u6392\u5217\u6811 for(int i = t; i <= n; i++)","title":"\u7ec4\u5408\u6811 \u4e0e \u6392\u5217\u6811"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/wikipedia-Backtracking/","text":"Backtracking \u201cbacktracking\u201d\u5373\u201c\u56de\u6eaf\u201d\u3002 wikipedia Backtracking Backtracking is a general algorithm for finding all (or some) solutions to some computational problems , notably constraint satisfaction problems , that incrementally builds candidates to the solutions, and abandons a candidate (\"backtracks\") as soon as it determines that the candidate cannot possibly be completed to a valid solution.[ 1] [ 2] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\uff0c\u5c31\u70b9\u660e\u4e86backtrack\u7684\u542b\u4e49\u6240\u5728\uff0c\u975e\u5e38\u7cbe\u51c6\u3002 Backtracking can be applied only for problems which admit the concept of a \"partial candidate solution\" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate\uff08\u6d88\u9664\uff09 many candidates with a single test. NOTE : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u8bf4\u660e\u4e86\u53ef\u4ee5\u4f7f\u7528backtrack\u89e3\u51b3\u7684\u95ee\u9898 Backtracking VS brute force enumeration Backtracking is an important tool for solving constraint satisfaction problems ,[ 3] such as crosswords , verbal arithmetic , Sudoku , and many other puzzles. It is often the most convenient (if not the most efficient[ citation needed ]) technique for parsing ,[ 4] for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon , Planner and Prolog . Backtracking depends on user-given \" black box procedures \" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic \uff08\u5143\u542f\u53d1\uff09rather than a specific algorithm \u2013 although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time. NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cbacktracking\u662f\u4e00\u79cd\u7b97\u6cd5\u6846\u67b6\uff0c\u6216\u8005\u8bf4\u662f\u4e00\u79cd\u7b97\u6cd5\u6280\u672f\uff0c\u800c\u4e0d\u662f\u4e00\u79cd\u4e13\u7528\u7684\u7b97\u6cd5\u3002 Description of the method The backtracking algorithm enumerates a set of partial candidates that, in principle, could be completed in various ways to give all the possible solutions to the given problem. The completion is done incrementally, by a sequence of candidate extension steps. Conceptually, the partial candidates are represented as the nodes of a tree structure , the potential search tree. Each partial candidate is the parent of the candidates that differ from it by a single extension step ; the leaves of the tree are the partial candidates that cannot be extended any further. The backtracking algorithm traverses this search tree recursively , from the root down, in depth-first order . At each node c , the algorithm checks whether c can be completed to a valid solution. If it cannot, the whole sub-tree rooted at c is skipped ( pruned \uff08\u526a\u679d\uff09). Otherwise, the algorithm (1) checks whether c itself is a valid solution, and if so reports it to the user; and (2) recursively enumerates all sub-trees of c . The two tests and the children of each node are defined by user-given procedures. Therefore, the actual search tree that is traversed by the algorithm is only a part of the potential tree. The total cost of the algorithm is the number of nodes of the actual tree times the cost of obtaining and processing each node. This fact should be considered when choosing the potential search tree and implementing the pruning test. Pseudocode In order to apply backtracking to a specific class of problems, one must provide the data P for the particular instance of the problem that is to be solved, and six procedural parameters , root , reject , accept , first , next , and output . These procedures should take the instance data P as a parameter and should do the following: root ( P ): return the partial candidate at the root of the search tree. reject ( P , c ): return true only if the partial candidate c is not worth completing. accept ( P , c ): return true if c is a solution of P , and false otherwise. first ( P , c ): generate the first extension of candidate c . next ( P , s ): generate the next alternative extension of a candidate, after the extension s . output ( P , c ): use the solution c of P , as appropriate to the application. The backtracking algorithm reduces the problem to the call bt ( root ( P )), where bt is the following recursive procedure: procedure bt(c) if reject(P,c) then return if accept(P,c) then output(P,c) s \u2190 first(P,c) while s \u2260 NULL do bt(s) s \u2190 next(P,s) Usage considerations The reject procedure should be a boolean-valued function that returns true only if it is certain that no possible extension of c is a valid solution for P . If the procedure cannot reach a definite conclusion, it should return false . An incorrect true result may cause the bt procedure to miss some valid solutions. The procedure may assume that reject ( P , t ) returned false for every ancestor t of c in the search tree. On the other hand, the efficiency of the backtracking algorithm depends on reject returning true for candidates that are as close to the root as possible. If reject always returns false , the algorithm will still find all solutions, but it will be equivalent to a brute-force search. The accept procedure should return true if c is a complete and valid solution for the problem instance P , and false otherwise. It may assume that the partial candidate c and all its ancestors in the tree have passed the *reject*test. The general pseudo-code above does not assume that the valid solutions are always leaves of the potential search tree. In other words, it admits the possibility that a valid solution for P can be further extended to yield other valid solutions. The first and next procedures are used by the backtracking algorithm to enumerate the children of a node c of the tree, that is, the candidates that differ from c by a single extension step. The call first ( P , c ) should yield the first child of c , in some order; and the call next ( P , s ) should return the next sibling of node s , in that order. Both functions should return a distinctive \"NULL\" candidate, if the requested child does not exist. Together, the root , first , and next functions define the set of partial candidates and the potential search tree. They should be chosen so that every solution of P occurs somewhere in the tree, and no partial candidate occurs more than once. Moreover, they should admit an efficient and effective reject predicate. Examples Examples where backtracking can be used to solve puzzles or problems include: Puzzles such as eight queens puzzle , crosswords , verbal arithmetic , Sudoku [ nb 1] , and Peg Solitaire . Combinatorial optimization problems such as parsing and the knapsack problem . Logic programming languages such as Icon , Planner and Prolog , which use backtracking internally to generate answers. The following is an example where backtracking is used for the constraint satisfaction problem : Constraint satisfaction The general constraint satisfaction problem consists in finding a list of integers x = ( x [1], x [2], \u2026, x [ n ]), each in some range {1, 2, \u2026, m }, that satisfies some arbitrary constraint (boolean function) F . For this class of problems, the instance data P would be the integers m and n , and the predicate F . In a typical backtracking solution to this problem, one could define a partial candidate as a list of integers c = ( c [1], c [2], \u2026, c [k]), for any k between 0 and n , that are to be assigned to the first k variables x [1], x [2], \u2026, x [ k ]. The root candidate would then be the empty list (). The first and next procedures would then be function first(P, c) k \u2190 length(c) if k = n then return NULL else return (c[1], c[2], \u2026, c[k], 1) function next(P, s) k \u2190 length(s) if s[k] = m then return NULL else return (s[1], s[2], \u2026, s[k - 1], 1 + s[k]) Here length ( c ) is the number of elements in the list c . The call reject ( P , c ) should return true if the constraint F cannot be satisfied by any list of n integers that begins with the k elements of c . For backtracking to be effective, there must be a way to detect this situation, at least for some candidates c , without enumerating all those m**n \u2212 k n -tuples. For example, if F is the conjunction of several boolean predicates, F = F [1] \u2227 F [2] \u2227 \u2026 \u2227 F [ p ], and each F [ i ] depends only on a small subset of the variables x [1], \u2026, x [ n ], then the reject procedure could simply check the terms F [ i ] that depend only on variables x [1], \u2026, x [ k ], and return true if any of those terms returns false . In fact, reject needs only check those terms that do depend on x [ k ], since the terms that depend only on x [1], \u2026, x [ k \u2212 1] will have been tested further up in the search tree. Assuming that reject is implemented as above, then accept ( P , c ) needs only check whether c is complete, that is, whether it has n elements. It is generally better to order the list of variables so that it begins with the most critical ones (i.e. the ones with fewest value options, or which have a greater impact on subsequent choices). One could also allow the next function to choose which variable should be assigned when extending a partial candidate, based on the values of the variables already assigned by it. Further improvements can be obtained by the technique of constraint propagation . In addition to retaining minimal recovery values used in backing up, backtracking implementations commonly keep a variable trail, to record value change history. An efficient implementation will avoid creating a variable trail entry between two successive changes when there is no choice point, as the backtracking will erase all of the changes as a single operation. An alternative to the variable trail is to keep a timestamp of when the last change was made to the variable. The timestamp is compared to the timestamp of a choice point. If the choice point has an associated time later than that of the variable, it is unnecessary to revert the variable when the choice point is backtracked, as it was changed before the choice point occurred.","title":"wikipedia-Backtracking"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/wikipedia-Backtracking/#backtracking","text":"\u201cbacktracking\u201d\u5373\u201c\u56de\u6eaf\u201d\u3002","title":"Backtracking"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/wikipedia-Backtracking/#wikipedia#backtracking","text":"Backtracking is a general algorithm for finding all (or some) solutions to some computational problems , notably constraint satisfaction problems , that incrementally builds candidates to the solutions, and abandons a candidate (\"backtracks\") as soon as it determines that the candidate cannot possibly be completed to a valid solution.[ 1] [ 2] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\uff0c\u5c31\u70b9\u660e\u4e86backtrack\u7684\u542b\u4e49\u6240\u5728\uff0c\u975e\u5e38\u7cbe\u51c6\u3002 Backtracking can be applied only for problems which admit the concept of a \"partial candidate solution\" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate\uff08\u6d88\u9664\uff09 many candidates with a single test. NOTE : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u8bf4\u660e\u4e86\u53ef\u4ee5\u4f7f\u7528backtrack\u89e3\u51b3\u7684\u95ee\u9898 Backtracking VS brute force enumeration Backtracking is an important tool for solving constraint satisfaction problems ,[ 3] such as crosswords , verbal arithmetic , Sudoku , and many other puzzles. It is often the most convenient (if not the most efficient[ citation needed ]) technique for parsing ,[ 4] for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon , Planner and Prolog . Backtracking depends on user-given \" black box procedures \" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic \uff08\u5143\u542f\u53d1\uff09rather than a specific algorithm \u2013 although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time. NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cbacktracking\u662f\u4e00\u79cd\u7b97\u6cd5\u6846\u67b6\uff0c\u6216\u8005\u8bf4\u662f\u4e00\u79cd\u7b97\u6cd5\u6280\u672f\uff0c\u800c\u4e0d\u662f\u4e00\u79cd\u4e13\u7528\u7684\u7b97\u6cd5\u3002","title":"wikipedia Backtracking"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/wikipedia-Backtracking/#description#of#the#method","text":"The backtracking algorithm enumerates a set of partial candidates that, in principle, could be completed in various ways to give all the possible solutions to the given problem. The completion is done incrementally, by a sequence of candidate extension steps. Conceptually, the partial candidates are represented as the nodes of a tree structure , the potential search tree. Each partial candidate is the parent of the candidates that differ from it by a single extension step ; the leaves of the tree are the partial candidates that cannot be extended any further. The backtracking algorithm traverses this search tree recursively , from the root down, in depth-first order . At each node c , the algorithm checks whether c can be completed to a valid solution. If it cannot, the whole sub-tree rooted at c is skipped ( pruned \uff08\u526a\u679d\uff09). Otherwise, the algorithm (1) checks whether c itself is a valid solution, and if so reports it to the user; and (2) recursively enumerates all sub-trees of c . The two tests and the children of each node are defined by user-given procedures. Therefore, the actual search tree that is traversed by the algorithm is only a part of the potential tree. The total cost of the algorithm is the number of nodes of the actual tree times the cost of obtaining and processing each node. This fact should be considered when choosing the potential search tree and implementing the pruning test.","title":"Description of the method"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/wikipedia-Backtracking/#pseudocode","text":"In order to apply backtracking to a specific class of problems, one must provide the data P for the particular instance of the problem that is to be solved, and six procedural parameters , root , reject , accept , first , next , and output . These procedures should take the instance data P as a parameter and should do the following: root ( P ): return the partial candidate at the root of the search tree. reject ( P , c ): return true only if the partial candidate c is not worth completing. accept ( P , c ): return true if c is a solution of P , and false otherwise. first ( P , c ): generate the first extension of candidate c . next ( P , s ): generate the next alternative extension of a candidate, after the extension s . output ( P , c ): use the solution c of P , as appropriate to the application. The backtracking algorithm reduces the problem to the call bt ( root ( P )), where bt is the following recursive procedure: procedure bt(c) if reject(P,c) then return if accept(P,c) then output(P,c) s \u2190 first(P,c) while s \u2260 NULL do bt(s) s \u2190 next(P,s)","title":"Pseudocode"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/wikipedia-Backtracking/#usage#considerations","text":"The reject procedure should be a boolean-valued function that returns true only if it is certain that no possible extension of c is a valid solution for P . If the procedure cannot reach a definite conclusion, it should return false . An incorrect true result may cause the bt procedure to miss some valid solutions. The procedure may assume that reject ( P , t ) returned false for every ancestor t of c in the search tree. On the other hand, the efficiency of the backtracking algorithm depends on reject returning true for candidates that are as close to the root as possible. If reject always returns false , the algorithm will still find all solutions, but it will be equivalent to a brute-force search. The accept procedure should return true if c is a complete and valid solution for the problem instance P , and false otherwise. It may assume that the partial candidate c and all its ancestors in the tree have passed the *reject*test. The general pseudo-code above does not assume that the valid solutions are always leaves of the potential search tree. In other words, it admits the possibility that a valid solution for P can be further extended to yield other valid solutions. The first and next procedures are used by the backtracking algorithm to enumerate the children of a node c of the tree, that is, the candidates that differ from c by a single extension step. The call first ( P , c ) should yield the first child of c , in some order; and the call next ( P , s ) should return the next sibling of node s , in that order. Both functions should return a distinctive \"NULL\" candidate, if the requested child does not exist. Together, the root , first , and next functions define the set of partial candidates and the potential search tree. They should be chosen so that every solution of P occurs somewhere in the tree, and no partial candidate occurs more than once. Moreover, they should admit an efficient and effective reject predicate.","title":"Usage considerations"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/wikipedia-Backtracking/#examples","text":"Examples where backtracking can be used to solve puzzles or problems include: Puzzles such as eight queens puzzle , crosswords , verbal arithmetic , Sudoku [ nb 1] , and Peg Solitaire . Combinatorial optimization problems such as parsing and the knapsack problem . Logic programming languages such as Icon , Planner and Prolog , which use backtracking internally to generate answers. The following is an example where backtracking is used for the constraint satisfaction problem :","title":"Examples"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Backtrack/wikipedia-Backtracking/#constraint#satisfaction","text":"The general constraint satisfaction problem consists in finding a list of integers x = ( x [1], x [2], \u2026, x [ n ]), each in some range {1, 2, \u2026, m }, that satisfies some arbitrary constraint (boolean function) F . For this class of problems, the instance data P would be the integers m and n , and the predicate F . In a typical backtracking solution to this problem, one could define a partial candidate as a list of integers c = ( c [1], c [2], \u2026, c [k]), for any k between 0 and n , that are to be assigned to the first k variables x [1], x [2], \u2026, x [ k ]. The root candidate would then be the empty list (). The first and next procedures would then be function first(P, c) k \u2190 length(c) if k = n then return NULL else return (c[1], c[2], \u2026, c[k], 1) function next(P, s) k \u2190 length(s) if s[k] = m then return NULL else return (s[1], s[2], \u2026, s[k - 1], 1 + s[k]) Here length ( c ) is the number of elements in the list c . The call reject ( P , c ) should return true if the constraint F cannot be satisfied by any list of n integers that begins with the k elements of c . For backtracking to be effective, there must be a way to detect this situation, at least for some candidates c , without enumerating all those m**n \u2212 k n -tuples. For example, if F is the conjunction of several boolean predicates, F = F [1] \u2227 F [2] \u2227 \u2026 \u2227 F [ p ], and each F [ i ] depends only on a small subset of the variables x [1], \u2026, x [ n ], then the reject procedure could simply check the terms F [ i ] that depend only on variables x [1], \u2026, x [ k ], and return true if any of those terms returns false . In fact, reject needs only check those terms that do depend on x [ k ], since the terms that depend only on x [1], \u2026, x [ k \u2212 1] will have been tested further up in the search tree. Assuming that reject is implemented as above, then accept ( P , c ) needs only check whether c is complete, that is, whether it has n elements. It is generally better to order the list of variables so that it begins with the most critical ones (i.e. the ones with fewest value options, or which have a greater impact on subsequent choices). One could also allow the next function to choose which variable should be assigned when extending a partial candidate, based on the values of the variables already assigned by it. Further improvements can be obtained by the technique of constraint propagation . In addition to retaining minimal recovery values used in backing up, backtracking implementations commonly keep a variable trail, to record value change history. An efficient implementation will avoid creating a variable trail entry between two successive changes when there is no choice point, as the backtracking will erase all of the changes as a single operation. An alternative to the variable trail is to keep a timestamp of when the last change was made to the variable. The timestamp is compared to the timestamp of a choice point. If the choice point has an associated time later than that of the variable, it is unnecessary to revert the variable when the choice point is backtracked, as it was changed before the choice point occurred.","title":"Constraint satisfaction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u7ed9\u51fabacktracking\u7b97\u6cd5\u7684\u4f8b\u5b50\u3002 \u4f8b\u5b50 \u89e3\u7a7a\u95f4 \u8bf4\u660e 5.2\u88c5\u8f7d\u95ee\u9898 \u7ec4\u5408\u6811 \u6bcf\u4e2a\u96c6\u88c5\u7bb1\u6709\u4e24\u79cd\u9009\u62e9\uff1a\u88c5\u5165\u82391\u6216\u8005\u82392 5.3\u6279\u5904\u7406\u4f5c\u4e1a\u8c03\u5ea6 \u7ec4\u5408\u6811 \u6bcf\u4e2a\u4f5c\u4e1a\u6709\u4e24\u79cd\u9009\u62e9\uff1a\u4f7f\u7528\u673a\u56681\u6216\u8005\u673a\u56682 5.4\u7b26\u53f7\u4e09\u89d2\u5f62\u95ee\u9898 \u7ec4\u5408\u6811 \u7b2c\u4e00\u6392\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u6709\u4e24\u4e2a\u9009\u62e9\uff1a\u653e\u7f6e\u6b63\u53f7\u6216\u8005\u8d1f\u53f7 5.5n\u540e\u95ee\u9898 \u7ec4\u5408\u6811 \u6bcf\u4e00\u884c\u6709n\u4e2a\u9009\u62e9\uff1an\u4e2a\u7687\u540e\u4e4b\u4e00 5.6 0-1\u80cc\u5305\u95ee\u9898 \u7ec4\u5408\u6811 \u6bcf\u4e2a\u80cc\u5305\u6709\u4e24\u4e2a\u9009\u62e9\uff1a\u88c5\u5165\u6216\u8005\u4e0d\u88c5\u5165 5.8\u56fe\u7684m\u7740\u8272\u95ee\u9898 \u7ec4\u5408\u6811 \u6bcf\u4e2a\u70b9\u6709m\u79cd\u9009\u62e9\uff1am\u79cd\u989c\u8272\u4e4b\u4e00 5.10 \u5706\u6392\u5217\u95ee\u9898 \u6392\u5217\u6811 \u975e\u5e38\u5178\u578b\u7684\u6392\u5217\u95ee\u9898\uff1a\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u6709n\u79cd\u9009\u62e9\uff0c\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e\u6709n-1\u79cd\u9009\u62e9\uff0c......\uff0c\u6700\u540e\u4e00\u4e2a\u4f4d\u7f6e\u53ea\u6709\u4e00\u4e2a\u9009\u62e9 5.11\u7535\u8def\u677f\u6392\u5217\u95ee\u9898 \u6392\u5217\u6811 \u975e\u5e38\u5178\u578b\u7684\u6392\u5217\u95ee\u9898 5.9\u65c5\u884c\u552e\u8d27\u5458\u95ee\u9898 \u6392\u5217\u6811 \u4e0d\u662f\u975e\u5e38\u5178\u578b\u7684\u6392\u5217\u6811\u95ee\u9898","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/#_1","text":"\u672c\u7ae0\u7ed9\u51fabacktracking\u7b97\u6cd5\u7684\u4f8b\u5b50\u3002 \u4f8b\u5b50 \u89e3\u7a7a\u95f4 \u8bf4\u660e 5.2\u88c5\u8f7d\u95ee\u9898 \u7ec4\u5408\u6811 \u6bcf\u4e2a\u96c6\u88c5\u7bb1\u6709\u4e24\u79cd\u9009\u62e9\uff1a\u88c5\u5165\u82391\u6216\u8005\u82392 5.3\u6279\u5904\u7406\u4f5c\u4e1a\u8c03\u5ea6 \u7ec4\u5408\u6811 \u6bcf\u4e2a\u4f5c\u4e1a\u6709\u4e24\u79cd\u9009\u62e9\uff1a\u4f7f\u7528\u673a\u56681\u6216\u8005\u673a\u56682 5.4\u7b26\u53f7\u4e09\u89d2\u5f62\u95ee\u9898 \u7ec4\u5408\u6811 \u7b2c\u4e00\u6392\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u6709\u4e24\u4e2a\u9009\u62e9\uff1a\u653e\u7f6e\u6b63\u53f7\u6216\u8005\u8d1f\u53f7 5.5n\u540e\u95ee\u9898 \u7ec4\u5408\u6811 \u6bcf\u4e00\u884c\u6709n\u4e2a\u9009\u62e9\uff1an\u4e2a\u7687\u540e\u4e4b\u4e00 5.6 0-1\u80cc\u5305\u95ee\u9898 \u7ec4\u5408\u6811 \u6bcf\u4e2a\u80cc\u5305\u6709\u4e24\u4e2a\u9009\u62e9\uff1a\u88c5\u5165\u6216\u8005\u4e0d\u88c5\u5165 5.8\u56fe\u7684m\u7740\u8272\u95ee\u9898 \u7ec4\u5408\u6811 \u6bcf\u4e2a\u70b9\u6709m\u79cd\u9009\u62e9\uff1am\u79cd\u989c\u8272\u4e4b\u4e00 5.10 \u5706\u6392\u5217\u95ee\u9898 \u6392\u5217\u6811 \u975e\u5e38\u5178\u578b\u7684\u6392\u5217\u95ee\u9898\uff1a\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u6709n\u79cd\u9009\u62e9\uff0c\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e\u6709n-1\u79cd\u9009\u62e9\uff0c......\uff0c\u6700\u540e\u4e00\u4e2a\u4f4d\u7f6e\u53ea\u6709\u4e00\u4e2a\u9009\u62e9 5.11\u7535\u8def\u677f\u6392\u5217\u95ee\u9898 \u6392\u5217\u6811 \u975e\u5e38\u5178\u578b\u7684\u6392\u5217\u95ee\u9898 5.9\u65c5\u884c\u552e\u8d27\u5458\u95ee\u9898 \u6392\u5217\u6811 \u4e0d\u662f\u975e\u5e38\u5178\u578b\u7684\u6392\u5217\u6811\u95ee\u9898","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/M-coloring-of-graph/","text":"M-coloring of graph \u95ee\u9898\u63cf\u8ff0 \u7b97\u6cd5\u8bbe\u8ba1 Implementation class Color { public : Color ( int n , int m , int ** a ) : n ( n ), m ( m ), a ( a ) { x = new int [ n + 1 ](); } ~ Color () { delete x ; } private : /** * \u53ef\u884c\u6027\u68c0\u9a8c\uff1a\u68c0\u67e5\u8282\u70b9k\u7684\u989c\u8272\u4e0e\u5b83\u7684\u76f8\u90bb\u8282\u70b9\u7684\u989c\u8272\uff0c\u5982\u679c\u4e00\u81f4\uff0c\u663e\u7136k\u7684\u5f53\u524d\u7740\u8272\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u5426\u5219\u662f\u53ef\u884c\u7684 */ bool OK ( int k ) { for ( int j = 1 ; j <= n ; j ++ ) { if ( ( a [ k ][ j ] == 1 ) && ( x [ j ] == x [ k ] )) { return false ; } } } /** * \u7ed9\u8282\u70b9t\u7740\u8272 */ void Backtrack ( int t ) { // \u83b7\u5f97\u4e86\u5b8c\u6574\u89e3 if ( t > n ) { sum ++ ; for ( int i = 1 ; i < n ; ++ 1 ) { std :: cout << x [ i ] << std :: endl ; } } // \u90e8\u5206\u89e3 else { for ( int i = 1 ; i <= m ; ++ 1 ) { x [ t ] = i ; // \u6ee1\u8db3\u53ef\u884c\u6027\u7ea6\u675f if ( OK ( t )) { Backtrack ( t + 1 ); } // \u4e0d\u6ee1\u8db3\u53ef\u884c\u6027\u7ea6\u675f\uff0c\u76f4\u63a5\u526a\u679d else { } x [ t ] = 0 ; // \u8fd8\u539f } } } private : int n ; // \u56fe\u7684\u9876\u70b9\u6570 int m ; // \u53ef\u7528\u7684\u989c\u8272 int ** a ; // \u56fe\u7684\u90bb\u63a5\u77e9\u9635 int * x ; // \u5f53\u524d\u89e3\uff0c\u5b83\u7684\u957f\u5ea6\u4e3a n + 1\uff0c\u56e0\u4e3a\u53ea\u6709\u7ed9\u6240\u6709\u7684\u8282\u70b9\u90fd\u7740\u8272\u540e\uff0c\u624d\u80fd\u591f\u5f97\u5230\u5b8c\u6574\u89e3 long sum { 0 }; //\u5f53\u524d\u5df2\u7ecf\u627e\u5230\u7684\u53efm\u7740\u8272\u65b9\u6848\u6570 }; /** * @param n \u56fe\u7684\u9876\u70b9\u6570 * @param m \u53ef\u7528\u7684\u989c\u8272 * @param a \u56fe\u7684\u90bb\u63a5\u77e9\u9635 */ int mColoring ( int n , int m , int ** a ) { Color X ( n , m , a ); X . Backtrack ( 1 ); return X . sum ; }","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/M-coloring-of-graph/#m-coloring#of#graph","text":"","title":"M-coloring of graph"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/M-coloring-of-graph/#_1","text":"","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/M-coloring-of-graph/#_2","text":"","title":"\u7b97\u6cd5\u8bbe\u8ba1"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/M-coloring-of-graph/#implementation","text":"class Color { public : Color ( int n , int m , int ** a ) : n ( n ), m ( m ), a ( a ) { x = new int [ n + 1 ](); } ~ Color () { delete x ; } private : /** * \u53ef\u884c\u6027\u68c0\u9a8c\uff1a\u68c0\u67e5\u8282\u70b9k\u7684\u989c\u8272\u4e0e\u5b83\u7684\u76f8\u90bb\u8282\u70b9\u7684\u989c\u8272\uff0c\u5982\u679c\u4e00\u81f4\uff0c\u663e\u7136k\u7684\u5f53\u524d\u7740\u8272\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u5426\u5219\u662f\u53ef\u884c\u7684 */ bool OK ( int k ) { for ( int j = 1 ; j <= n ; j ++ ) { if ( ( a [ k ][ j ] == 1 ) && ( x [ j ] == x [ k ] )) { return false ; } } } /** * \u7ed9\u8282\u70b9t\u7740\u8272 */ void Backtrack ( int t ) { // \u83b7\u5f97\u4e86\u5b8c\u6574\u89e3 if ( t > n ) { sum ++ ; for ( int i = 1 ; i < n ; ++ 1 ) { std :: cout << x [ i ] << std :: endl ; } } // \u90e8\u5206\u89e3 else { for ( int i = 1 ; i <= m ; ++ 1 ) { x [ t ] = i ; // \u6ee1\u8db3\u53ef\u884c\u6027\u7ea6\u675f if ( OK ( t )) { Backtrack ( t + 1 ); } // \u4e0d\u6ee1\u8db3\u53ef\u884c\u6027\u7ea6\u675f\uff0c\u76f4\u63a5\u526a\u679d else { } x [ t ] = 0 ; // \u8fd8\u539f } } } private : int n ; // \u56fe\u7684\u9876\u70b9\u6570 int m ; // \u53ef\u7528\u7684\u989c\u8272 int ** a ; // \u56fe\u7684\u90bb\u63a5\u77e9\u9635 int * x ; // \u5f53\u524d\u89e3\uff0c\u5b83\u7684\u957f\u5ea6\u4e3a n + 1\uff0c\u56e0\u4e3a\u53ea\u6709\u7ed9\u6240\u6709\u7684\u8282\u70b9\u90fd\u7740\u8272\u540e\uff0c\u624d\u80fd\u591f\u5f97\u5230\u5b8c\u6574\u89e3 long sum { 0 }; //\u5f53\u524d\u5df2\u7ecf\u627e\u5230\u7684\u53efm\u7740\u8272\u65b9\u6848\u6570 }; /** * @param n \u56fe\u7684\u9876\u70b9\u6570 * @param m \u53ef\u7528\u7684\u989c\u8272 * @param a \u56fe\u7684\u90bb\u63a5\u77e9\u9635 */ int mColoring ( int n , int m , int ** a ) { Color X ( n , m , a ); X . Backtrack ( 1 ); return X . sum ; }","title":"Implementation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/","text":"N queen \u95ee\u9898\u63cf\u8ff0 \u5728 n*n \u7684\u68cb\u76d8\u4e0a\u653e\u7f6e n \u4e2a\u7687\u540e\uff0c\u4efb\u4f55\u4e24\u4e2a\u7687\u540e\u5f7c\u6b64\u4e0d\u53d7\u653b\u51fb\u3002 \u6839\u636e\u56fd\u9645\u8c61\u68cb\u89c4\u5b9a\uff1a\u7687\u540e\u53ef\u4ee5\u653b\u51fb\u653e\u5728\u540c\u4e00\u884c\u3001\u540c\u4e00\u5217\u3001\u540c\u4e00\u659c\u7ebf\u4e0a\u7684\u5176\u4ed6\u7687\u540e\u3002 \u659c\u7ebf\u662f\u6307\u659c\u7387\u4e3a +-1 +-1 \u7684\u659c\u7ebf\u3002 \u7b97\u6cd5\u8bbe\u8ba1 \u89e3\u7684\u8868\u793a \u5bf9\u4e8e n*n \u7684\u68cb\u76d8\uff0c\u6211\u4eec\u81ea\u7136\u800c\u7136\u5730\u60f3\u5230\u4f7f\u7528 n*n \u4e8c\u7ef4\u6570\u7ec4\u6765\u8868\u793a\uff0c\u7687\u540e\u6240\u5904\u7684\u4f4d\u7f6e\u53ef\u4ee5\u4f7f\u7528\u4e8c\u7ef4\u6570\u7ec4\u7684\u5750\u6807\u6765\u8868\u793a\uff0c\u4e8c\u7ef4\u6570\u7ec4\u7684\u5750\u6807\u5b9a\u4e49\u5982\u4e0b\uff1a \u5176\u884c\u53f7\u4ece\u4e0a\u5230\u4e0b\u3001\u5217\u53f7\u4ece\u5de6\u5230\u53f3\uff0c\u7f16\u53f7\u4f9d\u6b21\u4e3a 1,2,\\dots, n 1,2,\\dots, n \u3002 \u90a3\u5982\u4f55\u6765\u8bb0\u5f55\u95ee\u9898\u7684\u89e3\uff0c\u5373\u7687\u540e\u5728\u68cb\u76d8\u4e0a\u7684\u4f4d\u7f6e\u5462\uff1f\u7687\u540e\u7684\u4f4d\u7f6e\u80af\u5b9a\u662f\u7531 (x, y) (x, y) \u5750\u6807\u7ec4\u6210\u7684\u5bf9\uff0c\u6240\u4ee5\u5bf9\u4e8e\u95ee\u9898\u7684\u89e3\uff0c\u6211\u4eec\u53ef\u4ee5\u91c7\u53d6\u5982\u4e0b\u7684\u65b9\u5f0f\uff1a std::vector<std::pair<int, int>> \u4f7f\u7528 std::pair<int, int> \u6765\u8868\u793a\u5750\u6807 \u7528 n \u5143\u7ec4 x[1:n] \u8868\u793a\u95ee\u9898\u7684\u89e3\uff0c x[i] \u8868\u793a\u7687\u540e i \u653e\u5728\u68cb\u76d8\u7684\u7b2c i \u884c\u7684\u7b2c x[i] \u5217\u4e0a\u3002 \u76f8\u6bd4\u4e8e\u65b9\u5f0f\u4e00\uff0c\u65b9\u5f0f\u4e8c\u662f\u66f4\u52a0\u7d27\u51d1\u3001\u8282\u7701\u7a7a\u95f4\u7684\uff0c\u5b83\u4f7f\u7528\u4e0b\u6807\u6765\u8868\u793a\u884c\u53f7\uff0c\u5bf9\u5e94\u7684\u6570\u7ec4\u503c\u6765\u8868\u793a\u5217\u53f7\u3002\u6240\u4ee5\u540e\u9762\u7684\u63cf\u8ff0\u91c7\u7528\u65b9\u5f0f\u4e8c\u3002 \u9650\u5236\u6761\u4ef6 \u9650\u5236\u6761\u4ef61 \u4e0d\u5141\u8bb8\u5c062\u4e2a\u7687\u540e\u653e\u5728\u540c\u4e00\u5217\uff0c\u4e0b\u9762\u7ed9\u51fa\u4e24\u4e2a\u7687\u540e i i \u3001 j j \u4f4d\u4e8e\u540c\u4e00\u5217\u7684\u6761\u4ef6: $$ x[i] == x[j] $$ \u9650\u5236\u6761\u4ef62 \u4e0d\u5141\u8bb8\u5c062\u4e2a\u7687\u540e\u653e\u5728\u540c\u4e00\u659c\u7ebf\uff0c\u6839\u636e\u659c\u7387\u7684\u5b9a\u4e49\uff0c\u4e0b\u9762\u7ed9\u51fa\u4e86\u4e24\u4e2a\u7687\u540e i i \u3001 j j \u4f4d\u4e8e\u540c\u4e00\u659c\u7ebf\u7684\u6761\u4ef6: $$ |{\\frac {x[i] - x[j] } {i -j}} | = 1 $$ \u56de\u6eaf\u6cd5 \u7528\u56de\u6eaf\u6cd5\u89e3n\u540e\u95ee\u9898\u65f6\uff0c\u4f7f\u7528\u5b8c\u5168 n \u53c9\u6811\u6765\u8868\u793a\u89e3\u7a7a\u95f4\u3002 \u4e0a\u8ff0\u7684\u53ef\u884c\u6027\u7ea6\u675f\u7531\u51fd\u6570 Place \u5b9e\u73b0\u3002 \u9012\u5f52\u51fd\u6570 Backtrack(1) \u5b9e\u73b0\u5bf9\u6574\u4e2a\u89e3\u7a7a\u95f4\u7684\u56de\u6eaf\u641c\u7d22\uff0c Backtrack(i) \u641c\u7d22\u89e3\u7a7a\u95f4\u4e2d\u7b2c i \u5c42\u5b50\u6811\u3002 \u5728 Backtrack \u4e2d\uff1a \u5f53 i>n \u65f6\uff0c\u7b97\u6cd5\u641c\u7d22\u5230\u4e86**\u53f6\u8282\u70b9**\uff0c\u9042\u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u89e3 \u5f53$i <= n $ \u65f6\uff0c \u5f53\u524d\u6269\u5c55\u8282\u70b9 Z **\u662f\u89e3\u7a7a\u95f4\u4e2d\u7684**\u5185\u90e8\u8282\u70b9 \uff0c\u8be5\u8282\u70b9\u6709 x[i]=1,2, \\dots, n x[i]=1,2, \\dots, n \uff0c\u5171 n \u4e2a\u5b50\u8282\u70b9\u3002\u5bf9**\u5f53\u524d\u6269\u5c55\u8282\u70b9 Z **\u7684\u6bcf\u4e2a\u5b50\u8282\u70b9\uff0c\u7531 Place \u68c0\u67e5\u5176\u53ef\u884c\u6027\uff1a \u5982\u679c\u53ef\u884c\uff0c\u5219\u9012\u5f52\u5730\u5bf9**\u5b50\u6811**\u8fdb\u884c\u641c\u7d22 \u5982\u679c\u4e0d\u53ef\u884c\uff0c\u5219\u526a\u679d\uff0c\u5373\u51cf\u53bb\u4e0d\u53ef\u884c\u7684\u5b50\u6811 Implementation Backtrack by recursion class Queen { friend int nQueen ( int ); private : int m_N ; // \u68cb\u76d8\u7684\u5927\u5c0f int * m_X ; // \u89e3 int m_Sum ; // \u89e3\u7684\u6570\u91cf private : /** * \u68c0\u67e5\u7687\u540ek\u7684\u4f4d\u7f6e\u662f\u5426\u6ee1\u8db3\u53ef\u884c\u6027\u7ea6\u675f */ bool Place ( int k ) { // \u68c0\u67e5k\u524d\u9762\u5df2\u7ecf\u653e\u7f6e\u7684\u7687\u540e for ( int j = 1 ; j < k ; ++ j ) { // \u68c0\u67e5\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6 if ( abs ( k - j ) == abs ( x [ j ] - x [ k ]) || x [ j ] == x [ k ]) { // \u4e0d\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6 return false ; } } return true ; } void Backtrack ( int t ) { if ( t > m_N ) { m_Sum ++ ; } else { for ( int i = 1 ; i < m_N ; ++ i ) { m_X [ t ] = i ; if ( Place ( t )) { Backtrack ( t + 1 ); } } } } }; int nQueen ( int n ) { Queen q ; q . m_N = n ; q . m_Sum = 0 ; int * p = new int [ n + 1 ](); q . m_X = p ; q . Backtrack ( 1 ); delete [] p ; return q . m_Sum ; } int main () { nQueue ( 5 ); } // g++ test.cpp Backtrack by iteration \u6570\u7ec4 m_X \u8bb0\u5f55\u4e86\u89e3\u7a7a\u95f4\u6811\u4e2d\u4ece**\u6839\u8282\u70b9**\u5230**\u5f53\u524d\u6269\u5c55\u8282\u70b9 Z **\u7684\u8def\u5f84\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5df2\u7ecf\u5305\u542b\u4e86\u56de\u6eaf\u6cd5\u5728\u56de\u6eaf\u65f6\u6240\u9700\u8981\u7684\u4fe1\u606f\u3002\u5229\u7528\u6570\u7ec4 m_X \u6240\u5305\u542b\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u5c06\u4e0a\u8ff0\u56de\u6eaf\u6cd5\u4f7f\u7528\u975e\u9012\u5f52\u65b9\u5f0f\u6765\u8fdb\u884c\u5b9e\u73b0\uff0c\u8fdb\u800c\u7701\u53bb O(n) O(n) \u9012\u5f52\u6808\u7a7a\u95f4\u3002 void Backtrack () { m_X [ 1 ] = 0 ; int k = 1 ; // k\u8868\u793a\u7b2ck\u4e2a\u7687\u540e while ( k > 0 ) { m_X [ k ] += 1 ; // /** * \u4e0b\u9762\u8fd9\u4e2awhile\u5faa\u73af\u5b9e\u73b0\u7684\u662f\uff1a\u7ed9\u7687\u540ek\u5bfb\u627e\u4e00\u4e2a\u7b26\u5408\u7ea6\u675f\u6761\u4ef6\u7684\u4f4d\u7f6e\uff0c\u5bfb\u627e\u7684\u7ed3\u679c\u6709\uff1a * - \u627e\u5230\u4e86\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e\uff1a\u5219\u9000\u51fa\u5faa\u73af\u7684\u65f6\u5019m_X[k]<=n * - \u6ca1\u6709\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e\uff1a\u5219\u9000\u51fa\u5faa\u73af\u7684\u65f6\u5019m_X[k] > n * \u6240\u4ee5\u6839\u636em_X[k]\u7684\u503c\uff0c\u53ef\u4ee5\u5224\u65ad\u5bf9\u4e8e\u7687\u540ek\uff0c\u662f\u5426\u627e\u5230\u4e86\u7b26\u5408\u7ea6\u675f\u6761\u4ef6\u7684\u4f4d\u7f6e */ while ( ( m_X [ k ] <= m_N ) && ! ( Place ( k )) ) { m_X [ k ] += 1 ; } // \u627e\u5230\u4e86\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e if ( m_X [ k ] <= n ) { // \u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u89e3\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f97\u5230\u5b8c\u6574\u89e3\u540e\uff0c\u5e76\u6ca1\u6709\u4fee\u6539k\u7684\u503c\uff0c // \u8fd9\u5c31\u4fdd\u8bc1\u4e86\u548cBacktrack by recursion\u4e2d\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff0c\u5373\uff1a // \u5b83\u4f1a\u5c06\u5e95\u5c42\u7684\u6240\u6709\u53ef\u80fd\u6027\u5c1d\u8bd5\u5b8c\u6210\u540e\uff0c\u518d\u56de\u6eaf\u5230\u4e0a\u4e00\u5c42\uff0c\u7136\u540e\u518d\u6df1\u5165\u5230\u4e0b\u4e00\u5c42\u3002 if ( k == m_N ) { m_Sum ++ ; } // \u89e3\u8fd8\u4e0d\u5b8c\u6574 else { // \u6df1\u5165 k ++ ; m_X [ k ] = 0 ; // \u521d\u59cb\u5316\u4e0b\u4e00\u4e2a\u7687\u540e\u7684\u521d\u59cb\u5316\u4f4d\u7f6e } } // \u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e else { // \u56de\u6eaf k -- ; } } } \u641c\u7d22\u987a\u5e8f \u4e0a\u8ff0\u7b97\u6cd5\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u7cbe\u5999\u7684\uff0c\u5b83\u5bf9**\u89e3\u7a7a\u95f4\u6811**\u7684\u641c\u7d22\u987a\u5e8f\u548cBacktrack by recursion\u4e2d\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u79cd\u987a\u5e8f\u7684**\u4e00\u81f4\u6027**\u975e\u5e38\u91cd\u8981\uff0c\u6211\u4eec\u5c06\u8fd9\u79cd\u987a\u5e8f\u6210\u4e3a**\u56de\u6eaf\u6cd5\u641c\u7d22\u987a\u5e8f**\uff0c\u51c6\u786e\u6765\u8bf4\u662f\uff1a\u6811\u7684**\u5148\u5e8f\u904d\u5386**\u3002 \u4e0b\u9762\u662f\u5173\u4e8e\u8fd9\u79cd\u641c\u7d22\u987a\u5e8f\u7684\u611f\u6027\u8ba4\u77e5: \u5b83\u4f1a\u5c06\u5e95\u5c42\u7684\u6240\u6709\u53ef\u80fd\u6027\u5c1d\u8bd5\u5b8c\u6210\u540e\uff0c\u518d\u56de\u6eaf\u5230\u4e0a\u4e00\u5c42\uff0c\u7136\u540e\u518d\u6df1\u5165\u5230\u4e0b\u4e00\u5c42\u3002 \u7ed3\u5408\u4ee3\u7801\u6765\u8fdb\u884c\u5206\u6790: // \u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u89e3\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f97\u5230\u5b8c\u6574\u89e3\u540e\uff0c\u5e76\u6ca1\u6709\u4fee\u6539k\u7684\u503c\uff0c // \u8fd9\u5c31\u4fdd\u8bc1\u4e86\u548cBacktrack by recursion\u4e2d\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff0c\u5373\uff1a // \u5b83\u4f1a\u5c06\u5e95\u5c42\u7684\u6240\u6709\u53ef\u80fd\u6027\u5c1d\u8bd5\u5b8c\u6210\u540e\uff0c\u518d\u56de\u6eaf\u5230\u4e0a\u4e00\u5c42\uff0c\u7136\u540e\u518d\u6df1\u5165\u5230\u4e0b\u4e00\u5c42\u3002 if ( k == n ) { m_Sum ++ ; } \u4f55\u65f6\u9000\u51fa\uff1f \u4e0a\u8ff0\u7b97\u6cd5\u7684\u9000\u51fa\u6761\u4ef6\u662f while(k > 0) \uff0c\u5373\u5f53 k<=0 \u65f6\uff0c\u7b97\u6cd5\u63a8\u51fa\uff0c\u90a3 k \u4f55\u65f6\u4f1a <=0 \u5462\uff1f \u5176\u5b9e\u6709\u4e86\u524d\u9762\u7684\u5bf9\u641c\u7d22\u987a\u5e8f\u7684\u5206\u6790\uff0c\u518d\u6765\u770b\u8fd9\u4e2a\u95ee\u9898\u5c31\u4e0d\u518d\u96be\u4ee5\u7406\u89e3\u4e86\u3002\u6211\u4eec\u7684\u9884\u671f\u662f\uff1a\u5f53\u7b97\u6cd5\u5c06\u6574\u4e2a\u89e3\u7a7a\u95f4\u6811\u90fd\u904d\u5386\u5b8c\u6210\u540e\uff0c\u5c31\u9000\u51fa\u6267\u884c\u3002\u90a3\u73b0\u5728\u6765\u770b\uff0c\u4e0a\u8ff0\u7b97\u6cd5\u80fd\u5426\u8fbe\u5230\u8fd9\u4e2a\u9884\u671f\uff1a \u5f53 k == 1 \u3001 m_X[k] == m_N \uff0c\u8868\u793a\u5df2\u7ecf\u5b8c\u6210\u4e86\u89e3\u7a7a\u95f4\u6811\u7684\u641c\u7d22\u4e86\uff0c\u6b64\u65f6\u518d\u6765\u8fdb\u5165\u4e3b\u5faa\u73af\u4f53\uff1a \u6267\u884c m_X[1] += 1; // \uff0c \u5219 m_X[1] == m_N + 1 \uff0c\u5219\u4e0d\u6ee1\u8db3\u4e0b\u9762\u5faa\u73af\u4f53\u7684\u6761\u4ef6\uff1a while ( ( m_X [ k ] <= m_N ) && ! ( Place ( k )) ) { m_X [ k ] += 1 ; } \u5219\u4f1a\u8fdb\u5165\u5982\u4e0b\u5206\u652f: // \u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e else { // \u56de\u6eaf k -- ; } \u663e\u7136\uff0c\u7ecf\u8fc7\u4e0a\u8ff0\u4ee3\u7801\u540e\uff0c k == 0 \uff0c\u6240\u4ee5\uff0c\u5c31\u9000\u51fa\u4e86\u4e3b\u5faa\u73af\u4f53\uff0c\u663e\u7136\u4e0a\u8ff0\u7b97\u6cd5\u662f\u7b26\u5408\u9884\u671f\u7684\u3002 wikipedia Eight queens puzzle","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#n#queen","text":"","title":"N queen"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#_1","text":"\u5728 n*n \u7684\u68cb\u76d8\u4e0a\u653e\u7f6e n \u4e2a\u7687\u540e\uff0c\u4efb\u4f55\u4e24\u4e2a\u7687\u540e\u5f7c\u6b64\u4e0d\u53d7\u653b\u51fb\u3002 \u6839\u636e\u56fd\u9645\u8c61\u68cb\u89c4\u5b9a\uff1a\u7687\u540e\u53ef\u4ee5\u653b\u51fb\u653e\u5728\u540c\u4e00\u884c\u3001\u540c\u4e00\u5217\u3001\u540c\u4e00\u659c\u7ebf\u4e0a\u7684\u5176\u4ed6\u7687\u540e\u3002 \u659c\u7ebf\u662f\u6307\u659c\u7387\u4e3a +-1 +-1 \u7684\u659c\u7ebf\u3002","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#_2","text":"","title":"\u7b97\u6cd5\u8bbe\u8ba1"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#_3","text":"\u5bf9\u4e8e n*n \u7684\u68cb\u76d8\uff0c\u6211\u4eec\u81ea\u7136\u800c\u7136\u5730\u60f3\u5230\u4f7f\u7528 n*n \u4e8c\u7ef4\u6570\u7ec4\u6765\u8868\u793a\uff0c\u7687\u540e\u6240\u5904\u7684\u4f4d\u7f6e\u53ef\u4ee5\u4f7f\u7528\u4e8c\u7ef4\u6570\u7ec4\u7684\u5750\u6807\u6765\u8868\u793a\uff0c\u4e8c\u7ef4\u6570\u7ec4\u7684\u5750\u6807\u5b9a\u4e49\u5982\u4e0b\uff1a \u5176\u884c\u53f7\u4ece\u4e0a\u5230\u4e0b\u3001\u5217\u53f7\u4ece\u5de6\u5230\u53f3\uff0c\u7f16\u53f7\u4f9d\u6b21\u4e3a 1,2,\\dots, n 1,2,\\dots, n \u3002 \u90a3\u5982\u4f55\u6765\u8bb0\u5f55\u95ee\u9898\u7684\u89e3\uff0c\u5373\u7687\u540e\u5728\u68cb\u76d8\u4e0a\u7684\u4f4d\u7f6e\u5462\uff1f\u7687\u540e\u7684\u4f4d\u7f6e\u80af\u5b9a\u662f\u7531 (x, y) (x, y) \u5750\u6807\u7ec4\u6210\u7684\u5bf9\uff0c\u6240\u4ee5\u5bf9\u4e8e\u95ee\u9898\u7684\u89e3\uff0c\u6211\u4eec\u53ef\u4ee5\u91c7\u53d6\u5982\u4e0b\u7684\u65b9\u5f0f\uff1a std::vector<std::pair<int, int>> \u4f7f\u7528 std::pair<int, int> \u6765\u8868\u793a\u5750\u6807 \u7528 n \u5143\u7ec4 x[1:n] \u8868\u793a\u95ee\u9898\u7684\u89e3\uff0c x[i] \u8868\u793a\u7687\u540e i \u653e\u5728\u68cb\u76d8\u7684\u7b2c i \u884c\u7684\u7b2c x[i] \u5217\u4e0a\u3002 \u76f8\u6bd4\u4e8e\u65b9\u5f0f\u4e00\uff0c\u65b9\u5f0f\u4e8c\u662f\u66f4\u52a0\u7d27\u51d1\u3001\u8282\u7701\u7a7a\u95f4\u7684\uff0c\u5b83\u4f7f\u7528\u4e0b\u6807\u6765\u8868\u793a\u884c\u53f7\uff0c\u5bf9\u5e94\u7684\u6570\u7ec4\u503c\u6765\u8868\u793a\u5217\u53f7\u3002\u6240\u4ee5\u540e\u9762\u7684\u63cf\u8ff0\u91c7\u7528\u65b9\u5f0f\u4e8c\u3002","title":"\u89e3\u7684\u8868\u793a"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#_4","text":"","title":"\u9650\u5236\u6761\u4ef6"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#1","text":"\u4e0d\u5141\u8bb8\u5c062\u4e2a\u7687\u540e\u653e\u5728\u540c\u4e00\u5217\uff0c\u4e0b\u9762\u7ed9\u51fa\u4e24\u4e2a\u7687\u540e i i \u3001 j j \u4f4d\u4e8e\u540c\u4e00\u5217\u7684\u6761\u4ef6: $$ x[i] == x[j] $$","title":"\u9650\u5236\u6761\u4ef61"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#2","text":"\u4e0d\u5141\u8bb8\u5c062\u4e2a\u7687\u540e\u653e\u5728\u540c\u4e00\u659c\u7ebf\uff0c\u6839\u636e\u659c\u7387\u7684\u5b9a\u4e49\uff0c\u4e0b\u9762\u7ed9\u51fa\u4e86\u4e24\u4e2a\u7687\u540e i i \u3001 j j \u4f4d\u4e8e\u540c\u4e00\u659c\u7ebf\u7684\u6761\u4ef6: $$ |{\\frac {x[i] - x[j] } {i -j}} | = 1 $$","title":"\u9650\u5236\u6761\u4ef62"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#_5","text":"\u7528\u56de\u6eaf\u6cd5\u89e3n\u540e\u95ee\u9898\u65f6\uff0c\u4f7f\u7528\u5b8c\u5168 n \u53c9\u6811\u6765\u8868\u793a\u89e3\u7a7a\u95f4\u3002 \u4e0a\u8ff0\u7684\u53ef\u884c\u6027\u7ea6\u675f\u7531\u51fd\u6570 Place \u5b9e\u73b0\u3002 \u9012\u5f52\u51fd\u6570 Backtrack(1) \u5b9e\u73b0\u5bf9\u6574\u4e2a\u89e3\u7a7a\u95f4\u7684\u56de\u6eaf\u641c\u7d22\uff0c Backtrack(i) \u641c\u7d22\u89e3\u7a7a\u95f4\u4e2d\u7b2c i \u5c42\u5b50\u6811\u3002 \u5728 Backtrack \u4e2d\uff1a \u5f53 i>n \u65f6\uff0c\u7b97\u6cd5\u641c\u7d22\u5230\u4e86**\u53f6\u8282\u70b9**\uff0c\u9042\u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u89e3 \u5f53$i <= n $ \u65f6\uff0c \u5f53\u524d\u6269\u5c55\u8282\u70b9 Z **\u662f\u89e3\u7a7a\u95f4\u4e2d\u7684**\u5185\u90e8\u8282\u70b9 \uff0c\u8be5\u8282\u70b9\u6709 x[i]=1,2, \\dots, n x[i]=1,2, \\dots, n \uff0c\u5171 n \u4e2a\u5b50\u8282\u70b9\u3002\u5bf9**\u5f53\u524d\u6269\u5c55\u8282\u70b9 Z **\u7684\u6bcf\u4e2a\u5b50\u8282\u70b9\uff0c\u7531 Place \u68c0\u67e5\u5176\u53ef\u884c\u6027\uff1a \u5982\u679c\u53ef\u884c\uff0c\u5219\u9012\u5f52\u5730\u5bf9**\u5b50\u6811**\u8fdb\u884c\u641c\u7d22 \u5982\u679c\u4e0d\u53ef\u884c\uff0c\u5219\u526a\u679d\uff0c\u5373\u51cf\u53bb\u4e0d\u53ef\u884c\u7684\u5b50\u6811","title":"\u56de\u6eaf\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#implementation","text":"","title":"Implementation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#backtrack#by#recursion","text":"class Queen { friend int nQueen ( int ); private : int m_N ; // \u68cb\u76d8\u7684\u5927\u5c0f int * m_X ; // \u89e3 int m_Sum ; // \u89e3\u7684\u6570\u91cf private : /** * \u68c0\u67e5\u7687\u540ek\u7684\u4f4d\u7f6e\u662f\u5426\u6ee1\u8db3\u53ef\u884c\u6027\u7ea6\u675f */ bool Place ( int k ) { // \u68c0\u67e5k\u524d\u9762\u5df2\u7ecf\u653e\u7f6e\u7684\u7687\u540e for ( int j = 1 ; j < k ; ++ j ) { // \u68c0\u67e5\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6 if ( abs ( k - j ) == abs ( x [ j ] - x [ k ]) || x [ j ] == x [ k ]) { // \u4e0d\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6 return false ; } } return true ; } void Backtrack ( int t ) { if ( t > m_N ) { m_Sum ++ ; } else { for ( int i = 1 ; i < m_N ; ++ i ) { m_X [ t ] = i ; if ( Place ( t )) { Backtrack ( t + 1 ); } } } } }; int nQueen ( int n ) { Queen q ; q . m_N = n ; q . m_Sum = 0 ; int * p = new int [ n + 1 ](); q . m_X = p ; q . Backtrack ( 1 ); delete [] p ; return q . m_Sum ; } int main () { nQueue ( 5 ); } // g++ test.cpp","title":"Backtrack by recursion"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#backtrack#by#iteration","text":"\u6570\u7ec4 m_X \u8bb0\u5f55\u4e86\u89e3\u7a7a\u95f4\u6811\u4e2d\u4ece**\u6839\u8282\u70b9**\u5230**\u5f53\u524d\u6269\u5c55\u8282\u70b9 Z **\u7684\u8def\u5f84\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5df2\u7ecf\u5305\u542b\u4e86\u56de\u6eaf\u6cd5\u5728\u56de\u6eaf\u65f6\u6240\u9700\u8981\u7684\u4fe1\u606f\u3002\u5229\u7528\u6570\u7ec4 m_X \u6240\u5305\u542b\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u5c06\u4e0a\u8ff0\u56de\u6eaf\u6cd5\u4f7f\u7528\u975e\u9012\u5f52\u65b9\u5f0f\u6765\u8fdb\u884c\u5b9e\u73b0\uff0c\u8fdb\u800c\u7701\u53bb O(n) O(n) \u9012\u5f52\u6808\u7a7a\u95f4\u3002 void Backtrack () { m_X [ 1 ] = 0 ; int k = 1 ; // k\u8868\u793a\u7b2ck\u4e2a\u7687\u540e while ( k > 0 ) { m_X [ k ] += 1 ; // /** * \u4e0b\u9762\u8fd9\u4e2awhile\u5faa\u73af\u5b9e\u73b0\u7684\u662f\uff1a\u7ed9\u7687\u540ek\u5bfb\u627e\u4e00\u4e2a\u7b26\u5408\u7ea6\u675f\u6761\u4ef6\u7684\u4f4d\u7f6e\uff0c\u5bfb\u627e\u7684\u7ed3\u679c\u6709\uff1a * - \u627e\u5230\u4e86\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e\uff1a\u5219\u9000\u51fa\u5faa\u73af\u7684\u65f6\u5019m_X[k]<=n * - \u6ca1\u6709\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e\uff1a\u5219\u9000\u51fa\u5faa\u73af\u7684\u65f6\u5019m_X[k] > n * \u6240\u4ee5\u6839\u636em_X[k]\u7684\u503c\uff0c\u53ef\u4ee5\u5224\u65ad\u5bf9\u4e8e\u7687\u540ek\uff0c\u662f\u5426\u627e\u5230\u4e86\u7b26\u5408\u7ea6\u675f\u6761\u4ef6\u7684\u4f4d\u7f6e */ while ( ( m_X [ k ] <= m_N ) && ! ( Place ( k )) ) { m_X [ k ] += 1 ; } // \u627e\u5230\u4e86\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e if ( m_X [ k ] <= n ) { // \u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u89e3\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f97\u5230\u5b8c\u6574\u89e3\u540e\uff0c\u5e76\u6ca1\u6709\u4fee\u6539k\u7684\u503c\uff0c // \u8fd9\u5c31\u4fdd\u8bc1\u4e86\u548cBacktrack by recursion\u4e2d\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff0c\u5373\uff1a // \u5b83\u4f1a\u5c06\u5e95\u5c42\u7684\u6240\u6709\u53ef\u80fd\u6027\u5c1d\u8bd5\u5b8c\u6210\u540e\uff0c\u518d\u56de\u6eaf\u5230\u4e0a\u4e00\u5c42\uff0c\u7136\u540e\u518d\u6df1\u5165\u5230\u4e0b\u4e00\u5c42\u3002 if ( k == m_N ) { m_Sum ++ ; } // \u89e3\u8fd8\u4e0d\u5b8c\u6574 else { // \u6df1\u5165 k ++ ; m_X [ k ] = 0 ; // \u521d\u59cb\u5316\u4e0b\u4e00\u4e2a\u7687\u540e\u7684\u521d\u59cb\u5316\u4f4d\u7f6e } } // \u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e else { // \u56de\u6eaf k -- ; } } }","title":"Backtrack by iteration"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#_6","text":"\u4e0a\u8ff0\u7b97\u6cd5\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u7cbe\u5999\u7684\uff0c\u5b83\u5bf9**\u89e3\u7a7a\u95f4\u6811**\u7684\u641c\u7d22\u987a\u5e8f\u548cBacktrack by recursion\u4e2d\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u79cd\u987a\u5e8f\u7684**\u4e00\u81f4\u6027**\u975e\u5e38\u91cd\u8981\uff0c\u6211\u4eec\u5c06\u8fd9\u79cd\u987a\u5e8f\u6210\u4e3a**\u56de\u6eaf\u6cd5\u641c\u7d22\u987a\u5e8f**\uff0c\u51c6\u786e\u6765\u8bf4\u662f\uff1a\u6811\u7684**\u5148\u5e8f\u904d\u5386**\u3002 \u4e0b\u9762\u662f\u5173\u4e8e\u8fd9\u79cd\u641c\u7d22\u987a\u5e8f\u7684\u611f\u6027\u8ba4\u77e5: \u5b83\u4f1a\u5c06\u5e95\u5c42\u7684\u6240\u6709\u53ef\u80fd\u6027\u5c1d\u8bd5\u5b8c\u6210\u540e\uff0c\u518d\u56de\u6eaf\u5230\u4e0a\u4e00\u5c42\uff0c\u7136\u540e\u518d\u6df1\u5165\u5230\u4e0b\u4e00\u5c42\u3002 \u7ed3\u5408\u4ee3\u7801\u6765\u8fdb\u884c\u5206\u6790: // \u5f97\u5230\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u89e3\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f97\u5230\u5b8c\u6574\u89e3\u540e\uff0c\u5e76\u6ca1\u6709\u4fee\u6539k\u7684\u503c\uff0c // \u8fd9\u5c31\u4fdd\u8bc1\u4e86\u548cBacktrack by recursion\u4e2d\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff0c\u5373\uff1a // \u5b83\u4f1a\u5c06\u5e95\u5c42\u7684\u6240\u6709\u53ef\u80fd\u6027\u5c1d\u8bd5\u5b8c\u6210\u540e\uff0c\u518d\u56de\u6eaf\u5230\u4e0a\u4e00\u5c42\uff0c\u7136\u540e\u518d\u6df1\u5165\u5230\u4e0b\u4e00\u5c42\u3002 if ( k == n ) { m_Sum ++ ; }","title":"\u641c\u7d22\u987a\u5e8f"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#_7","text":"\u4e0a\u8ff0\u7b97\u6cd5\u7684\u9000\u51fa\u6761\u4ef6\u662f while(k > 0) \uff0c\u5373\u5f53 k<=0 \u65f6\uff0c\u7b97\u6cd5\u63a8\u51fa\uff0c\u90a3 k \u4f55\u65f6\u4f1a <=0 \u5462\uff1f \u5176\u5b9e\u6709\u4e86\u524d\u9762\u7684\u5bf9\u641c\u7d22\u987a\u5e8f\u7684\u5206\u6790\uff0c\u518d\u6765\u770b\u8fd9\u4e2a\u95ee\u9898\u5c31\u4e0d\u518d\u96be\u4ee5\u7406\u89e3\u4e86\u3002\u6211\u4eec\u7684\u9884\u671f\u662f\uff1a\u5f53\u7b97\u6cd5\u5c06\u6574\u4e2a\u89e3\u7a7a\u95f4\u6811\u90fd\u904d\u5386\u5b8c\u6210\u540e\uff0c\u5c31\u9000\u51fa\u6267\u884c\u3002\u90a3\u73b0\u5728\u6765\u770b\uff0c\u4e0a\u8ff0\u7b97\u6cd5\u80fd\u5426\u8fbe\u5230\u8fd9\u4e2a\u9884\u671f\uff1a \u5f53 k == 1 \u3001 m_X[k] == m_N \uff0c\u8868\u793a\u5df2\u7ecf\u5b8c\u6210\u4e86\u89e3\u7a7a\u95f4\u6811\u7684\u641c\u7d22\u4e86\uff0c\u6b64\u65f6\u518d\u6765\u8fdb\u5165\u4e3b\u5faa\u73af\u4f53\uff1a \u6267\u884c m_X[1] += 1; // \uff0c \u5219 m_X[1] == m_N + 1 \uff0c\u5219\u4e0d\u6ee1\u8db3\u4e0b\u9762\u5faa\u73af\u4f53\u7684\u6761\u4ef6\uff1a while ( ( m_X [ k ] <= m_N ) && ! ( Place ( k )) ) { m_X [ k ] += 1 ; } \u5219\u4f1a\u8fdb\u5165\u5982\u4e0b\u5206\u652f: // \u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u4f4d\u7f6e else { // \u56de\u6eaf k -- ; } \u663e\u7136\uff0c\u7ecf\u8fc7\u4e0a\u8ff0\u4ee3\u7801\u540e\uff0c k == 0 \uff0c\u6240\u4ee5\uff0c\u5c31\u9000\u51fa\u4e86\u4e3b\u5faa\u73af\u4f53\uff0c\u663e\u7136\u4e0a\u8ff0\u7b97\u6cd5\u662f\u7b26\u5408\u9884\u671f\u7684\u3002","title":"\u4f55\u65f6\u9000\u51fa\uff1f"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/N-queen/N-queen/#wikipedia#eight#queens#puzzle","text":"","title":"wikipedia Eight queens puzzle"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Permutation-of-circle/","text":"Permutation of circle \u5706\u6392\u5217\u95ee\u9898\u3002 \u95ee\u9898\u63cf\u8ff0 \u4e0e \u7b97\u6cd5\u8bbe\u8ba1 \u5b9e\u73b0 #include <algorithm> // std::copy_n /** * @brief \u5706\u6392\u5217\u5668 * */ class CirclePermutator { public : /** * @brief * * @param n \u5706\u4e2a\u6570 * @param a \u5706\u534a\u5f84 */ CirclePermutator ( int n , float * a ) : n { n }, r { new float [ n ] }, x { new float [ n ] } { std :: copy_n ( a , n , r ); } /** * @brief \u5706\u6392\u5217 * * @param n \u5706\u4e2a\u6570 * @param a \u5706\u534a\u5f84 * @return */ friend float CirclePerm ( int n , float * a ) { } private : /** * @brief \u56de\u6eaf * * @param t \u7b2ct\u4e2a\u89e3\u3001\u7b2ct\u4e2a\u5706 */ void Backtrack ( int t ) { } private : float min { 0.0 }; // \u5f53\u524d\u6700\u4f18\u503c float * x { nullptr }; //\u5f53\u524d\u5706\u6392\u5217\u5bf9\u5e94\u7684\u6bcf\u4e2a\u5706\u5fc3\u6a2a\u5750\u6807 float * r { nullptr }; //\u5f53\u524d\u5706\u6392\u5217\u5bf9\u5e94\u7684\u6bcf\u4e2a\u5706\u7684\u534a\u5f84 int n { 0 }; //\u5f85\u6392\u5217\u7684\u5706\u7684\u4e2a\u6570 };","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Permutation-of-circle/#permutation#of#circle","text":"\u5706\u6392\u5217\u95ee\u9898\u3002","title":"Permutation of circle"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Permutation-of-circle/#_1","text":"","title":"\u95ee\u9898\u63cf\u8ff0 \u4e0e \u7b97\u6cd5\u8bbe\u8ba1"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Permutation-of-circle/#_2","text":"#include <algorithm> // std::copy_n /** * @brief \u5706\u6392\u5217\u5668 * */ class CirclePermutator { public : /** * @brief * * @param n \u5706\u4e2a\u6570 * @param a \u5706\u534a\u5f84 */ CirclePermutator ( int n , float * a ) : n { n }, r { new float [ n ] }, x { new float [ n ] } { std :: copy_n ( a , n , r ); } /** * @brief \u5706\u6392\u5217 * * @param n \u5706\u4e2a\u6570 * @param a \u5706\u534a\u5f84 * @return */ friend float CirclePerm ( int n , float * a ) { } private : /** * @brief \u56de\u6eaf * * @param t \u7b2ct\u4e2a\u89e3\u3001\u7b2ct\u4e2a\u5706 */ void Backtrack ( int t ) { } private : float min { 0.0 }; // \u5f53\u524d\u6700\u4f18\u503c float * x { nullptr }; //\u5f53\u524d\u5706\u6392\u5217\u5bf9\u5e94\u7684\u6bcf\u4e2a\u5706\u5fc3\u6a2a\u5750\u6807 float * r { nullptr }; //\u5f53\u524d\u5706\u6392\u5217\u5bf9\u5e94\u7684\u6bcf\u4e2a\u5706\u7684\u534a\u5f84 int n { 0 }; //\u5f85\u6392\u5217\u7684\u5706\u7684\u4e2a\u6570 };","title":"\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Sudoku-solving/","text":"Sudoku solving algorithms Sudoku solving algorithms","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Sudoku-solving/#sudoku#solving#algorithms","text":"","title":"Sudoku solving algorithms"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Symbol-triangle/Symbol-triangle/","text":"Symbol triangle Implementation \u6574\u4e2a\u7b26\u53f7\u4e09\u89d2\u5f62\uff0c\u662f\u7531\u7b2c\u4e00\u884c\u7684\u7b26\u53f7\u786e\u5b9a\u7684\uff0c\u56e0\u6b64\u5b83\u7684\u89e3\u7a7a\u95f4\u5b8c\u5168\u662f\u7531\u5b83\u7684\u7b2c\u4e00\u884c\u7684\u7b26\u53f7\u7ec4\u6210\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e00\u68f5\u5b8c\u5168\u4e8c\u53c9\u6811\u6765\u8868\u793a\u5176\u89e3\u7a7a\u95f4\u3002 \u5982\u4f55\u6765\u5b9e\u73b0\u5462\uff1f\u4f9d\u7136\u662fone-by-one\uff0c\u7ed9\u51fa\u6240\u6709\u7684\u7ec4\u5408\u3002 class Triangle { public : friend int Compute ( int n ); Triangle ( int n ) : n ( n ) { } bool Init () { int total = ( n * ( n + 1 ) / 2 ); // \u603b\u4e2a\u6570 if ( total % 2 == 1 ) // \u603b\u4e2a\u6570\u4e3a\u5947\u6570\uff0c\u663e\u7136\u4e0d\u5b58\u5728\u53ef\u884c\u89e3 { return false ; } half /= 2 ; p = new int * [ n + 1 ](); for ( int i = 0 ; i <= n ; ++ i ) { p [ i ] = new int [ n + 1 ](); } return true ; } private : int ** p { nullptr }; // \u7b26\u5408\u4e09\u89d2\u5f62\u77e9\u9635 int n ; // \u7b2c\u4e00\u884c\u7684\u7b26\u53f7\u7684\u4e2a\u6570 int count { 0 }; // \u5f53\u524d+\u53f7\u7684\u4e2a\u6570\uff0c\u56e0\u4e3a\u201c+\u201d\u53f7\u75281\u8868\u793a\uff0c\u201c-\u201d\u53f7\u75280\u8868\u793a int half { 0 }; // (n * ( n + 1 ))/4 long sum { 0 }; //\u5df2\u7ecf\u627e\u5230\u7684\u89e3\u7684\u4e2a\u6570 void Backtack ( int t ) { // \u5982\u679c\u5728\u5df2\u7ecf\u786e\u5b9a\u7684t-1\u884c\u4e09\u89d2\u5f62\u4e2d\uff0c\u201c+\u201d\u6216\u201c-\u201d\u7684\u4e2a\u6570\u8d85\u8fc7half\uff0c\u5219\u663e\u7136\u4e0d\u53ef\u80fd\u5305\u542b\u53ef\u884c\u89e3\uff0c\u53ef\u4ee5\u76f4\u63a5\u526a\u6389 // t * (t - 1)) / 2 \u662f\u201c\u5df2\u7ecf\u786e\u5b9a\u7684t-1\u884c\u4e09\u89d2\u5f62\u201d\u4e2d\u7b26\u5408\u7684\u603b\u4e2a\u6570 if (( count > half ) || ( t * ( t - 1 )) / 2 - count > half ) { return ; } // \u5f97\u5230\u4e86\u5b8c\u6574\u89e3 if ( t > n ) { sum ++ ; } // \u89e3\u4e0d\u5b8c\u5168 else { for ( int i = 0 ; i < 2 ; i ++ ) // \u6bcf\u4e2a\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u53ef\u4ee5\u53d6\u4e24\u4e2a\u503c0\uff0c1\u53d60\u8868\u793a\u201c-\u201d\uff0c\u53d61\u8868\u793a\u201c+\u201d { p [ 1 ][ t ] = i ; //\u7b2c\u4e00\u884c\u7b2ct\u5217\u586b\u5165\u7b26\u5408 count += i ; //\u66f4\u65b0+\u53f7\u7684\u4e2a\u6570 for ( int j = 2 ; j <= t ; j ++ ) //\u6309\u7167\u659c\u7ebf\u65b9\u5411\u6709\u4e0a\u7f51\u4e0b\u8fdb\u884c\u586b\u503c { p [ j ][ t - j + 1 ] = p [ j - 1 ][ t - j + 1 ] * p [ j - 1 ][ t - j + 2 ]; count += p [ j ][ t - j + 1 ]; } Backtack ( t + 1 ); // \u56de\u6eda for ( int j = 2 ; j <= t ; j ++ ) { //\u8fd9\u4e09\u53e5\u8fdb\u884c\u56de\u6eda count -= p [ j ][ t - j + 1 ]; } count -= i ; } } } }; int Compute ( int n ) { Triangle X ; if ( X . Init ()) { X . Backtack ( 1 ); } return X . sum ; }","title":"Symbol-triangle"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Symbol-triangle/Symbol-triangle/#symbol#triangle","text":"","title":"Symbol triangle"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Examples/Symbol-triangle/Symbol-triangle/#implementation","text":"\u6574\u4e2a\u7b26\u53f7\u4e09\u89d2\u5f62\uff0c\u662f\u7531\u7b2c\u4e00\u884c\u7684\u7b26\u53f7\u786e\u5b9a\u7684\uff0c\u56e0\u6b64\u5b83\u7684\u89e3\u7a7a\u95f4\u5b8c\u5168\u662f\u7531\u5b83\u7684\u7b2c\u4e00\u884c\u7684\u7b26\u53f7\u7ec4\u6210\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e00\u68f5\u5b8c\u5168\u4e8c\u53c9\u6811\u6765\u8868\u793a\u5176\u89e3\u7a7a\u95f4\u3002 \u5982\u4f55\u6765\u5b9e\u73b0\u5462\uff1f\u4f9d\u7136\u662fone-by-one\uff0c\u7ed9\u51fa\u6240\u6709\u7684\u7ec4\u5408\u3002 class Triangle { public : friend int Compute ( int n ); Triangle ( int n ) : n ( n ) { } bool Init () { int total = ( n * ( n + 1 ) / 2 ); // \u603b\u4e2a\u6570 if ( total % 2 == 1 ) // \u603b\u4e2a\u6570\u4e3a\u5947\u6570\uff0c\u663e\u7136\u4e0d\u5b58\u5728\u53ef\u884c\u89e3 { return false ; } half /= 2 ; p = new int * [ n + 1 ](); for ( int i = 0 ; i <= n ; ++ i ) { p [ i ] = new int [ n + 1 ](); } return true ; } private : int ** p { nullptr }; // \u7b26\u5408\u4e09\u89d2\u5f62\u77e9\u9635 int n ; // \u7b2c\u4e00\u884c\u7684\u7b26\u53f7\u7684\u4e2a\u6570 int count { 0 }; // \u5f53\u524d+\u53f7\u7684\u4e2a\u6570\uff0c\u56e0\u4e3a\u201c+\u201d\u53f7\u75281\u8868\u793a\uff0c\u201c-\u201d\u53f7\u75280\u8868\u793a int half { 0 }; // (n * ( n + 1 ))/4 long sum { 0 }; //\u5df2\u7ecf\u627e\u5230\u7684\u89e3\u7684\u4e2a\u6570 void Backtack ( int t ) { // \u5982\u679c\u5728\u5df2\u7ecf\u786e\u5b9a\u7684t-1\u884c\u4e09\u89d2\u5f62\u4e2d\uff0c\u201c+\u201d\u6216\u201c-\u201d\u7684\u4e2a\u6570\u8d85\u8fc7half\uff0c\u5219\u663e\u7136\u4e0d\u53ef\u80fd\u5305\u542b\u53ef\u884c\u89e3\uff0c\u53ef\u4ee5\u76f4\u63a5\u526a\u6389 // t * (t - 1)) / 2 \u662f\u201c\u5df2\u7ecf\u786e\u5b9a\u7684t-1\u884c\u4e09\u89d2\u5f62\u201d\u4e2d\u7b26\u5408\u7684\u603b\u4e2a\u6570 if (( count > half ) || ( t * ( t - 1 )) / 2 - count > half ) { return ; } // \u5f97\u5230\u4e86\u5b8c\u6574\u89e3 if ( t > n ) { sum ++ ; } // \u89e3\u4e0d\u5b8c\u5168 else { for ( int i = 0 ; i < 2 ; i ++ ) // \u6bcf\u4e2a\u5f53\u524d\u6269\u5c55\u7ed3\u70b9\u53ef\u4ee5\u53d6\u4e24\u4e2a\u503c0\uff0c1\u53d60\u8868\u793a\u201c-\u201d\uff0c\u53d61\u8868\u793a\u201c+\u201d { p [ 1 ][ t ] = i ; //\u7b2c\u4e00\u884c\u7b2ct\u5217\u586b\u5165\u7b26\u5408 count += i ; //\u66f4\u65b0+\u53f7\u7684\u4e2a\u6570 for ( int j = 2 ; j <= t ; j ++ ) //\u6309\u7167\u659c\u7ebf\u65b9\u5411\u6709\u4e0a\u7f51\u4e0b\u8fdb\u884c\u586b\u503c { p [ j ][ t - j + 1 ] = p [ j - 1 ][ t - j + 1 ] * p [ j - 1 ][ t - j + 2 ]; count += p [ j ][ t - j + 1 ]; } Backtack ( t + 1 ); // \u56de\u6eda for ( int j = 2 ; j <= t ; j ++ ) { //\u8fd9\u4e09\u53e5\u8fdb\u884c\u56de\u6eda count -= p [ j ][ t - j + 1 ]; } count -= i ; } } } }; int Compute ( int n ) { Triangle X ; if ( X . Init ()) { X . Backtack ( 1 ); } return X . sum ; }","title":"Implementation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Permutation/","text":"Permutation Permutation Permutation wikipedia Permutations in computing Algorithms to generate permutations In computing it may be required to generate permutations of a given sequence of values. The methods best adapted to do this depend on whether one wants some randomly chosen permutations, or all permutations, and in the latter case if a specific ordering is required. Another question is whether possible equality among entries in the given sequence is to be taken into account; if so, one should only generate distinct multiset permutations of the sequence. An obvious way to generate permutations of n is to generate values for the Lehmer code (possibly using the factorial number system representation of integers up to n !), and convert those into the corresponding permutations. However, the latter step, while straightforward, is hard to implement efficiently, because it requires n operations each of selection from a sequence and deletion from it, at an arbitrary position; of the obvious representations of the sequence as an array or a linked list , both require (for different reasons) about n*2/4 operations to perform the conversion. With *n likely to be rather small (especially if generation of all permutations is needed) that is not too much of a problem, but it turns out that both for random and for systematic generation there are simple alternatives that do considerably better. For this reason it does not seem useful, although certainly possible, to employ a special data structure that would allow performing the conversion from Lehmer code to permutation in O ( n log n ) time. \u751f\u6210n\u7684\u6392\u5217\u7684\u4e00\u4e2a\u660e\u663e\u7684\u65b9\u6cd5\u662f\u751f\u6210Lehmer\u4ee3\u7801\u7684\u503c(\u53ef\u80fd\u4f7f\u7528\u5230n\u7684\u6574\u6570\u7684\u9636\u4e58\u6570\u5b57\u7cfb\u7edf\u8868\u793a)\uff0c\u5e76\u5c06\u5b83\u4eec\u8f6c\u6362\u6210\u76f8\u5e94\u7684\u6392\u5217\u3002 \u4f46\u662f\uff0c\u540e\u9762\u7684\u6b65\u9aa4\u867d\u7136\u7b80\u5355\uff0c\u4f46\u662f\u5f88\u96be\u6709\u6548\u5730\u5b9e\u73b0\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5728\u4efb\u610f\u4f4d\u7f6e\u5bf9\u5e8f\u5217\u8fdb\u884cn\u6b21\u9009\u62e9\u548c\u5220\u9664\u64cd\u4f5c; \u5e8f\u5217\u7684\u660e\u663e\u8868\u793a\u5f62\u5f0f\u4e3a\u6570\u7ec4\u6216\u94fe\u8868\uff0c\u4e24\u8005\u90fd\u9700\u8981(\u51fa\u4e8e\u4e0d\u540c\u7684\u539f\u56e0)\u5927\u7ea6n2/4\u64cd\u4f5c\u6765\u6267\u884c\u8f6c\u6362\u3002 n\u53ef\u80fd\u5f88\u5c0f(\u7279\u522b\u662f\u5982\u679c\u9700\u8981\u751f\u6210\u6240\u6709\u6392\u5217)\uff0c\u8fd9\u4e0d\u662f\u4ec0\u4e48\u5927\u95ee\u9898\uff0c\u4f46\u4e8b\u5b9e\u8bc1\u660e\uff0c\u65e0\u8bba\u662f\u968f\u673a\u751f\u6210\u8fd8\u662f\u7cfb\u7edf\u751f\u6210\uff0c\u90fd\u6709\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6cd5\u53ef\u4ee5\u505a\u5f97\u66f4\u597d\u3002 \u56e0\u6b64\uff0c\u4f7f\u7528\u4e00\u79cd\u7279\u6b8a\u7684\u6570\u636e\u7ed3\u6784\uff0c\u5141\u8bb8\u5728O(n log n)\u65f6\u95f4\u5185\u6267\u884c\u4eceLehmer\u4ee3\u7801\u5230\u7f6e\u6362\u7684\u8f6c\u6362\uff0c\u867d\u7136\u8fd9\u79cd\u6570\u636e\u7ed3\u6784\u80af\u5b9a\u662f\u53ef\u884c\u7684\uff0c\u4f46\u4f3c\u4e4e\u5e76\u4e0d\u6709\u7528\u3002 TO READ geeksforgeeks Write a program to print all permutations of a given string https://leetcode-cn.com/problems/permutations-ii/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liwe-2/","title":"Permutation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Permutation/#permutation","text":"","title":"Permutation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Permutation/#permutation#permutation","text":"","title":"Permutation Permutation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Permutation/#wikipedia#permutations#in#computing","text":"","title":"wikipedia Permutations in computing"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Permutation/#algorithms#to#generate#permutations","text":"In computing it may be required to generate permutations of a given sequence of values. The methods best adapted to do this depend on whether one wants some randomly chosen permutations, or all permutations, and in the latter case if a specific ordering is required. Another question is whether possible equality among entries in the given sequence is to be taken into account; if so, one should only generate distinct multiset permutations of the sequence. An obvious way to generate permutations of n is to generate values for the Lehmer code (possibly using the factorial number system representation of integers up to n !), and convert those into the corresponding permutations. However, the latter step, while straightforward, is hard to implement efficiently, because it requires n operations each of selection from a sequence and deletion from it, at an arbitrary position; of the obvious representations of the sequence as an array or a linked list , both require (for different reasons) about n*2/4 operations to perform the conversion. With *n likely to be rather small (especially if generation of all permutations is needed) that is not too much of a problem, but it turns out that both for random and for systematic generation there are simple alternatives that do considerably better. For this reason it does not seem useful, although certainly possible, to employ a special data structure that would allow performing the conversion from Lehmer code to permutation in O ( n log n ) time. \u751f\u6210n\u7684\u6392\u5217\u7684\u4e00\u4e2a\u660e\u663e\u7684\u65b9\u6cd5\u662f\u751f\u6210Lehmer\u4ee3\u7801\u7684\u503c(\u53ef\u80fd\u4f7f\u7528\u5230n\u7684\u6574\u6570\u7684\u9636\u4e58\u6570\u5b57\u7cfb\u7edf\u8868\u793a)\uff0c\u5e76\u5c06\u5b83\u4eec\u8f6c\u6362\u6210\u76f8\u5e94\u7684\u6392\u5217\u3002 \u4f46\u662f\uff0c\u540e\u9762\u7684\u6b65\u9aa4\u867d\u7136\u7b80\u5355\uff0c\u4f46\u662f\u5f88\u96be\u6709\u6548\u5730\u5b9e\u73b0\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5728\u4efb\u610f\u4f4d\u7f6e\u5bf9\u5e8f\u5217\u8fdb\u884cn\u6b21\u9009\u62e9\u548c\u5220\u9664\u64cd\u4f5c; \u5e8f\u5217\u7684\u660e\u663e\u8868\u793a\u5f62\u5f0f\u4e3a\u6570\u7ec4\u6216\u94fe\u8868\uff0c\u4e24\u8005\u90fd\u9700\u8981(\u51fa\u4e8e\u4e0d\u540c\u7684\u539f\u56e0)\u5927\u7ea6n2/4\u64cd\u4f5c\u6765\u6267\u884c\u8f6c\u6362\u3002 n\u53ef\u80fd\u5f88\u5c0f(\u7279\u522b\u662f\u5982\u679c\u9700\u8981\u751f\u6210\u6240\u6709\u6392\u5217)\uff0c\u8fd9\u4e0d\u662f\u4ec0\u4e48\u5927\u95ee\u9898\uff0c\u4f46\u4e8b\u5b9e\u8bc1\u660e\uff0c\u65e0\u8bba\u662f\u968f\u673a\u751f\u6210\u8fd8\u662f\u7cfb\u7edf\u751f\u6210\uff0c\u90fd\u6709\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6cd5\u53ef\u4ee5\u505a\u5f97\u66f4\u597d\u3002 \u56e0\u6b64\uff0c\u4f7f\u7528\u4e00\u79cd\u7279\u6b8a\u7684\u6570\u636e\u7ed3\u6784\uff0c\u5141\u8bb8\u5728O(n log n)\u65f6\u95f4\u5185\u6267\u884c\u4eceLehmer\u4ee3\u7801\u5230\u7f6e\u6362\u7684\u8f6c\u6362\uff0c\u867d\u7136\u8fd9\u79cd\u6570\u636e\u7ed3\u6784\u80af\u5b9a\u662f\u53ef\u884c\u7684\uff0c\u4f46\u4f3c\u4e4e\u5e76\u4e0d\u6709\u7528\u3002","title":"Algorithms to generate permutations"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Permutation/#to#read","text":"geeksforgeeks Write a program to print all permutations of a given string https://leetcode-cn.com/problems/permutations-ii/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liwe-2/","title":"TO READ"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Reading-list/","text":"Algorithm to return all combinations of k elements from n The coolest way to generate combinations Algorithms: Generating Combinations #100DaysOfCode GENERATING COMBINATIONS Print all possible combinations of r elements in a given array of size n Print all possible strings of length k that can be formed from a set of n characters","title":"Reading-list"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Reading-list/#algorithm#to#return#all#combinations#of#k#elements#from#n","text":"","title":"Algorithm to return all combinations of k elements from n"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Reading-list/#the#coolest#way#to#generate#combinations","text":"","title":"The coolest way to generate combinations"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Reading-list/#algorithms#generating#combinations#100daysofcode","text":"","title":"Algorithms: Generating Combinations #100DaysOfCode"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Reading-list/#generating#combinations","text":"","title":"GENERATING COMBINATIONS"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Reading-list/#print#all#possible#combinations#of#r#elements#in#a#given#array#of#size#n","text":"","title":"Print all possible combinations of r elements in a given array of size n"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Reading-list/#print#all#possible#strings#of#length#k#that#can#be#formed#from#a#set#of#n#characters","text":"","title":"Print all possible strings of length k that can be formed from a set of n characters"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/","text":"Write a program to print all permutations of a given string \u95ee\u9898\u63cf\u8ff0 A permutation, also called an \u201carrangement number\u201d or \u201corder,\u201d is a rearrangement of the elements of an ordered list S into a one-to-one correspondence with S itself. A string of length n has n! permutation. Source: Mathword( http://mathworld.wolfram.com/Permutation.html ) Below are the permutations of string ABC. ABC ACB BAC BCA CBA CAB \u7b97\u6cd5 Here is a solution that is used as a basis in backtracking. SUMMARY : \u4e0a\u8ff0\u56fe\u5df2\u7ecf\u975e\u5e38\u597d\u5730\u5c55\u793a\u4e86 backtracking \u7b97\u6cd5\u7684\u8c03\u7528\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e0b\u9762\u7684\u4ee3\u7801\u6765\u770b\u7684\u8bdd\uff0c\u5bf9\u4e0a\u9762\u7684recursion tree\u8fdb\u884c\u5148\u5e8f\u904d\u5386\u5c31\u662f\u4e0b\u9762\u7684\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\uff1b\u8be5\u51fd\u6570\u7684\u76ee\u7684\u662f\u751f\u6210\u6240\u6709\u7684permutation\u7684\uff0c\u5373\u5b83\u9700\u8981\u7a77\u4e3e\uff1b \u751f\u6210permutation\u7684\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u8ff0\u4e3a\uff1a \u4ecen\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c0\u4e2a\u5143\u7d20 \u4ecen-1\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c1\u4e2a\u5143\u7d20 \u4ecen-2\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c2\u4e2a\u5143\u7d20 ... \u4ece2\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2cn-2\u4e2a\u5143\u7d20 \u4ece1\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2cn-1\u4e2a\u5143\u7d20 \u663e\u7136\u4e0a\u8ff0\u8fc7\u7a0b\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u4e0a\u8ff0\u8fc7\u7a0b\u5982\u679c\u4f7f\u7528\u56fe\u5f62\u5316\u6765\u5c55\u793a\u7684\u8bdd\uff0c\u5176\u5b9e\u548c\u4e0a\u9762\u7684recursion tree\u662f\u5b8c\u7f8e\u5bf9\u5e94\u7684\uff1a recursion tree\u7684\u7b2c\u4e00\u5c42\u8282\u70b9\uff0c\u5bf9\u5e94\u4e86\u4ecen\uff08n=3\uff09\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c0\u4e2a\u5143\u7d20 recursion tree\u7684\u7b2c\u4e8c\u5c42\u8282\u70b9\uff0c\u5bf9\u5e94\u4e86\u4ecen-1\uff08n-1=2\uff09\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c1\u4e2a\u5143\u7d20 recursion tree\u4e2d\u7684\u6bcf\u4e00\u6761\u8def\u5f84\u5c31\u5bf9\u5e94\u4e86\u4e00\u4e2a\u7ec4\u5408\uff1b \u6240\u4ee5\uff0c\u5171\u6709 n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1 n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1 \u79cd\u6392\u5217\uff0c n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1 n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1 \u662f\u548c\u4e0a\u8ff0recursion tree\u5b8c\u7f8e\u5bf9\u5e94\u7684: \u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \uff0c\u5bf9\u5e94\u4e86\u7b2c\u4e00\u68f5\u5b50\u6811\uff0c\u663e\u7136\u5171\u6709n\u68f5\u7c7b\u4f3c\u8fd9\u6837\u7684\u5b50\u6811\uff0c\u6240\u4ee5\u662f n \\times n \\times \u7531\u4e8e\u6211\u4eec\u7684backtrack\u7b97\u6cd5\u76ee\u7684\u662f**\u7a77\u4e3e**\uff0c\u6240\u4ee5\u5728\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \u540e\uff0c\u8fd8\u9700\u8981\u53bb\u5c1d\u8bd5\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a B \u7684\u60c5\u51b5\uff0c\u8fd8\u9700\u8981\u53bb\u4ea7\u751f\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a C \u7684\u60c5\u51b5\uff1b\u4ee5\u6b64\u7c7b\u63a8\uff0c\u5728\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \u540e\uff0c\u8fd8\u9700\u8981\u53bb\u5c1d\u8bd5\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a B \u7684\u60c5\u51b5\uff0c\u8fd8\u9700\u8981\u53bb\u4ea7\u751f\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a C \u7684\u60c5\u51b5\uff1b\u6240\u4ee5\u6211\u4eec\u9700\u8981\u56de\u6eaf\uff0c\u5373\u56de\u9000\u5230\u4e0a\u4e00\u5c42\u7684\u72b6\u6001\uff0c\u8fd9\u6837\u4e0a\u4e00\u5c42\u5c31\u80fd\u591f\u5c1d\u8bd5\u53e6\u5916\u4e00\u79cd\u60c5\u51b5\u4e86\uff1b\u5176\u5b9e\u8fd9\u5c31\u662f backtracking \u7b97\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\u6240\u5728\u4e86\uff1b \u5bf9\u4e0a\u8ff0\u9012\u5f52\u6570\u8fdb\u884c\u5148\u5e8f\u904d\u5386\u5c31\u5bf9\u5e94\u4e86\u9012\u5f52\u51fd\u6570\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u7b2c\u4e00\u6b21\u7531\u4e0a\u5230\u4e0b\u7ecf\u8fc7\u4e00\u6761\u8fb9\u7684\u65f6\u5019\uff0c\u6267\u884cswap\u51fd\u6570\uff0c\u5f53\u7b2c\u4e8c\u6b21\u7531\u4e0b\u5230\u4e0a\u7ecf\u8fc7\u8be5\u8fb9\u7684\u65f6\u5019\uff0c\u6267\u884c\u65b9\u5411\u76f8\u53cd\u7684swap\u51fd\u6570\uff1b\u8fd9\u4e5f\u8bf4\u660e\u9012\u5f52\u8c03\u7528\u662f\u6df1\u5ea6\u4f18\u5148\u5730\u904d\u5386\u3002 \u90a3\u4e0a\u8ff0\u8fc7\u7a0b\u5982\u4f55\u4f7f\u7528\u9012\u5f52\u51fd\u6570\u6765\u8fdb\u884c\u5b9e\u73b0\u5462\uff1f\u8fd9\u5c31\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5458\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\u4e86\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5e38\u5e38\u8003\u8651\u7684\u662f\u4f7f\u7528 backtracking \u7b97\u6cd5\uff1a C++ // C++ program to print all // permutations with duplicates allowed #include <bits/stdc++.h> using namespace std ; // Function to print permutations of string // This function takes three parameters: // 1. String // 2. Starting index of the string // 3. Ending index of the string. void permute ( string a , int l , int r ) { // Base case if ( l == r ) cout << a << endl ; else { // Permutations made for ( int i = l ; i <= r ; i ++ ) { // Swapping done swap ( a [ l ], a [ i ]); // Recursion called permute ( a , l + 1 , r ); //backtrack swap ( a [ l ], a [ i ]); } } } // Driver Code int main () { string str = \"ABC\" ; int n = str . size (); permute ( str , 0 , n -1 ); return 0 ; } // This is code is contributed by rathbhupendra C // C program to print all permutations with duplicates allowed #include <stdio.h> #include <string.h> /* Function to swap values at two pointers */ void swap ( char * x , char * y ) { char temp ; temp = * x ; * x = * y ; * y = temp ; } /* Function to print permutations of string This function takes three parameters: 1. String 2. Starting index of the string 3. Ending index of the string. */ void permute ( char * a , int l , int r ) { int i ; if ( l == r ) printf ( \"%s \\n \" , a ); else { for ( i = l ; i <= r ; i ++ ) { swap (( a + l ), ( a + i )); permute ( a , l + 1 , r ); swap (( a + l ), ( a + i )); //backtrack } } } /* Driver program to test above functions */ int main () { char str [] = \"ABC\" ; int n = strlen ( str ); permute ( str , 0 , n -1 ); return 0 ; }","title":"Backtrack-enum-all-permutations-of-string"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#write#a#program#to#print#all#permutations#of#a#given#string","text":"","title":"Write a program to print all permutations of a given string"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#_1","text":"A permutation, also called an \u201carrangement number\u201d or \u201corder,\u201d is a rearrangement of the elements of an ordered list S into a one-to-one correspondence with S itself. A string of length n has n! permutation. Source: Mathword( http://mathworld.wolfram.com/Permutation.html ) Below are the permutations of string ABC. ABC ACB BAC BCA CBA CAB","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#_2","text":"Here is a solution that is used as a basis in backtracking. SUMMARY : \u4e0a\u8ff0\u56fe\u5df2\u7ecf\u975e\u5e38\u597d\u5730\u5c55\u793a\u4e86 backtracking \u7b97\u6cd5\u7684\u8c03\u7528\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e0b\u9762\u7684\u4ee3\u7801\u6765\u770b\u7684\u8bdd\uff0c\u5bf9\u4e0a\u9762\u7684recursion tree\u8fdb\u884c\u5148\u5e8f\u904d\u5386\u5c31\u662f\u4e0b\u9762\u7684\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\uff1b\u8be5\u51fd\u6570\u7684\u76ee\u7684\u662f\u751f\u6210\u6240\u6709\u7684permutation\u7684\uff0c\u5373\u5b83\u9700\u8981\u7a77\u4e3e\uff1b \u751f\u6210permutation\u7684\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u8ff0\u4e3a\uff1a \u4ecen\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c0\u4e2a\u5143\u7d20 \u4ecen-1\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c1\u4e2a\u5143\u7d20 \u4ecen-2\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c2\u4e2a\u5143\u7d20 ... \u4ece2\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2cn-2\u4e2a\u5143\u7d20 \u4ece1\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2cn-1\u4e2a\u5143\u7d20 \u663e\u7136\u4e0a\u8ff0\u8fc7\u7a0b\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u4e0a\u8ff0\u8fc7\u7a0b\u5982\u679c\u4f7f\u7528\u56fe\u5f62\u5316\u6765\u5c55\u793a\u7684\u8bdd\uff0c\u5176\u5b9e\u548c\u4e0a\u9762\u7684recursion tree\u662f\u5b8c\u7f8e\u5bf9\u5e94\u7684\uff1a recursion tree\u7684\u7b2c\u4e00\u5c42\u8282\u70b9\uff0c\u5bf9\u5e94\u4e86\u4ecen\uff08n=3\uff09\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c0\u4e2a\u5143\u7d20 recursion tree\u7684\u7b2c\u4e8c\u5c42\u8282\u70b9\uff0c\u5bf9\u5e94\u4e86\u4ecen-1\uff08n-1=2\uff09\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c1\u4e2a\u5143\u7d20 recursion tree\u4e2d\u7684\u6bcf\u4e00\u6761\u8def\u5f84\u5c31\u5bf9\u5e94\u4e86\u4e00\u4e2a\u7ec4\u5408\uff1b \u6240\u4ee5\uff0c\u5171\u6709 n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1 n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1 \u79cd\u6392\u5217\uff0c n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1 n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1 \u662f\u548c\u4e0a\u8ff0recursion tree\u5b8c\u7f8e\u5bf9\u5e94\u7684: \u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \uff0c\u5bf9\u5e94\u4e86\u7b2c\u4e00\u68f5\u5b50\u6811\uff0c\u663e\u7136\u5171\u6709n\u68f5\u7c7b\u4f3c\u8fd9\u6837\u7684\u5b50\u6811\uff0c\u6240\u4ee5\u662f n \\times n \\times \u7531\u4e8e\u6211\u4eec\u7684backtrack\u7b97\u6cd5\u76ee\u7684\u662f**\u7a77\u4e3e**\uff0c\u6240\u4ee5\u5728\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \u540e\uff0c\u8fd8\u9700\u8981\u53bb\u5c1d\u8bd5\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a B \u7684\u60c5\u51b5\uff0c\u8fd8\u9700\u8981\u53bb\u4ea7\u751f\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a C \u7684\u60c5\u51b5\uff1b\u4ee5\u6b64\u7c7b\u63a8\uff0c\u5728\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \u540e\uff0c\u8fd8\u9700\u8981\u53bb\u5c1d\u8bd5\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a B \u7684\u60c5\u51b5\uff0c\u8fd8\u9700\u8981\u53bb\u4ea7\u751f\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a C \u7684\u60c5\u51b5\uff1b\u6240\u4ee5\u6211\u4eec\u9700\u8981\u56de\u6eaf\uff0c\u5373\u56de\u9000\u5230\u4e0a\u4e00\u5c42\u7684\u72b6\u6001\uff0c\u8fd9\u6837\u4e0a\u4e00\u5c42\u5c31\u80fd\u591f\u5c1d\u8bd5\u53e6\u5916\u4e00\u79cd\u60c5\u51b5\u4e86\uff1b\u5176\u5b9e\u8fd9\u5c31\u662f backtracking \u7b97\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\u6240\u5728\u4e86\uff1b \u5bf9\u4e0a\u8ff0\u9012\u5f52\u6570\u8fdb\u884c\u5148\u5e8f\u904d\u5386\u5c31\u5bf9\u5e94\u4e86\u9012\u5f52\u51fd\u6570\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u7b2c\u4e00\u6b21\u7531\u4e0a\u5230\u4e0b\u7ecf\u8fc7\u4e00\u6761\u8fb9\u7684\u65f6\u5019\uff0c\u6267\u884cswap\u51fd\u6570\uff0c\u5f53\u7b2c\u4e8c\u6b21\u7531\u4e0b\u5230\u4e0a\u7ecf\u8fc7\u8be5\u8fb9\u7684\u65f6\u5019\uff0c\u6267\u884c\u65b9\u5411\u76f8\u53cd\u7684swap\u51fd\u6570\uff1b\u8fd9\u4e5f\u8bf4\u660e\u9012\u5f52\u8c03\u7528\u662f\u6df1\u5ea6\u4f18\u5148\u5730\u904d\u5386\u3002 \u90a3\u4e0a\u8ff0\u8fc7\u7a0b\u5982\u4f55\u4f7f\u7528\u9012\u5f52\u51fd\u6570\u6765\u8fdb\u884c\u5b9e\u73b0\u5462\uff1f\u8fd9\u5c31\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5458\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\u4e86\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5e38\u5e38\u8003\u8651\u7684\u662f\u4f7f\u7528 backtracking \u7b97\u6cd5\uff1a","title":"\u7b97\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#c","text":"// C++ program to print all // permutations with duplicates allowed #include <bits/stdc++.h> using namespace std ; // Function to print permutations of string // This function takes three parameters: // 1. String // 2. Starting index of the string // 3. Ending index of the string. void permute ( string a , int l , int r ) { // Base case if ( l == r ) cout << a << endl ; else { // Permutations made for ( int i = l ; i <= r ; i ++ ) { // Swapping done swap ( a [ l ], a [ i ]); // Recursion called permute ( a , l + 1 , r ); //backtrack swap ( a [ l ], a [ i ]); } } } // Driver Code int main () { string str = \"ABC\" ; int n = str . size (); permute ( str , 0 , n -1 ); return 0 ; } // This is code is contributed by rathbhupendra","title":"C++"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#c_1","text":"// C program to print all permutations with duplicates allowed #include <stdio.h> #include <string.h> /* Function to swap values at two pointers */ void swap ( char * x , char * y ) { char temp ; temp = * x ; * x = * y ; * y = temp ; } /* Function to print permutations of string This function takes three parameters: 1. String 2. Starting index of the string 3. Ending index of the string. */ void permute ( char * a , int l , int r ) { int i ; if ( l == r ) printf ( \"%s \\n \" , a ); else { for ( i = l ; i <= r ; i ++ ) { swap (( a + l ), ( a + i )); permute ( a , l + 1 , r ); swap (( a + l ), ( a + i )); //backtrack } } } /* Driver program to test above functions */ int main () { char str [] = \"ABC\" ; int n = strlen ( str ); permute ( str , 0 , n -1 ); return 0 ; }","title":"C"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-distinct-permutation-of-with-duplicate/","text":"Print all distinct permutations of a given string with duplicates \u95ee\u9898\u63cf\u8ff0 \u7b97\u6cd5 Print all distinct permutations of a given string with duplicates \u95ee\u9898\u63cf\u8ff0 Given a string that may contain duplicates, write a function to print all permutations of given string such that no permutation is repeated in output. Examples: Input: str[] = \"AB\" Output: AB BA Input: str[] = \"AA\" Output: AA Input: str[] = \"ABC\" Output: ABC ACB BAC BCA CBA CAB Input: str[] = \"ABA\" Output: ABA AAB BAA Input: str[] = \"ABCA\" Output: AABC AACB ABAC ABCA ACBA ACAB BAAC BACA BCAA CABA CAAB CBAA We have discussed an algorithm to print all permutations in below post. It is strongly recommended to refer below post as a prerequisite of this post. \u7b97\u6cd5 Write a C program to print all permutations of a given string The algorithm discussed on above link doesn\u2019t handle duplicates. // Program to print all permutations of a // string in sorted order. #include <stdio.h> #include <stdlib.h> #include <string.h> /* Following function is needed for library function qsort(). */ int compare ( const void * a , const void * b ) { return ( * ( char * ) a - * ( char * ) b ); } // A utility function two swap two characters // a and b void swap ( char * a , char * b ) { char t = * a ; * a = * b ; * b = t ; } // This function finds the index of the // smallest character which is greater // than 'first' and is present in str[l..h] int findCeil ( char str [], char first , int l , int h ) { // initialize index of ceiling element int ceilIndex = l ; // Now iterate through rest of the // elements and find the smallest // character greater than 'first' for ( int i = l + 1 ; i <= h ; i ++ ) if ( str [ i ] > first && str [ i ] < str [ ceilIndex ]) ceilIndex = i ; return ceilIndex ; } // Print all permutations of str in sorted order void sortedPermutations ( char str []) { // Get size of string int size = strlen ( str ); // Sort the string in increasing order qsort ( str , size , sizeof ( str [ 0 ]), compare ); // Print permutations one by one bool isFinished = false ; while ( ! isFinished ) { // print this permutation static int x = 1 ; printf ( \"%d %s \\n \" , x ++ , str ); // Find the rightmost character // which is smaller than its next // character. Let us call it 'first // char' int i ; for ( i = size - 2 ; i >= 0 ; -- i ) if ( str [ i ] < str [ i + 1 ]) break ; // If there is no such character, all // are sorted in decreasing order, // means we just printed the last // permutation and we are done. if ( i == -1 ) isFinished = true ; else { // Find the ceil of 'first char' // in right of first character. // Ceil of a character is the // smallest character greater // than it int ceilIndex = findCeil ( str , str [ i ], i + 1 , size - 1 ); // Swap first and second characters swap ( & str [ i ], & str [ ceilIndex ]); // Sort the string on right of 'first char' qsort ( str + i + 1 , size - i - 1 , sizeof ( str [ 0 ]), compare ); } } } // Driver program to test above function int main () { char str [] = \"ACBC\" ; sortedPermutations ( str ); return 0 ; }","title":"Enum-distinct-permutation-of-with-duplicate"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-distinct-permutation-of-with-duplicate/#print#all#distinct#permutations#of#a#given#string#with#duplicates","text":"","title":"Print all distinct permutations of a given string with duplicates"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-distinct-permutation-of-with-duplicate/#_1","text":"Given a string that may contain duplicates, write a function to print all permutations of given string such that no permutation is repeated in output. Examples: Input: str[] = \"AB\" Output: AB BA Input: str[] = \"AA\" Output: AA Input: str[] = \"ABC\" Output: ABC ACB BAC BCA CBA CAB Input: str[] = \"ABA\" Output: ABA AAB BAA Input: str[] = \"ABCA\" Output: AABC AACB ABAC ABCA ACBA ACAB BAAC BACA BCAA CABA CAAB CBAA We have discussed an algorithm to print all permutations in below post. It is strongly recommended to refer below post as a prerequisite of this post.","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-distinct-permutation-of-with-duplicate/#_2","text":"Write a C program to print all permutations of a given string The algorithm discussed on above link doesn\u2019t handle duplicates. // Program to print all permutations of a // string in sorted order. #include <stdio.h> #include <stdlib.h> #include <string.h> /* Following function is needed for library function qsort(). */ int compare ( const void * a , const void * b ) { return ( * ( char * ) a - * ( char * ) b ); } // A utility function two swap two characters // a and b void swap ( char * a , char * b ) { char t = * a ; * a = * b ; * b = t ; } // This function finds the index of the // smallest character which is greater // than 'first' and is present in str[l..h] int findCeil ( char str [], char first , int l , int h ) { // initialize index of ceiling element int ceilIndex = l ; // Now iterate through rest of the // elements and find the smallest // character greater than 'first' for ( int i = l + 1 ; i <= h ; i ++ ) if ( str [ i ] > first && str [ i ] < str [ ceilIndex ]) ceilIndex = i ; return ceilIndex ; } // Print all permutations of str in sorted order void sortedPermutations ( char str []) { // Get size of string int size = strlen ( str ); // Sort the string in increasing order qsort ( str , size , sizeof ( str [ 0 ]), compare ); // Print permutations one by one bool isFinished = false ; while ( ! isFinished ) { // print this permutation static int x = 1 ; printf ( \"%d %s \\n \" , x ++ , str ); // Find the rightmost character // which is smaller than its next // character. Let us call it 'first // char' int i ; for ( i = size - 2 ; i >= 0 ; -- i ) if ( str [ i ] < str [ i + 1 ]) break ; // If there is no such character, all // are sorted in decreasing order, // means we just printed the last // permutation and we are done. if ( i == -1 ) isFinished = true ; else { // Find the ceil of 'first char' // in right of first character. // Ceil of a character is the // smallest character greater // than it int ceilIndex = findCeil ( str , str [ i ], i + 1 , size - 1 ); // Swap first and second characters swap ( & str [ i ], & str [ ceilIndex ]); // Sort the string on right of 'first char' qsort ( str + i + 1 , size - i - 1 , sizeof ( str [ 0 ]), compare ); } } } // Driver program to test above function int main () { char str [] = \"ACBC\" ; sortedPermutations ( str ); return 0 ; }","title":"\u7b97\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/","text":"Generate permutation Generate list of all possible permutations of a string Write a program to print all permutations of a given string Generating all permutations of a given string Iterative program to generate distinct Permutations of a String Generate list of all possible permutations of a string Write a program to print all permutations of a given string Generating all permutations of a given string","title":"Enum-permutation-reading-list"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generate#permutation","text":"","title":"Generate permutation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generate#list#of#all#possible#permutations#of#a#string","text":"","title":"Generate list of all possible permutations of a string"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#write#a#program#to#print#all#permutations#of#a#given#string","text":"","title":"Write a program to print all permutations of a given string"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generating#all#permutations#of#a#given#string","text":"","title":"Generating all permutations of a given string"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#iterative#program#to#generate#distinct#permutations#of#a#string","text":"","title":"Iterative program to generate distinct Permutations of a String"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generate#list#of#all#possible#permutations#of#a#string_1","text":"","title":"Generate list of all possible permutations of a string"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#write#a#program#to#print#all#permutations#of#a#given#string_1","text":"","title":"Write a program to print all permutations of a given string"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generating#all#permutations#of#a#given#string_1","text":"","title":"Generating all permutations of a given string"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Branch-and-Bound/Branch-and-bound/","text":"Branch and bound Branch and bound","title":"Branch and bound"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Branch-and-Bound/Branch-and-bound/#branch#and#bound","text":"","title":"Branch and bound"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Branch-and-Bound/Branch-and-cut/","text":"Branch and cut Branch and cut","title":"Branch and cut"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Implementation/Branch-and-Bound/Branch-and-cut/#branch#and#cut","text":"","title":"Branch and cut"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Property-of-search-algorithm/Completeness/","text":"Completeness \"completeness\"\u662fsearch algorithm\u4e2d\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6982\u5ff5\uff0c\u7ef4\u57fa\u767e\u79d1 Search algorithm \u4e2d\u5bf9\u5b83\u7684\u89e3\u91ca\u662f\uff1a Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these tree-search methods are guaranteed to find the exact or optimal solution, if given enough time. This is called \" completeness \". \u5b83\u5e76\u6ca1\u6709\u7ed9\u51fa\u51c6\u786e\u7684\u63cf\u8ff0\uff0c\" completeness \"\u6240\u94fe\u63a5\u5230\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Completeness (logic) \uff0c\u8fd9\u7bc7\u6587\u7ae0\u662f\u4e0d\u6613\u7406\u89e3\u7684\u3002\u4e0b\u9762\u662f\u4e00\u4e9b\u5206\u6790\uff1a What is meant by search algorithm completeness? If an algorithm is complete, it means that if at least one solution exists then the algorithm is guaranteed find a solution in a finite amount of time.","title":"Completeness"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Property-of-search-algorithm/Completeness/#completeness","text":"\"completeness\"\u662fsearch algorithm\u4e2d\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6982\u5ff5\uff0c\u7ef4\u57fa\u767e\u79d1 Search algorithm \u4e2d\u5bf9\u5b83\u7684\u89e3\u91ca\u662f\uff1a Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these tree-search methods are guaranteed to find the exact or optimal solution, if given enough time. This is called \" completeness \". \u5b83\u5e76\u6ca1\u6709\u7ed9\u51fa\u51c6\u786e\u7684\u63cf\u8ff0\uff0c\" completeness \"\u6240\u94fe\u63a5\u5230\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Completeness (logic) \uff0c\u8fd9\u7bc7\u6587\u7ae0\u662f\u4e0d\u6613\u7406\u89e3\u7684\u3002\u4e0b\u9762\u662f\u4e00\u4e9b\u5206\u6790\uff1a","title":"Completeness"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Property-of-search-algorithm/Completeness/#what#is#meant#by#search#algorithm#completeness","text":"If an algorithm is complete, it means that if at least one solution exists then the algorithm is guaranteed find a solution in a finite amount of time.","title":"What is meant by search algorithm completeness?"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Property-of-search-algorithm/Optimality/","text":"Optimality What is meant by search algorithm optimality? If a search algorithm is optimal, then when it finds a solution it finds the best solution.","title":"Optimality"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Property-of-search-algorithm/Optimality/#optimality","text":"","title":"Optimality"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/Property-of-search-algorithm/Optimality/#what#is#meant#by#search#algorithm#optimality","text":"If a search algorithm is optimal, then when it finds a solution it finds the best solution.","title":"What is meant by search algorithm optimality?"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/State-space/State-space-search/","text":"State space search wikipedia State space search wikipedia State space \u72b6\u6001\u7a7a\u95f4VS\u89e3\u7a7a\u95f4 **\u89e3\u7a7a\u95f4**\u662f\u4e00\u79cd\u975e\u5e38\u5178\u578b\u7684state space\u3002","title":"State-space-search"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/State-space/State-space-search/#state#space#search","text":"","title":"State space search"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/State-space/State-space-search/#wikipedia#state#space#search","text":"","title":"wikipedia State space search"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/State-space/State-space-search/#wikipedia#state#space","text":"","title":"wikipedia State space"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Search/State-space/State-space-search/#vs","text":"**\u89e3\u7a7a\u95f4**\u662f\u4e00\u79cd\u975e\u5e38\u5178\u578b\u7684state space\u3002","title":"\u72b6\u6001\u7a7a\u95f4VS\u89e3\u7a7a\u95f4"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0\u6392\u5e8f\u7b97\u6cd5\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u6392\u5e8f\u7b97\u6cd5\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Summary/","text":"Summary Timsort vs quicksort Comparison between timsort and quicksort","title":"Summary"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Summary/#summary","text":"","title":"Summary"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Summary/#timsort#vs#quicksort","text":"Comparison between timsort and quicksort","title":"Timsort vs quicksort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Quick-sort/","text":"Quick sort \u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790 #include <cstdio> #include <iostream> #include <algorithm> /** * @param start \u6570\u7ec4\u8d77\u59cb\u4e0b\u6807 * @param end \u6570\u7ec4\u7ec8\u6b62\u4e0b\u6807 **/ template < typename T > void QuickSort ( T a [], int start , int end ); template < typename T > int Partition ( T a [], int start , int end ); template < typename T > void Swap ( T & a , T & b ) { T tmp = a ; a = b ; b = tmp ; } /*Displays the array, passed to this method*/ template < typename T > void display ( T arr [], int n ); int main () { int n ; std :: cout << \"Enter size of array: \\n \" ; std :: cin >> n ; // E.g. 8 std :: cout << \"Enter the elements of the array \\n \" ; int i ; int * arr = new int [ n ]; for ( i = 0 ; i < n ; i ++ ) { std :: cin >> arr [ i ]; } std :: cout << \"Original array: \" ; display ( arr , n ); // Original array : 10 11 9 8 4 7 3 8 QuickSort ( arr , 0 , n - 1 ); std :: cout << \"Sorted array: \" ; display ( arr , n ); // Sorted array : 3 4 7 8 8 9 10 11 getchar (); return 0 ; } /** * @param start \u6570\u7ec4\u8d77\u59cb\u4e0b\u6807 * @param end \u6570\u7ec4\u7ec8\u6b62\u4e0b\u6807 **/ template < typename T > void QuickSort ( T a [], int start , int end ) { if ( start < end ) { int p = Partition ( a , start , end ); // pivot index QuickSort ( a , start , p - 1 ); // \u5bf9\u5de6\u534a\u6bb5\u6392\u5e8f QuickSort ( a , p + 1 , end ); // \u5bf9\u53f3\u534a\u6bb5\u6392\u5e8f } } template < typename T > int Partition ( T a [], int start , int end ) { int i = start , j = end + 1 ; T pivot = a [ start ]; while ( true ) { while ( a [ ++ i ] < pivot and i < end ) ; while ( a [ -- j ] > pivot ) ; if ( i >= j ) break ; std :: swap ( a [ i ], a [ j ]); // \u4f7f\u7528\u6807\u51c6\u5e93\u7684swap\u51fd\u6570 } a [ start ] = a [ j ]; a [ j ] = pivot ; return j ; } /*Displays the array, passed to this method*/ template < typename T > void display ( T arr [], int n ) { int i ; for ( i = 0 ; i < n ; i ++ ) { std :: cout << arr [ i ] << \" \" ; } std :: cout << \" \\n \" ; } \u6d4b\u8bd5\u7528\u4f8b\uff1a [5, 4, 3, 2, 1] \u4f7f\u7528\u4e0a\u8ff0\u7b97\u6cd5\u5bf9 [5, 4, 3, 2, 1] \u8fdb\u884c\u6392\u5e8f\uff1a while(a[++i]<x and i<end); \u9000\u51fa\u65f6\uff0c i \u7684\u503c\u4e3a end \u53734\uff1b while(a[--j]>x); \u9000\u51fa\u65f6\uff0c j \u7684\u503c\u4e3a 4 \u3002 \u6d4b\u8bd5\u7528\u4f8b\uff1a [10, 11, 9, 8, 4, 7, 3, 8] hackerearth Quick Sort \u8bb2\u89e3\u5730\u975e\u5e38\u4e0d\u9519\u3002 int partition ( int A [], int start , int end ) { int i = start + 1 ; int piv = A [ start ] ; //make the first element as pivot element. for ( int j = start + 1 ; j <= end ; j ++ ) { /*rearrange the array by putting elements which are less than pivot on one side and which are greater that on other. */ if ( A [ j ] < piv ) { swap ( A [ i ], A [ j ]); i += 1 ; } } swap ( A [ start ] , A [ i -1 ] ) ; //put the pivot element in its proper place. return i -1 ; //return the position of the pivot } i \u662f\u7528\u4e8e\u5b9a\u754c\u7684\u3002 Now, let us see the recursive function Quick_sort : void quick_sort ( int A [ ] , int start , int end ) { if ( start < end ) { //stores the position of pivot element int piv_pos = partition ( A , start , end ) ; quick_sort ( A , start , piv_pos -1 ); //sorts the left side of pivot. quick_sort ( A , piv_pos + 1 , end ) ; //sorts the right side of pivot. } } \u5b8c\u6574\u7a0b\u5e8f\u5982\u4e0b\uff1a #include <stdio.h> #include <stdlib.h> /*Displays the array, passed to this method*/ void display ( int arr [], int n ) { int i ; for ( i = 0 ; i < n ; i ++ ) { printf ( \"%d \" , arr [ i ]); } printf ( \" \\n \" ); } /*Swap function to swap two values*/ void swap ( int * first , int * second ) { int temp = * first ; * first = * second ; * second = temp ; } int partition ( int A [], int start , int end ) { int i = start + 1 ; int piv = A [ start ] ; //make the first element as pivot element. for ( int j = start + 1 ; j <= end ; j ++ ) { /*rearrange the array by putting elements which are less than pivot on one side and which are greater that on other. */ if ( A [ j ] < piv ) { swap ( A [ i ], A [ j ]); i += 1 ; } } swap ( A [ start ] , A [ i -1 ] ) ; //put the pivot element in its proper place. return i -1 ; //return the position of the pivot } /*This is where the sorting of the array takes place arr[] --- Array to be sorted lower --- Starting index upper --- Ending index */ void quickSort ( int arr [], int lower , int upper ) { if ( upper > lower ) { // partitioning index is returned by the partition method , partition element is at its correct poition int partitionIndex = partition ( arr , lower , upper ); // Sorting elements before and after the partition index quickSort ( arr , lower , partitionIndex - 1 ); quickSort ( arr , partitionIndex + 1 , upper ); } } int main () { int n ; printf ( \"Enter size of array: \\n \" ); scanf ( \"%d\" , & n ); // E.g. 8 printf ( \"Enter the elements of the array \\n \" ); int i ; int * arr = ( int * ) malloc ( sizeof ( int ) * n ); for ( i = 0 ; i < n ; i ++ ) { scanf ( \"%d\" , & arr [ i ]); } printf ( \"Original array: \" ); display ( arr , n ); // Original array : 10 11 9 8 4 7 3 8 quickSort ( arr , 0 , n - 1 ); printf ( \"Sorted array: \" ); display ( arr , n ); // Sorted array : 3 4 7 8 8 9 10 11 getchar (); return 0 ; } \u5bf9\u6bd4\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f \u201c\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790\u201d\u4e2d\u7684\u5b9e\u73b0\u7684\u601d\u8def\u662f\uff1a\u7531\u4e24\u7aef\u5411\u4e2d\u95f4\u6269\u5c55 \u201chackerearth Quick Sort \u201d\u4e2d\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u601d\u8def\u662f\uff1a\u81ea\u5de6\u5411\u53f3\u8fdb\u884c\u6269\u5c55\u3002 \u5206\u4e09\u6bb5\uff0c\u53ea\u9700\u8981\u4e24\u4e2aboundary\uff0c\u5206\u522b\u5bf9\u5e94 i \u548c j \u3002 TODO developer.51cto \u5750\u5728\u9a6c\u6876\u4e0a\u770b\u7b97\u6cd5\uff1a\u5feb\u901f\u6392\u5e8f","title":"Quick-sort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Quick-sort/#quick#sort","text":"","title":"Quick sort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Quick-sort/#_1","text":"#include <cstdio> #include <iostream> #include <algorithm> /** * @param start \u6570\u7ec4\u8d77\u59cb\u4e0b\u6807 * @param end \u6570\u7ec4\u7ec8\u6b62\u4e0b\u6807 **/ template < typename T > void QuickSort ( T a [], int start , int end ); template < typename T > int Partition ( T a [], int start , int end ); template < typename T > void Swap ( T & a , T & b ) { T tmp = a ; a = b ; b = tmp ; } /*Displays the array, passed to this method*/ template < typename T > void display ( T arr [], int n ); int main () { int n ; std :: cout << \"Enter size of array: \\n \" ; std :: cin >> n ; // E.g. 8 std :: cout << \"Enter the elements of the array \\n \" ; int i ; int * arr = new int [ n ]; for ( i = 0 ; i < n ; i ++ ) { std :: cin >> arr [ i ]; } std :: cout << \"Original array: \" ; display ( arr , n ); // Original array : 10 11 9 8 4 7 3 8 QuickSort ( arr , 0 , n - 1 ); std :: cout << \"Sorted array: \" ; display ( arr , n ); // Sorted array : 3 4 7 8 8 9 10 11 getchar (); return 0 ; } /** * @param start \u6570\u7ec4\u8d77\u59cb\u4e0b\u6807 * @param end \u6570\u7ec4\u7ec8\u6b62\u4e0b\u6807 **/ template < typename T > void QuickSort ( T a [], int start , int end ) { if ( start < end ) { int p = Partition ( a , start , end ); // pivot index QuickSort ( a , start , p - 1 ); // \u5bf9\u5de6\u534a\u6bb5\u6392\u5e8f QuickSort ( a , p + 1 , end ); // \u5bf9\u53f3\u534a\u6bb5\u6392\u5e8f } } template < typename T > int Partition ( T a [], int start , int end ) { int i = start , j = end + 1 ; T pivot = a [ start ]; while ( true ) { while ( a [ ++ i ] < pivot and i < end ) ; while ( a [ -- j ] > pivot ) ; if ( i >= j ) break ; std :: swap ( a [ i ], a [ j ]); // \u4f7f\u7528\u6807\u51c6\u5e93\u7684swap\u51fd\u6570 } a [ start ] = a [ j ]; a [ j ] = pivot ; return j ; } /*Displays the array, passed to this method*/ template < typename T > void display ( T arr [], int n ) { int i ; for ( i = 0 ; i < n ; i ++ ) { std :: cout << arr [ i ] << \" \" ; } std :: cout << \" \\n \" ; } \u6d4b\u8bd5\u7528\u4f8b\uff1a [5, 4, 3, 2, 1] \u4f7f\u7528\u4e0a\u8ff0\u7b97\u6cd5\u5bf9 [5, 4, 3, 2, 1] \u8fdb\u884c\u6392\u5e8f\uff1a while(a[++i]<x and i<end); \u9000\u51fa\u65f6\uff0c i \u7684\u503c\u4e3a end \u53734\uff1b while(a[--j]>x); \u9000\u51fa\u65f6\uff0c j \u7684\u503c\u4e3a 4 \u3002 \u6d4b\u8bd5\u7528\u4f8b\uff1a [10, 11, 9, 8, 4, 7, 3, 8]","title":"\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Quick-sort/#hackerearth#quick#sort","text":"\u8bb2\u89e3\u5730\u975e\u5e38\u4e0d\u9519\u3002 int partition ( int A [], int start , int end ) { int i = start + 1 ; int piv = A [ start ] ; //make the first element as pivot element. for ( int j = start + 1 ; j <= end ; j ++ ) { /*rearrange the array by putting elements which are less than pivot on one side and which are greater that on other. */ if ( A [ j ] < piv ) { swap ( A [ i ], A [ j ]); i += 1 ; } } swap ( A [ start ] , A [ i -1 ] ) ; //put the pivot element in its proper place. return i -1 ; //return the position of the pivot } i \u662f\u7528\u4e8e\u5b9a\u754c\u7684\u3002 Now, let us see the recursive function Quick_sort : void quick_sort ( int A [ ] , int start , int end ) { if ( start < end ) { //stores the position of pivot element int piv_pos = partition ( A , start , end ) ; quick_sort ( A , start , piv_pos -1 ); //sorts the left side of pivot. quick_sort ( A , piv_pos + 1 , end ) ; //sorts the right side of pivot. } } \u5b8c\u6574\u7a0b\u5e8f\u5982\u4e0b\uff1a #include <stdio.h> #include <stdlib.h> /*Displays the array, passed to this method*/ void display ( int arr [], int n ) { int i ; for ( i = 0 ; i < n ; i ++ ) { printf ( \"%d \" , arr [ i ]); } printf ( \" \\n \" ); } /*Swap function to swap two values*/ void swap ( int * first , int * second ) { int temp = * first ; * first = * second ; * second = temp ; } int partition ( int A [], int start , int end ) { int i = start + 1 ; int piv = A [ start ] ; //make the first element as pivot element. for ( int j = start + 1 ; j <= end ; j ++ ) { /*rearrange the array by putting elements which are less than pivot on one side and which are greater that on other. */ if ( A [ j ] < piv ) { swap ( A [ i ], A [ j ]); i += 1 ; } } swap ( A [ start ] , A [ i -1 ] ) ; //put the pivot element in its proper place. return i -1 ; //return the position of the pivot } /*This is where the sorting of the array takes place arr[] --- Array to be sorted lower --- Starting index upper --- Ending index */ void quickSort ( int arr [], int lower , int upper ) { if ( upper > lower ) { // partitioning index is returned by the partition method , partition element is at its correct poition int partitionIndex = partition ( arr , lower , upper ); // Sorting elements before and after the partition index quickSort ( arr , lower , partitionIndex - 1 ); quickSort ( arr , partitionIndex + 1 , upper ); } } int main () { int n ; printf ( \"Enter size of array: \\n \" ); scanf ( \"%d\" , & n ); // E.g. 8 printf ( \"Enter the elements of the array \\n \" ); int i ; int * arr = ( int * ) malloc ( sizeof ( int ) * n ); for ( i = 0 ; i < n ; i ++ ) { scanf ( \"%d\" , & arr [ i ]); } printf ( \"Original array: \" ); display ( arr , n ); // Original array : 10 11 9 8 4 7 3 8 quickSort ( arr , 0 , n - 1 ); printf ( \"Sorted array: \" ); display ( arr , n ); // Sorted array : 3 4 7 8 8 9 10 11 getchar (); return 0 ; }","title":"hackerearth Quick Sort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Quick-sort/#_2","text":"\u201c\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790\u201d\u4e2d\u7684\u5b9e\u73b0\u7684\u601d\u8def\u662f\uff1a\u7531\u4e24\u7aef\u5411\u4e2d\u95f4\u6269\u5c55 \u201chackerearth Quick Sort \u201d\u4e2d\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u601d\u8def\u662f\uff1a\u81ea\u5de6\u5411\u53f3\u8fdb\u884c\u6269\u5c55\u3002 \u5206\u4e09\u6bb5\uff0c\u53ea\u9700\u8981\u4e24\u4e2aboundary\uff0c\u5206\u522b\u5bf9\u5e94 i \u548c j \u3002","title":"\u5bf9\u6bd4\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Quick-sort/#todo","text":"developer.51cto \u5750\u5728\u9a6c\u6876\u4e0a\u770b\u7b97\u6cd5\uff1a\u5feb\u901f\u6392\u5e8f","title":"TODO"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Timsort/Timsort/","text":"Timsort \u7ef4\u57fa\u767e\u79d1 Timsort hackernoon Timsort drmaciver Understanding timsort Implementation cpp-TimSort","title":"Timsort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Timsort/Timsort/#timsort","text":"","title":"Timsort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Timsort/Timsort/#timsort_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Timsort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Timsort/Timsort/#hackernoon#timsort","text":"","title":"hackernoon Timsort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Timsort/Timsort/#drmaciver#understanding#timsort","text":"","title":"drmaciver Understanding timsort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Timsort/Timsort/#implementation","text":"","title":"Implementation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/Timsort/Timsort/#cpp-timsort","text":"","title":"cpp-TimSort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/merge-sorts/Merge-algorithm/","text":"Merge algorithm Application Merging two lists K-way merging Merge algorithm Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms , most famously merge sort . Application The merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm . Conceptually, merge sort algorithm consists of two steps: Recursively divide the list into sublists of (roughly) equal length, until each sublist contains only one element, or in the case of iterative (bottom up) merge sort, consider a list of n elements as n sub-lists of size 1. A list containing a single element is, by definition, sorted. Repeatedly merge sublists to create a new sorted sublist until the single list contains all elements. The single list is the sorted list. The merge algorithm is used repeatedly in the merge sort algorithm . An example merge sort is given in the illustration. It starts with an unsorted array of 7 integers. The array is divided into 7 partitions; each partition contains 1 element and is sorted. The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left. Merging two lists Merging two sorted lists into one can be done in linear time and linear space. The following pseudocode demonstrates an algorithm that merges input lists (either linked lists or arrays ) A and B into a new list C .[ 1] [ 2] :104 The function head yields the first element of a list; \"dropping\" an element means removing it from its list, typically by incrementing a pointer or index. algorithm merge(A, B) is inputs A, B : list returns list C := new empty list while A is not empty and B is not empty do if head(A) \u2264 head(B) then append head(A) to C drop the head of A else append head(B) to C drop the head of B // By now, either A or B is empty. It remains to empty the other input list. while A is not empty do append head(A) to C drop the head of A while B is not empty do append head(B) to C drop the head of B return C When the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list. In the merge sort algorithm, this subroutine is typically used to merge two sub-arrays A[lo..mid], A[mid..hi] of a single array A. This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above.[ 1] The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised,[ 3] sometimes sacrificing the linear-time bound to produce an O ( n log n ) algorithm;[ 4] see Merge sort \u00a7 Variants for discussion. K-way merging Main article: K-way merge algorithm k -way merging generalizes binary merging to an arbitrary number k of sorted input lists. Applications of k -way merging arise in various sorting algorithms, including patience sorting [ 5] and an external sorting algorithm that divides its input into k = 1/ M \u2212 1 blocks that fit in memory, sorts these one by one, then merges these blocks.[ 2] :119\u2013120 Several solutions to this problem exist. A naive solution is to do a loop over the k lists to pick off the minimum element each time, and repeat this loop until all lists are empty: Input: a list of k lists. While any of the lists is non-empty: Loop over the lists to find the one with the minimum first element. Output the minimum element and remove it from its list. In the worst case , this algorithm performs ( k \u22121)( n \u2212 k /2) element comparisons to perform its work if there are a total of n elements in the lists.[ 6] It can be improved by storing the lists in a priority queue ( min-heap ) keyed by their first element: Build a min-heap h of the k lists, using the first element as the key. While any of the lists is non-empty: Let i = find-min( h ). Output the first element of list i and remove it from its list. Re-heapify h .","title":"Merge-algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/merge-sorts/Merge-algorithm/#merge#algorithm","text":"Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms , most famously merge sort .","title":"Merge algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/merge-sorts/Merge-algorithm/#application","text":"The merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm . Conceptually, merge sort algorithm consists of two steps: Recursively divide the list into sublists of (roughly) equal length, until each sublist contains only one element, or in the case of iterative (bottom up) merge sort, consider a list of n elements as n sub-lists of size 1. A list containing a single element is, by definition, sorted. Repeatedly merge sublists to create a new sorted sublist until the single list contains all elements. The single list is the sorted list. The merge algorithm is used repeatedly in the merge sort algorithm . An example merge sort is given in the illustration. It starts with an unsorted array of 7 integers. The array is divided into 7 partitions; each partition contains 1 element and is sorted. The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left.","title":"Application"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/merge-sorts/Merge-algorithm/#merging#two#lists","text":"Merging two sorted lists into one can be done in linear time and linear space. The following pseudocode demonstrates an algorithm that merges input lists (either linked lists or arrays ) A and B into a new list C .[ 1] [ 2] :104 The function head yields the first element of a list; \"dropping\" an element means removing it from its list, typically by incrementing a pointer or index. algorithm merge(A, B) is inputs A, B : list returns list C := new empty list while A is not empty and B is not empty do if head(A) \u2264 head(B) then append head(A) to C drop the head of A else append head(B) to C drop the head of B // By now, either A or B is empty. It remains to empty the other input list. while A is not empty do append head(A) to C drop the head of A while B is not empty do append head(B) to C drop the head of B return C When the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list. In the merge sort algorithm, this subroutine is typically used to merge two sub-arrays A[lo..mid], A[mid..hi] of a single array A. This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above.[ 1] The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised,[ 3] sometimes sacrificing the linear-time bound to produce an O ( n log n ) algorithm;[ 4] see Merge sort \u00a7 Variants for discussion.","title":"Merging two lists"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/merge-sorts/Merge-algorithm/#k-way#merging","text":"Main article: K-way merge algorithm k -way merging generalizes binary merging to an arbitrary number k of sorted input lists. Applications of k -way merging arise in various sorting algorithms, including patience sorting [ 5] and an external sorting algorithm that divides its input into k = 1/ M \u2212 1 blocks that fit in memory, sorts these one by one, then merges these blocks.[ 2] :119\u2013120 Several solutions to this problem exist. A naive solution is to do a loop over the k lists to pick off the minimum element each time, and repeat this loop until all lists are empty: Input: a list of k lists. While any of the lists is non-empty: Loop over the lists to find the one with the minimum first element. Output the minimum element and remove it from its list. In the worst case , this algorithm performs ( k \u22121)( n \u2212 k /2) element comparisons to perform its work if there are a total of n elements in the lists.[ 6] It can be improved by storing the lists in a priority queue ( min-heap ) keyed by their first element: Build a min-heap h of the k lists, using the first element as the key. While any of the lists is non-empty: Let i = find-min( h ). Output the first element of list i and remove it from its list. Re-heapify h .","title":"K-way merging"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/merge-sorts/Merge-sort/","text":"Merge sort Merge sort","title":"Merge-sort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/merge-sorts/Merge-sort/#merge#sort","text":"","title":"Merge sort"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/Sorting/merge-sorts/k-way-merge-algorithm/","text":"","title":"K-way-merge-algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8bb2\u8ff0\u4e0e\u5b57\u7b26\u4e32\u76f8\u5173\u7684\u7b97\u6cd5\u3002 algorithm to judge whether two string has substring in common Finding all the common substrings of given two strings Check if two strings have a common substring longest common substring problem suffix tree suffix tree Ukkonen's suffix tree algorithm in plain English https://stackoverflow.com/questions/tagged/suffix-tree?tab=Active \u4f7f\u7528\u7f16\u8f91\u8ddd\u79bb \u4e00\u79cdpython\u5b9e\u73b0 def get_num_of_word_in_common ( word1 , word2 ): \"\"\"\u516c\u5171\u8bcd\u7684\u4e2a\u6570\"\"\" return len ( Counter ( word1 ) & Counter ( word2 ))","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/#_1","text":"\u672c\u7ae0\u8bb2\u8ff0\u4e0e\u5b57\u7b26\u4e32\u76f8\u5173\u7684\u7b97\u6cd5\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/#algorithm#to#judge#whether#two#string#has#substring#in#common","text":"Finding all the common substrings of given two strings Check if two strings have a common substring longest common substring problem suffix tree","title":"algorithm to judge whether two string has substring in common"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/#suffix#tree","text":"Ukkonen's suffix tree algorithm in plain English https://stackoverflow.com/questions/tagged/suffix-tree?tab=Active","title":"suffix tree"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/#_2","text":"","title":"\u4f7f\u7528\u7f16\u8f91\u8ddd\u79bb"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/#python","text":"def get_num_of_word_in_common ( word1 , word2 ): \"\"\"\u516c\u5171\u8bcd\u7684\u4e2a\u6570\"\"\" return len ( Counter ( word1 ) & Counter ( word2 ))","title":"\u4e00\u79cdpython\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/String%28computer-science%29/","text":"","title":"String(computer-science)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/Pattern-matching/Pattern-matching/","text":"Pattern matching","title":"[Pattern matching](https://en.wikipedia.org/wiki/Pattern_matching)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/Pattern-matching/Pattern-matching/#pattern#matching","text":"","title":"Pattern matching"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/Regular-expression/wikipedia-Powerset-construction/","text":"Powerset construction In the theory of computation and automata theory , the powerset construction or subset construction is a standard method for converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language . It is important in theory because it establishes that NFAs, despite their additional flexibility, are unable to recognize any language that cannot be recognized by some DFA. It is also important in practice for converting easier-to-construct NFAs into more efficiently executable DFAs. However, if the NFA has n states, the resulting DFA may have up to 2^n 2^n states, an exponentially larger number, which sometimes makes the construction impractical for large NFAs. The construction, sometimes called the Rabin\u2013Scott powerset construction (or subset construction) to distinguish it from similar constructions for other types of automata, was first published by Michael O. Rabin and Dana Scott in 1959.[ 1] Intuition To simulate the operation of a DFA on a given input string, one needs to keep track of a single state at any time: the state that the automaton will reach after seeing a prefix of the input. In contrast, to simulate an NFA, one needs to keep track of a set of states : all of the states that the automaton could reach after seeing the same prefix of the input, according to the nondeterministic choices made by the automaton. If, after a certain prefix of the input, a set S of states can be reached, then after the next input symbol x the set of reachable states is a deterministic function of S and x . Therefore, the sets of reachable NFA states play the same role in the NFA simulation as single DFA states play in the DFA simulation, and in fact the sets of NFA states appearing in this simulation may be re-interpreted as being states of a DFA.[ 2] NOTE: The difference between DFA and NFA can help to understand why a single state in DFA versus a set of states in NFA, below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.","title":"[Powerset construction](https://en.wikipedia.org/wiki/Powerset_construction)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/Regular-expression/wikipedia-Powerset-construction/#powerset#construction","text":"In the theory of computation and automata theory , the powerset construction or subset construction is a standard method for converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language . It is important in theory because it establishes that NFAs, despite their additional flexibility, are unable to recognize any language that cannot be recognized by some DFA. It is also important in practice for converting easier-to-construct NFAs into more efficiently executable DFAs. However, if the NFA has n states, the resulting DFA may have up to 2^n 2^n states, an exponentially larger number, which sometimes makes the construction impractical for large NFAs. The construction, sometimes called the Rabin\u2013Scott powerset construction (or subset construction) to distinguish it from similar constructions for other types of automata, was first published by Michael O. Rabin and Dana Scott in 1959.[ 1]","title":"Powerset construction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/Regular-expression/wikipedia-Powerset-construction/#intuition","text":"To simulate the operation of a DFA on a given input string, one needs to keep track of a single state at any time: the state that the automaton will reach after seeing a prefix of the input. In contrast, to simulate an NFA, one needs to keep track of a set of states : all of the states that the automaton could reach after seeing the same prefix of the input, according to the nondeterministic choices made by the automaton. If, after a certain prefix of the input, a set S of states can be reached, then after the next input symbol x the set of reachable states is a deterministic function of S and x . Therefore, the sets of reachable NFA states play the same role in the NFA simulation as single DFA states play in the DFA simulation, and in fact the sets of NFA states appearing in this simulation may be re-interpreted as being states of a DFA.[ 2] NOTE: The difference between DFA and NFA can help to understand why a single state in DFA versus a set of states in NFA, below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.","title":"Intuition"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/String-metric/wikipedia-Edit-distance/","text":"Edit distance Edit distance","title":"wikipedia Edit distance"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/String-metric/wikipedia-Edit-distance/#edit#distance","text":"","title":"Edit distance"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/String-metric/wikipedia-String-metric/","text":"String metric","title":"[String metric](https://en.wikipedia.org/wiki/String_metric)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/String-metric/wikipedia-String-metric/#string#metric","text":"","title":"String metric"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/","text":"String-searching algorithm Basic classification of search algorithms Single-pattern algorithms Algorithms using a finite set of patterns Algorithms using an infinite number of patterns Other classification Na\u00efve string search Finite-state-automaton-based search Stubs Index methods Other variants String-searching algorithm Basic classification of search algorithms The various algorithms can be classified by the number of patterns each uses. Single-pattern algorithms Let m be the length of the pattern, n be the length of the searchable text and k = |\u03a3| be the size of the alphabet. Algorithm Preprocessing time Matching time[ 1] Space Na\u00efve string-search algorithm none \u0398(nm) none Rabin\u2013Karp algorithm \u0398(m) average \u0398(n + m), worst \u0398((n\u2212m)m) O(1) Knuth\u2013Morris\u2013Pratt algorithm \u0398(m) \u0398(n) \u0398(m) Boyer\u2013Moore string-search algorithm \u0398(m + k) best \u03a9(n/m), worst O(mn) \u0398(k) Bitap algorithm ( shift-or , shift-and , Baeza\u2013Yates\u2013Gonnet ; fuzzy; agrep) \u0398(m + k) O(mn) Two-way string-matching algorithm (glibc memmem/strstr)[ 3] \u0398(m) O(n+m) O(1) BNDM (Backward Non-Deterministic DAWG Matching) (fuzzy + regex; nrgrep)[ 4] O(m) O(n) BOM (Backward Oracle Matching)[ 5] O(m) O(mn) 1. ^ Asymptotic times are expressed using O, \u03a9, and \u0398 notation . The Boyer\u2013Moore string-search algorithm has been the standard benchmark for the practical string-search literature.[ 6] Algorithms using a finite set of patterns Aho\u2013Corasick string matching algorithm (extension of Knuth-Morris-Pratt) Commentz-Walter algorithm (extension of Boyer-Moore) Set-BOM (extension of Backward Oracle Matching) Rabin\u2013Karp string search algorithm Algorithms using an infinite number of patterns Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression . Other classification Other classification approaches are possible. One of the most common uses preprocessing as main criteria. Classes of string searching algorithms [ 7] Text not preprocessed Text preprocessed Patterns not preprocessed Elementary algorithms Index methods Patterns preprocessed Constructed search engines Signature methods :[ 8] Another one classifies the algorithms by their matching strategy:[ 9] Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick) Match the suffix first (Boyer-Moore and variants, Commentz-Walter) Match the best factor first (BNDM, BOM, Set-BOM) Other strategy (Naive, Rabin-Karp) Na\u00efve string search A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O ( n + m ) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like \"aaaab\" in a string like \"aaaaaaaaab\", it takes O ( nm ) Finite-state-automaton-based search In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct\u2014they are usually created using the powerset construction \u2014but are very quick to use. For example, the DFA shown to the right recognizes the word \"MOMMY\". This approach is frequently generalized in practice to search for arbitrary regular expressions . Stubs Knuth\u2013Morris\u2013Pratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer\u2013Moore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza\u2013Yates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching . The bitap algorithm is an application of Baeza\u2013Yates' approach. Index methods Faster search algorithms preprocess the text. After building a substring index , for example a suffix tree or suffix array , the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in $ \\Theta (n) $ time, and all $ z $ occurrences of a pattern can be found in $ O(m) $ time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree. NOTE: https://github.com/fxsjy/jieba Other variants Some search methods, for instance trigram search , are intended to find a \"closeness\" score between the search string and the text rather than a \"match/non-match\". These are sometimes called \"fuzzy\" searches .","title":"wikipedia String searching algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#string-searching#algorithm","text":"","title":"String-searching algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#basic#classification#of#search#algorithms","text":"The various algorithms can be classified by the number of patterns each uses.","title":"Basic classification of search algorithms"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#single-pattern#algorithms","text":"Let m be the length of the pattern, n be the length of the searchable text and k = |\u03a3| be the size of the alphabet. Algorithm Preprocessing time Matching time[ 1] Space Na\u00efve string-search algorithm none \u0398(nm) none Rabin\u2013Karp algorithm \u0398(m) average \u0398(n + m), worst \u0398((n\u2212m)m) O(1) Knuth\u2013Morris\u2013Pratt algorithm \u0398(m) \u0398(n) \u0398(m) Boyer\u2013Moore string-search algorithm \u0398(m + k) best \u03a9(n/m), worst O(mn) \u0398(k) Bitap algorithm ( shift-or , shift-and , Baeza\u2013Yates\u2013Gonnet ; fuzzy; agrep) \u0398(m + k) O(mn) Two-way string-matching algorithm (glibc memmem/strstr)[ 3] \u0398(m) O(n+m) O(1) BNDM (Backward Non-Deterministic DAWG Matching) (fuzzy + regex; nrgrep)[ 4] O(m) O(n) BOM (Backward Oracle Matching)[ 5] O(m) O(mn) 1. ^ Asymptotic times are expressed using O, \u03a9, and \u0398 notation . The Boyer\u2013Moore string-search algorithm has been the standard benchmark for the practical string-search literature.[ 6]","title":"Single-pattern algorithms"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#algorithms#using#a#finite#set#of#patterns","text":"Aho\u2013Corasick string matching algorithm (extension of Knuth-Morris-Pratt) Commentz-Walter algorithm (extension of Boyer-Moore) Set-BOM (extension of Backward Oracle Matching) Rabin\u2013Karp string search algorithm","title":"Algorithms using a finite set of patterns"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#algorithms#using#an#infinite#number#of#patterns","text":"Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression .","title":"Algorithms using an infinite number of patterns"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#other#classification","text":"Other classification approaches are possible. One of the most common uses preprocessing as main criteria. Classes of string searching algorithms [ 7] Text not preprocessed Text preprocessed Patterns not preprocessed Elementary algorithms Index methods Patterns preprocessed Constructed search engines Signature methods :[ 8] Another one classifies the algorithms by their matching strategy:[ 9] Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick) Match the suffix first (Boyer-Moore and variants, Commentz-Walter) Match the best factor first (BNDM, BOM, Set-BOM) Other strategy (Naive, Rabin-Karp)","title":"Other classification"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#naive#string#search","text":"A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O ( n + m ) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like \"aaaab\" in a string like \"aaaaaaaaab\", it takes O ( nm )","title":"Na\u00efve string search"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#finite-state-automaton-based#search","text":"In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct\u2014they are usually created using the powerset construction \u2014but are very quick to use. For example, the DFA shown to the right recognizes the word \"MOMMY\". This approach is frequently generalized in practice to search for arbitrary regular expressions .","title":"Finite-state-automaton-based search"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#stubs","text":"Knuth\u2013Morris\u2013Pratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer\u2013Moore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza\u2013Yates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching . The bitap algorithm is an application of Baeza\u2013Yates' approach.","title":"Stubs"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#index#methods","text":"Faster search algorithms preprocess the text. After building a substring index , for example a suffix tree or suffix array , the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in $ \\Theta (n) $ time, and all $ z $ occurrences of a pattern can be found in $ O(m) $ time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree. NOTE: https://github.com/fxsjy/jieba","title":"Index methods"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/wikipedia-String-searching-algorithm/#other#variants","text":"Some search methods, for instance trigram search , are intended to find a \"closeness\" score between the search string and the text rather than a \"match/non-match\". These are sometimes called \"fuzzy\" searches .","title":"Other variants"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/Aho-Corasick-algorithm/","text":"Aho-Corasick algorithm Construction of the trie Construction of an automaton Aho-Corasick algorithm Let there be a set of strings with the total length mm (sum of all lengths). The Aho-Corasick algorithm constructs a data structure similar to a trie with some additional links, and then constructs a finite state machine (automaton) in O(mk) O(mk) time, where k k is the size of the used alphabet. The algorithm was proposed by Alfred Aho and Margaret Corasick in 1975. Construction of the trie Formally a trie is a rooted tree, where each edge of the tree is labeled by some letter. All outgoing edge from one vertex mush have different labels. Consider any path in the trie from the root to any vertex. If we write out the labels of all edges on the path, we get a string that corresponds to this path. For any vertex in the trie we will associate the string from the root to the vertex. Each vertex will also have a flag leaf which will be true, if any string from the given set corresponds to this vertex. Accordingly to build a trie for a set of strings means to build a trie such that each leaf vertex will correspond to one string from the set, and conversely that each string of the set corresponds to one leaf vertex. We now describe how to construct a trie for a given set of strings in linear time with respect to their total length. We introduce a structure for the vertices of the tree. const int K = 26 ; struct Vertex { int next [ K ]; bool leaf = false ; Vertex () { fill ( begin ( next ), end ( next ), -1 ); } }; vector < Vertex > trie ( 1 ); Here we store the trie as an array of Vertex . Each Vertex contains the flag leaf , and the edges in the form of ans array next[] , where next[i] is the index to the vertex that we reach by following the character i , or \u22121, if there is no such edge. Initially the trie consists of only one vertex - the root - with the index 0. Now we implement a function that will add a string s to the trie. The implementation is extremely simple: we start at the root node, and as long as there are edges corresponding to the characters of s we follow them. If there is no edge for one character, we simply generate a new vertex and connect it via an edge. At the end of the process we mark the last vertex with flag leaf . void add_string ( string const & s ) { int v = 0 ; for ( char ch : s ) { int c = ch - 'a' ; if ( trie [ v ]. next [ c ] == -1 ) { trie [ v ]. next [ c ] = trie . size (); trie . emplace_back (); } v = trie [ v ]. next [ c ]; } trie [ v ]. leaf = true ; } The implementation obviously runs in linear time. And since every vertex store k links, it will use O(mk) memory. It is possible to decrease the memory consumption to O(m) by using a map instead of an array in each vertex. However this will increase the complexity to O(nlogk) . Construction of an automaton Suppose we have built a trie for the given set of strings. Now let's look at it from a different side. If we look at any vertex. The string that corresponds to it is a prefix of one or more strings in the set, thus each vertex of the trie can be interpreted as a position in one or more strings from the set. In fact the trie vertices can be interpreted as states in a finite deterministic automaton . From any state we can transition - using some input letter - to other states, i.e. to another position in the set of strings. For example, if there is only one string in the trie abc , and we are standing at vertex 2 (which corresponds to the string ab ), then using the letter c we can transition to the state 3 . Thus we can understand the edges of the trie as transitions in an automaton according to the corresponding letter. However for an automaton we cannot restrict the possible transitions for each state. If we try to perform a transition using a letter, and there is no corresponding edge in the trie, then we nevertheless must go into some state. Thus we can understand the edges of the trie as transitions in an automaton according to the corresponding letter. However for an automaton we cannot restrict the possible transitions for each state. If we try to perform a transition using a letter, and there is no corresponding edge in the trie, then we nevertheless must go into some state.","title":"Aho Corasick algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/Aho-Corasick-algorithm/#aho-corasick#algorithm","text":"Let there be a set of strings with the total length mm (sum of all lengths). The Aho-Corasick algorithm constructs a data structure similar to a trie with some additional links, and then constructs a finite state machine (automaton) in O(mk) O(mk) time, where k k is the size of the used alphabet. The algorithm was proposed by Alfred Aho and Margaret Corasick in 1975.","title":"Aho-Corasick algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/Aho-Corasick-algorithm/#construction#of#the#trie","text":"Formally a trie is a rooted tree, where each edge of the tree is labeled by some letter. All outgoing edge from one vertex mush have different labels. Consider any path in the trie from the root to any vertex. If we write out the labels of all edges on the path, we get a string that corresponds to this path. For any vertex in the trie we will associate the string from the root to the vertex. Each vertex will also have a flag leaf which will be true, if any string from the given set corresponds to this vertex. Accordingly to build a trie for a set of strings means to build a trie such that each leaf vertex will correspond to one string from the set, and conversely that each string of the set corresponds to one leaf vertex. We now describe how to construct a trie for a given set of strings in linear time with respect to their total length. We introduce a structure for the vertices of the tree. const int K = 26 ; struct Vertex { int next [ K ]; bool leaf = false ; Vertex () { fill ( begin ( next ), end ( next ), -1 ); } }; vector < Vertex > trie ( 1 ); Here we store the trie as an array of Vertex . Each Vertex contains the flag leaf , and the edges in the form of ans array next[] , where next[i] is the index to the vertex that we reach by following the character i , or \u22121, if there is no such edge. Initially the trie consists of only one vertex - the root - with the index 0. Now we implement a function that will add a string s to the trie. The implementation is extremely simple: we start at the root node, and as long as there are edges corresponding to the characters of s we follow them. If there is no edge for one character, we simply generate a new vertex and connect it via an edge. At the end of the process we mark the last vertex with flag leaf . void add_string ( string const & s ) { int v = 0 ; for ( char ch : s ) { int c = ch - 'a' ; if ( trie [ v ]. next [ c ] == -1 ) { trie [ v ]. next [ c ] = trie . size (); trie . emplace_back (); } v = trie [ v ]. next [ c ]; } trie [ v ]. leaf = true ; } The implementation obviously runs in linear time. And since every vertex store k links, it will use O(mk) memory. It is possible to decrease the memory consumption to O(m) by using a map instead of an array in each vertex. However this will increase the complexity to O(nlogk) .","title":"Construction of the trie"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/Aho-Corasick-algorithm/#construction#of#an#automaton","text":"Suppose we have built a trie for the given set of strings. Now let's look at it from a different side. If we look at any vertex. The string that corresponds to it is a prefix of one or more strings in the set, thus each vertex of the trie can be interpreted as a position in one or more strings from the set. In fact the trie vertices can be interpreted as states in a finite deterministic automaton . From any state we can transition - using some input letter - to other states, i.e. to another position in the set of strings. For example, if there is only one string in the trie abc , and we are standing at vertex 2 (which corresponds to the string ab ), then using the letter c we can transition to the state 3 . Thus we can understand the edges of the trie as transitions in an automaton according to the corresponding letter. However for an automaton we cannot restrict the possible transitions for each state. If we try to perform a transition using a letter, and there is no corresponding edge in the trie, then we nevertheless must go into some state. Thus we can understand the edges of the trie as transitions in an automaton according to the corresponding letter. However for an automaton we cannot restrict the possible transitions for each state. If we try to perform a transition using a letter, and there is no corresponding edge in the trie, then we nevertheless must go into some state.","title":"Construction of an automaton"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/","text":"Aho-Corasick Algorithm for Pattern Searching Prepocessing : 1. Go To 2. Failure 3. Output Matching Preprocessing: Aho-Corasick Algorithm for Pattern Searching Given an input text and an array of k words, arr[] , find all occurrences of all words in the input text. Let n be the length of text and m be the total number characters in all words, i.e. m = length(arr[0]) + length(arr[1]) + \u2026 + length(arr[k-1]) . Here k is total numbers of input words. Example: Input: text = \"ahishers\" arr[] = {\"he\", \"she\", \"hers\", \"his\"} Output: Word his appears from 1 to 3 Word he appears from 4 to 5 Word she appears from 3 to 5 Word hers appears from 4 to 7 If we use a linear time searching algorithm like KMP , then we need to one by one search all words in text[]. This gives us total time complexity as O(n + length(word[0]) + O(n + length(word[1]) + O(n + length(word[2]) + \u2026 O(n + length(word[k-1]) . This time complexity can be written as O(n*k + m) . Aho-Corasick Algorithm finds all words in O(n + m + z) time where z is total number of occurrences of words in text. The Aho\u2013Corasick string matching algorithm formed the basis of the original Unix command fgrep. Prepocessing : Build an automaton of all words in arr[] The automaton has mainly three functions: 1. Go To This function simply follows edges of Trie of all words in arr[] . It is represented as 2D array g[][] where we store next state for current state and character. 2. Failure This function stores all edges that are followed when current character doesn't have edge in Trie. It is represented as 1D array f[] where we store next state for current state. 3. Output Stores indexes of all words that end at current state. It is represented as 1D array o[] where we store indexes of all matching words as a bitmap for current state. Matching Traverse the given text over built automaton to find all matching words. Preprocessing: We first Build a Trie (or Keyword Tree) of all words. NOTE: The figure above has a mistake that edge i should start from node after h . This part fills entries in goto g[][] and output o[] . Next we extend Trie into an automaton to support linear time matching. This part fills entries in failure f[] and output o[] . Go to : We build Trie . And for all characters which don\u2019t have an edge at root, we add an edge back to root. Failure : For a state s , we find the longest proper suffix which is a proper prefix of some pattern. This is done using Breadth First Traversal of Trie. Output : For a state s , indexes of all words ending at s are stored. These indexes are stored as bitwise map (by doing bitwise OR of values). This is also computing using Breadth First Traversal with Failure. Below is C++ implementation of Aho-Corasick Algorithm","title":"geeksforgeeks Aho Corasick Algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#aho-corasick#algorithm#for#pattern#searching","text":"Given an input text and an array of k words, arr[] , find all occurrences of all words in the input text. Let n be the length of text and m be the total number characters in all words, i.e. m = length(arr[0]) + length(arr[1]) + \u2026 + length(arr[k-1]) . Here k is total numbers of input words. Example: Input: text = \"ahishers\" arr[] = {\"he\", \"she\", \"hers\", \"his\"} Output: Word his appears from 1 to 3 Word he appears from 4 to 5 Word she appears from 3 to 5 Word hers appears from 4 to 7 If we use a linear time searching algorithm like KMP , then we need to one by one search all words in text[]. This gives us total time complexity as O(n + length(word[0]) + O(n + length(word[1]) + O(n + length(word[2]) + \u2026 O(n + length(word[k-1]) . This time complexity can be written as O(n*k + m) . Aho-Corasick Algorithm finds all words in O(n + m + z) time where z is total number of occurrences of words in text. The Aho\u2013Corasick string matching algorithm formed the basis of the original Unix command fgrep.","title":"Aho-Corasick Algorithm for Pattern Searching"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#prepocessing","text":"Build an automaton of all words in arr[] The automaton has mainly three functions:","title":"Prepocessing :"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#1#go#to","text":"This function simply follows edges of Trie of all words in arr[] . It is represented as 2D array g[][] where we store next state for current state and character.","title":"1. Go To"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#2#failure","text":"This function stores all edges that are followed when current character doesn't have edge in Trie. It is represented as 1D array f[] where we store next state for current state.","title":"2. Failure"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#3#output","text":"Stores indexes of all words that end at current state. It is represented as 1D array o[] where we store indexes of all matching words as a bitmap for current state.","title":"3. Output"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#matching","text":"Traverse the given text over built automaton to find all matching words.","title":"Matching"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#preprocessing","text":"We first Build a Trie (or Keyword Tree) of all words. NOTE: The figure above has a mistake that edge i should start from node after h . This part fills entries in goto g[][] and output o[] . Next we extend Trie into an automaton to support linear time matching. This part fills entries in failure f[] and output o[] . Go to : We build Trie . And for all characters which don\u2019t have an edge at root, we add an edge back to root. Failure : For a state s , we find the longest proper suffix which is a proper prefix of some pattern. This is done using Breadth First Traversal of Trie. Output : For a state s , indexes of all words ending at s are stored. These indexes are stored as bitwise map (by doing bitwise OR of values). This is also computing using Breadth First Traversal with Failure. Below is C++ implementation of Aho-Corasick Algorithm","title":"Preprocessing:"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/wikipedia-Aho-Corasick-algorithm/","text":"Aho\u2013Corasick algorithm Example Implementation Aho\u2013Corasick algorithm In computer science , the Aho\u2013Corasick algorithm is a string-searching algorithm invented by Alfred V. Aho and Margaret J. Corasick.[ 1] It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the \"dictionary\") within an input text. It matches all strings simultaneously. The complexity of the algorithm is linear in the length of the strings plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a , aa , aaa , aaaa and input string is aaaa ). Informally, the algorithm constructs a finite-state machine that resembles a trie with additional links between the various internal nodes. These extra internal links allow fast transitions between failed string matches (e.g. a search for cat in a trie that does not contain cat , but contains cart , and thus would fail at the node prefixed by ca ), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). This allows the automaton to transition between string matches without the need for backtracking. When the string dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. In this case, its run time is linear in the length of the input plus the number of matched entries. The Aho\u2013Corasick string-matching algorithm formed the basis of the original Unix command fgrep . Example In this example, we will consider a dictionary consisting of the following words: { a , ab , bab , bc , bca , c , caa }. The graph below is the Aho\u2013Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node. A visualization of the trie for the dictionary on the right. Suffix links are in blue; dictionary suffix links in green. Nodes corresponding to dictionary entries are highlighted in blue. The data structure has one node for every prefix of every string in the dictionary. So if ( bca ) is in the dictionary, then there will be nodes for ( bca ), ( bc ), ( b ), and (). If a node is in the dictionary then it is a blue node . Otherwise it is a grey node . There is a black directed \"child\" arc from each node to a node whose name is found by appending one character. So there is a black arc from ( bc ) to ( bca ). There is a blue directed \"suffix\" arc from each node to the node that is the longest possible strict suffix of it in the graph . For example, for node ( caa ), its strict suffixes are ( aa ) and ( a ) and (). The longest of these that exists in the graph is ( a ). So there is a blue arc from ( caa ) to ( a ). The blue arcs can be computed in linear time by repeatedly traversing the blue arcs of a node's parent until the traversing node has a child matching the character of the target node. There is a green \"dictionary suffix\" arc from each node to the next node in the dictionary that can be reached by following blue arcs . For example, there is a green arc from ( bca ) to ( a ) because ( a ) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to ( ca ) and then on to (a). The green arcs can be computed in linear time by repeatedly traversing blue arcs until a filled in node is found, and memoizing this information. Dictionary {a, ab, bab, bc, bca, c, caa} Path In dictionary Suffix link Dict suffix link () \u2013 (a) + () (ab) + (b) (b) \u2013 () (ba) \u2013 (a) (a) (bab) + (ab) (ab) (bc) + \u00a9 \u00a9 (bca) + (ca) (a) \u00a9 + () (ca) \u2013 (a) (a) (caa) + (a) (a) At each step, the current node is extended by finding its child, and if that doesn't exist, finding its suffix's child, and if that doesn't work, finding its suffix's suffix's child, and so on, finally ending in the root node if nothing's seen before. When the algorithm reaches a node, it outputs all the dictionary entries that end at the current character position in the input text. This is done by printing every node reached by following the dictionary suffix links, starting from that node, and continuing until it reaches a node with no dictionary suffix link. In addition, the node itself is printed, if it is a dictionary entry. Execution on input string abccab yields the following steps: Implementation https://github.com/WojciechMula/pyahocorasick","title":"wikipedia Aho Corasick algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/wikipedia-Aho-Corasick-algorithm/#ahocorasick#algorithm","text":"In computer science , the Aho\u2013Corasick algorithm is a string-searching algorithm invented by Alfred V. Aho and Margaret J. Corasick.[ 1] It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the \"dictionary\") within an input text. It matches all strings simultaneously. The complexity of the algorithm is linear in the length of the strings plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a , aa , aaa , aaaa and input string is aaaa ). Informally, the algorithm constructs a finite-state machine that resembles a trie with additional links between the various internal nodes. These extra internal links allow fast transitions between failed string matches (e.g. a search for cat in a trie that does not contain cat , but contains cart , and thus would fail at the node prefixed by ca ), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). This allows the automaton to transition between string matches without the need for backtracking. When the string dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. In this case, its run time is linear in the length of the input plus the number of matched entries. The Aho\u2013Corasick string-matching algorithm formed the basis of the original Unix command fgrep .","title":"Aho\u2013Corasick algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/wikipedia-Aho-Corasick-algorithm/#example","text":"In this example, we will consider a dictionary consisting of the following words: { a , ab , bab , bc , bca , c , caa }. The graph below is the Aho\u2013Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node. A visualization of the trie for the dictionary on the right. Suffix links are in blue; dictionary suffix links in green. Nodes corresponding to dictionary entries are highlighted in blue. The data structure has one node for every prefix of every string in the dictionary. So if ( bca ) is in the dictionary, then there will be nodes for ( bca ), ( bc ), ( b ), and (). If a node is in the dictionary then it is a blue node . Otherwise it is a grey node . There is a black directed \"child\" arc from each node to a node whose name is found by appending one character. So there is a black arc from ( bc ) to ( bca ). There is a blue directed \"suffix\" arc from each node to the node that is the longest possible strict suffix of it in the graph . For example, for node ( caa ), its strict suffixes are ( aa ) and ( a ) and (). The longest of these that exists in the graph is ( a ). So there is a blue arc from ( caa ) to ( a ). The blue arcs can be computed in linear time by repeatedly traversing the blue arcs of a node's parent until the traversing node has a child matching the character of the target node. There is a green \"dictionary suffix\" arc from each node to the next node in the dictionary that can be reached by following blue arcs . For example, there is a green arc from ( bca ) to ( a ) because ( a ) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to ( ca ) and then on to (a). The green arcs can be computed in linear time by repeatedly traversing blue arcs until a filled in node is found, and memoizing this information. Dictionary {a, ab, bab, bc, bca, c, caa} Path In dictionary Suffix link Dict suffix link () \u2013 (a) + () (ab) + (b) (b) \u2013 () (ba) \u2013 (a) (a) (bab) + (ab) (ab) (bc) + \u00a9 \u00a9 (bca) + (ca) (a) \u00a9 + () (ca) \u2013 (a) (a) (caa) + (a) (a) At each step, the current node is extended by finding its child, and if that doesn't exist, finding its suffix's child, and if that doesn't work, finding its suffix's suffix's child, and so on, finally ending in the root node if nothing's seen before. When the algorithm reaches a node, it outputs all the dictionary entries that end at the current character position in the input text. This is done by printing every node reached by following the dictionary suffix links, starting from that node, and continuing until it reaches a node with no dictionary suffix link. In addition, the node itself is printed, if it is a dictionary entry. Execution on input string abccab yields the following steps:","title":"Example"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Aho-Corasick-algorithm/wikipedia-Aho-Corasick-algorithm/#implementation","text":"https://github.com/WojciechMula/pyahocorasick","title":"Implementation"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/","text":"Introduction 1. Knuth\u2013Morris\u2013Pratt algorithm 2. \u8be6\u89e3KMP\u7b97\u6cd5 \u6c42\u89e3next\u6570\u7ec4 \u5f53P[k] == P[j]\u65f6 \u5f53P[k] != P[j]\u65f6, 3. Computing the KMP failure function (f(k)) definition of f(k) Naive way to find f(k): Relating f(k) to f(k\u22121) Fact between f(k) and f(k\u22121) Computation trick 1 Prelude to computation trick 2 Computation trick #2 Algorithm to compute KMP failure function KMP\u5b9e\u73b0\u5206\u6790 \u8ba1\u7b97KMP failure function\u7684\u9012\u5f52\u516c\u5f0f \u8ba1\u7b97KMP failure function\u7684python\u5b9e\u73b0 \u8ba1\u7b97KMP failure function \u548c dynamic programming KMP\u7684\u5b9e\u73b0 KMP in leetcode Introduction It takes me some effort to master KMP algorithm. Here are three articles that helped me solve the mystery as I learned. 1. Knuth\u2013Morris\u2013Pratt algorithm In computer science , the Knuth\u2013Morris\u2013Pratt string-searching algorithm (or KMP algorithm ) searches for occurrences of a \"word\" W within a main \"text string\" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters. The algorithm was conceived by James H. Morris and independently discovered by Donald Knuth \"a few weeks later\" from automata theory.[ 1] [ 2] Morris and Vaughan Pratt published a technical report in 1970.[ 3] The three also published the algorithm jointly in 1977.[ 1] Independently, in 1969, Matiyasevich [ 4] [ 5] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem over a binary alphabet. This was the first linear-time algorithm for string matching. 2. \u8be6\u89e3KMP\u7b97\u6cd5 KMP\u7b97\u6cd5\u8981\u89e3\u51b3\u7684\u95ee\u9898\u5c31\u662f\u5728\u5b57\u7b26\u4e32\uff08\u4e5f\u53eb\u4e3b\u4e32\uff09\u4e2d\u7684\u6a21\u5f0f\uff08pattern\uff09\u5b9a\u4f4d\u95ee\u9898\u3002\u8bf4\u7b80\u5355\u70b9\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u5e38\u8bf4\u7684\u5173\u952e\u5b57\u641c\u7d22\u3002\u6a21\u5f0f\u4e32\u5c31\u662f\u5173\u952e\u5b57\uff08\u63a5\u4e0b\u6765\u79f0\u5b83\u4e3a P \uff09\uff0c\u5982\u679c\u5b83\u5728\u4e00\u4e2a\u4e3b\u4e32\uff08\u63a5\u4e0b\u6765\u79f0\u4e3a T \uff09\u4e2d\u51fa\u73b0\uff0c\u5c31\u8fd4\u56de\u5b83\u7684\u5177\u4f53\u4f4d\u7f6e\uff0c\u5426\u5219\u8fd4\u56de -1 \uff08\u5e38\u7528\u624b\u6bb5\uff09\u3002 \u9996\u5148\uff0c\u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\u6709\u4e00\u4e2a\u5f88\u5355\u7eaf\u7684\u60f3\u6cd5\uff1a\u4ece\u5de6\u5230\u53f3\u4e00\u4e2a\u4e2a\u5339\u914d\uff0c\u5982\u679c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u6709\u67d0\u4e2a\u5b57\u7b26\u4e0d\u5339\u914d\uff0c\u5c31\u8df3\u56de\u53bb\uff0c\u5c06\u6a21\u5f0f\u4e32\u5411\u53f3\u79fb\u52a8\u4e00\u4f4d\u3002\u8fd9\u6709\u4ec0\u4e48\u96be\u7684\uff1f \u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u521d\u59cb\u5316\uff1a \u4e4b\u540e\u6211\u4eec\u53ea\u9700\u8981\u6bd4\u8f83** i \u6307\u9488**\u6307\u5411\u7684\u5b57\u7b26\u548c** j \u6307\u9488**\u6307\u5411\u7684\u5b57\u7b26\u662f\u5426\u4e00\u81f4\u3002\u5982\u679c\u4e00\u81f4\u5c31\u90fd\u5411\u540e\u79fb\u52a8\uff0c\u5982\u679c\u4e0d\u4e00\u81f4\uff0c\u5982\u4e0b\u56fe\uff1a A \u548c E \u4e0d\u76f8\u7b49\uff0c\u90a3\u5c31\u628a** i \u6307\u9488**\u79fb\u56de\u7b2c1\u4f4d\uff08\u5047\u8bbe\u4e0b\u6807\u4ece0\u5f00\u59cb\uff09\uff0c j \u79fb\u52a8\u5230\u6a21\u5f0f\u4e32\u7684\u7b2c0\u4f4d\uff0c\u7136\u540e\u53c8\u91cd\u65b0\u5f00\u59cb\u8fd9\u4e2a\u6b65\u9aa4\uff1a \u57fa\u4e8e\u8fd9\u4e2a\u60f3\u6cd5\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u7684\u7a0b\u5e8f\uff1a /** * \u66b4\u529b\u7834\u89e3\u6cd5 * @param ts \u4e3b\u4e32 * @param ps \u6a21\u5f0f\u4e32 * @return \u5982\u679c\u627e\u5230\uff0c\u8fd4\u56de\u5728\u4e3b\u4e32\u4e2d\u7b2c\u4e00\u4e2a\u5b57\u7b26\u51fa\u73b0\u7684\u4e0b\u6807\uff0c\u5426\u5219\u4e3a-1 */ public static int bf ( String ts , String ps ) { char [] t = ts . toCharArray (); char [] p = ps . toCharArray (); int i = 0 ; // \u4e3b\u4e32\u7684\u4f4d\u7f6e int j = 0 ; // \u6a21\u5f0f\u4e32\u7684\u4f4d\u7f6e while ( i < t . length && j < p . length ) { if ( t [ i ] == p [ j ] ) { // \u5f53\u4e24\u4e2a\u5b57\u7b26\u76f8\u540c\uff0c\u5c31\u6bd4\u8f83\u4e0b\u4e00\u4e2a i ++ ; j ++ ; } else { i = i - j + 1 ; // \u4e00\u65e6\u4e0d\u5339\u914d\uff0ci\u540e\u9000 j = 0 ; // j\u5f520 } } if ( j == p . length ) { return i - j ; } else { return - 1 ; } } \u4e0a\u9762\u7684\u7a0b\u5e8f\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u4f46\u4e0d\u591f\u597d\uff01 NOTE: geeksforgeeks\u7684\u6587\u7ae0 Naive algorithm for Pattern Searching \u4e2d\u7ed9\u51fa\u7684\u4ee3\u7801\u662f\u6bd4\u4e0a\u8ff0\u4ee3\u7801\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u5982\u679c\u662f\u4eba\u4e3a\u6765\u5bfb\u627e\u7684\u8bdd\uff0c\u80af\u5b9a\u4e0d\u4f1a\u518d\u628a i \u79fb\u52a8\u56de\u7b2c1\u4f4d\uff0c \u56e0\u4e3a\u4e3b\u4e32\u5339\u914d\u5931\u8d25\u7684\u4f4d\u7f6e\u524d\u9762\u9664\u4e86\u7b2c\u4e00\u4e2a A \u4e4b\u5916\u518d\u4e5f\u6ca1\u6709 A **\u4e86\uff0c\u6211\u4eec\u4e3a\u4ec0\u4e48\u80fd\u77e5\u9053\u4e3b\u4e32\u524d\u9762\u53ea\u6709\u4e00\u4e2a A \uff1f**\u56e0\u4e3a\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u524d\u9762\u4e09\u4e2a\u5b57\u7b26\u90fd\u662f\u5339\u914d\u7684\uff01\uff08\u8fd9\u5f88\u91cd\u8981\uff09 \u3002\u79fb\u52a8\u8fc7\u53bb\u80af\u5b9a\u4e5f\u662f\u4e0d\u5339\u914d\u7684\uff01\u6709\u4e00\u4e2a\u60f3\u6cd5\uff0c i \u53ef\u4ee5\u4e0d\u52a8\uff0c\u6211\u4eec\u53ea\u9700\u8981\u79fb\u52a8 j \u5373\u53ef\uff0c\u5982\u4e0b\u56fe\uff1a \u4e0a\u9762\u7684\u8fd9\u79cd\u60c5\u51b5\u8fd8\u662f\u6bd4\u8f83\u7406\u60f3\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u6700\u591a\u4e5f\u5c31\u591a\u6bd4\u8f83\u4e86\u4e24\u6b21\u3002\u4f46\u5047\u5982\u662f\u5728\u4e3b\u4e32 SSSSSSSSSSSSSA \u4e2d\u67e5\u627e SSSSB \uff0c\u6bd4\u8f83\u5230\u6700\u540e\u4e00\u4e2a\u624d\u77e5\u9053\u4e0d\u5339\u914d\uff0c\u7136\u540e i \u56de\u6eaf \uff0c\u8fd9\u4e2a\u7684\u6548\u7387\u662f\u663e\u7136\u662f\u6700\u4f4e\u7684\u3002 NOTE: \u5173\u4e8e\u56de\u6eaf\uff0c\u53c2\u89c1 Backtracking \u5927\u725b\u4eec\u662f\u65e0\u6cd5\u5fcd\u53d7\u201c\u66b4\u529b\u7834\u89e3\u201d\u8fd9\u79cd\u4f4e\u6548\u7684\u624b\u6bb5\u7684\uff0c\u4e8e\u662f\u4ed6\u4eec\u4e09\u4e2a\u7814\u7a76\u51fa\u4e86KMP\u7b97\u6cd5\u3002\u5176\u601d\u60f3\u5c31\u5982\u540c\u6211\u4eec\u4e0a\u8fb9\u6240\u770b\u5230\u7684\u4e00\u6837\uff1a\u201c \u5229\u7528\u5df2\u7ecf\u90e8\u5206\u5339\u914d\u8fd9\u4e2a\u6709\u6548\u4fe1\u606f\uff0c\u4fdd\u6301 i \u6307\u9488\u4e0d\u56de\u6eaf\uff0c\u901a\u8fc7\u4fee\u6539 j \u6307\u9488\uff0c\u8ba9\u6a21\u5f0f\u4e32\u5c3d\u91cf\u5730\u79fb\u52a8\u5230\u6709\u6548\u7684\u4f4d\u7f6e \u3002\u201d NOTE: \u63d0\u9192\u4f60\u6ce8\u610f**\u5c3d\u91cf\u5730**\u8fd9\u4e2a\u4fee\u9970\u8bed\uff0c\u7b49\u4f60\u5b8c\u5168\u7406\u89e3\u4e86KMP\u7b97\u6cd5\uff0c\u4f60\u5c31\u5e61\u7136\u9192\u609f\u8fd9\u4e2a\u4fee\u9970\u8bed\u662f\u975e\u5e38\u5999\u7684\u3002\u5176\u5b9e\u5728\u8fd9\u91cc\uff0c\u6211\u662f\u53ef\u4ee5\u5411\u4f60\u63d0\u524d\u900f\u9732\u7684\uff0c\u65e2\u7136\u8bf4\u662f\u5c3d\u91cf\uff0c\u90a3\u4e48\u4e5f\u5c31\u662f\u8bf4\u79fb\u52a8\u5230\u7684\u4f4d\u7f6e\u4e0d\u4e00\u5b9a\u662f\u6700\u6700\u6709\u6548\u7684\u4f4d\u7f6e\uff0c\u800c\u662f\u4e00\u4e2a\u76f8\u5bf9\u6709\u6548\u7684\u4f4d\u7f6e\uff0c\u53ef\u80fd\u9700\u8981\u7ecf\u8fc7\u591a\u6b21\u79fb\u52a8\u624d\u80fd\u591f\u5230\u8fbe\u6b63\u786e\u7684\u4f4d\u7f6e\uff0c\u6bd5\u7adf\u8ba1\u7b97\u673a\u4e0d\u662f\u50cf\u6211\u4eec\u4eba\u8fd9\u6837\u7684\u667a\u80fd\u3002 \u6240\u4ee5\uff0c\u6574\u4e2aKMP\u7684\u91cd\u70b9\u5c31\u5728\u4e8e**\u5f53\u67d0\u4e00\u4e2a\u5b57\u7b26\u4e0e\u4e3b\u4e32\u4e0d\u5339\u914d\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u77e5\u9053 j \u6307\u9488\u8981\u79fb\u52a8\u5230\u54ea**\uff1f \u63a5\u4e0b\u6765\u6211\u4eec\u81ea\u5df1\u6765\u53d1\u73b0 j \u7684\u79fb\u52a8\u89c4\u5f8b\uff1a \u5982\u56fe\uff1a C \u548c D \u4e0d\u5339\u914d\u4e86\uff0c\u6211\u4eec\u8981\u628a j \u79fb\u52a8\u5230\u54ea\uff1f\u663e\u7136\u662f\u7b2c1\u4f4d\u3002\u4e3a\u4ec0\u4e48\uff1f\u56e0\u4e3a\u524d\u9762\u6709\u4e00\u4e2a A \u76f8\u540c\u554a\uff1a \u5982\u4e0b\u56fe\u4e5f\u662f\u4e00\u6837\u7684\u60c5\u51b5\uff1a \u53ef\u4ee5\u628a j \u6307\u9488\u79fb\u52a8\u5230\u7b2c2\u4f4d\uff0c\u56e0\u4e3a\u524d\u9762\u6709\u4e24\u4e2a\u5b57\u6bcd\u662f\u4e00\u6837\u7684\uff1a \u81f3\u6b64\u6211\u4eec\u53ef\u4ee5\u5927\u6982\u770b\u51fa\u4e00\u70b9\u7aef\u502a\uff0c\u5f53\u5339\u914d\u5931\u8d25\u65f6\uff0c j \u8981\u79fb\u52a8\u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e k \u3002\u5b58\u5728\u7740\u8fd9\u6837\u7684\u6027\u8d28\uff1a \u6700\u524d\u9762\u7684 k \u5b57\u7b26\u548c j \u4e4b\u524d\u7684\u6700\u540e k \u4e2a\u5b57\u7b26\u662f\u4e00\u6837\u7684 \u3002 \u5982\u679c\u7528\u6570\u5b66\u516c\u5f0f\u6765\u8868\u793a\u662f\u8fd9\u6837\u7684 P[0 ~ k-1] == P[j-k ~ j-1] \u8fd9\u4e2a\u76f8\u5f53\u91cd\u8981\uff0c\u5982\u679c\u89c9\u5f97\u4e0d\u597d\u8bb0\u7684\u8bdd\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e0b\u56fe\u6765\u7406\u89e3\uff1a \u5f04\u660e\u767d\u4e86\u8fd9\u4e2a\u5c31\u5e94\u8be5\u53ef\u80fd\u660e\u767d\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u76f4\u63a5\u5c06 j \u79fb\u52a8\u5230 k \u4f4d\u7f6e\u4e86\u3002 \u56e0\u4e3a: \u5f53 T[i] != P[j] \u65f6 \u6709 T[i-j ~ i-1] == P[0 ~ j-1] \u7531 P[0 ~ k-1] == P[j-k ~ j-1] \u5fc5\u7136\uff1a T[i-k ~ i-1] == P[0 ~ k-1] NOTE: \u4e0a\u8ff0\u516c\u5f0f\u5176\u5b9e\u5c31\u662fa==b, b==c,\u5219a==c \u516c\u5f0f\u5f88\u65e0\u804a\uff0c\u80fd\u770b\u660e\u767d\u5c31\u884c\u4e86\uff0c\u4e0d\u9700\u8981\u8bb0\u4f4f\u3002 NOTE: \u4f5c\u8005\u8fd9\u91cc\u7684\u603b\u7ed3\u4e0d\u591f\u76f4\u63a5\uff0c\u4e0b\u9762\u662f\u6458\u81ea\u767e\u5ea6\u767e\u79d1 kmp\u7b97\u6cd5 \u4e2d\u5bf9\u8fd9\u4e2a\u7ed3\u8bba\u7684\u603b\u7ed3\uff0c\u5b83\u975e\u5e38\u76f4\u63a5\uff1a \u7528\u66b4\u529b\u7b97\u6cd5\u5339\u914d\u5b57\u7b26\u4e32\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f1a\u628a T[0] \u8ddf W[0] \u5339\u914d\uff0c\u5982\u679c\u76f8\u540c\u5219\u5339\u914d\u4e0b\u4e00\u4e2a\u5b57\u7b26\uff0c\u76f4\u5230\u51fa\u73b0\u4e0d\u76f8\u540c\u7684\u60c5\u51b5\uff0c\u6b64\u65f6\u6211\u4eec\u4f1a\u4e22\u5f03\u524d\u9762\u7684\u5339\u914d\u4fe1\u606f\uff0c\u7136\u540e\u628a T[1] \u8ddf W[0] \u5339\u914d\uff0c\u5faa\u73af\u8fdb\u884c\uff0c\u76f4\u5230\u4e3b\u4e32\u7ed3\u675f\uff0c\u6216\u8005\u51fa\u73b0\u5339\u914d\u6210\u529f\u7684\u60c5\u51b5\u3002\u8fd9\u79cd\u4e22\u5f03\u524d\u9762\u7684\u5339\u914d\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5339\u914d\u6548\u7387\u3002 \u800c\u5728KMP\u7b97\u6cd5\u4e2d\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u6a21\u5f0f\u4e32\u6211\u4eec\u4f1a\u4e8b\u5148\u8ba1\u7b97\u51fa\u6a21\u5f0f\u4e32\u7684\u5185\u90e8\u5339\u914d\u4fe1\u606f\uff0c\u5728\u5339\u914d\u5931\u8d25\u65f6\u6700\u5927\u7684\u79fb\u52a8\u6a21\u5f0f\u4e32\uff0c\u4ee5\u51cf\u5c11\u5339\u914d\u6b21\u6570\u3002 \u6bd4\u5982\uff0c\u5728\u7b80\u5355\u7684\u4e00\u6b21\u5339\u914d\u5931\u8d25\u540e\uff0c\u6211\u4eec\u4f1a\u60f3\u5c06\u6a21\u5f0f\u4e32\u5c3d\u91cf\u7684\u53f3\u79fb\u548c\u4e3b\u4e32\u8fdb\u884c\u5339\u914d\u3002\u53f3\u79fb\u7684\u8ddd\u79bb\u5728KMP\u7b97\u6cd5\u4e2d\u662f\u5982\u6b64\u8ba1\u7b97\u7684\uff1a\u5728**\u5df2\u7ecf\u5339\u914d\u7684\u6a21\u5f0f\u4e32\u5b50\u4e32**\u4e2d\uff0c\u627e\u51fa\u6700\u957f\u7684\u76f8\u540c\u7684 \u524d\u7f00 \u548c \u540e\u7f00 \uff0c\u7136\u540e\u79fb\u52a8\u4f7f\u5b83\u4eec\u91cd\u53e0\u3002 \u8fd9\u4e00\u6bb5\u53ea\u662f\u4e3a\u4e86\u8bc1\u660e\u6211\u4eec\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u76f4\u63a5\u5c06 j \u79fb\u52a8\u5230 k \u800c\u65e0\u987b\u518d\u6bd4\u8f83\u524d\u9762\u7684 k \u4e2a\u5b57\u7b26\u3002 \u6c42\u89e3next\u6570\u7ec4 \u597d\uff0c\u63a5\u4e0b\u6765\u5c31\u662f\u91cd\u70b9\u4e86\uff0c\u600e\u4e48\u6c42\u8fd9\u4e2a\uff08\u8fd9\u4e9b\uff09 k \u5462\uff1f\u56e0\u4e3a\u5728 P \u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e\u90fd\u53ef\u80fd\u53d1\u751f\u4e0d\u5339\u914d\uff0c\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u8981\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u4f4d\u7f6e j \u5bf9\u5e94\u7684 k \uff0c\u6240\u4ee5\u7528\u4e00\u4e2a\u6570\u7ec4 next \u6765\u4fdd\u5b58\uff0c next[j] = k \uff0c\u8868\u793a\u5f53 T[i] != P[j] \u65f6\uff0c** j \u6307\u9488**\u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u3002 \u5f88\u591a\u6559\u6750\u6216\u535a\u6587\u5728\u8fd9\u4e2a\u5730\u65b9\u90fd\u662f\u8bb2\u5f97\u6bd4\u8f83\u542b\u7cca\u6216\u662f\u6839\u672c\u5c31\u4e00\u7b14\u5e26\u8fc7\uff0c\u751a\u81f3\u5c31\u662f\u8d34\u4e00\u6bb5\u4ee3\u7801\u4e0a\u6765\uff0c\u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u6c42\uff1f\u600e\u4e48\u53ef\u4ee5\u8fd9\u6837\u6c42\uff1f\u6839\u672c\u5c31\u6ca1\u6709\u8bf4\u6e05\u695a\u3002\u800c\u8fd9\u91cc\u6070\u6070\u662f\u6574\u4e2a\u7b97\u6cd5\u6700\u5173\u952e\u7684\u5730\u65b9\u3002 public static int [] getNext ( String ps ) { char [] p = ps . toCharArray (); int [] next = new int [ p . length ] ; next [ 0 ] = - 1 ; int j = 0 ; int k = - 1 ; while ( j < p . length - 1 ) { if ( k == - 1 || p [ j ] == p [ k ] ) { next [++ j ] = ++ k ; } else { k = next [ k ] ; } } return next ; } \u8fd9\u4e2a\u7248\u672c\u7684\u6c42 next \u6570\u7ec4\u7684\u7b97\u6cd5\u5e94\u8be5\u662f\u6d41\u4f20\u6700\u5e7f\u6cdb\u7684\uff0c\u4ee3\u7801\u662f\u5f88\u7b80\u6d01\u3002\u53ef\u662f\u771f\u7684\u5f88\u8ba9\u4eba\u6478\u4e0d\u5230\u5934\u8111\uff0c\u5b83\u8fd9\u6837\u8ba1\u7b97\u7684\u4f9d\u636e\u5230\u5e95\u662f\u4ec0\u4e48\uff1f \u597d\uff0c\u5148\u628a\u8fd9\u4e2a\u653e\u4e00\u8fb9\uff0c\u6211\u4eec\u81ea\u5df1\u6765\u63a8\u5bfc\u601d\u8def\uff0c\u73b0\u5728\u8981\u59cb\u7ec8\u8bb0\u4f4f\u4e00\u70b9\uff0c next[j] \u7684\u503c\uff08\u4e5f\u5c31\u662f k \uff09\u8868\u793a\uff0c\u5f53 P[j] != T[i] \u65f6\uff0c j \u6307\u9488\u7684\u4e0b\u4e00\u6b65\u79fb\u52a8\u4f4d\u7f6e\u3002 \u5148\u6765\u770b\u7b2c\u4e00\u4e2a\uff1a\u5f53 j \u4e3a0\u65f6\uff0c\u5982\u679c\u8fd9\u65f6\u5019\u4e0d\u5339\u914d\uff0c\u600e\u4e48\u529e\uff1f \u50cf\u4e0a\u56fe\u8fd9\u79cd\u60c5\u51b5\uff0c j \u5df2\u7ecf\u5728\u6700\u5de6\u8fb9\u4e86\uff0c\u4e0d\u53ef\u80fd\u518d\u79fb\u52a8\u4e86\uff0c\u8fd9\u65f6\u5019\u8981\u5e94\u8be5\u662f i \u6307\u9488\u540e\u79fb\u3002\u6240\u4ee5\u5728\u4ee3\u7801\u4e2d\u624d\u4f1a\u6709 next[0] = -1; \u8fd9\u4e2a\u521d\u59cb\u5316\u3002 NOTE: \u770b\u4e86\u4e0b\u9762\u7684\u5b8c\u6574\u7684\u4ee3\u7801\u5c31\u77e5\u9053\u4e3a\u4ec0\u4e48\u4f7f\u7528 -1 \u6765\u4f5c\u4e3a\u521d\u59cb\u503c\uff0c\u56e0\u4e3a i++ \u548c j++ \u662f\u5728\u76f8\u540c\u7684\u5206\u652f\u4e2d\uff0c j++ \u540e j \u4e3a0\uff0c\u8fd9\u5c31\u4fdd\u8bc1\u4e86\u4eceP\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u5f00\u59cb\u5339\u914d\u3002 \u5982\u679c\u662f\u5f53 j \u4e3a1\u7684\u65f6\u5019\u5462\uff1f \u663e\u7136\uff0c j \u6307\u9488\u4e00\u5b9a\u662f\u540e\u79fb\u52300\u4f4d\u7f6e\u7684\u3002\u56e0\u4e3a\u5b83\u524d\u9762\u4e5f\u5c31\u53ea\u6709\u8fd9\u4e00\u4e2a\u4f4d\u7f6e\u4e86\u3002 \u4e0b\u9762\u8fd9\u4e2a\u662f\u6700\u91cd\u8981\u7684\uff0c\u8bf7\u770b\u5982\u4e0b\u56fe\uff1a \u8bf7\u4ed4\u7ec6\u5bf9\u6bd4\u8fd9\u4e24\u4e2a\u56fe\u3002 \u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u89c4\u5f8b\uff1a \u5f53 P[k] == P[j] \u65f6 \u5f53 P[k] == P[j] \u65f6\uff0c\u6709 next[j+1] == next[j] + 1 \u5176\u5b9e\u8fd9\u4e2a\u662f\u53ef\u4ee5\u8bc1\u660e\u7684\uff1a \u56e0\u4e3a\u5728 P[j] \u4e4b\u524d\u5df2\u7ecf\u6709 P[0 ~ k-1] == p[j-k ~ j-1] \u3002\uff08 next[j] == k \uff09 \u8fd9\u65f6\u5019\u73b0\u6709 P[k] == P[j] \uff0c\u6211\u4eec\u662f\u4e0d\u662f\u53ef\u4ee5\u5f97\u5230 P[0 ~ k-1] + P[k] == p[j-k ~ j-1] + P[j] \u3002 \u5373\uff1a P[0 ~ k] == P[j-k ~ j] \uff0c\u5373 next[j+1] == k + 1 == next[j] + 1 \u3002 \u8fd9\u91cc\u7684\u516c\u5f0f\u4e0d\u662f\u5f88\u597d\u61c2\uff0c\u8fd8\u662f\u770b\u56fe\u4f1a\u5bb9\u6613\u7406\u89e3\u4e9b\u3002 \u5f53 P[k] != P[j] \u65f6, \u5f53 P[k] != P[j] \u65f6\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u50cf\u8fd9\u79cd\u60c5\u51b5\uff0c\u5982\u679c\u4f60\u4ece\u4ee3\u7801\u4e0a\u770b\u5e94\u8be5\u662f\u8fd9\u4e00\u53e5\uff1a k = next[k]; \u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u5b50\uff1f\u4f60\u770b\u4e0b\u9762\u5e94\u8be5\u5c31\u660e\u767d\u4e86\u3002 \u73b0\u5728\u4f60\u5e94\u8be5\u77e5\u9053\u4e3a\u4ec0\u4e48\u8981 k = next[k] \u4e86\u5427\uff01\u50cf\u4e0a\u8fb9\u7684\u4f8b\u5b50\uff0c\u6211\u4eec\u5df2\u7ecf\u4e0d\u53ef\u80fd\u627e\u5230 [ A\uff0cB\uff0cA\uff0cB ] \u8fd9\u4e2a\u6700\u957f\u7684\u540e\u7f00\u4e32\u4e86\uff0c\u4f46\u6211\u4eec\u8fd8\u662f\u53ef\u80fd\u627e\u5230 [ A\uff0cB ] \u3001 [ B ] \u8fd9\u6837\u7684\u524d\u7f00\u4e32\u7684\u3002\u6240\u4ee5\u8fd9\u4e2a\u8fc7\u7a0b\u50cf\u4e0d\u50cf\u5728\u5b9a\u4f4d [ A\uff0cB\uff0cA\uff0cC ] \u8fd9\u4e2a\u4e32\uff0c\u5f53 C \u548c\u4e3b\u4e32\u4e0d\u4e00\u6837\u4e86\uff08\u4e5f\u5c31\u662f k \u4f4d\u7f6e\u4e0d\u4e00\u6837\u4e86\uff09\uff0c\u90a3\u5f53\u7136\u662f\u628a\u6307\u9488\u79fb\u52a8\u5230 next[k] \u5566\u3002 NOTE: \u8fd9\u7bc7\u6587\u7ae0\u8fd9\u91cc\u7684\u5206\u6790\u8fd8\u662f\u6bd4\u8f83\u96be\u4ee5\u7406\u89e3\u7684\uff0c\u4e0b\u4e00\u7bc7\u5728\u5206\u6790\u66f4\u52a0\u900f\u5f7b\u3002 NOTE: \u6784\u5efa next \u6570\u7ec4\u7684\u7b97\u6cd5\u662f\u4f7f\u7528\u7684\u6570\u5b66\u5f52\u7eb3\u6cd5\u6765\u6c42\u89e3next\u6570\u7ec4\u7684\u6bcf\u4e2a\u503c\uff0c\u5373\u6839\u636e next \u6570\u7ec4\u4e2d\u524d j \u4e2a\u5143\u7d20\u7684\u503c\u6765\u6c42\u89e3 next[j+1] \u7684\u503c\u3002 \u6709\u4e86 next \u6570\u7ec4\u4e4b\u540e\u5c31\u4e00\u5207\u597d\u529e\u4e86\uff0c\u6211\u4eec\u53ef\u4ee5\u52a8\u624b\u5199KMP\u7b97\u6cd5\u4e86\uff1a public static int KMP ( String ts , String ps ) { char [] t = ts . toCharArray (); char [] p = ps . toCharArray (); int i = 0 ; // \u4e3b\u4e32\u7684\u4f4d\u7f6e int j = 0 ; // \u6a21\u5f0f\u4e32\u7684\u4f4d\u7f6e int [] next = getNext ( ps ); while ( i < t . length && j < p . length ) { if ( j == - 1 || t [ i ] == p [ j ] ) { // \u5f53j\u4e3a-1\u65f6\uff0c\u8981\u79fb\u52a8\u7684\u662fi\uff0c\u5f53\u7136j\u4e5f\u8981\u5f520 i ++ ; j ++ ; } else { // i\u4e0d\u9700\u8981\u56de\u6eaf\u4e86 // i = i - j + 1; j = next [ j ] ; // j\u56de\u5230\u6307\u5b9a\u4f4d\u7f6e } } if ( j == p . length ) { return i - j ; } else { return - 1 ; } } \u548c\u66b4\u529b\u7834\u89e3\u76f8\u6bd4\uff0c\u5c31\u6539\u52a8\u4e864\u4e2a\u5730\u65b9\u3002\u5176\u4e2d\u6700\u4e3b\u8981\u7684\u4e00\u70b9\u5c31\u662f\uff0c i \u4e0d\u9700\u8981\u56de\u6eaf\u4e86\u3002 \u6700\u540e\uff0c\u6765\u770b\u4e00\u4e0b\u4e0a\u8fb9\u7684\u7b97\u6cd5\u5b58\u5728\u7684\u7f3a\u9677\u3002\u6765\u770b\u7b2c\u4e00\u4e2a\u4f8b\u5b50\uff1a \u663e\u7136\uff0c\u5f53\u6211\u4eec\u4e0a\u8fb9\u7684\u7b97\u6cd5\u5f97\u5230\u7684 next \u6570\u7ec4\u5e94\u8be5\u662f [ -1\uff0c0\uff0c0\uff0c1 ] \u6240\u4ee5\u4e0b\u4e00\u6b65\u6211\u4eec\u5e94\u8be5\u662f\u628a j \u79fb\u52a8\u5230\u7b2c1\u4e2a\u5143\u7d20\u54af\uff1a \u4e0d\u96be\u53d1\u73b0\uff0c \u8fd9\u4e00\u6b65\u662f\u5b8c\u5168\u6ca1\u6709\u610f\u4e49\u7684\u3002\u56e0\u4e3a\u540e\u9762\u7684 B \u5df2\u7ecf\u4e0d\u5339\u914d\u4e86\uff0c\u90a3\u524d\u9762\u7684 B \u4e5f\u4e00\u5b9a\u662f\u4e0d\u5339\u914d\u7684 \uff0c\u540c\u6837\u7684\u60c5\u51b5\u5176\u5b9e\u8fd8\u53d1\u751f\u5728\u7b2c2\u4e2a\u5143\u7d20 A \u4e0a\u3002 \u663e\u7136\uff0c \u53d1\u751f\u95ee\u9898\u7684\u539f\u56e0\u5728\u4e8e P[j] == P[next[j]] \u3002 \u6240\u4ee5\u6211\u4eec\u4e5f\u53ea\u9700\u8981\u6dfb\u52a0\u4e00\u4e2a\u5224\u65ad\u6761\u4ef6\u5373\u53ef\uff1a public static int [] getNext ( String ps ) { char [] p = ps . toCharArray (); int [] next = new int [ p . length ] ; next [ 0 ] = - 1 ; int j = 0 ; int k = - 1 ; while ( j < p . length - 1 ) { if ( k == - 1 || p [ j ] == p [ k ] ) { if ( p [++ j ] == p [++ k ] ) { // \u5f53\u4e24\u4e2a\u5b57\u7b26\u76f8\u7b49\u65f6\u8981\u8df3\u8fc7 next [ j ] = next [ k ] ; } else { next [ j ] = k ; } } else { k = next [ k ] ; } } return next ; } \u8be5\u7b97\u6cd5\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8e\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684 next[j] \u7684\u503c k \u5c31\u662f j \u4f4d\u4e4b\u524d\u7684\u5b50\u4e32\u4e2d\uff0c\u524d\u7f00\u96c6\u548c\u540e\u7f00\u96c6\u4e2d\u7684\u6700\u5927\u91cd\u590d\u5b50\u4e32\u7684\u957f\u5ea6\u3002 abacabac next[0]=-1;j=0;k=-1 \u6761\u4ef61 \u6761\u4ef62 \u5206\u652f2 \u5206\u652f1 k==-1 next[1]=0;j=1;k=0 p[1]!=p[0]; k=next[0]=-1 k==-1 next[2]=0;j=2;k=0 p[2]==p[0]; next[3]=1;j=3;k=1 p[3]!=p[1]; k=next[1]=0 p[3]!=p[0]; k=next[0]=-1 k==-1; next[4]=0;j=4;k=0 p[4]==p[0] next[5]=1;j=5;k=1 p[5]==p[1] next[6]=2;j=6;k=2 p[6]==p[2] next[7]=3;j=7;k=3 p[7]==p[3] next[8]=4;j=8;k=4 \u8981\u60f3\u5f97\u5230 p[j+1] \uff0c\u53ea\u9700\u8981\u6bd4\u8f83 p[j] \u548c p[k] \u5373\u53ef\uff1b 3. Computing the KMP failure function (f(k)) definition of f(k) f(k) = MaxOverlap ( \"p0 p1 ... pk\" ) where: \"p0 p1 ... pk\" = the prefix of length k+1 of pattern P Graphically: Naive way to find f(k) : Given P = \"p0 p1 ... pm-1\" Given k = 1, 2, ..., m-1 (k = 0 ==> f(0) = 0) 1. Extract the sub-pattern: \"p0 p1 ... pk\" 2. Find the first (= largest) overlap: Try: (p0) p1 p2 ... pk-1 p0 p1 ... pk-1 pk If (no match) Try: (p0) p1 p2 ... pk-1 p0 p1 ... pk-1 pk And so on... The first overlap is the longest ! NOTE: \u4e0a\u8ff0\u7b97\u6cd5\u662f\u4e00\u4e2a\u5faa\u73af\u7b97\u6cd5\uff0c\u5373 for k in range(1, m) \uff0c\u4e0b\u9762\u662f\u4e0a\u8ff0\u7b97\u6cd5\u7684python\u5b9e\u73b0\uff1a def build_failure_table ( p ): \"\"\" \u6784\u5efa\u5b57\u7b26\u4e32p\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u6570\u7ec4 :param p: :return: \"\"\" failure_table = list () len_of_p = len ( p ) for len_of_sub_str in range ( 1 , len_of_p + 1 ): max_len_of_overlap = int ( len_of_sub_str / 2 ) # \u6700\u5927\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6 print ( \"\u5b50\u4e32\u957f\u5ea6: {} ,\u6700\u5927\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u957f\u5ea6: {} \" . format ( len_of_sub_str , max_len_of_overlap )) if max_len_of_overlap == 0 : # \u957f\u5ea6\u4e3a1\u7684\u4e32\uff0c\u662f\u6ca1\u6709\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684 failure_table . append ( 0 ) else : found = False # \u662f\u5426\u627e\u5230\u91cd\u53e0\u524d\u7f00\u540e\u7f00 for len_of_overlap in range ( max_len_of_overlap , 0 , - 1 ): print ( \"\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u957f\u5ea6: {} \" . format ( len_of_overlap )) # len_of_overlap \u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6 for prefix_index in range ( len_of_overlap ): suffix_index = prefix_index + ( len_of_sub_str - len_of_overlap ) print ( \"\u524d\u7f00\u8d77\u59cb\u4f4d\u7f6e: {} ,\u540e\u7f00\u8d77\u59cb\u4f4d\u7f6e: {} \" . format ( prefix_index , suffix_index )) if p [ prefix_index ] == p [ suffix_index ]: if suffix_index == len_of_sub_str - 1 : # \u627e\u5230\u4e86\u91cd\u53e0\u90e8\u5206 failure_table . append ( len_of_overlap ) found = True break else : break if found : break if not found : failure_table . append ( 0 ) return failure_table Relating f(k) to f(k\u22121) The values f(k) are computed easily using existing prefix overlap information : f(0) = 0 ( f(0) is always 0) f(1) is computing using (already computed) value f(0) f(2) is computing using (already computed) value f(0) , f(1) f(3) is computing using (already computed) value f(0) , f(1) , f(2) And so on According to the definition of f(k) : NOTE: \u4e0a\u9762\u8fd9\u79cd\u8868\u793a\u95ee\u9898\u7684\u65b9\u5f0f\u662f\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u5373\u5728\u539f\u95ee\u9898\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u65b0\u5143\u7d20\u4ece\u800c\u6784\u6210\u4e86\u4e00\u4e2a\u89c4\u6a21\u66f4\u5927\u7684\u95ee\u9898\u3002 Suppose that we know that: f(k\u22121) = x In other words: the longest overlapping suffix and prefix in \" p0 p1 ... pk-1 \" has x characters: f(k-1) = x characters <-----------------------> p1 p2 p3 ... pk-x-2 pk-x-3 pk-x-4 .... pk-1 ^ ^ ^ ^ | | | equal | v v v v p0 p1 p2 .... px-1 px ... pk-1 question: Can we use the fact that f(k\u22121) = x to compute f(k) ? answer: Yes, because f(k) is computed using a similar prefix as f(k\u22121): prefix used to compute f(k-1) +--------------------------------+ | | p0 p1 p2 .... px-1 ... pk-1 pk | | +------------------------------------+ prefix used to compute f(k) We will next learn how to exploit the similarity to compute f(k) Fact between f(k) and f(k\u22121) Fact: f(k) \u2264 f(k\u22121) + 1 Computation trick 1 Let use denote: f(k\u22121) = x (Note: f(k\u22121) is equal to some value. The above assumption simply gave a more convenient notation for this value). If px == pk , then: f(k) = x+1 (i.e., the maximum overlap of the prefix p0 p1 p2 .... pk-1 pk has x+1 characters Proof: These x+1 characters match IF pk == px! <----------------------------> p1 p2 p3 ... pk-x-2 pk-x-3 pk-x-4 .... pk-1 pk ^ ^ ^ ^ ^ | | | equal | |equal v v v v v p0 p1 p2 .... px-1 px ... pk-1 pk | | +--------------------------+ These characters matches because f(k-1) = x Prelude to computation trick 2 Consider the prefix ababyabab where f(8) = 4: 012345678 prefix = ababyabab f(8) = 4 because: ababyabab ababyabab <--> 4 characters overlap We want to compute f(9) using f(8) , but now the next character does not match(that is the next char is not equal to y): 0123456789 prefix = ababyababa ababyababa ababyababa Conclusion: *** We CANNOT use f(8) to compute f(9) *** question: What should we try next to find the maximum overlap for the prefix \"ababyababa\" answer: To find the maximum overlap, we must slide the prefix down and look for matching letters !!! NOTE: \u601d\u8def\u662f\u4f7f\u7528\u5df2\u7ecf\u5339\u914d\u7684\u5b57\u7b26\u4e32\u6765\u5c3d\u53ef\u80fd\u51cf\u5c11\u5339\u914d\u6b21\u6570\u5e76\u4e14\u5bfb\u627e\u7b2c\u4e00\u4e2a\u6700\u53ef\u80fd\u7684\u4f4d\u7f6e\u3002 Now, let us use only the matching prefix information: ababyababa ababyababa Look only at these characters: ?????abab? abab?????? We can know for sure that the overlap cannot be found starting at these positions: ?????abab? abab?????? NOTE: \u56e0\u4e3a\u6211\u4eec\u77e5\u9053\u4e32 abab \u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u662f2\uff0c\u5373 f(3) \uff0c\u6240\u4ee5\u5b83\u7684\u524d\u4e24\u4e2a\u5143\u7d20\u53ef\u4ee5\u5339\u914d\u4e0a\u7684\uff0c\u6240\u4ee5\u7b2c\u4e00\u4e2a\u53ef\u80fd\u4f4d\u7f6e\u662f\u5982\u4e0b\u56fe\u6240\u793a\u7684\uff0c\u8fd9\u5c31\u662f\u5bf9\u5df2\u7ecf\u5339\u914d\u4fe1\u606f\u7684\u5145\u5206\u8fd0\u7528\u3002\u81f3\u4e8e\u7b2c\u4e09\u4e2a\u5143\u7d20\u662f\u5426\u80fd\u591f\u5339\u914d\u4e0a\uff0c\u5c31\u8981\u6bd4\u8f83\u7684\u7ed3\u679c\u4e86\u3002 The first possible way that overlap can be found is starting here: ?????abab? abab?????? In other words: we can compute f(9) using f(3) : 0123 prefix = abab abab abab f(3) = 2 Notice that: 3 = 4\u22121 and f(8) = 4 Worked out further: 0123456789 prefix = ababyababa ababyababa ababyababa ^ | compare the character at position 2 (f(3) = 2) Note: The prefix abab is hightlighted in yellow Because the characters are equal, we have found the maximum overlap: f(9) = f(3) + 1 = 2 + 1 = 3 !!! NOTE: \u8fd9\u91cc\u53ef\u4ee5\u5047\u8bbe\uff0c\u5982\u679c p[3] \u548c p[9] \u5e76\u4e0d\u76f8\u7b49\uff0c\u5219\u4e0a\u8ff0\u6d41\u7a0b\u9700\u8981\u7ee7\u7eed\u4e0b\u53bb\uff0c\u81f3\u4e8e\u7ec8\u6b62\u6761\u4ef6\uff0c\u663e\u7136\u662f\u76f4\u81f3\u6bd4\u8f83\u5230\u7b2c\u4e00\u4e2a\u5143\u7d20\u90fd\u4e0d\u76f8\u7b49\u3002 Computation trick #2 Let: f(k\u22121) = x (Note: f(k\u22121) is equal to some value. The above assumption simply gave a more convenient notation for this value). If px \u2260 pk , then: The next prefix that can be used to compute f(k) is: p0 p1 .... px-1 In pseudo code: i = k-1; // Try to use f(k-1) to compute f(k) x = f(i); // x = character position to match against pk if ( P[k] == P[x] ) then f(k) = f(x\u22121) + 1 else Use: p0 p1 .... px-1 to compute f(k) What that means in terms of program statements: i = x-1; // Try to use f(x-1) to compute f(k) x = f(i); // x = character position to match against pk Note: We must repeat trick #2 as long as i \u2265 0, In other words: use a while loop instead of an if statement ! Algorithm to compute KMP failure function Java code: public static int [] KMP_failure_function ( String P ) { int k , i , x , m ; int f [] = new int [ P . length () ] ; m = P . length (); f [ 0 ] = 0 ; // f(0) is always 0 for ( k = 1 ; k < m ; k ++ ) { // Compute f[k] i = k - 1 ; // First try to use f(k-1) to compute f(k) x = f [ i ] ; while ( P . charAt ( x ) != P . charAt ( k ) ) { i = x - 1 ; // Try the next candidate f(.) to compute f(k) if ( i < 0 ) // Make sure x is valid break ; // STOP the search !!! x = f [ i ] ; } if ( i < 0 ) f [ k ] = 0 ; // No overlap at all: max overlap = 0 characters else f [ k ] = f [ i ] + 1 ; // We can compute f(k) using f(i) } return ( f ); } \u5b8c\u6574\u6d4b\u8bd5\u7a0b\u5e8f /* ---------------------------------- My own KMP Failure function alg S.Y. Cheung - 3/3/2013 ---------------------------------- */ import java.util.* ; public class ComputeF { public static int [] KMP_failure_function ( String P ) { int k , i , x , m ; int f [] = new int [ P . length () ] ; String s ; m = P . length (); f [ 0 ] = 0 ; for ( k = 1 ; k < m ; k ++ ) { // Compute f[k] s = P . substring ( 0 , k + 1 ); System . out . println ( \"-----------------------------------------------\" ); System . out . println ( \"Prefix = \" + s + \" --- Computing f(\" + k + \"):\" ); i = k - 1 ; // First try to use f(k-1) to compute f(k) x = f [ i ] ; System . out . println ( \"===================================\" ); System . out . println ( \"Try using: f(\" + i + \") = \" + x ); printState ( s , s , k , x ); while ( P . charAt ( x ) != P . charAt ( k ) ) { i = f [ i ]- 1 ; // Try the next candidate f(.) to compute f(k) if ( i < 0 ) // Search ended in failure.... break ; x = f [ i ] ; System . out . println ( \"===================================\" ); System . out . println ( \"Try using: f(\" + i + \") = \" + x ); printState ( s , s , k , x ); } if ( i < 0 ) { System . out . println ( \"No overlap possible... --> f[\" + k + \"] = 0\" ); f [ k ] = 0 ; // No overlap possible } else { f [ k ] = f [ i ] + 1 ; // Compute f(k) using f(i) System . out . println ( \"Overlap found ... --> f[\" + k + \"] = \" + f [ k ] ); } } return ( f ); } public static void main ( String [] args ) { String P ; Scanner in ; int [] f ; in = new Scanner ( System . in ); System . out . print ( \"P = \" ); P = in . nextLine (); System . out . println (); f = KMP_failure_function ( P ); for ( int i = 0 ; i < P . length (); i ++ ) { System . out . println ( \"f(\" + i + \") = \" + f [ i ] ); } System . out . println (); } /* ===================================================== Variables and Methods to make the algorithm visual ===================================================== */ public static String T_ruler , P_ruler ; public static String ruler ( int n ) { String out = \"\" ; char x = '0' ; for ( int i = 0 ; i < n ; i ++ ) { out = out + x ; x ++ ; if ( x > '9' ) x = '0' ; } return out ; } public static void printState ( String T , String P , int i , int j ) { T_ruler = ruler ( T . length () ); P_ruler = ruler ( P . length () ); System . out . println ( \"=====================================\" ); System . out . println ( \"Matching: i = \" + i + \", j = \" + j ); System . out . println ( \" \" + T_ruler ); System . out . println ( \" \" + T ); System . out . print ( \" \" ); for ( int k = 0 ; k < i - j ; k ++ ) System . out . print ( \" \" ); System . out . println ( P ); System . out . print ( \" \" ); for ( int k = 0 ; k < i - j ; k ++ ) System . out . print ( \" \" ); System . out . println ( P_ruler ); System . out . print ( \" \" ); for ( int k = 0 ; k < i ; k ++ ) System . out . print ( \" \" ); System . out . println ( \"^\" ); System . out . print ( \" \" ); for ( int k = 0 ; k < i ; k ++ ) System . out . print ( \" \" ); System . out . println ( \"|\" ); System . out . println (); } } KMP\u5b9e\u73b0\u5206\u6790 \u901a\u8fc7\u4e0a\u8ff0\u4e09\u7bc7\u6587\u7ae0\uff0c\u80fd\u591f\u77e5\u9053KMP\u7b97\u6cd5\u7684\u539f\u7406\uff0c\u73b0\u5728\u9700\u8981\u8003\u8651\u7684\u662f\u5982\u4f55\u6765\u8fdb\u884c\u5b9e\u73b0\u3002 \u8ba1\u7b97KMP failure function\u7684\u9012\u5f52\u516c\u5f0f \u5f53 pattern[j] \u4e0e pattern[f[j-1]] \u4e0d\u76f8\u7b49\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u9012\u5f52\u516c\u5f0f\u4e2d\u6d89\u53ca\u5230\u4e86\u4e0d\u65ad\u5730\u5faa\u73af\u9012\u5f52\uff0c\u4f7f\u7528\u6570\u5b66\u516c\u5f0f\u4e0d\u65b9\u4fbf\u63cf\u8ff0\uff0c\u4e0b\u9762\u7684python\u7a0b\u5e8f\u662f\u975e\u5e38\u7b80\u6d01\u6613\u61c2\u7684\uff0c\u5e76\u4e14\u662f\u975e\u5e38\u63a5\u8fd1\u6570\u5b66\u516c\u5f0f\u7684\uff0c\u6240\u4ee5\u8fd9\u91cc\u5c31\u7701\u7565\u6389\u9012\u5f52\u516c\u5f0f\u3002 \u8ba1\u7b97KMP failure function\u7684python\u5b9e\u73b0 failure function f(j) \u8868\u793a\u7684\u662f\u4ece pattern[0] \u5230 pattern[j] \u7684\u5e8f\u5217\uff08\u663e\u7136\u8fd9\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\u662f j+1 \uff09\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684**\u957f\u5ea6**\uff0c\u5373 f(j) \u6240\u8868\u793a\u7684\u662f\u957f\u5ea6\u4e3a j+1 \u7684\u5e8f\u5217\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u3002\u663e\u7136 f[0]==0 \uff0c\u56e0\u4e3a\u957f\u5ea6\u4e3a1\u7684\u5e8f\u5217\u7684\u6700\u957f\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u4e3a0\u3002\u6240\u4ee5\uff0c\u5f53\u5df2\u77e5\u5e8f\u5217\u7684\u957f\u5ea6\u4e3a i \uff0c\u6765\u67e5\u8be2\u5176\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u7684\u662f f(i-1) \u3002\u56e0\u4e3a i \u8868\u793a\u7684\u662f\u957f\u5ea6\uff0c\u6240\u4ee5 pattern[i] \u5f15\u7528\u7684\u662f\u6570\u7ec4\u7684\u7b2c i+1 \u4e2a\u5143\u7d20\u3002 def get_failure_array ( pattern ): failure = [ 0 ] # \u521d\u59cb\u6761\u4ef6 i = 0 # f(j-1)\u7684\u503c\uff0c\u662f\u5df2\u77e5\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u7684\u542b\u4e49\u662f\u957f\u5ea6 j = 1 # f(j)\u662f\u672a\u77e5\u7684\uff0cj\u8868\u793a\u7684\u662findex while j < len ( pattern ): if pattern [ i ] == pattern [ j ]: i += 1 elif i > 0 : i = failure [ i - 1 ] continue j += 1 failure . append ( i ) return failure \u8ba1\u7b97KMP failure function \u548c dynamic programming KMP\u7684failure function\u7684\u6c42\u89e3\u8fc7\u7a0b\u5728\u8ba1\u7b97 f(k+1) \u7684\u65f6\u6240\u4f9d\u8d56\u7684 f(0),f(1)...,f(k) \u90fd\u662f\u901a\u8fc7\u67e5failure table\u800c\u83b7\u5f97\u7684\uff0c\u800c\u4e0d\u662f\u91cd\u65b0\u8ba1\u7b97\uff0c\u8fd9\u5176\u5b9e\u5c31\u662f\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684\u601d\u60f3\u3002\u5728\u4e0a\u8ff0\u4ee3\u7801\u4e2d\uff0c i \u5c31\u8868\u793a\u8ba1\u7b97 f(k+1) \u6240\u4f9d\u8d56\u7684\u6570\u636e\uff0c\u5b83\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8e\u8fed\u4ee3\u7248\u7684\u6590\u6ce2\u90a3\u5951\u6570\u5217\u3002 KMP\u7684\u5b9e\u73b0 def kmp_search ( pattern , text ): \"\"\" :param pattern: :param text: :return: \"\"\" # 1) Construct the failure array failure = get_failure_array ( pattern ) # 2) Step through text searching for pattern i , j = 0 , 0 # index into text, pattern while i < len ( text ): if pattern [ j ] == text [ i ]: if j == ( len ( pattern ) - 1 ): return True j += 1 elif j > 0 : # if this is a prefix in our pattern # just go back far enough to continue j = failure [ j - 1 ] continue i += 1 return False \u601d\u8003\uff1a\u4e3a\u4ec0\u4e48 j = failure[j - 1] \uff1f\u5176\u5b9e\u7ed3\u5408\u524d\u9762\u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u77e5\u9053\u4e86\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002 KMP in leetcode http://www.voidcn.com/article/p-uuefgkai-bnw.html https://leetcode-cn.com/problems/implement-strstr/comments/ https://leetcode.com/problems/shortest-palindrome/discuss/60113/clean-kmp-solution-with-super-detailed-explanation","title":"Knuth\u2013Morris\u2013Pratt algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#introduction","text":"It takes me some effort to master KMP algorithm. Here are three articles that helped me solve the mystery as I learned.","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#1#knuthmorrispratt#algorithm","text":"In computer science , the Knuth\u2013Morris\u2013Pratt string-searching algorithm (or KMP algorithm ) searches for occurrences of a \"word\" W within a main \"text string\" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters. The algorithm was conceived by James H. Morris and independently discovered by Donald Knuth \"a few weeks later\" from automata theory.[ 1] [ 2] Morris and Vaughan Pratt published a technical report in 1970.[ 3] The three also published the algorithm jointly in 1977.[ 1] Independently, in 1969, Matiyasevich [ 4] [ 5] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem over a binary alphabet. This was the first linear-time algorithm for string matching.","title":"1. Knuth\u2013Morris\u2013Pratt algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#2#kmp","text":"KMP\u7b97\u6cd5\u8981\u89e3\u51b3\u7684\u95ee\u9898\u5c31\u662f\u5728\u5b57\u7b26\u4e32\uff08\u4e5f\u53eb\u4e3b\u4e32\uff09\u4e2d\u7684\u6a21\u5f0f\uff08pattern\uff09\u5b9a\u4f4d\u95ee\u9898\u3002\u8bf4\u7b80\u5355\u70b9\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u5e38\u8bf4\u7684\u5173\u952e\u5b57\u641c\u7d22\u3002\u6a21\u5f0f\u4e32\u5c31\u662f\u5173\u952e\u5b57\uff08\u63a5\u4e0b\u6765\u79f0\u5b83\u4e3a P \uff09\uff0c\u5982\u679c\u5b83\u5728\u4e00\u4e2a\u4e3b\u4e32\uff08\u63a5\u4e0b\u6765\u79f0\u4e3a T \uff09\u4e2d\u51fa\u73b0\uff0c\u5c31\u8fd4\u56de\u5b83\u7684\u5177\u4f53\u4f4d\u7f6e\uff0c\u5426\u5219\u8fd4\u56de -1 \uff08\u5e38\u7528\u624b\u6bb5\uff09\u3002 \u9996\u5148\uff0c\u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\u6709\u4e00\u4e2a\u5f88\u5355\u7eaf\u7684\u60f3\u6cd5\uff1a\u4ece\u5de6\u5230\u53f3\u4e00\u4e2a\u4e2a\u5339\u914d\uff0c\u5982\u679c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u6709\u67d0\u4e2a\u5b57\u7b26\u4e0d\u5339\u914d\uff0c\u5c31\u8df3\u56de\u53bb\uff0c\u5c06\u6a21\u5f0f\u4e32\u5411\u53f3\u79fb\u52a8\u4e00\u4f4d\u3002\u8fd9\u6709\u4ec0\u4e48\u96be\u7684\uff1f \u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u521d\u59cb\u5316\uff1a \u4e4b\u540e\u6211\u4eec\u53ea\u9700\u8981\u6bd4\u8f83** i \u6307\u9488**\u6307\u5411\u7684\u5b57\u7b26\u548c** j \u6307\u9488**\u6307\u5411\u7684\u5b57\u7b26\u662f\u5426\u4e00\u81f4\u3002\u5982\u679c\u4e00\u81f4\u5c31\u90fd\u5411\u540e\u79fb\u52a8\uff0c\u5982\u679c\u4e0d\u4e00\u81f4\uff0c\u5982\u4e0b\u56fe\uff1a A \u548c E \u4e0d\u76f8\u7b49\uff0c\u90a3\u5c31\u628a** i \u6307\u9488**\u79fb\u56de\u7b2c1\u4f4d\uff08\u5047\u8bbe\u4e0b\u6807\u4ece0\u5f00\u59cb\uff09\uff0c j \u79fb\u52a8\u5230\u6a21\u5f0f\u4e32\u7684\u7b2c0\u4f4d\uff0c\u7136\u540e\u53c8\u91cd\u65b0\u5f00\u59cb\u8fd9\u4e2a\u6b65\u9aa4\uff1a \u57fa\u4e8e\u8fd9\u4e2a\u60f3\u6cd5\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u7684\u7a0b\u5e8f\uff1a /** * \u66b4\u529b\u7834\u89e3\u6cd5 * @param ts \u4e3b\u4e32 * @param ps \u6a21\u5f0f\u4e32 * @return \u5982\u679c\u627e\u5230\uff0c\u8fd4\u56de\u5728\u4e3b\u4e32\u4e2d\u7b2c\u4e00\u4e2a\u5b57\u7b26\u51fa\u73b0\u7684\u4e0b\u6807\uff0c\u5426\u5219\u4e3a-1 */ public static int bf ( String ts , String ps ) { char [] t = ts . toCharArray (); char [] p = ps . toCharArray (); int i = 0 ; // \u4e3b\u4e32\u7684\u4f4d\u7f6e int j = 0 ; // \u6a21\u5f0f\u4e32\u7684\u4f4d\u7f6e while ( i < t . length && j < p . length ) { if ( t [ i ] == p [ j ] ) { // \u5f53\u4e24\u4e2a\u5b57\u7b26\u76f8\u540c\uff0c\u5c31\u6bd4\u8f83\u4e0b\u4e00\u4e2a i ++ ; j ++ ; } else { i = i - j + 1 ; // \u4e00\u65e6\u4e0d\u5339\u914d\uff0ci\u540e\u9000 j = 0 ; // j\u5f520 } } if ( j == p . length ) { return i - j ; } else { return - 1 ; } } \u4e0a\u9762\u7684\u7a0b\u5e8f\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u4f46\u4e0d\u591f\u597d\uff01 NOTE: geeksforgeeks\u7684\u6587\u7ae0 Naive algorithm for Pattern Searching \u4e2d\u7ed9\u51fa\u7684\u4ee3\u7801\u662f\u6bd4\u4e0a\u8ff0\u4ee3\u7801\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u5982\u679c\u662f\u4eba\u4e3a\u6765\u5bfb\u627e\u7684\u8bdd\uff0c\u80af\u5b9a\u4e0d\u4f1a\u518d\u628a i \u79fb\u52a8\u56de\u7b2c1\u4f4d\uff0c \u56e0\u4e3a\u4e3b\u4e32\u5339\u914d\u5931\u8d25\u7684\u4f4d\u7f6e\u524d\u9762\u9664\u4e86\u7b2c\u4e00\u4e2a A \u4e4b\u5916\u518d\u4e5f\u6ca1\u6709 A **\u4e86\uff0c\u6211\u4eec\u4e3a\u4ec0\u4e48\u80fd\u77e5\u9053\u4e3b\u4e32\u524d\u9762\u53ea\u6709\u4e00\u4e2a A \uff1f**\u56e0\u4e3a\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u524d\u9762\u4e09\u4e2a\u5b57\u7b26\u90fd\u662f\u5339\u914d\u7684\uff01\uff08\u8fd9\u5f88\u91cd\u8981\uff09 \u3002\u79fb\u52a8\u8fc7\u53bb\u80af\u5b9a\u4e5f\u662f\u4e0d\u5339\u914d\u7684\uff01\u6709\u4e00\u4e2a\u60f3\u6cd5\uff0c i \u53ef\u4ee5\u4e0d\u52a8\uff0c\u6211\u4eec\u53ea\u9700\u8981\u79fb\u52a8 j \u5373\u53ef\uff0c\u5982\u4e0b\u56fe\uff1a \u4e0a\u9762\u7684\u8fd9\u79cd\u60c5\u51b5\u8fd8\u662f\u6bd4\u8f83\u7406\u60f3\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u6700\u591a\u4e5f\u5c31\u591a\u6bd4\u8f83\u4e86\u4e24\u6b21\u3002\u4f46\u5047\u5982\u662f\u5728\u4e3b\u4e32 SSSSSSSSSSSSSA \u4e2d\u67e5\u627e SSSSB \uff0c\u6bd4\u8f83\u5230\u6700\u540e\u4e00\u4e2a\u624d\u77e5\u9053\u4e0d\u5339\u914d\uff0c\u7136\u540e i \u56de\u6eaf \uff0c\u8fd9\u4e2a\u7684\u6548\u7387\u662f\u663e\u7136\u662f\u6700\u4f4e\u7684\u3002 NOTE: \u5173\u4e8e\u56de\u6eaf\uff0c\u53c2\u89c1 Backtracking \u5927\u725b\u4eec\u662f\u65e0\u6cd5\u5fcd\u53d7\u201c\u66b4\u529b\u7834\u89e3\u201d\u8fd9\u79cd\u4f4e\u6548\u7684\u624b\u6bb5\u7684\uff0c\u4e8e\u662f\u4ed6\u4eec\u4e09\u4e2a\u7814\u7a76\u51fa\u4e86KMP\u7b97\u6cd5\u3002\u5176\u601d\u60f3\u5c31\u5982\u540c\u6211\u4eec\u4e0a\u8fb9\u6240\u770b\u5230\u7684\u4e00\u6837\uff1a\u201c \u5229\u7528\u5df2\u7ecf\u90e8\u5206\u5339\u914d\u8fd9\u4e2a\u6709\u6548\u4fe1\u606f\uff0c\u4fdd\u6301 i \u6307\u9488\u4e0d\u56de\u6eaf\uff0c\u901a\u8fc7\u4fee\u6539 j \u6307\u9488\uff0c\u8ba9\u6a21\u5f0f\u4e32\u5c3d\u91cf\u5730\u79fb\u52a8\u5230\u6709\u6548\u7684\u4f4d\u7f6e \u3002\u201d NOTE: \u63d0\u9192\u4f60\u6ce8\u610f**\u5c3d\u91cf\u5730**\u8fd9\u4e2a\u4fee\u9970\u8bed\uff0c\u7b49\u4f60\u5b8c\u5168\u7406\u89e3\u4e86KMP\u7b97\u6cd5\uff0c\u4f60\u5c31\u5e61\u7136\u9192\u609f\u8fd9\u4e2a\u4fee\u9970\u8bed\u662f\u975e\u5e38\u5999\u7684\u3002\u5176\u5b9e\u5728\u8fd9\u91cc\uff0c\u6211\u662f\u53ef\u4ee5\u5411\u4f60\u63d0\u524d\u900f\u9732\u7684\uff0c\u65e2\u7136\u8bf4\u662f\u5c3d\u91cf\uff0c\u90a3\u4e48\u4e5f\u5c31\u662f\u8bf4\u79fb\u52a8\u5230\u7684\u4f4d\u7f6e\u4e0d\u4e00\u5b9a\u662f\u6700\u6700\u6709\u6548\u7684\u4f4d\u7f6e\uff0c\u800c\u662f\u4e00\u4e2a\u76f8\u5bf9\u6709\u6548\u7684\u4f4d\u7f6e\uff0c\u53ef\u80fd\u9700\u8981\u7ecf\u8fc7\u591a\u6b21\u79fb\u52a8\u624d\u80fd\u591f\u5230\u8fbe\u6b63\u786e\u7684\u4f4d\u7f6e\uff0c\u6bd5\u7adf\u8ba1\u7b97\u673a\u4e0d\u662f\u50cf\u6211\u4eec\u4eba\u8fd9\u6837\u7684\u667a\u80fd\u3002 \u6240\u4ee5\uff0c\u6574\u4e2aKMP\u7684\u91cd\u70b9\u5c31\u5728\u4e8e**\u5f53\u67d0\u4e00\u4e2a\u5b57\u7b26\u4e0e\u4e3b\u4e32\u4e0d\u5339\u914d\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u77e5\u9053 j \u6307\u9488\u8981\u79fb\u52a8\u5230\u54ea**\uff1f \u63a5\u4e0b\u6765\u6211\u4eec\u81ea\u5df1\u6765\u53d1\u73b0 j \u7684\u79fb\u52a8\u89c4\u5f8b\uff1a \u5982\u56fe\uff1a C \u548c D \u4e0d\u5339\u914d\u4e86\uff0c\u6211\u4eec\u8981\u628a j \u79fb\u52a8\u5230\u54ea\uff1f\u663e\u7136\u662f\u7b2c1\u4f4d\u3002\u4e3a\u4ec0\u4e48\uff1f\u56e0\u4e3a\u524d\u9762\u6709\u4e00\u4e2a A \u76f8\u540c\u554a\uff1a \u5982\u4e0b\u56fe\u4e5f\u662f\u4e00\u6837\u7684\u60c5\u51b5\uff1a \u53ef\u4ee5\u628a j \u6307\u9488\u79fb\u52a8\u5230\u7b2c2\u4f4d\uff0c\u56e0\u4e3a\u524d\u9762\u6709\u4e24\u4e2a\u5b57\u6bcd\u662f\u4e00\u6837\u7684\uff1a \u81f3\u6b64\u6211\u4eec\u53ef\u4ee5\u5927\u6982\u770b\u51fa\u4e00\u70b9\u7aef\u502a\uff0c\u5f53\u5339\u914d\u5931\u8d25\u65f6\uff0c j \u8981\u79fb\u52a8\u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e k \u3002\u5b58\u5728\u7740\u8fd9\u6837\u7684\u6027\u8d28\uff1a \u6700\u524d\u9762\u7684 k \u5b57\u7b26\u548c j \u4e4b\u524d\u7684\u6700\u540e k \u4e2a\u5b57\u7b26\u662f\u4e00\u6837\u7684 \u3002 \u5982\u679c\u7528\u6570\u5b66\u516c\u5f0f\u6765\u8868\u793a\u662f\u8fd9\u6837\u7684 P[0 ~ k-1] == P[j-k ~ j-1] \u8fd9\u4e2a\u76f8\u5f53\u91cd\u8981\uff0c\u5982\u679c\u89c9\u5f97\u4e0d\u597d\u8bb0\u7684\u8bdd\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e0b\u56fe\u6765\u7406\u89e3\uff1a \u5f04\u660e\u767d\u4e86\u8fd9\u4e2a\u5c31\u5e94\u8be5\u53ef\u80fd\u660e\u767d\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u76f4\u63a5\u5c06 j \u79fb\u52a8\u5230 k \u4f4d\u7f6e\u4e86\u3002 \u56e0\u4e3a: \u5f53 T[i] != P[j] \u65f6 \u6709 T[i-j ~ i-1] == P[0 ~ j-1] \u7531 P[0 ~ k-1] == P[j-k ~ j-1] \u5fc5\u7136\uff1a T[i-k ~ i-1] == P[0 ~ k-1] NOTE: \u4e0a\u8ff0\u516c\u5f0f\u5176\u5b9e\u5c31\u662fa==b, b==c,\u5219a==c \u516c\u5f0f\u5f88\u65e0\u804a\uff0c\u80fd\u770b\u660e\u767d\u5c31\u884c\u4e86\uff0c\u4e0d\u9700\u8981\u8bb0\u4f4f\u3002 NOTE: \u4f5c\u8005\u8fd9\u91cc\u7684\u603b\u7ed3\u4e0d\u591f\u76f4\u63a5\uff0c\u4e0b\u9762\u662f\u6458\u81ea\u767e\u5ea6\u767e\u79d1 kmp\u7b97\u6cd5 \u4e2d\u5bf9\u8fd9\u4e2a\u7ed3\u8bba\u7684\u603b\u7ed3\uff0c\u5b83\u975e\u5e38\u76f4\u63a5\uff1a \u7528\u66b4\u529b\u7b97\u6cd5\u5339\u914d\u5b57\u7b26\u4e32\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f1a\u628a T[0] \u8ddf W[0] \u5339\u914d\uff0c\u5982\u679c\u76f8\u540c\u5219\u5339\u914d\u4e0b\u4e00\u4e2a\u5b57\u7b26\uff0c\u76f4\u5230\u51fa\u73b0\u4e0d\u76f8\u540c\u7684\u60c5\u51b5\uff0c\u6b64\u65f6\u6211\u4eec\u4f1a\u4e22\u5f03\u524d\u9762\u7684\u5339\u914d\u4fe1\u606f\uff0c\u7136\u540e\u628a T[1] \u8ddf W[0] \u5339\u914d\uff0c\u5faa\u73af\u8fdb\u884c\uff0c\u76f4\u5230\u4e3b\u4e32\u7ed3\u675f\uff0c\u6216\u8005\u51fa\u73b0\u5339\u914d\u6210\u529f\u7684\u60c5\u51b5\u3002\u8fd9\u79cd\u4e22\u5f03\u524d\u9762\u7684\u5339\u914d\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5339\u914d\u6548\u7387\u3002 \u800c\u5728KMP\u7b97\u6cd5\u4e2d\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u6a21\u5f0f\u4e32\u6211\u4eec\u4f1a\u4e8b\u5148\u8ba1\u7b97\u51fa\u6a21\u5f0f\u4e32\u7684\u5185\u90e8\u5339\u914d\u4fe1\u606f\uff0c\u5728\u5339\u914d\u5931\u8d25\u65f6\u6700\u5927\u7684\u79fb\u52a8\u6a21\u5f0f\u4e32\uff0c\u4ee5\u51cf\u5c11\u5339\u914d\u6b21\u6570\u3002 \u6bd4\u5982\uff0c\u5728\u7b80\u5355\u7684\u4e00\u6b21\u5339\u914d\u5931\u8d25\u540e\uff0c\u6211\u4eec\u4f1a\u60f3\u5c06\u6a21\u5f0f\u4e32\u5c3d\u91cf\u7684\u53f3\u79fb\u548c\u4e3b\u4e32\u8fdb\u884c\u5339\u914d\u3002\u53f3\u79fb\u7684\u8ddd\u79bb\u5728KMP\u7b97\u6cd5\u4e2d\u662f\u5982\u6b64\u8ba1\u7b97\u7684\uff1a\u5728**\u5df2\u7ecf\u5339\u914d\u7684\u6a21\u5f0f\u4e32\u5b50\u4e32**\u4e2d\uff0c\u627e\u51fa\u6700\u957f\u7684\u76f8\u540c\u7684 \u524d\u7f00 \u548c \u540e\u7f00 \uff0c\u7136\u540e\u79fb\u52a8\u4f7f\u5b83\u4eec\u91cd\u53e0\u3002 \u8fd9\u4e00\u6bb5\u53ea\u662f\u4e3a\u4e86\u8bc1\u660e\u6211\u4eec\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u76f4\u63a5\u5c06 j \u79fb\u52a8\u5230 k \u800c\u65e0\u987b\u518d\u6bd4\u8f83\u524d\u9762\u7684 k \u4e2a\u5b57\u7b26\u3002","title":"2. \u8be6\u89e3KMP\u7b97\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#next","text":"\u597d\uff0c\u63a5\u4e0b\u6765\u5c31\u662f\u91cd\u70b9\u4e86\uff0c\u600e\u4e48\u6c42\u8fd9\u4e2a\uff08\u8fd9\u4e9b\uff09 k \u5462\uff1f\u56e0\u4e3a\u5728 P \u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e\u90fd\u53ef\u80fd\u53d1\u751f\u4e0d\u5339\u914d\uff0c\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u8981\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u4f4d\u7f6e j \u5bf9\u5e94\u7684 k \uff0c\u6240\u4ee5\u7528\u4e00\u4e2a\u6570\u7ec4 next \u6765\u4fdd\u5b58\uff0c next[j] = k \uff0c\u8868\u793a\u5f53 T[i] != P[j] \u65f6\uff0c** j \u6307\u9488**\u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u3002 \u5f88\u591a\u6559\u6750\u6216\u535a\u6587\u5728\u8fd9\u4e2a\u5730\u65b9\u90fd\u662f\u8bb2\u5f97\u6bd4\u8f83\u542b\u7cca\u6216\u662f\u6839\u672c\u5c31\u4e00\u7b14\u5e26\u8fc7\uff0c\u751a\u81f3\u5c31\u662f\u8d34\u4e00\u6bb5\u4ee3\u7801\u4e0a\u6765\uff0c\u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u6c42\uff1f\u600e\u4e48\u53ef\u4ee5\u8fd9\u6837\u6c42\uff1f\u6839\u672c\u5c31\u6ca1\u6709\u8bf4\u6e05\u695a\u3002\u800c\u8fd9\u91cc\u6070\u6070\u662f\u6574\u4e2a\u7b97\u6cd5\u6700\u5173\u952e\u7684\u5730\u65b9\u3002 public static int [] getNext ( String ps ) { char [] p = ps . toCharArray (); int [] next = new int [ p . length ] ; next [ 0 ] = - 1 ; int j = 0 ; int k = - 1 ; while ( j < p . length - 1 ) { if ( k == - 1 || p [ j ] == p [ k ] ) { next [++ j ] = ++ k ; } else { k = next [ k ] ; } } return next ; } \u8fd9\u4e2a\u7248\u672c\u7684\u6c42 next \u6570\u7ec4\u7684\u7b97\u6cd5\u5e94\u8be5\u662f\u6d41\u4f20\u6700\u5e7f\u6cdb\u7684\uff0c\u4ee3\u7801\u662f\u5f88\u7b80\u6d01\u3002\u53ef\u662f\u771f\u7684\u5f88\u8ba9\u4eba\u6478\u4e0d\u5230\u5934\u8111\uff0c\u5b83\u8fd9\u6837\u8ba1\u7b97\u7684\u4f9d\u636e\u5230\u5e95\u662f\u4ec0\u4e48\uff1f \u597d\uff0c\u5148\u628a\u8fd9\u4e2a\u653e\u4e00\u8fb9\uff0c\u6211\u4eec\u81ea\u5df1\u6765\u63a8\u5bfc\u601d\u8def\uff0c\u73b0\u5728\u8981\u59cb\u7ec8\u8bb0\u4f4f\u4e00\u70b9\uff0c next[j] \u7684\u503c\uff08\u4e5f\u5c31\u662f k \uff09\u8868\u793a\uff0c\u5f53 P[j] != T[i] \u65f6\uff0c j \u6307\u9488\u7684\u4e0b\u4e00\u6b65\u79fb\u52a8\u4f4d\u7f6e\u3002 \u5148\u6765\u770b\u7b2c\u4e00\u4e2a\uff1a\u5f53 j \u4e3a0\u65f6\uff0c\u5982\u679c\u8fd9\u65f6\u5019\u4e0d\u5339\u914d\uff0c\u600e\u4e48\u529e\uff1f \u50cf\u4e0a\u56fe\u8fd9\u79cd\u60c5\u51b5\uff0c j \u5df2\u7ecf\u5728\u6700\u5de6\u8fb9\u4e86\uff0c\u4e0d\u53ef\u80fd\u518d\u79fb\u52a8\u4e86\uff0c\u8fd9\u65f6\u5019\u8981\u5e94\u8be5\u662f i \u6307\u9488\u540e\u79fb\u3002\u6240\u4ee5\u5728\u4ee3\u7801\u4e2d\u624d\u4f1a\u6709 next[0] = -1; \u8fd9\u4e2a\u521d\u59cb\u5316\u3002 NOTE: \u770b\u4e86\u4e0b\u9762\u7684\u5b8c\u6574\u7684\u4ee3\u7801\u5c31\u77e5\u9053\u4e3a\u4ec0\u4e48\u4f7f\u7528 -1 \u6765\u4f5c\u4e3a\u521d\u59cb\u503c\uff0c\u56e0\u4e3a i++ \u548c j++ \u662f\u5728\u76f8\u540c\u7684\u5206\u652f\u4e2d\uff0c j++ \u540e j \u4e3a0\uff0c\u8fd9\u5c31\u4fdd\u8bc1\u4e86\u4eceP\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u5f00\u59cb\u5339\u914d\u3002 \u5982\u679c\u662f\u5f53 j \u4e3a1\u7684\u65f6\u5019\u5462\uff1f \u663e\u7136\uff0c j \u6307\u9488\u4e00\u5b9a\u662f\u540e\u79fb\u52300\u4f4d\u7f6e\u7684\u3002\u56e0\u4e3a\u5b83\u524d\u9762\u4e5f\u5c31\u53ea\u6709\u8fd9\u4e00\u4e2a\u4f4d\u7f6e\u4e86\u3002 \u4e0b\u9762\u8fd9\u4e2a\u662f\u6700\u91cd\u8981\u7684\uff0c\u8bf7\u770b\u5982\u4e0b\u56fe\uff1a \u8bf7\u4ed4\u7ec6\u5bf9\u6bd4\u8fd9\u4e24\u4e2a\u56fe\u3002 \u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u89c4\u5f8b\uff1a","title":"\u6c42\u89e3next\u6570\u7ec4"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#pk#pj","text":"\u5f53 P[k] == P[j] \u65f6\uff0c\u6709 next[j+1] == next[j] + 1 \u5176\u5b9e\u8fd9\u4e2a\u662f\u53ef\u4ee5\u8bc1\u660e\u7684\uff1a \u56e0\u4e3a\u5728 P[j] \u4e4b\u524d\u5df2\u7ecf\u6709 P[0 ~ k-1] == p[j-k ~ j-1] \u3002\uff08 next[j] == k \uff09 \u8fd9\u65f6\u5019\u73b0\u6709 P[k] == P[j] \uff0c\u6211\u4eec\u662f\u4e0d\u662f\u53ef\u4ee5\u5f97\u5230 P[0 ~ k-1] + P[k] == p[j-k ~ j-1] + P[j] \u3002 \u5373\uff1a P[0 ~ k] == P[j-k ~ j] \uff0c\u5373 next[j+1] == k + 1 == next[j] + 1 \u3002 \u8fd9\u91cc\u7684\u516c\u5f0f\u4e0d\u662f\u5f88\u597d\u61c2\uff0c\u8fd8\u662f\u770b\u56fe\u4f1a\u5bb9\u6613\u7406\u89e3\u4e9b\u3002","title":"\u5f53P[k] == P[j]\u65f6"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#pk#pj_1","text":"\u5f53 P[k] != P[j] \u65f6\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u50cf\u8fd9\u79cd\u60c5\u51b5\uff0c\u5982\u679c\u4f60\u4ece\u4ee3\u7801\u4e0a\u770b\u5e94\u8be5\u662f\u8fd9\u4e00\u53e5\uff1a k = next[k]; \u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u5b50\uff1f\u4f60\u770b\u4e0b\u9762\u5e94\u8be5\u5c31\u660e\u767d\u4e86\u3002 \u73b0\u5728\u4f60\u5e94\u8be5\u77e5\u9053\u4e3a\u4ec0\u4e48\u8981 k = next[k] \u4e86\u5427\uff01\u50cf\u4e0a\u8fb9\u7684\u4f8b\u5b50\uff0c\u6211\u4eec\u5df2\u7ecf\u4e0d\u53ef\u80fd\u627e\u5230 [ A\uff0cB\uff0cA\uff0cB ] \u8fd9\u4e2a\u6700\u957f\u7684\u540e\u7f00\u4e32\u4e86\uff0c\u4f46\u6211\u4eec\u8fd8\u662f\u53ef\u80fd\u627e\u5230 [ A\uff0cB ] \u3001 [ B ] \u8fd9\u6837\u7684\u524d\u7f00\u4e32\u7684\u3002\u6240\u4ee5\u8fd9\u4e2a\u8fc7\u7a0b\u50cf\u4e0d\u50cf\u5728\u5b9a\u4f4d [ A\uff0cB\uff0cA\uff0cC ] \u8fd9\u4e2a\u4e32\uff0c\u5f53 C \u548c\u4e3b\u4e32\u4e0d\u4e00\u6837\u4e86\uff08\u4e5f\u5c31\u662f k \u4f4d\u7f6e\u4e0d\u4e00\u6837\u4e86\uff09\uff0c\u90a3\u5f53\u7136\u662f\u628a\u6307\u9488\u79fb\u52a8\u5230 next[k] \u5566\u3002 NOTE: \u8fd9\u7bc7\u6587\u7ae0\u8fd9\u91cc\u7684\u5206\u6790\u8fd8\u662f\u6bd4\u8f83\u96be\u4ee5\u7406\u89e3\u7684\uff0c\u4e0b\u4e00\u7bc7\u5728\u5206\u6790\u66f4\u52a0\u900f\u5f7b\u3002 NOTE: \u6784\u5efa next \u6570\u7ec4\u7684\u7b97\u6cd5\u662f\u4f7f\u7528\u7684\u6570\u5b66\u5f52\u7eb3\u6cd5\u6765\u6c42\u89e3next\u6570\u7ec4\u7684\u6bcf\u4e2a\u503c\uff0c\u5373\u6839\u636e next \u6570\u7ec4\u4e2d\u524d j \u4e2a\u5143\u7d20\u7684\u503c\u6765\u6c42\u89e3 next[j+1] \u7684\u503c\u3002 \u6709\u4e86 next \u6570\u7ec4\u4e4b\u540e\u5c31\u4e00\u5207\u597d\u529e\u4e86\uff0c\u6211\u4eec\u53ef\u4ee5\u52a8\u624b\u5199KMP\u7b97\u6cd5\u4e86\uff1a public static int KMP ( String ts , String ps ) { char [] t = ts . toCharArray (); char [] p = ps . toCharArray (); int i = 0 ; // \u4e3b\u4e32\u7684\u4f4d\u7f6e int j = 0 ; // \u6a21\u5f0f\u4e32\u7684\u4f4d\u7f6e int [] next = getNext ( ps ); while ( i < t . length && j < p . length ) { if ( j == - 1 || t [ i ] == p [ j ] ) { // \u5f53j\u4e3a-1\u65f6\uff0c\u8981\u79fb\u52a8\u7684\u662fi\uff0c\u5f53\u7136j\u4e5f\u8981\u5f520 i ++ ; j ++ ; } else { // i\u4e0d\u9700\u8981\u56de\u6eaf\u4e86 // i = i - j + 1; j = next [ j ] ; // j\u56de\u5230\u6307\u5b9a\u4f4d\u7f6e } } if ( j == p . length ) { return i - j ; } else { return - 1 ; } } \u548c\u66b4\u529b\u7834\u89e3\u76f8\u6bd4\uff0c\u5c31\u6539\u52a8\u4e864\u4e2a\u5730\u65b9\u3002\u5176\u4e2d\u6700\u4e3b\u8981\u7684\u4e00\u70b9\u5c31\u662f\uff0c i \u4e0d\u9700\u8981\u56de\u6eaf\u4e86\u3002 \u6700\u540e\uff0c\u6765\u770b\u4e00\u4e0b\u4e0a\u8fb9\u7684\u7b97\u6cd5\u5b58\u5728\u7684\u7f3a\u9677\u3002\u6765\u770b\u7b2c\u4e00\u4e2a\u4f8b\u5b50\uff1a \u663e\u7136\uff0c\u5f53\u6211\u4eec\u4e0a\u8fb9\u7684\u7b97\u6cd5\u5f97\u5230\u7684 next \u6570\u7ec4\u5e94\u8be5\u662f [ -1\uff0c0\uff0c0\uff0c1 ] \u6240\u4ee5\u4e0b\u4e00\u6b65\u6211\u4eec\u5e94\u8be5\u662f\u628a j \u79fb\u52a8\u5230\u7b2c1\u4e2a\u5143\u7d20\u54af\uff1a \u4e0d\u96be\u53d1\u73b0\uff0c \u8fd9\u4e00\u6b65\u662f\u5b8c\u5168\u6ca1\u6709\u610f\u4e49\u7684\u3002\u56e0\u4e3a\u540e\u9762\u7684 B \u5df2\u7ecf\u4e0d\u5339\u914d\u4e86\uff0c\u90a3\u524d\u9762\u7684 B \u4e5f\u4e00\u5b9a\u662f\u4e0d\u5339\u914d\u7684 \uff0c\u540c\u6837\u7684\u60c5\u51b5\u5176\u5b9e\u8fd8\u53d1\u751f\u5728\u7b2c2\u4e2a\u5143\u7d20 A \u4e0a\u3002 \u663e\u7136\uff0c \u53d1\u751f\u95ee\u9898\u7684\u539f\u56e0\u5728\u4e8e P[j] == P[next[j]] \u3002 \u6240\u4ee5\u6211\u4eec\u4e5f\u53ea\u9700\u8981\u6dfb\u52a0\u4e00\u4e2a\u5224\u65ad\u6761\u4ef6\u5373\u53ef\uff1a public static int [] getNext ( String ps ) { char [] p = ps . toCharArray (); int [] next = new int [ p . length ] ; next [ 0 ] = - 1 ; int j = 0 ; int k = - 1 ; while ( j < p . length - 1 ) { if ( k == - 1 || p [ j ] == p [ k ] ) { if ( p [++ j ] == p [++ k ] ) { // \u5f53\u4e24\u4e2a\u5b57\u7b26\u76f8\u7b49\u65f6\u8981\u8df3\u8fc7 next [ j ] = next [ k ] ; } else { next [ j ] = k ; } } else { k = next [ k ] ; } } return next ; } \u8be5\u7b97\u6cd5\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8e\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684 next[j] \u7684\u503c k \u5c31\u662f j \u4f4d\u4e4b\u524d\u7684\u5b50\u4e32\u4e2d\uff0c\u524d\u7f00\u96c6\u548c\u540e\u7f00\u96c6\u4e2d\u7684\u6700\u5927\u91cd\u590d\u5b50\u4e32\u7684\u957f\u5ea6\u3002 abacabac next[0]=-1;j=0;k=-1 \u6761\u4ef61 \u6761\u4ef62 \u5206\u652f2 \u5206\u652f1 k==-1 next[1]=0;j=1;k=0 p[1]!=p[0]; k=next[0]=-1 k==-1 next[2]=0;j=2;k=0 p[2]==p[0]; next[3]=1;j=3;k=1 p[3]!=p[1]; k=next[1]=0 p[3]!=p[0]; k=next[0]=-1 k==-1; next[4]=0;j=4;k=0 p[4]==p[0] next[5]=1;j=5;k=1 p[5]==p[1] next[6]=2;j=6;k=2 p[6]==p[2] next[7]=3;j=7;k=3 p[7]==p[3] next[8]=4;j=8;k=4 \u8981\u60f3\u5f97\u5230 p[j+1] \uff0c\u53ea\u9700\u8981\u6bd4\u8f83 p[j] \u548c p[k] \u5373\u53ef\uff1b","title":"\u5f53P[k] != P[j]\u65f6,"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#3#computing#the#kmp#failure#function#fk","text":"","title":"3. Computing the KMP failure function (f(k))"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#definition#of#fk","text":"f(k) = MaxOverlap ( \"p0 p1 ... pk\" ) where: \"p0 p1 ... pk\" = the prefix of length k+1 of pattern P Graphically:","title":"definition of f(k)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#naive#way#to#find#fk","text":"Given P = \"p0 p1 ... pm-1\" Given k = 1, 2, ..., m-1 (k = 0 ==> f(0) = 0) 1. Extract the sub-pattern: \"p0 p1 ... pk\" 2. Find the first (= largest) overlap: Try: (p0) p1 p2 ... pk-1 p0 p1 ... pk-1 pk If (no match) Try: (p0) p1 p2 ... pk-1 p0 p1 ... pk-1 pk And so on... The first overlap is the longest ! NOTE: \u4e0a\u8ff0\u7b97\u6cd5\u662f\u4e00\u4e2a\u5faa\u73af\u7b97\u6cd5\uff0c\u5373 for k in range(1, m) \uff0c\u4e0b\u9762\u662f\u4e0a\u8ff0\u7b97\u6cd5\u7684python\u5b9e\u73b0\uff1a def build_failure_table ( p ): \"\"\" \u6784\u5efa\u5b57\u7b26\u4e32p\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u6570\u7ec4 :param p: :return: \"\"\" failure_table = list () len_of_p = len ( p ) for len_of_sub_str in range ( 1 , len_of_p + 1 ): max_len_of_overlap = int ( len_of_sub_str / 2 ) # \u6700\u5927\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6 print ( \"\u5b50\u4e32\u957f\u5ea6: {} ,\u6700\u5927\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u957f\u5ea6: {} \" . format ( len_of_sub_str , max_len_of_overlap )) if max_len_of_overlap == 0 : # \u957f\u5ea6\u4e3a1\u7684\u4e32\uff0c\u662f\u6ca1\u6709\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684 failure_table . append ( 0 ) else : found = False # \u662f\u5426\u627e\u5230\u91cd\u53e0\u524d\u7f00\u540e\u7f00 for len_of_overlap in range ( max_len_of_overlap , 0 , - 1 ): print ( \"\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u957f\u5ea6: {} \" . format ( len_of_overlap )) # len_of_overlap \u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6 for prefix_index in range ( len_of_overlap ): suffix_index = prefix_index + ( len_of_sub_str - len_of_overlap ) print ( \"\u524d\u7f00\u8d77\u59cb\u4f4d\u7f6e: {} ,\u540e\u7f00\u8d77\u59cb\u4f4d\u7f6e: {} \" . format ( prefix_index , suffix_index )) if p [ prefix_index ] == p [ suffix_index ]: if suffix_index == len_of_sub_str - 1 : # \u627e\u5230\u4e86\u91cd\u53e0\u90e8\u5206 failure_table . append ( len_of_overlap ) found = True break else : break if found : break if not found : failure_table . append ( 0 ) return failure_table","title":"Naive way to find f(k):"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#relating#fk#to#fk1","text":"The values f(k) are computed easily using existing prefix overlap information : f(0) = 0 ( f(0) is always 0) f(1) is computing using (already computed) value f(0) f(2) is computing using (already computed) value f(0) , f(1) f(3) is computing using (already computed) value f(0) , f(1) , f(2) And so on According to the definition of f(k) : NOTE: \u4e0a\u9762\u8fd9\u79cd\u8868\u793a\u95ee\u9898\u7684\u65b9\u5f0f\u662f\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u5373\u5728\u539f\u95ee\u9898\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u65b0\u5143\u7d20\u4ece\u800c\u6784\u6210\u4e86\u4e00\u4e2a\u89c4\u6a21\u66f4\u5927\u7684\u95ee\u9898\u3002 Suppose that we know that: f(k\u22121) = x In other words: the longest overlapping suffix and prefix in \" p0 p1 ... pk-1 \" has x characters: f(k-1) = x characters <-----------------------> p1 p2 p3 ... pk-x-2 pk-x-3 pk-x-4 .... pk-1 ^ ^ ^ ^ | | | equal | v v v v p0 p1 p2 .... px-1 px ... pk-1 question: Can we use the fact that f(k\u22121) = x to compute f(k) ? answer: Yes, because f(k) is computed using a similar prefix as f(k\u22121): prefix used to compute f(k-1) +--------------------------------+ | | p0 p1 p2 .... px-1 ... pk-1 pk | | +------------------------------------+ prefix used to compute f(k) We will next learn how to exploit the similarity to compute f(k)","title":"Relating f(k) to f(k\u22121)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#fact#between#fk#and#fk1","text":"Fact: f(k) \u2264 f(k\u22121) + 1","title":"Fact between f(k) and f(k\u22121)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#computation#trick#1","text":"Let use denote: f(k\u22121) = x (Note: f(k\u22121) is equal to some value. The above assumption simply gave a more convenient notation for this value). If px == pk , then: f(k) = x+1 (i.e., the maximum overlap of the prefix p0 p1 p2 .... pk-1 pk has x+1 characters Proof: These x+1 characters match IF pk == px! <----------------------------> p1 p2 p3 ... pk-x-2 pk-x-3 pk-x-4 .... pk-1 pk ^ ^ ^ ^ ^ | | | equal | |equal v v v v v p0 p1 p2 .... px-1 px ... pk-1 pk | | +--------------------------+ These characters matches because f(k-1) = x","title":"Computation trick 1"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#prelude#to#computation#trick#2","text":"Consider the prefix ababyabab where f(8) = 4: 012345678 prefix = ababyabab f(8) = 4 because: ababyabab ababyabab <--> 4 characters overlap We want to compute f(9) using f(8) , but now the next character does not match(that is the next char is not equal to y): 0123456789 prefix = ababyababa ababyababa ababyababa Conclusion: *** We CANNOT use f(8) to compute f(9) *** question: What should we try next to find the maximum overlap for the prefix \"ababyababa\" answer: To find the maximum overlap, we must slide the prefix down and look for matching letters !!! NOTE: \u601d\u8def\u662f\u4f7f\u7528\u5df2\u7ecf\u5339\u914d\u7684\u5b57\u7b26\u4e32\u6765\u5c3d\u53ef\u80fd\u51cf\u5c11\u5339\u914d\u6b21\u6570\u5e76\u4e14\u5bfb\u627e\u7b2c\u4e00\u4e2a\u6700\u53ef\u80fd\u7684\u4f4d\u7f6e\u3002 Now, let us use only the matching prefix information: ababyababa ababyababa Look only at these characters: ?????abab? abab?????? We can know for sure that the overlap cannot be found starting at these positions: ?????abab? abab?????? NOTE: \u56e0\u4e3a\u6211\u4eec\u77e5\u9053\u4e32 abab \u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u662f2\uff0c\u5373 f(3) \uff0c\u6240\u4ee5\u5b83\u7684\u524d\u4e24\u4e2a\u5143\u7d20\u53ef\u4ee5\u5339\u914d\u4e0a\u7684\uff0c\u6240\u4ee5\u7b2c\u4e00\u4e2a\u53ef\u80fd\u4f4d\u7f6e\u662f\u5982\u4e0b\u56fe\u6240\u793a\u7684\uff0c\u8fd9\u5c31\u662f\u5bf9\u5df2\u7ecf\u5339\u914d\u4fe1\u606f\u7684\u5145\u5206\u8fd0\u7528\u3002\u81f3\u4e8e\u7b2c\u4e09\u4e2a\u5143\u7d20\u662f\u5426\u80fd\u591f\u5339\u914d\u4e0a\uff0c\u5c31\u8981\u6bd4\u8f83\u7684\u7ed3\u679c\u4e86\u3002 The first possible way that overlap can be found is starting here: ?????abab? abab?????? In other words: we can compute f(9) using f(3) : 0123 prefix = abab abab abab f(3) = 2 Notice that: 3 = 4\u22121 and f(8) = 4 Worked out further: 0123456789 prefix = ababyababa ababyababa ababyababa ^ | compare the character at position 2 (f(3) = 2) Note: The prefix abab is hightlighted in yellow Because the characters are equal, we have found the maximum overlap: f(9) = f(3) + 1 = 2 + 1 = 3 !!! NOTE: \u8fd9\u91cc\u53ef\u4ee5\u5047\u8bbe\uff0c\u5982\u679c p[3] \u548c p[9] \u5e76\u4e0d\u76f8\u7b49\uff0c\u5219\u4e0a\u8ff0\u6d41\u7a0b\u9700\u8981\u7ee7\u7eed\u4e0b\u53bb\uff0c\u81f3\u4e8e\u7ec8\u6b62\u6761\u4ef6\uff0c\u663e\u7136\u662f\u76f4\u81f3\u6bd4\u8f83\u5230\u7b2c\u4e00\u4e2a\u5143\u7d20\u90fd\u4e0d\u76f8\u7b49\u3002","title":"Prelude to computation trick 2"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#computation#trick#2","text":"Let: f(k\u22121) = x (Note: f(k\u22121) is equal to some value. The above assumption simply gave a more convenient notation for this value). If px \u2260 pk , then: The next prefix that can be used to compute f(k) is: p0 p1 .... px-1 In pseudo code: i = k-1; // Try to use f(k-1) to compute f(k) x = f(i); // x = character position to match against pk if ( P[k] == P[x] ) then f(k) = f(x\u22121) + 1 else Use: p0 p1 .... px-1 to compute f(k) What that means in terms of program statements: i = x-1; // Try to use f(x-1) to compute f(k) x = f(i); // x = character position to match against pk Note: We must repeat trick #2 as long as i \u2265 0, In other words: use a while loop instead of an if statement !","title":"Computation trick #2"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#algorithm#to#compute#kmp#failure#function","text":"Java code: public static int [] KMP_failure_function ( String P ) { int k , i , x , m ; int f [] = new int [ P . length () ] ; m = P . length (); f [ 0 ] = 0 ; // f(0) is always 0 for ( k = 1 ; k < m ; k ++ ) { // Compute f[k] i = k - 1 ; // First try to use f(k-1) to compute f(k) x = f [ i ] ; while ( P . charAt ( x ) != P . charAt ( k ) ) { i = x - 1 ; // Try the next candidate f(.) to compute f(k) if ( i < 0 ) // Make sure x is valid break ; // STOP the search !!! x = f [ i ] ; } if ( i < 0 ) f [ k ] = 0 ; // No overlap at all: max overlap = 0 characters else f [ k ] = f [ i ] + 1 ; // We can compute f(k) using f(i) } return ( f ); } \u5b8c\u6574\u6d4b\u8bd5\u7a0b\u5e8f /* ---------------------------------- My own KMP Failure function alg S.Y. Cheung - 3/3/2013 ---------------------------------- */ import java.util.* ; public class ComputeF { public static int [] KMP_failure_function ( String P ) { int k , i , x , m ; int f [] = new int [ P . length () ] ; String s ; m = P . length (); f [ 0 ] = 0 ; for ( k = 1 ; k < m ; k ++ ) { // Compute f[k] s = P . substring ( 0 , k + 1 ); System . out . println ( \"-----------------------------------------------\" ); System . out . println ( \"Prefix = \" + s + \" --- Computing f(\" + k + \"):\" ); i = k - 1 ; // First try to use f(k-1) to compute f(k) x = f [ i ] ; System . out . println ( \"===================================\" ); System . out . println ( \"Try using: f(\" + i + \") = \" + x ); printState ( s , s , k , x ); while ( P . charAt ( x ) != P . charAt ( k ) ) { i = f [ i ]- 1 ; // Try the next candidate f(.) to compute f(k) if ( i < 0 ) // Search ended in failure.... break ; x = f [ i ] ; System . out . println ( \"===================================\" ); System . out . println ( \"Try using: f(\" + i + \") = \" + x ); printState ( s , s , k , x ); } if ( i < 0 ) { System . out . println ( \"No overlap possible... --> f[\" + k + \"] = 0\" ); f [ k ] = 0 ; // No overlap possible } else { f [ k ] = f [ i ] + 1 ; // Compute f(k) using f(i) System . out . println ( \"Overlap found ... --> f[\" + k + \"] = \" + f [ k ] ); } } return ( f ); } public static void main ( String [] args ) { String P ; Scanner in ; int [] f ; in = new Scanner ( System . in ); System . out . print ( \"P = \" ); P = in . nextLine (); System . out . println (); f = KMP_failure_function ( P ); for ( int i = 0 ; i < P . length (); i ++ ) { System . out . println ( \"f(\" + i + \") = \" + f [ i ] ); } System . out . println (); } /* ===================================================== Variables and Methods to make the algorithm visual ===================================================== */ public static String T_ruler , P_ruler ; public static String ruler ( int n ) { String out = \"\" ; char x = '0' ; for ( int i = 0 ; i < n ; i ++ ) { out = out + x ; x ++ ; if ( x > '9' ) x = '0' ; } return out ; } public static void printState ( String T , String P , int i , int j ) { T_ruler = ruler ( T . length () ); P_ruler = ruler ( P . length () ); System . out . println ( \"=====================================\" ); System . out . println ( \"Matching: i = \" + i + \", j = \" + j ); System . out . println ( \" \" + T_ruler ); System . out . println ( \" \" + T ); System . out . print ( \" \" ); for ( int k = 0 ; k < i - j ; k ++ ) System . out . print ( \" \" ); System . out . println ( P ); System . out . print ( \" \" ); for ( int k = 0 ; k < i - j ; k ++ ) System . out . print ( \" \" ); System . out . println ( P_ruler ); System . out . print ( \" \" ); for ( int k = 0 ; k < i ; k ++ ) System . out . print ( \" \" ); System . out . println ( \"^\" ); System . out . print ( \" \" ); for ( int k = 0 ; k < i ; k ++ ) System . out . print ( \" \" ); System . out . println ( \"|\" ); System . out . println (); } }","title":"Algorithm to compute KMP failure function"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#kmp","text":"\u901a\u8fc7\u4e0a\u8ff0\u4e09\u7bc7\u6587\u7ae0\uff0c\u80fd\u591f\u77e5\u9053KMP\u7b97\u6cd5\u7684\u539f\u7406\uff0c\u73b0\u5728\u9700\u8981\u8003\u8651\u7684\u662f\u5982\u4f55\u6765\u8fdb\u884c\u5b9e\u73b0\u3002","title":"KMP\u5b9e\u73b0\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#kmp#failure#function","text":"\u5f53 pattern[j] \u4e0e pattern[f[j-1]] \u4e0d\u76f8\u7b49\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u9012\u5f52\u516c\u5f0f\u4e2d\u6d89\u53ca\u5230\u4e86\u4e0d\u65ad\u5730\u5faa\u73af\u9012\u5f52\uff0c\u4f7f\u7528\u6570\u5b66\u516c\u5f0f\u4e0d\u65b9\u4fbf\u63cf\u8ff0\uff0c\u4e0b\u9762\u7684python\u7a0b\u5e8f\u662f\u975e\u5e38\u7b80\u6d01\u6613\u61c2\u7684\uff0c\u5e76\u4e14\u662f\u975e\u5e38\u63a5\u8fd1\u6570\u5b66\u516c\u5f0f\u7684\uff0c\u6240\u4ee5\u8fd9\u91cc\u5c31\u7701\u7565\u6389\u9012\u5f52\u516c\u5f0f\u3002","title":"\u8ba1\u7b97KMP failure function\u7684\u9012\u5f52\u516c\u5f0f"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#kmp#failure#functionpython","text":"failure function f(j) \u8868\u793a\u7684\u662f\u4ece pattern[0] \u5230 pattern[j] \u7684\u5e8f\u5217\uff08\u663e\u7136\u8fd9\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\u662f j+1 \uff09\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684**\u957f\u5ea6**\uff0c\u5373 f(j) \u6240\u8868\u793a\u7684\u662f\u957f\u5ea6\u4e3a j+1 \u7684\u5e8f\u5217\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u3002\u663e\u7136 f[0]==0 \uff0c\u56e0\u4e3a\u957f\u5ea6\u4e3a1\u7684\u5e8f\u5217\u7684\u6700\u957f\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u4e3a0\u3002\u6240\u4ee5\uff0c\u5f53\u5df2\u77e5\u5e8f\u5217\u7684\u957f\u5ea6\u4e3a i \uff0c\u6765\u67e5\u8be2\u5176\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u7684\u662f f(i-1) \u3002\u56e0\u4e3a i \u8868\u793a\u7684\u662f\u957f\u5ea6\uff0c\u6240\u4ee5 pattern[i] \u5f15\u7528\u7684\u662f\u6570\u7ec4\u7684\u7b2c i+1 \u4e2a\u5143\u7d20\u3002 def get_failure_array ( pattern ): failure = [ 0 ] # \u521d\u59cb\u6761\u4ef6 i = 0 # f(j-1)\u7684\u503c\uff0c\u662f\u5df2\u77e5\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u7684\u542b\u4e49\u662f\u957f\u5ea6 j = 1 # f(j)\u662f\u672a\u77e5\u7684\uff0cj\u8868\u793a\u7684\u662findex while j < len ( pattern ): if pattern [ i ] == pattern [ j ]: i += 1 elif i > 0 : i = failure [ i - 1 ] continue j += 1 failure . append ( i ) return failure","title":"\u8ba1\u7b97KMP failure function\u7684python\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#kmp#failure#function#dynamic#programming","text":"KMP\u7684failure function\u7684\u6c42\u89e3\u8fc7\u7a0b\u5728\u8ba1\u7b97 f(k+1) \u7684\u65f6\u6240\u4f9d\u8d56\u7684 f(0),f(1)...,f(k) \u90fd\u662f\u901a\u8fc7\u67e5failure table\u800c\u83b7\u5f97\u7684\uff0c\u800c\u4e0d\u662f\u91cd\u65b0\u8ba1\u7b97\uff0c\u8fd9\u5176\u5b9e\u5c31\u662f\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684\u601d\u60f3\u3002\u5728\u4e0a\u8ff0\u4ee3\u7801\u4e2d\uff0c i \u5c31\u8868\u793a\u8ba1\u7b97 f(k+1) \u6240\u4f9d\u8d56\u7684\u6570\u636e\uff0c\u5b83\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8e\u8fed\u4ee3\u7248\u7684\u6590\u6ce2\u90a3\u5951\u6570\u5217\u3002","title":"\u8ba1\u7b97KMP failure function \u548c dynamic programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#kmp_1","text":"def kmp_search ( pattern , text ): \"\"\" :param pattern: :param text: :return: \"\"\" # 1) Construct the failure array failure = get_failure_array ( pattern ) # 2) Step through text searching for pattern i , j = 0 , 0 # index into text, pattern while i < len ( text ): if pattern [ j ] == text [ i ]: if j == ( len ( pattern ) - 1 ): return True j += 1 elif j > 0 : # if this is a prefix in our pattern # just go back far enough to continue j = failure [ j - 1 ] continue i += 1 return False \u601d\u8003\uff1a\u4e3a\u4ec0\u4e48 j = failure[j - 1] \uff1f\u5176\u5b9e\u7ed3\u5408\u524d\u9762\u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u77e5\u9053\u4e86\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002","title":"KMP\u7684\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/#kmp#in#leetcode","text":"http://www.voidcn.com/article/p-uuefgkai-bnw.html https://leetcode-cn.com/problems/implement-strstr/comments/ https://leetcode.com/problems/shortest-palindrome/discuss/60113/clean-kmp-solution-with-super-detailed-explanation","title":"KMP in leetcode"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/","text":"KMP Algorithm for Pattern Searching KMP (Knuth Morris Pratt) Pattern Searching Preprocessing Overview: Searching Algorithm: KMP Algorithm for Pattern Searching KMP (Knuth Morris Pratt) Pattern Searching The Naive pattern searching algorithm doesn\u2019t work well in cases where we see many matching characters followed by a mismatching character. Following are some examples. txt[] = \"AAAAAAAAAAAAAAAAAB\" pat[] = \"AAAAB\" txt[] = \"ABABABCABABABCABABABC\" pat[] = \"ABABAC\" (not a worst case, but a bad case for Naive) The KMP matching algorithm uses degenerating property (pattern having same sub-patterns appearing more than once in the pattern) of the pattern and improves the worst case complexity to O(n) . The basic idea behind KMP\u2019s algorithm is: whenever we detect a mismatch (after some matches), we already know some of the characters in the text of the next window. We take advantage of this information to avoid matching the characters that we know will anyway match. Let us consider below example to understand this. Matching Overview txt = \"AAAAABAAABA\" pat = \"AAAA\" We compare first window of txt with pat txt = \"AAAAABAAABA\" pat = \"AAAA\" [Initial position] We find a match. This is same as Naive String Matching. In the next step, we compare next window of txt with pat. txt = \"AAAAABAAABA\" pat = \"AAAA\" [Pattern shifted one position] This is where KMP does optimization over Naive. In this second window, we only compare fourth A of pattern with fourth character of current window of text to decide whether current window matches or not. Since we know first three characters will anyway match, we skipped matching first three characters. Need of Preprocessing? An important question arises from the above explanation, how to know how many characters to be skipped. To know this, we pre-process pattern and prepare an integer array lps[] that tells us the count of characters to be skipped. Preprocessing Overview: KMP algorithm preprocesses pat[] and constructs an auxiliary lps[] of size m (same as size of pattern) which is used to skip characters while matching. name lps indicates longest proper prefix which is also suffix. . A proper prefix is prefix with whole string not allowed. For example, prefixes of \u201cABC\u201d are \u201c\u201d, \u201cA\u201d, \u201cAB\u201d and \u201cABC\u201d. Proper prefixes are \u201c\u201d, \u201cA\u201d and \u201cAB\u201d. Suffixes of the string are \u201c\u201d, \u201cC\u201d, \u201cBC\u201d and \u201cABC\u201d. We search for lps in sub-patterns. More clearly we focus on sub-strings of patterns that are either prefix and suffix. For each sub-pattern pat[0..i] where i = 0 to m-1 , lps[i] stores length of the maximum matching proper prefix which is also a suffix of the sub-pattern pat[0..i] . lps[i] = the longest proper prefix of pat[0..i] which is also a suffix of pat[0..i]. Note : lps[i] could also be defined as longest prefix which is also proper suffix. We need to use properly at one place to make sure that the whole substring is not considered. Searching Algorithm: Unlike Naive algorithm , where we slide the pattern by one and compare all characters at each shift, we use a value from lps[] to decide the next characters to be matched. The idea is to not match a character that we know will anyway match. How to use lps[] to decide next positions (or to know a number of characters to be skipped)? We start comparison of pat[j] with j = 0 with characters of current window of text. We keep matching characters txt[i] and pat[j] and keep incrementing i and j while pat[j] and txt[i] keep matching . When we see a mismatch We know that characters pat[0..j-1] match with txt[i-j\u2026i-1] (Note that j starts with 0 and increment it only when there is a match). We also know (from above definition) that lps[j-1] is count of characters of pat[0\u2026j-1] that are both proper prefix and suffix. From above two points, we can conclude that we do not need to match these lps[j-1] characters with txt[i-j\u2026i-1] because we know that these characters will anyway match. Let us consider above example to understand this.","title":"geeksforgeeks KMP Algorithm for Pattern Searching"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/#kmp#algorithm#for#pattern#searching","text":"","title":"KMP Algorithm for Pattern Searching"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/#kmp#knuth#morris#pratt#pattern#searching","text":"The Naive pattern searching algorithm doesn\u2019t work well in cases where we see many matching characters followed by a mismatching character. Following are some examples. txt[] = \"AAAAAAAAAAAAAAAAAB\" pat[] = \"AAAAB\" txt[] = \"ABABABCABABABCABABABC\" pat[] = \"ABABAC\" (not a worst case, but a bad case for Naive) The KMP matching algorithm uses degenerating property (pattern having same sub-patterns appearing more than once in the pattern) of the pattern and improves the worst case complexity to O(n) . The basic idea behind KMP\u2019s algorithm is: whenever we detect a mismatch (after some matches), we already know some of the characters in the text of the next window. We take advantage of this information to avoid matching the characters that we know will anyway match. Let us consider below example to understand this. Matching Overview txt = \"AAAAABAAABA\" pat = \"AAAA\" We compare first window of txt with pat txt = \"AAAAABAAABA\" pat = \"AAAA\" [Initial position] We find a match. This is same as Naive String Matching. In the next step, we compare next window of txt with pat. txt = \"AAAAABAAABA\" pat = \"AAAA\" [Pattern shifted one position] This is where KMP does optimization over Naive. In this second window, we only compare fourth A of pattern with fourth character of current window of text to decide whether current window matches or not. Since we know first three characters will anyway match, we skipped matching first three characters. Need of Preprocessing? An important question arises from the above explanation, how to know how many characters to be skipped. To know this, we pre-process pattern and prepare an integer array lps[] that tells us the count of characters to be skipped.","title":"KMP (Knuth Morris Pratt) Pattern Searching"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/#preprocessing#overview","text":"KMP algorithm preprocesses pat[] and constructs an auxiliary lps[] of size m (same as size of pattern) which is used to skip characters while matching. name lps indicates longest proper prefix which is also suffix. . A proper prefix is prefix with whole string not allowed. For example, prefixes of \u201cABC\u201d are \u201c\u201d, \u201cA\u201d, \u201cAB\u201d and \u201cABC\u201d. Proper prefixes are \u201c\u201d, \u201cA\u201d and \u201cAB\u201d. Suffixes of the string are \u201c\u201d, \u201cC\u201d, \u201cBC\u201d and \u201cABC\u201d. We search for lps in sub-patterns. More clearly we focus on sub-strings of patterns that are either prefix and suffix. For each sub-pattern pat[0..i] where i = 0 to m-1 , lps[i] stores length of the maximum matching proper prefix which is also a suffix of the sub-pattern pat[0..i] . lps[i] = the longest proper prefix of pat[0..i] which is also a suffix of pat[0..i]. Note : lps[i] could also be defined as longest prefix which is also proper suffix. We need to use properly at one place to make sure that the whole substring is not considered.","title":"Preprocessing Overview:"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Knuth%E2%80%93Morris%E2%80%93Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/#searching#algorithm","text":"Unlike Naive algorithm , where we slide the pattern by one and compare all characters at each shift, we use a value from lps[] to decide the next characters to be matched. The idea is to not match a character that we know will anyway match. How to use lps[] to decide next positions (or to know a number of characters to be skipped)? We start comparison of pat[j] with j = 0 with characters of current window of text. We keep matching characters txt[i] and pat[j] and keep incrementing i and j while pat[j] and txt[i] keep matching . When we see a mismatch We know that characters pat[0..j-1] match with txt[i-j\u2026i-1] (Note that j starts with 0 and increment it only when there is a match). We also know (from above definition) that lps[j-1] is count of characters of pat[0\u2026j-1] that are both proper prefix and suffix. From above two points, we can conclude that we do not need to match these lps[j-1] characters with txt[i-j\u2026i-1] because we know that these characters will anyway match. Let us consider above example to understand this.","title":"Searching Algorithm:"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Rabin%E2%80%93Karp-algorithm/Rabin%E2%80%93Karp-algorithm/","text":"Rabin\u2013Karp algorithm NOTE: \u8fd9\u4e2a\u7b97\u6cd5\u5c55\u793a\u4e86\u8fdb\u884c\u5b57\u7b26\u4e32\u6bd4\u8f83\u7684\u53e6\u5916\u4e00\u79cd\u601d\u8def\uff1a hashing In computer science , the Rabin\u2013Karp algorithm or Karp\u2013Rabin algorithm is a string-searching algorithm created by Richard M. Karp and Michael O. Rabin ( 1987 ) that uses hashing to find an exact match of a pattern string in a text. It uses a rolling hash to quickly filter out positions of the text that cannot match the pattern, and then checks for a match at the remaining positions. Generalizations of the same idea can be used to find more than one match of a single pattern, or to find matches for more than one pattern. A practical application of the algorithm is detecting plagiarism . Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical. NOTE: Detecting plagiarism \u7684\u4e00\u4e2a\u7279\u70b9\u5c31\u662f\u5b83\u5e76\u4e0d\u8981\u6c42\u4e00\u5b57\u4e0d\u5dee\u7684\u7cbe\u51c6\u5339\u914d\u3002 Overview Several string-matching algorithms, including the Knuth\u2013Morris\u2013Pratt algorithm and the Boyer\u2013Moore string-search algorithm , reduce the worst-case time for string matching by extracting more information from each mismatch, allowing them to skip over positions of the text that are guaranteed not to match the pattern. The Rabin\u2013Karp algorithm instead achieves its speedup by using a hash function to quickly perform an approximate check for each position, and then only performing an exact comparison at the positions that pass this approximate check. A hash function is a function which converts every string into a numeric value, called its hash value ; for example, we might have hash(\"hello\")=5 . If two strings are equal, their hash values are also equal. For a well-designed hash function, the opposite is true, in an approximate sense: strings that are unequal are very unlikely to have equal hash values. The Rabin\u2013Karp algorithm proceeds by computing, at each position of the text, the hash value of a string starting at that position with the same length as the pattern. If this hash value equals the hash value of the pattern, it performs a full comparison at that position. The algorithm The algorithm is as shown: 1function RabinKarp(string s[1..n], string pattern[1..m]) 2 hpattern := hash(pattern[1..m]); 3 for i from 1 to n-m+1 4 hs := hash(s[i..i+m-1]) 5 if hs = hpattern 6 if s[i..i+m-1] = pattern[1..m] 7 return i 8 return not found Lines 2, 4, and 6 each require O ( m ) time. However, line 2 is only executed once, and line 6 is only executed if the hash values match, which is unlikely to happen more than a few times. Line 5 is executed O( n ) times, but each comparison only requires constant time, so its impact is O( n ). The issue is line 4. Naively computing the hash value for the substring s[i+1..i+m] requires O ( m ) time because each character is examined. Since the hash computation is done on each loop, the algorithm with a na\u00efve hash computation requires O (mn) time, the same complexity as a straightforward string matching algorithms. For speed, the hash must be computed in constant time. The trick is the variable hs already contains the previous hash value of s[i..i+m-1] . If that value can be used to compute the next hash value in constant time, then computing successive hash values will be fast. The trick can be exploited using a rolling hash . A rolling hash is a hash function specially designed to enable this operation. A trivial (but not very good) rolling hash function just adds the values of each character in the substring. This rolling hash formula can compute the next hash value from the previous value in constant time: s[i+1..i+m] = s[i..i+m-1] - s[i] + s[i+m] This simple function works, but will result in statement 5 being executed more often than other more sophisticated rolling hash functions such as those discussed in the next section. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\u4e0a\u9762\u5c55\u793a\u7684rolling hash function\u662f\u4e0d\u591f\u597d\u7684\uff0c\u5728\u4e0b\u4e00\u8282\u4e2d\u4f1a\u4ecb\u7ecd\u66f4\u597d\u7684\u540c\u65f6\u4e5f\u66f4\u52a0\u590d\u6742\u7684hash function Good performance requires a good hashing function for the encountered data. If the hashing is poor (such as producing the same hash value for every input), then line 6 would be executed O ( n ) times (i.e. on every iteration of the loop). Because character-by-character comparison of strings with length m takes O (m) time, the whole algorithm then takes a worst-case O ( mn ) time. Hash function used","title":"[Rabin\u2013Karp algorithm](https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm)"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Rabin%E2%80%93Karp-algorithm/Rabin%E2%80%93Karp-algorithm/#rabinkarp#algorithm","text":"NOTE: \u8fd9\u4e2a\u7b97\u6cd5\u5c55\u793a\u4e86\u8fdb\u884c\u5b57\u7b26\u4e32\u6bd4\u8f83\u7684\u53e6\u5916\u4e00\u79cd\u601d\u8def\uff1a hashing In computer science , the Rabin\u2013Karp algorithm or Karp\u2013Rabin algorithm is a string-searching algorithm created by Richard M. Karp and Michael O. Rabin ( 1987 ) that uses hashing to find an exact match of a pattern string in a text. It uses a rolling hash to quickly filter out positions of the text that cannot match the pattern, and then checks for a match at the remaining positions. Generalizations of the same idea can be used to find more than one match of a single pattern, or to find matches for more than one pattern. A practical application of the algorithm is detecting plagiarism . Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical. NOTE: Detecting plagiarism \u7684\u4e00\u4e2a\u7279\u70b9\u5c31\u662f\u5b83\u5e76\u4e0d\u8981\u6c42\u4e00\u5b57\u4e0d\u5dee\u7684\u7cbe\u51c6\u5339\u914d\u3002","title":"Rabin\u2013Karp algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Rabin%E2%80%93Karp-algorithm/Rabin%E2%80%93Karp-algorithm/#overview","text":"Several string-matching algorithms, including the Knuth\u2013Morris\u2013Pratt algorithm and the Boyer\u2013Moore string-search algorithm , reduce the worst-case time for string matching by extracting more information from each mismatch, allowing them to skip over positions of the text that are guaranteed not to match the pattern. The Rabin\u2013Karp algorithm instead achieves its speedup by using a hash function to quickly perform an approximate check for each position, and then only performing an exact comparison at the positions that pass this approximate check. A hash function is a function which converts every string into a numeric value, called its hash value ; for example, we might have hash(\"hello\")=5 . If two strings are equal, their hash values are also equal. For a well-designed hash function, the opposite is true, in an approximate sense: strings that are unequal are very unlikely to have equal hash values. The Rabin\u2013Karp algorithm proceeds by computing, at each position of the text, the hash value of a string starting at that position with the same length as the pattern. If this hash value equals the hash value of the pattern, it performs a full comparison at that position.","title":"Overview"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Rabin%E2%80%93Karp-algorithm/Rabin%E2%80%93Karp-algorithm/#the#algorithm","text":"The algorithm is as shown: 1function RabinKarp(string s[1..n], string pattern[1..m]) 2 hpattern := hash(pattern[1..m]); 3 for i from 1 to n-m+1 4 hs := hash(s[i..i+m-1]) 5 if hs = hpattern 6 if s[i..i+m-1] = pattern[1..m] 7 return i 8 return not found Lines 2, 4, and 6 each require O ( m ) time. However, line 2 is only executed once, and line 6 is only executed if the hash values match, which is unlikely to happen more than a few times. Line 5 is executed O( n ) times, but each comparison only requires constant time, so its impact is O( n ). The issue is line 4. Naively computing the hash value for the substring s[i+1..i+m] requires O ( m ) time because each character is examined. Since the hash computation is done on each loop, the algorithm with a na\u00efve hash computation requires O (mn) time, the same complexity as a straightforward string matching algorithms. For speed, the hash must be computed in constant time. The trick is the variable hs already contains the previous hash value of s[i..i+m-1] . If that value can be used to compute the next hash value in constant time, then computing successive hash values will be fast. The trick can be exploited using a rolling hash . A rolling hash is a hash function specially designed to enable this operation. A trivial (but not very good) rolling hash function just adds the values of each character in the substring. This rolling hash formula can compute the next hash value from the previous value in constant time: s[i+1..i+m] = s[i..i+m-1] - s[i] + s[i+m] This simple function works, but will result in statement 5 being executed more often than other more sophisticated rolling hash functions such as those discussed in the next section. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\u4e0a\u9762\u5c55\u793a\u7684rolling hash function\u662f\u4e0d\u591f\u597d\u7684\uff0c\u5728\u4e0b\u4e00\u8282\u4e2d\u4f1a\u4ecb\u7ecd\u66f4\u597d\u7684\u540c\u65f6\u4e5f\u66f4\u52a0\u590d\u6742\u7684hash function Good performance requires a good hashing function for the encountered data. If the hashing is poor (such as producing the same hash value for every input), then line 6 would be executed O ( n ) times (i.e. on every iteration of the loop). Because character-by-character comparison of strings with length m takes O (m) time, the whole algorithm then takes a worst-case O ( mn ) time.","title":"The algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Application/String/string-searching/Rabin%E2%80%93Karp-algorithm/Rabin%E2%80%93Karp-algorithm/#hash#function#used","text":"","title":"Hash function used"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0\u4e00\u4e9b\u5e38\u89c1\u7684\u7b97\u6cd5paradigm\u3002 https://online.wlu.ca/news/2019/02/12/how-algorithm-design-applied","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u4e00\u4e9b\u5e38\u89c1\u7684\u7b97\u6cd5paradigm\u3002 https://online.wlu.ca/news/2019/02/12/how-algorithm-design-applied","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/00-Brute-force/Brute-force-search/","text":"Brute-force search","title":"Brute-force-search"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/00-Brute-force/Brute-force-search/#brute-force#search","text":"","title":"Brute-force search"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/01-Divide-and-Conquer/Divide-and-conquer-algorithm/","text":"Divide-and-conquer algorithm","title":"Divide-and-conquer-algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/01-Divide-and-Conquer/Divide-and-conquer-algorithm/#divide-and-conquer#algorithm","text":"","title":"Divide-and-conquer algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/","text":"\u5173\u4e8e\u672c\u7ae0 \u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u548c\u8d2a\u5fc3\u7b97\u6cd5\u6bd4\u8f83\u7c7b\u4f3c\uff0c\u4e24\u8005\u90fd\u7528\u4e8e\u89e3\u51b3optimization\u95ee\u9898/\u6700\u503c\u95ee\u9898\uff0c\u56e0\u4e3a\u80fd\u591f\u7528\u8fd9\u4e24\u79cd\u7b97\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u5177\u6709\u76f8\u540c\u7684\u6027\u8d28\uff1a Optimal substructure \u5982\u679c\u95ee\u9898\u5177\u6709 Overlapping subproblems \u6027\u8d28\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528 Dynamic programming \uff0c\u5426\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 Greedy algorithm \u3002 TODO https://stackoverflow.com/questions/tagged/dynamic-programming?tab=Active \u5e8f\u5217\u95ee\u9898\u4e0e\u591a\u6761\u89c4\u5212\u7b97\u6cd5","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/#_1","text":"\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u548c\u8d2a\u5fc3\u7b97\u6cd5\u6bd4\u8f83\u7c7b\u4f3c\uff0c\u4e24\u8005\u90fd\u7528\u4e8e\u89e3\u51b3optimization\u95ee\u9898/\u6700\u503c\u95ee\u9898\uff0c\u56e0\u4e3a\u80fd\u591f\u7528\u8fd9\u4e24\u79cd\u7b97\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u5177\u6709\u76f8\u540c\u7684\u6027\u8d28\uff1a Optimal substructure \u5982\u679c\u95ee\u9898\u5177\u6709 Overlapping subproblems \u6027\u8d28\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528 Dynamic programming \uff0c\u5426\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 Greedy algorithm \u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/#todo","text":"https://stackoverflow.com/questions/tagged/dynamic-programming?tab=Active \u5e8f\u5217\u95ee\u9898\u4e0e\u591a\u6761\u89c4\u5212\u7b97\u6cd5","title":"TODO"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Bellman-equation/","text":"Bellman equation Bellman equation","title":"Bellman-equation"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Bellman-equation/#bellman#equation","text":"","title":"Bellman equation"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/","text":"VS \u52a8\u6001\u89c4\u5212VS\u8d2a\u5fc3\u7b97\u6cd5 \u4e24\u8005\u4e4b\u95f4\u7684\u5171\u540c\u70b9\u662f Optimal substructure \uff1b\u5982\u679c\u5177\u6709 Overlapping subproblems \uff0c\u5219\u4f7f\u7528 dynamic programming \uff0c\u5426\u5219\u4f7f\u7528 greedy algorithm \uff1b\u8fd9\u4e2a\u89c2\u70b9\u5728 Optimal substructure \u4e2d\u7ed9\u51fa\u4e86\uff1b \u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e0e\u641c\u7d22\u7b97\u6cd5\u6bd4\u8f83\u5206\u6790 \u65e0\u8bba\u662f**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u6291\u6216\u662f**\u57fa\u4e8e\u9012\u5f52\u7684\u641c\u7d22\u7b97\u6cd5**\uff0c\u4ed6\u4eec\u7684\u5b9e\u73b0\u63a5\u4f9d\u8d56\u4e8e**\u9012\u5f52\u5173\u7cfb**\u7684\u5efa\u7acb\u56e0\u6b64\u5efa\u7acb\u6b63\u786e\u7684**\u9012\u5f52\u5173\u7cfb**\u662f\u89e3\u51b3\u95ee\u9898\u7684\u6838\u5fc3\u6240\u5728\u3002 \u4e0b\u9762\u5206\u6790\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u4e0d\u540c\u3002\u603b\u7684\u6765\u8bf4\u4ed6\u4eec\u5bf9**\u9012\u5f52\u5173\u7cfb**\uff0c\u5b9e\u73b0\u7684\u65b9\u5f0f\u662f**\u76f8\u53cd**\u7684\uff1a \u641c\u7d22\u7b97\u6cd5**\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5b9e\u73b0\u662f**\u81ea\u5de6\u81f3\u53f3**\u7684\uff0c\u56e0\u4e3a\u5b83\u91c7\u7528\u9012\u5f52\u6765\u5b9e\u73b0\uff0c\u56e0\u6b64\u4ed6\u7684\u8ba1\u7b97\u662f**\u81ea\u9876\u5411\u4e0b**\u8fdb\u884c\u7684\u3002\u4ed6\u501f\u4f4f**\u7cfb\u7edf\u6808**\u4e0d\u65ad\u7684\u6309\u7167**\u9012\u5f52\u5173\u7cfb**\u81ea\u5de6\u5411\u53f3\u7684\u8fdb\u884c**\u5206\u6790 \uff0c\u76f4\u81f3**\u6700\u5c0f\u5b50\u95ee\u9898**\u53ef\u4ee5\u6c42\u89e3\u5f97\u5230\uff1b\u5728**\u641c\u7d22\u7b97\u6cd5**\u4e2d\uff0c \u6700\u5c0f\u5b50\u95ee\u9898**\u662f**\u9012\u5f52\u7ec8\u6b62\u6761\u4ef6 \uff1b \u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5b9e\u73b0\u662f**\u81ea\u53f3\u81f3\u5de6**\u7684\uff0c\u56e0\u6b64\u4ed6\u9996\u5148\u8ba1\u7b97\u7684\u662f**\u6700\u5c0f\u5b50\u95ee\u9898 \uff0c\u7136\u540e\u6309\u7167\u9012\u5f52\u5173\u7cfb\uff08 synthetically \u6216\u8005 \u5f52\u7eb3\uff09\u8ba1\u7b97\u51fa\u66f4\u5927\u7684\u95ee\u9898\u76f4\u81f3\u6c42\u89e3\u9664\u76ee\u6807\u95ee\u9898\uff0c\u5373\u66f4\u5927\u95ee\u9898\u7684\u6c42\u89e3\u662f\u4f9d\u8d56\u4e8e\u5c0f\u95ee\u9898\u7684\u89e3\u7684\uff0c\u6240\u4ee5\u5728\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e2d\uff0c\u5fc5\u987b\u8981\u8003\u8651\u7684\u4e00\u4e2a\u95ee\u9898\u662f\u4fdd\u5b58\u5c0f\u95ee\u9898\u7684\u89e3\uff08\u5982 Fibonacci sequence \u4e2d\u9700\u8981\u4fdd\u5b58\u524d\u4e24\u9879\uff0c\u6700\u5927\u5b57\u6bb5\u548c\u95ee\u9898\u4e2d\u9700\u8981\u4fdd\u5b58\u524d\u4e00\u9879 \uff09\u3002\u4fdd\u5b58\u5c0f\u95ee\u9898\u7684\u89e3\u6240\u5e26\u6765\u7684\u53e6\u5916\u4e00\u4e2a\u597d\u5904\u662f\uff1a\u907f\u514d\u4e86\u91cd\u590d\u8ba1\u7b97\u76f8\u540c\u7684\u95ee\u9898\uff0c\u5728\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e2d\u5e38\u5e38\u9700\u8981\u4f7f\u7528\u4e00\u4e2a\u5bb9\u5668\u5c06\u5404\u4e2a\u6b64\u95ee\u9898\u7684\u8ba1\u7b97\u7ed3\u679c\u7ed9\u4fdd\u5b58\u8d77\u6765\uff0c\u8fd9\u6837\u5728\u540e\u7eed\u8ba1\u7b97\u4e2d\u9700\u8981\u88ab\u4f7f\u7528\u65f6\u5c31\u53ef\u76f4\u63a5\u53d6\u5f97\uff08 Overlapping subproblems \uff09\uff1b\u5728**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u4e2d\uff0c**\u6700\u5c0f\u5b50\u95ee\u9898**\u662f\u65e0\u9700\u8ba1\u7b97\u7684\uff0c\u5b83\u662f\u5728\u7b97\u6cd5\u5f00\u59cb\u4e4b\u521d\u5c31\u53ef\u4ee5\u76f4\u63a5**\u521d\u59cb\u5316**\u5230\u4fdd\u5b58\u95ee\u9898\u89e3\u7684\u5bb9\u5668\u4e2d\uff1b SUMMARY : \u8fd9\u79cd\u5173\u7cfb\u5728 wikipedia Corecursion \u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\uff1b **\u641c\u7d22\u7b97\u6cd5**\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5904\u7406\u662f**\u7531\u5de6\u5411\u53f3**\u7684\u56e0\u6b64\u4ed6\u662f**\u7531\u9876\u5411\u4e0b**\u8fdb\u884c\u8ba1\u7b97\u7684\uff1b**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5904\u7406\u662f**\u7531\u53f3\u5411\u5de6**\u7684\u56e0\u6b64\u4ed6\u7684\u8ba1\u7b97\u662f**\u7531\u5e95\u5411\u4e0a**\u7684\uff1b\u65e0\u8bba\u662f**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u6291\u6216\u662f**\u56de\u6eaf\u6cd5**\u4ed6\u4eec\u63a5\u4f9d\u8d56\u4e8e\u9012\u5f52\u8868\u8fbe\u5f0f\u7684\u5efa\u7acb\u4e24\u79cd\u7b97\u6cd5\u5bf9\u9012\u5f52\u8868\u8fbe\u5f0f\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u5b9e\u73b0\u65b9\u5f0f\u663e\u7136\u3002\u662f\u5b9e\u73b0\u91c7\u7528\u7684\u662f\u66f4\u52a0\u7eaf\u7cb9\u7684\u9012\u5f52\u65b9\u5f0f\u4ed6\u501f\u52a9\u4e8e\u7cfb\u7edf\u7ad9\u6765\u9010\u6b65\u5206\u89e3\u76f4\u5230\u6700\u5c0f\u5b50\u95ee\u9898\u800c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5219\u5b8c\u5168\u662f\u7531\u7a0b\u5e8f\u7684\u8bed\u8a00\u6765\u5b89\u6392\u8ba1\u7b97\u6b64\u95ee\u9898\u7684\u6b21\u5e8f\u3002D\u5b89\u6392\u6570\u7ec4\u548c\u53d8\u4eae\u6765\u4fdd\u5b58\u8ba1\u7b97\u7684\u5b50\u95ee\u9898\u7684\u7ed3\u679c\u6211\u731c\u60f3\u5e94\u8be5\u662f\u5148\u6709\u56de\u8083\u53cd\u7136\u540e\u624d\u6709\u4e86\u9488\u5bf9\u56de\u6eaf\u53d1\u8fdb\u884c\u6539\u8fdb\u7684\u66f4\u52a0\u7075\u6d3b\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u3002 \u52a8\u6001\u89c4\u5212VS\u56de\u6eaf\u6cd5VS\u5206\u652f\u9650\u754c\u6cd5VS\u8d2a\u5fc3\u7b97\u6cd5 \u52a8\u6001\u89c4\u5212\u3001\u56de\u6eaf\u6cd5\u3001\u5206\u652f\u9650\u754c\u6cd5\u90fd\u5c1d\u8bd5\u5728\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\u4e2d\u9009\u53d6\u6700\u4f18\u89e3\uff0c\u800c\u8d2a\u5fc3\u7b97\u6cd5\u5219\u4e0d\u540c\uff0c\u5b83\u4e0d\u65ad\u5730\u9009\u53d6\u5f53\u524d\u6700\u4f18\u89e3\u3002","title":"Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/#vs","text":"","title":"VS"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/#vs_1","text":"\u4e24\u8005\u4e4b\u95f4\u7684\u5171\u540c\u70b9\u662f Optimal substructure \uff1b\u5982\u679c\u5177\u6709 Overlapping subproblems \uff0c\u5219\u4f7f\u7528 dynamic programming \uff0c\u5426\u5219\u4f7f\u7528 greedy algorithm \uff1b\u8fd9\u4e2a\u89c2\u70b9\u5728 Optimal substructure \u4e2d\u7ed9\u51fa\u4e86\uff1b","title":"\u52a8\u6001\u89c4\u5212VS\u8d2a\u5fc3\u7b97\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/#_1","text":"\u65e0\u8bba\u662f**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u6291\u6216\u662f**\u57fa\u4e8e\u9012\u5f52\u7684\u641c\u7d22\u7b97\u6cd5**\uff0c\u4ed6\u4eec\u7684\u5b9e\u73b0\u63a5\u4f9d\u8d56\u4e8e**\u9012\u5f52\u5173\u7cfb**\u7684\u5efa\u7acb\u56e0\u6b64\u5efa\u7acb\u6b63\u786e\u7684**\u9012\u5f52\u5173\u7cfb**\u662f\u89e3\u51b3\u95ee\u9898\u7684\u6838\u5fc3\u6240\u5728\u3002 \u4e0b\u9762\u5206\u6790\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u4e0d\u540c\u3002\u603b\u7684\u6765\u8bf4\u4ed6\u4eec\u5bf9**\u9012\u5f52\u5173\u7cfb**\uff0c\u5b9e\u73b0\u7684\u65b9\u5f0f\u662f**\u76f8\u53cd**\u7684\uff1a \u641c\u7d22\u7b97\u6cd5**\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5b9e\u73b0\u662f**\u81ea\u5de6\u81f3\u53f3**\u7684\uff0c\u56e0\u4e3a\u5b83\u91c7\u7528\u9012\u5f52\u6765\u5b9e\u73b0\uff0c\u56e0\u6b64\u4ed6\u7684\u8ba1\u7b97\u662f**\u81ea\u9876\u5411\u4e0b**\u8fdb\u884c\u7684\u3002\u4ed6\u501f\u4f4f**\u7cfb\u7edf\u6808**\u4e0d\u65ad\u7684\u6309\u7167**\u9012\u5f52\u5173\u7cfb**\u81ea\u5de6\u5411\u53f3\u7684\u8fdb\u884c**\u5206\u6790 \uff0c\u76f4\u81f3**\u6700\u5c0f\u5b50\u95ee\u9898**\u53ef\u4ee5\u6c42\u89e3\u5f97\u5230\uff1b\u5728**\u641c\u7d22\u7b97\u6cd5**\u4e2d\uff0c \u6700\u5c0f\u5b50\u95ee\u9898**\u662f**\u9012\u5f52\u7ec8\u6b62\u6761\u4ef6 \uff1b \u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5b9e\u73b0\u662f**\u81ea\u53f3\u81f3\u5de6**\u7684\uff0c\u56e0\u6b64\u4ed6\u9996\u5148\u8ba1\u7b97\u7684\u662f**\u6700\u5c0f\u5b50\u95ee\u9898 \uff0c\u7136\u540e\u6309\u7167\u9012\u5f52\u5173\u7cfb\uff08 synthetically \u6216\u8005 \u5f52\u7eb3\uff09\u8ba1\u7b97\u51fa\u66f4\u5927\u7684\u95ee\u9898\u76f4\u81f3\u6c42\u89e3\u9664\u76ee\u6807\u95ee\u9898\uff0c\u5373\u66f4\u5927\u95ee\u9898\u7684\u6c42\u89e3\u662f\u4f9d\u8d56\u4e8e\u5c0f\u95ee\u9898\u7684\u89e3\u7684\uff0c\u6240\u4ee5\u5728\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e2d\uff0c\u5fc5\u987b\u8981\u8003\u8651\u7684\u4e00\u4e2a\u95ee\u9898\u662f\u4fdd\u5b58\u5c0f\u95ee\u9898\u7684\u89e3\uff08\u5982 Fibonacci sequence \u4e2d\u9700\u8981\u4fdd\u5b58\u524d\u4e24\u9879\uff0c\u6700\u5927\u5b57\u6bb5\u548c\u95ee\u9898\u4e2d\u9700\u8981\u4fdd\u5b58\u524d\u4e00\u9879 \uff09\u3002\u4fdd\u5b58\u5c0f\u95ee\u9898\u7684\u89e3\u6240\u5e26\u6765\u7684\u53e6\u5916\u4e00\u4e2a\u597d\u5904\u662f\uff1a\u907f\u514d\u4e86\u91cd\u590d\u8ba1\u7b97\u76f8\u540c\u7684\u95ee\u9898\uff0c\u5728\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e2d\u5e38\u5e38\u9700\u8981\u4f7f\u7528\u4e00\u4e2a\u5bb9\u5668\u5c06\u5404\u4e2a\u6b64\u95ee\u9898\u7684\u8ba1\u7b97\u7ed3\u679c\u7ed9\u4fdd\u5b58\u8d77\u6765\uff0c\u8fd9\u6837\u5728\u540e\u7eed\u8ba1\u7b97\u4e2d\u9700\u8981\u88ab\u4f7f\u7528\u65f6\u5c31\u53ef\u76f4\u63a5\u53d6\u5f97\uff08 Overlapping subproblems \uff09\uff1b\u5728**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u4e2d\uff0c**\u6700\u5c0f\u5b50\u95ee\u9898**\u662f\u65e0\u9700\u8ba1\u7b97\u7684\uff0c\u5b83\u662f\u5728\u7b97\u6cd5\u5f00\u59cb\u4e4b\u521d\u5c31\u53ef\u4ee5\u76f4\u63a5**\u521d\u59cb\u5316**\u5230\u4fdd\u5b58\u95ee\u9898\u89e3\u7684\u5bb9\u5668\u4e2d\uff1b SUMMARY : \u8fd9\u79cd\u5173\u7cfb\u5728 wikipedia Corecursion \u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\uff1b **\u641c\u7d22\u7b97\u6cd5**\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5904\u7406\u662f**\u7531\u5de6\u5411\u53f3**\u7684\u56e0\u6b64\u4ed6\u662f**\u7531\u9876\u5411\u4e0b**\u8fdb\u884c\u8ba1\u7b97\u7684\uff1b**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u5bf9**\u9012\u5f52\u5173\u7cfb**\u7684\u5904\u7406\u662f**\u7531\u53f3\u5411\u5de6**\u7684\u56e0\u6b64\u4ed6\u7684\u8ba1\u7b97\u662f**\u7531\u5e95\u5411\u4e0a**\u7684\uff1b\u65e0\u8bba\u662f**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u6291\u6216\u662f**\u56de\u6eaf\u6cd5**\u4ed6\u4eec\u63a5\u4f9d\u8d56\u4e8e\u9012\u5f52\u8868\u8fbe\u5f0f\u7684\u5efa\u7acb\u4e24\u79cd\u7b97\u6cd5\u5bf9\u9012\u5f52\u8868\u8fbe\u5f0f\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u5b9e\u73b0\u65b9\u5f0f\u663e\u7136\u3002\u662f\u5b9e\u73b0\u91c7\u7528\u7684\u662f\u66f4\u52a0\u7eaf\u7cb9\u7684\u9012\u5f52\u65b9\u5f0f\u4ed6\u501f\u52a9\u4e8e\u7cfb\u7edf\u7ad9\u6765\u9010\u6b65\u5206\u89e3\u76f4\u5230\u6700\u5c0f\u5b50\u95ee\u9898\u800c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5219\u5b8c\u5168\u662f\u7531\u7a0b\u5e8f\u7684\u8bed\u8a00\u6765\u5b89\u6392\u8ba1\u7b97\u6b64\u95ee\u9898\u7684\u6b21\u5e8f\u3002D\u5b89\u6392\u6570\u7ec4\u548c\u53d8\u4eae\u6765\u4fdd\u5b58\u8ba1\u7b97\u7684\u5b50\u95ee\u9898\u7684\u7ed3\u679c\u6211\u731c\u60f3\u5e94\u8be5\u662f\u5148\u6709\u56de\u8083\u53cd\u7136\u540e\u624d\u6709\u4e86\u9488\u5bf9\u56de\u6eaf\u53d1\u8fdb\u884c\u6539\u8fdb\u7684\u66f4\u52a0\u7075\u6d3b\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u3002","title":"\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e0e\u641c\u7d22\u7b97\u6cd5\u6bd4\u8f83\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/#vsvsvs","text":"\u52a8\u6001\u89c4\u5212\u3001\u56de\u6eaf\u6cd5\u3001\u5206\u652f\u9650\u754c\u6cd5\u90fd\u5c1d\u8bd5\u5728\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\u4e2d\u9009\u53d6\u6700\u4f18\u89e3\uff0c\u800c\u8d2a\u5fc3\u7b97\u6cd5\u5219\u4e0d\u540c\uff0c\u5b83\u4e0d\u65ad\u5730\u9009\u53d6\u5f53\u524d\u6700\u4f18\u89e3\u3002","title":"\u52a8\u6001\u89c4\u5212VS\u56de\u6eaf\u6cd5VS\u5206\u652f\u9650\u754c\u6cd5VS\u8d2a\u5fc3\u7b97\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Optimal-substructure/","text":"\u7ef4\u57fa\u767e\u79d1 Optimal substructure In computer science , a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine the usefulness of dynamic programming and greedy algorithms for a problem.[ 1] NOTE: optimal substructure\u5176\u5b9e\u4e5f\u4f53\u73b0\u4e86 \u9012\u5f52\u5173\u7cfb \uff1b Typically, a greedy algorithm is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step.[ 1] Otherwise, provided the problem exhibits overlapping subproblems as well, dynamic programming is used. If there are no appropriate greedy algorithms and the problem fails to exhibit overlapping subproblems, often a lengthy but straightforward search of the solution space is the best alternative\uff08\u4f7f\u7528\u641c\u7d22\u7b97\u6cd5\uff09. In the application of dynamic programming to mathematical optimization , Richard Bellman 's Principle of Optimality is based on the idea that in order to solve a dynamic optimization problem from some starting period t to some ending period T , one implicitly has to solve subproblems starting from later dates s , where t<s<T . This is an example of optimal substructure . The Principle of Optimality is used to derive the Bellman equation , which shows how the value of the problem starting from t is related to the value of the problem starting from s . Problems with optimal substructure Longest common subsequence problem Longest increasing subsequence Longest palindromic substring All-Pairs Shortest Path Any problem that can be solved by dynamic programming . Problems without optimal substructure Longest path problem Least-cost airline fare. (Using online flight search, we will frequently find that the cheapest flight from airport A to airport B involves a single connection through airport C, but the cheapest flight from airport A to airport C involves a connection through some other airport D.) 20191112 \u6700\u4f18\u5b50\u7ed3\u6784\u5176\u5b9e\u5c31\u5177\u5907\u9012\u5f52\u6027\u8d28\uff1a\u5168\u95ee\u9898\u7684\u6700\u4f18\u89e3\u8574\u542b\u7740\u5b50\u95ee\u9898\u7684\u6700\u4f18\u89e3\u3002 \u6240\u4ee5\u6211\u4eec\u5373\u53ef\u4ee5\u81ea\u9876\u5411\u4e0b\u6765\u8fd0\u7528\u9012\u5f52\u5173\u7cfb\u4e5f\u53ef\u4ee5\u81ea\u5e95\u5411\u4e0a\u6765\u8fd0\u7528\u9012\u5f52\u5173\u7cfb\u3002\u52a8\u6001\u89c4\u5212\u5c31\u662f\u81ea\u5e95\u5411\u4e0a\u7684\u3002","title":"Optimal-substructure"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Optimal-substructure/#optimal#substructure","text":"In computer science , a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine the usefulness of dynamic programming and greedy algorithms for a problem.[ 1] NOTE: optimal substructure\u5176\u5b9e\u4e5f\u4f53\u73b0\u4e86 \u9012\u5f52\u5173\u7cfb \uff1b Typically, a greedy algorithm is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step.[ 1] Otherwise, provided the problem exhibits overlapping subproblems as well, dynamic programming is used. If there are no appropriate greedy algorithms and the problem fails to exhibit overlapping subproblems, often a lengthy but straightforward search of the solution space is the best alternative\uff08\u4f7f\u7528\u641c\u7d22\u7b97\u6cd5\uff09. In the application of dynamic programming to mathematical optimization , Richard Bellman 's Principle of Optimality is based on the idea that in order to solve a dynamic optimization problem from some starting period t to some ending period T , one implicitly has to solve subproblems starting from later dates s , where t<s<T . This is an example of optimal substructure . The Principle of Optimality is used to derive the Bellman equation , which shows how the value of the problem starting from t is related to the value of the problem starting from s .","title":"\u7ef4\u57fa\u767e\u79d1Optimal substructure"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Optimal-substructure/#problems#with#optimal#substructure","text":"Longest common subsequence problem Longest increasing subsequence Longest palindromic substring All-Pairs Shortest Path Any problem that can be solved by dynamic programming .","title":"Problems with optimal substructure"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Optimal-substructure/#problems#without#optimal#substructure","text":"Longest path problem Least-cost airline fare. (Using online flight search, we will frequently find that the cheapest flight from airport A to airport B involves a single connection through airport C, but the cheapest flight from airport A to airport C involves a connection through some other airport D.)","title":"Problems without optimal substructure"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Optimal-substructure/#20191112","text":"\u6700\u4f18\u5b50\u7ed3\u6784\u5176\u5b9e\u5c31\u5177\u5907\u9012\u5f52\u6027\u8d28\uff1a\u5168\u95ee\u9898\u7684\u6700\u4f18\u89e3\u8574\u542b\u7740\u5b50\u95ee\u9898\u7684\u6700\u4f18\u89e3\u3002 \u6240\u4ee5\u6211\u4eec\u5373\u53ef\u4ee5\u81ea\u9876\u5411\u4e0b\u6765\u8fd0\u7528\u9012\u5f52\u5173\u7cfb\u4e5f\u53ef\u4ee5\u81ea\u5e95\u5411\u4e0a\u6765\u8fd0\u7528\u9012\u5f52\u5173\u7cfb\u3002\u52a8\u6001\u89c4\u5212\u5c31\u662f\u81ea\u5e95\u5411\u4e0a\u7684\u3002","title":"20191112"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Catalan/","text":"","title":"Catalan"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/","text":"Dynamic programming Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering \uff08\u822a\u5929\u5de5\u7a0b\uff09 to economics . In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems \uff08\u51b3\u7b56\u95ee\u9898\uff09 cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems , then it is said to have optimal substructure \uff08\u6700\u4f18\u5b50\u7ed3\u6784\uff09. If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems.[ 1] In the optimization literature this relationship is called the Bellman equation . Overview Mathematical optimization In terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time. This is done by defining a sequence of value functions V*1, *V*2, ..., *V**n taking y as an argument representing the state of the system at times i from 1 to n . The definition of V**n ( y ) is the value obtained in state y at the last time n . The values V**i at earlier times i = n \u22121, n \u2212 2, ..., 2, 1 can be found by working backwards, using a recursive relationship called the Bellman equation . For i = 2, ..., n , V**i \u22121 at any state y is calculated from V**i by maximizing a simple function (usually the sum) of the gain from a decision at time i \u2212 1 and the function V**i at the new state of the system if this decision is made. Since V**i has already been calculated for the needed states, the above operation yields V**i \u22121 for those states. Finally, *V*1 at the initial state of the system is the value of the optimal solution. The optimal values of the decision variables can be recovered, one by one, by tracking back the calculations already performed. Computer programming There are two key attributes that a problem must have in order for dynamic programming to be applicable: optimal substructure and overlapping sub-problems . If a problem can be solved by combining optimal solutions to non-overlapping sub-problems, the strategy is called \" divide and conquer \" instead.[ 1] This is why merge sort and quick sort are not classified as dynamic programming problems. SUMMARY : sorting\u5e76\u4e0d\u5177\u5907 overlapping sub-problems \u7684\u7279\u6027\uff1b Optimal substructure means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems . Such optimal substructures are usually described by means of recursion . For example, given a graph G=(V,E) , the shortest path p from a vertex u to a vertex v exhibits optimal substructure : take any intermediate vertex w on this shortest path p . If p is truly the shortest path, then it can be split into sub-paths p1 from u to w and p2 from w to v such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in Introduction to Algorithms ). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the Bellman\u2013Ford algorithm or the Floyd\u2013Warshall algorithm does. Overlapping sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems. For example, consider the recursive formulation for generating the Fibonacci series: F_i = F_{i\u22121} + F_{i\u22122} F_i = F_{i\u22121} + F_{i\u22122} , with base case F_1 = F_2 = 1 F_1 = F_2 = 1 . Then F_{43} = F_{42} + F_{41} F_{43} = F_{42} + F_{41} , and F_{42} = F_{41} + F_{40} F_{42} = F_{41} + F_{40} . Now F_{41} F_{41} is being solved in the recursive sub-trees of both F_{43} F_{43} as well as F_{42} F_{42} . Even though the total number of sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once. Figure 2. This can be achieved in either of two ways:[ citation needed ] Top-down approach : This is the direct fall-out of the recursive formulation of any problem. If the solution to any problem can be formulated recursively using the solution to its sub-problems, and if its sub-problems are overlapping, then one can easily memoize or store the solutions to the sub-problems in a table. Whenever we attempt to solve a new sub-problem, we first check the table to see if it is already solved. If a solution has been recorded, we can use it directly, otherwise we solve the sub-problem and add its solution to the table. Bottom-up approach : Once we formulate the solution to a problem recursively as in terms of its sub-problems, we can try reformulating the problem in a bottom-up fashion: try solving the sub-problems first and use their solutions to build-on and arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger and bigger sub-problems by using the solutions to small sub-problems. For example, if we already know the values of *F*41 and *F*40, we can directly calculate the value of *F*42. Some programming languages can automatically memoize the result of a function call with a particular set of arguments, in order to speed up call-by-name evaluation (this mechanism is referred to as call-by-need ). Some languages make it possible portably (e.g. Scheme , Common Lisp , Perl or D ). Some languages have automatic memoization built in, such as tabled Prolog and J , which supports memoization with the M. adverb.[ 4] In any case, this is only possible for a referentially transparent function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages such as Wolfram Language . Examples: Computer algorithms Dijkstra's algorithm for the shortest path problem Fibonacci sequence A type of balanced 0\u20131 matrix Checkerboard Sequence alignment Tower of Hanoi puzzle Egg dropping puzzle Matrix chain multiplication Main article: Matrix chain multiplication Algorithms that use dynamic programming Recurrent solutions to lattice models for protein-DNA binding Backward induction as a solution method for finite-horizon discrete-time dynamic optimization problems Method of undetermined coefficients can be used to solve the Bellman equation in infinite-horizon, discrete-time, discounted , time-invariant dynamic optimization problems Many string algorithms including longest common subsequence , longest increasing subsequence , longest common substring , Levenshtein distance (edit distance) Many algorithmic problems on graphs can be solved efficiently for graphs of bounded treewidth or bounded clique-width by using dynamic programming on a tree decomposition of the graph. The Cocke\u2013Younger\u2013Kasami (CYK) algorithm which determines whether and how a given string can be generated by a given context-free grammar Knuth's word wrapping algorithm that minimizes raggedness when word wrapping text The use of transposition tables and refutation tables in computer chess The Viterbi algorithm (used for hidden Markov models , and particularly in part of speech tagging ) The Earley algorithm (a type of chart parser ) The Needleman\u2013Wunsch algorithm and other algorithms used in bioinformatics , including sequence alignment , structural alignment , RNA structure prediction Floyd's all-pairs shortest path algorithm Optimizing the order for chain matrix multiplication Pseudo-polynomial time algorithms for the subset sum , knapsack and partition problems The dynamic time warping algorithm for computing the global distance between two time series The Selinger (a.k.a. System R ) algorithm for relational database query optimization De Boor algorithm for evaluating B-spline curves Duckworth\u2013Lewis method for resolving the problem when games of cricket are interrupted The value iteration method for solving Markov decision processes Some graphic image edge following selection methods such as the \"magnet\" selection tool in Photoshop Some methods for solving interval scheduling problems Some methods for solving the travelling salesman problem , either exactly (in exponential time ) or approximately (e.g. via the bitonic tour ) Recursive least squares method Beat tracking in music information retrieval Adaptive-critic training strategy for artificial neural networks Stereo algorithms for solving the correspondence problem used in stereo vision Seam carving (content-aware image resizing) The Bellman\u2013Ford algorithm for finding the shortest distance in a graph Some approximate solution methods for the linear search problem Kadane's algorithm for the maximum subarray problem Optimization of electric generation expansion plans in the Wein Automatic System Planning (WASP) package See also Convexity in economics Greedy algorithm Non-convexity (economics) Stochastic programming Stochastic dynamic programming","title":"Dynamic-programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#dynamic#programming","text":"Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering \uff08\u822a\u5929\u5de5\u7a0b\uff09 to economics . In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems \uff08\u51b3\u7b56\u95ee\u9898\uff09 cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems , then it is said to have optimal substructure \uff08\u6700\u4f18\u5b50\u7ed3\u6784\uff09. If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems.[ 1] In the optimization literature this relationship is called the Bellman equation .","title":"Dynamic programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#overview","text":"","title":"Overview"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#mathematical#optimization","text":"In terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time. This is done by defining a sequence of value functions V*1, *V*2, ..., *V**n taking y as an argument representing the state of the system at times i from 1 to n . The definition of V**n ( y ) is the value obtained in state y at the last time n . The values V**i at earlier times i = n \u22121, n \u2212 2, ..., 2, 1 can be found by working backwards, using a recursive relationship called the Bellman equation . For i = 2, ..., n , V**i \u22121 at any state y is calculated from V**i by maximizing a simple function (usually the sum) of the gain from a decision at time i \u2212 1 and the function V**i at the new state of the system if this decision is made. Since V**i has already been calculated for the needed states, the above operation yields V**i \u22121 for those states. Finally, *V*1 at the initial state of the system is the value of the optimal solution. The optimal values of the decision variables can be recovered, one by one, by tracking back the calculations already performed.","title":"Mathematical optimization"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#computer#programming","text":"There are two key attributes that a problem must have in order for dynamic programming to be applicable: optimal substructure and overlapping sub-problems . If a problem can be solved by combining optimal solutions to non-overlapping sub-problems, the strategy is called \" divide and conquer \" instead.[ 1] This is why merge sort and quick sort are not classified as dynamic programming problems. SUMMARY : sorting\u5e76\u4e0d\u5177\u5907 overlapping sub-problems \u7684\u7279\u6027\uff1b Optimal substructure means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems . Such optimal substructures are usually described by means of recursion . For example, given a graph G=(V,E) , the shortest path p from a vertex u to a vertex v exhibits optimal substructure : take any intermediate vertex w on this shortest path p . If p is truly the shortest path, then it can be split into sub-paths p1 from u to w and p2 from w to v such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in Introduction to Algorithms ). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the Bellman\u2013Ford algorithm or the Floyd\u2013Warshall algorithm does. Overlapping sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems. For example, consider the recursive formulation for generating the Fibonacci series: F_i = F_{i\u22121} + F_{i\u22122} F_i = F_{i\u22121} + F_{i\u22122} , with base case F_1 = F_2 = 1 F_1 = F_2 = 1 . Then F_{43} = F_{42} + F_{41} F_{43} = F_{42} + F_{41} , and F_{42} = F_{41} + F_{40} F_{42} = F_{41} + F_{40} . Now F_{41} F_{41} is being solved in the recursive sub-trees of both F_{43} F_{43} as well as F_{42} F_{42} . Even though the total number of sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once. Figure 2. This can be achieved in either of two ways:[ citation needed ] Top-down approach : This is the direct fall-out of the recursive formulation of any problem. If the solution to any problem can be formulated recursively using the solution to its sub-problems, and if its sub-problems are overlapping, then one can easily memoize or store the solutions to the sub-problems in a table. Whenever we attempt to solve a new sub-problem, we first check the table to see if it is already solved. If a solution has been recorded, we can use it directly, otherwise we solve the sub-problem and add its solution to the table. Bottom-up approach : Once we formulate the solution to a problem recursively as in terms of its sub-problems, we can try reformulating the problem in a bottom-up fashion: try solving the sub-problems first and use their solutions to build-on and arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger and bigger sub-problems by using the solutions to small sub-problems. For example, if we already know the values of *F*41 and *F*40, we can directly calculate the value of *F*42. Some programming languages can automatically memoize the result of a function call with a particular set of arguments, in order to speed up call-by-name evaluation (this mechanism is referred to as call-by-need ). Some languages make it possible portably (e.g. Scheme , Common Lisp , Perl or D ). Some languages have automatic memoization built in, such as tabled Prolog and J , which supports memoization with the M. adverb.[ 4] In any case, this is only possible for a referentially transparent function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages such as Wolfram Language .","title":"Computer programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#examples#computer#algorithms","text":"","title":"Examples: Computer algorithms"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#dijkstras#algorithm#for#the#shortest#path#problem","text":"","title":"Dijkstra's algorithm for the shortest path problem"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#fibonacci#sequence","text":"","title":"Fibonacci sequence"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#a#type#of#balanced#01#matrix","text":"","title":"A type of balanced 0\u20131 matrix"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#checkerboard","text":"","title":"Checkerboard"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#sequence#alignment","text":"","title":"Sequence alignment"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#tower#of#hanoi#puzzle","text":"","title":"Tower of Hanoi puzzle"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#egg#dropping#puzzle","text":"","title":"Egg dropping puzzle"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#matrix#chain#multiplication","text":"Main article: Matrix chain multiplication","title":"Matrix chain multiplication"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#algorithms#that#use#dynamic#programming","text":"Recurrent solutions to lattice models for protein-DNA binding Backward induction as a solution method for finite-horizon discrete-time dynamic optimization problems Method of undetermined coefficients can be used to solve the Bellman equation in infinite-horizon, discrete-time, discounted , time-invariant dynamic optimization problems Many string algorithms including longest common subsequence , longest increasing subsequence , longest common substring , Levenshtein distance (edit distance) Many algorithmic problems on graphs can be solved efficiently for graphs of bounded treewidth or bounded clique-width by using dynamic programming on a tree decomposition of the graph. The Cocke\u2013Younger\u2013Kasami (CYK) algorithm which determines whether and how a given string can be generated by a given context-free grammar Knuth's word wrapping algorithm that minimizes raggedness when word wrapping text The use of transposition tables and refutation tables in computer chess The Viterbi algorithm (used for hidden Markov models , and particularly in part of speech tagging ) The Earley algorithm (a type of chart parser ) The Needleman\u2013Wunsch algorithm and other algorithms used in bioinformatics , including sequence alignment , structural alignment , RNA structure prediction Floyd's all-pairs shortest path algorithm Optimizing the order for chain matrix multiplication Pseudo-polynomial time algorithms for the subset sum , knapsack and partition problems The dynamic time warping algorithm for computing the global distance between two time series The Selinger (a.k.a. System R ) algorithm for relational database query optimization De Boor algorithm for evaluating B-spline curves Duckworth\u2013Lewis method for resolving the problem when games of cricket are interrupted The value iteration method for solving Markov decision processes Some graphic image edge following selection methods such as the \"magnet\" selection tool in Photoshop Some methods for solving interval scheduling problems Some methods for solving the travelling salesman problem , either exactly (in exponential time ) or approximately (e.g. via the bitonic tour ) Recursive least squares method Beat tracking in music information retrieval Adaptive-critic training strategy for artificial neural networks Stereo algorithms for solving the correspondence problem used in stereo vision Seam carving (content-aware image resizing) The Bellman\u2013Ford algorithm for finding the shortest distance in a graph Some approximate solution methods for the linear search problem Kadane's algorithm for the maximum subarray problem Optimization of electric generation expansion plans in the Wein Automatic System Planning (WASP) package","title":"Algorithms that use dynamic programming"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#see#also","text":"Convexity in economics Greedy algorithm Non-convexity (economics) Stochastic programming Stochastic dynamic programming","title":"See also"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Overlapping-subproblems/","text":"Overlapping subproblems Overlapping subproblems In computer science , a problem is said to have overlapping subproblems if the problem can be broken down into subproblems which are reused several times or a recursive algorithm for the problem solves the same subproblem over and over rather than always generating new subproblems.[ 1] [ 2] [ 3] For example, the problem of computing the Fibonacci sequence exhibits overlapping subproblems. The problem of computing the n*th Fibonacci number *F ( n ), can be broken down into the subproblems of computing F ( n \u2212 1) and F ( n \u2212 2), and then adding the two. The subproblem of computing F ( n \u2212 1) can itself be broken down into a subproblem that involves computing F ( n \u2212 2). Therefore, the computation of F ( n \u2212 2) is reused, and the Fibonacci sequence thus exhibits overlapping subproblems. A naive recursive approach to such a problem generally fails due to an exponential complexity . If the problem also shares an optimal substructure property, dynamic programming is a good way to work it out.","title":"Overlapping-subproblems"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Overlapping-subproblems/#overlapping#subproblems","text":"In computer science , a problem is said to have overlapping subproblems if the problem can be broken down into subproblems which are reused several times or a recursive algorithm for the problem solves the same subproblem over and over rather than always generating new subproblems.[ 1] [ 2] [ 3] For example, the problem of computing the Fibonacci sequence exhibits overlapping subproblems. The problem of computing the n*th Fibonacci number *F ( n ), can be broken down into the subproblems of computing F ( n \u2212 1) and F ( n \u2212 2), and then adding the two. The subproblem of computing F ( n \u2212 1) can itself be broken down into a subproblem that involves computing F ( n \u2212 2). Therefore, the computation of F ( n \u2212 2) is reused, and the Fibonacci sequence thus exhibits overlapping subproblems. A naive recursive approach to such a problem generally fails due to an exponential complexity . If the problem also shares an optimal substructure property, dynamic programming is a good way to work it out.","title":"Overlapping subproblems"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Reading-list/","text":"Reading list Chapter 6 Dynamic programming https://www.geeksforgeeks.org/top-20-dynamic-programming-interview-questions/","title":"Reading-list"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/Reading-list/#reading#list","text":"Chapter 6 Dynamic programming https://www.geeksforgeeks.org/top-20-dynamic-programming-interview-questions/","title":"Reading list"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/","text":"\u524d\u8a00 \u5728\u300a\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790\u300b\u7b2c\u56db\u7248\u738b\u6653\u4e1c\u8457\u7684chapter 3 dynamic programming\u4e2d\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\u4f5c\u4e3a\u4f8b\u5b50\u6765\u8bb2\u8ff0\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\uff0c\u4eca\u5929\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\u770b\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u8fd9\u4e09\u4e2a\u95ee\u9898\u5176\u5b9e\u662f\u5b58\u5728\u7740\u8fd9\u6837\u7684\u5171\u6027\uff1a\u5bf9**\u5e8f\u5217**\u8fdb\u884c\u64cd\u4f5c\uff1a \u77e9\u9635\u8fde\u4e58\u95ee\u9898\u662f\u5bf9**\u77e9\u9635\u94fe**\u8fdb\u884c\u64cd\u4f5c\uff08\u5c06\u77e9\u9635\u94fe\u5206\u5272\u4e3a\u5b50\u94fe\uff09 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u662f\u5bf9\u4e24\u4e2a\u5e8f\u5217\u8fdb\u884c\u64cd\u4f5c\uff08\u524d\u7f00\uff09 \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\u662f\u5bf9\u4e00\u4e2a\u6570\u4e32\u8fdb\u884c\u64cd\u4f5c \u8fd9\u4e09\u4e2a\u95ee\u9898\u7684\u5206\u6790\u5982\u4e0b\u3002 \u95ee\u9898\uff1a\u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898 \u95ee\u9898\u63cf\u8ff0 \u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u6b21\u5e8f\u95ee\u9898**\u89e3\u51b3\u7684\u662f\u591a\u4e2a\u77e9\u9635\u8fde\u4e58\u6b21\u6570\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u77e9\u9635\u770b\u505a\u662f\u4e00\u4e2a**\u77e9\u9635\u94fe \uff1b\u5bf9\u5b83\u8fdb\u884c\u9012\u5f52\u5c31\u53ef\u4ee5\u5c06\u539f\u957f\u7684\u77e9\u9635\u94fe\u5207\u5272\u4e3a\u4e24\u4e2a**\u5b50\u94fe**\uff0c\u7136\u540e\u5c06\u4e24\u4e2a**\u5b50\u94fe**\u76f8\u4e58\uff1b\u4e0d\u540c\u7684\u5207\u5272\u65b9\u6848\u5bfc\u81f4**\u5b50\u94fe**\u7684\u957f\u5ea6\u95ee\u9898\uff0c\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u94fe\u5b83\u4eec\u7684\u4e58\u6cd5\u6b21\u6570\u4e5f\u662f\u4e0d\u540c\u7684\uff1b\u53ef\u4ee5\u6839\u636e\u5b83\u5f97\u5230\u66f4\u957f\u7684**\u77e9\u9635\u94fe**\u7684\u4e58\u6cd5\u6b21\u6570\uff1b step1\u5206\u6790\u6700\u4f18\u89e3\u7684\u7ed3\u6784 \u5c06**\u77e9\u9635\u94fe**\u8bb0\u4e3a A_iA_{i+1} \\dots A_j A_iA_{i+1} \\dots A_j \uff0c\u7b80\u8bb0\u4e3a A[i:j] A[i:j] \u3002 \u8003\u5bdf\u8ba1\u7b97 A[1:n] A[1:n] \u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898\uff1a\u8bbe\u8fd9\u4e2a\u8ba1\u7b97\u6b21\u5e8f\u5728 A_k A_k \u548c A_{k+1} A_{k+1} \u4e4b\u95f4\u5c06**\u77e9\u9635\u94fe**\u65ad\u5f00\uff0c 1 \\le k \\lt n 1 \\le k \\lt n \uff0c\u5219\u5176\u76f8\u5e94\u7684\u5b8c\u5168\u52a0\u62ec\u53f7\u65b9\u5f0f\u4e3a$ (A_1 \\dots A_{k}) (A_{k+1} \\ldots A_n) \u3002\u4f9d\u7167\u6b64\u6b21\u5e8f\uff0c\u5148\u8ba1\u7b97 \u3002\u4f9d\u7167\u6b64\u6b21\u5e8f\uff0c\u5148\u8ba1\u7b97 A[1:k] \u548c \u548c A[k+1, n] \uff0c\u7136\u540e\u5c06\u8ba1\u7b97\u7ed3\u679c\u76f8\u4e58\u5f97\u5230 \uff0c\u7136\u540e\u5c06\u8ba1\u7b97\u7ed3\u679c\u76f8\u4e58\u5f97\u5230 A[1:n]$\u3002 \u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u5173\u952e\u7279\u5f81\u662f\uff1a\u8ba1\u7b97 A[1:n] A[1:n] \u6700\u4f18\u6b21\u5e8f\u6240\u5305\u542b\u7684\u8ba1\u7b97\u77e9\u9635\u5b50\u94fe A[1:k] A[1:k] \u548c A[k+1, n] A[k+1, n] \u7684\u6b21\u5e8f\u4e5f\u662f\u6700\u4f18\u7684\u3002\u56e0\u6b64\uff0c\u77e9\u9635\u8fde\u4e58\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898\u7684\u6700\u4f18\u89e3\u8574\u542b\u7740\u5176\u5b50\u95ee\u9898\u7684\u6700\u4f18\u89e3\uff0c\u8fd9\u79cd\u6027\u8d28\u79f0\u4e3a**\u6700\u4f18\u5b50\u7ed3\u6784\u6027\u8d28**\u3002 step2\u5efa\u7acb\u9012\u5f52\u5173\u7cfb \u8bbe\u8ba1**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u7684\u7b2c2\u6b65\u662f\u9012\u5f52\u5730\u5b9a\u4e49**\u6700\u4f18\u503c**\u3002\u8bbe A[i:j], 1 \\le i\\le j \\le n A[i:j], 1 \\le i\\le j \\le n \uff0c\u6240\u9700\u7684\u6700\u5c11\u4e58\u6cd5\u6b21\u6570\u662f m[i][j] m[i][j] \uff0c\u5219\u539f\u95ee\u9898\u7684**\u6700\u4f18\u503c**\u4e3a m[1][n] m[1][n] \u3002 $$ m[i][j] = \\begin{cases} 0, & i=j \\ \\min\\limits_{i \\le k \\lt j} { m[i][k] + m[k+i][j] + p_{i-1}p_k p_j } & i \\lt j \\end{cases} $$ m[i][j] m[i][j] \u7ed9\u51fa\u4e86\u6700\u4f18\u89e3\uff0c\u540c\u65f6\u8fd8\u786e\u5b9a\u4e86 A[i:j] A[i:j] \u7684\u6700\u4f18\u6b21\u5e8f\u4e2d\u7684\u65ad\u5f00\u4f4d\u7f6e k k \uff0c\u82e5\u5c06\u5bf9\u5e94\u4e8e m[i][j] m[i][j] \u7684\u65ad\u5f00\u4f4d\u7f6e k k \u8bb0\u4e3a s[i][j] s[i][j] \uff0c\u5728\u8ba1\u7b97\u51fa\u6700\u4f18\u503c m[i][j] m[i][j] \u540e\uff0c\u53ef\u9012\u5f52\u5730\u7531 s[i][j] s[i][j] \u6784\u9020\u51fa\u76f8\u5e94\u7684\u6700\u4f18\u89e3\u3002 SUMMARY : i=j i=j \u662f\u9012\u5f52\u5173\u7cfb\u4e2d\u7684base case\uff0c\u662f\u9012\u5f52\u7684\u7ec8\u6b62\u6761\u4ef6\uff1b \u5728\u77e9\u9635\u8fde\u4e58\u95ee\u9898\u4e2d\uff0c\u5982\u4f55\u6765\u5b9a\u4e49\u548c\u4fdd\u5b58\u5b50\u95ee\u9898\u7684\u89e3\u5462\uff1f\u4f7f\u7528\u4e8c\u7ef4\u8868\u6765\u4fdd\u5b58\u7684\u662f\u4e0d\u540c\u957f\u5ea6\u7684\u77e9\u9635\u94fe\u7684\u4e58\u6cd5\u6b21\u6570\uff0c\u4e8c\u7ef4\u8868\u7684\u7b2c\u4e00\u7ef4\u662f\u77e9\u9635\u94fe\u7684\u8d77\u59cb\u4f4d\u7f6e\uff0c\u7b2c\u4e8c\u7ef4\u662f\u77e9\u9635\u94fe\u7684\u7ec8\u6b62\u4f4d\u7f6e\uff1b step3\u8ba1\u7b97\u6700\u4f18\u503c \u8f93\u5165\u53c2\u6570 p_0, p_1, \\dots , p_n p_0, p_1, \\dots , p_n \u5b58\u50a8\u4e8e\u6570\u7ec4 p p \u4e2d\uff0c\u7b97\u6cd5\u9664\u4e86\u8f93\u51fa\u6700\u4f18\u503c\u6570\u7ec4 m \u5916\uff0c\u8fd8\u8f93\u51fa\u8bb0\u5f55\u6700\u4f18\u65ad\u5f00\u4f4d\u7f6e\u7684\u6570\u7ec4 s \u3002 void MatrixChain ( int * p , int n , int ** m , int ** s ){ for ( int i = 0 ; i < n ; ++ i ) m [ i ][ i ] = 0 ; int chain_length = 2 ; //\u77e9\u9635\u94fe\u7684\u957f\u5ea6 for (; chain_length <= n ; ++ chain_length ){ for ( int i = 0 ; i < n - chain_length ; ++ i ){ j = i + chain_length -1 ; //\u5b50\u94fe\u7684\u7ec8\u6b62\u4f4d\u7f6e // \u4e0b\u9762\u4f7f\u7528\u6253\u64c2\u53f0\u7684\u65b9\u5f0f\u9009\u51fa\u6700\u4f18\u89e3 m [ i ][ j ] = m [ i + 1 ][ j ] + p [ i -1 ] * p [ i ] * p [ j ]; s [ i ][ j ] = i ; for ( int k = i + 1 ; k < j ; ++ k ){ int t = m [ i ][ k ] + m [ k + 1 ][ j ] + p [ k -1 ] * p [ k ] * p [ j ]; if ( t < m [ i ][ j ]){ m [ i ][ j ] = t ; s [ i ][ j ] = k ; } } } } } \u65f6\u95f4\u590d\u6742\u5ea6\uff1a O(n^3) O(n^3) \u7a7a\u95f4\u590d\u6742\u5ea6\uff1a O(n^2) O(n^2) step4\u6784\u9020\u6700\u4f18\u89e3 s[i][j] s[i][j] \u8868\u793a\u7684\u662f\u8ba1\u7b97 A[i:j] A[i:j] \u7684\u6700\u4f73\u65ad\u5f00\u4f4d\u7f6e\uff1b\u6240\u4ee5\u77e9\u9635\u94fe A[1:n] A[1:n] \u7684\u6700\u4f73\u65ad\u5f00\u4f4d\u7f6e\u4e3a s[1][n] s[1][n] \uff0c\u4ece\u5b83\u53ef\u4ee5\u5f97\u5230\u4e24\u4e2a\u5b50\u94fe A[1:s[1][n]] A[1:s[1][n]] \u3001 A[s[1][n] + 1:n] A[s[1][n] + 1:n] \uff0c\u800c\u8fd9\u4e24\u4e2a\u5b50\u94fe\u7684\u65ad\u5f00\u4f4d\u7f6e\u53c8\u53ef\u4ee5\u901a\u8fc7\u67e5\u8868\u5f97\u5230\uff0c\u663e\u7136\u8fd9\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5982\u4e0b\u51fd\u6570\u53ef\u4ee5\u5b9e\u73b0\u5c06\u89e3\u8f93\u51fa\uff1a void TraceBack ( int i , int j , int ** s ){ if ( i == j ) return ; TraceBack ( i , s [ i ][ j ], s ); TraceBack ( s [ i ][ j ] + 1 , j , s ); cout << \"Multiply A\" << i << \",\" << s [ i ][ j ]; cout << \"and A\" << ( s [ i ][ j ] + 1 ) << \",\" << j << endl ; } SUMMARY : \u4e8c\u53c9\u6811\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22 \u95ee\u9898\uff1a\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217 \u95ee\u9898\u63cf\u8ff0 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217**\u89e3\u51b3\u7684\u662f**\u4e24\u4e2a**\u5e8f\u5217\u7684\u95ee\u9898\uff0c\u5bf9\u5b83\u8fdb\u884c**\u9012\u5f52**\u5c31\u53ef\u4ee5\u5c06\u539f\u95ee\u9898reduce\u5230\u4e24\u4e2a\u5b50\u5e8f\u5217\uff1b\u7531\u4e8e**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u81ea\u5e95\u5411\u4e0a\u5730\u8fd0\u7528**\u9012\u5f52\u5173\u7cfb \uff0c\u6240\u4ee5\u5b83\u9700\u8981\u9996\u5148\u8ba1\u7b97\u5b50\u95ee\u9898\uff0c\u7136\u540e\u7531\u5b50\u95ee\u9898\u7684\u89e3**\u63a8\u5bfc**\u51fa\u66f4\u5927\u7684\u95ee\u9898\u7684\u89e3\uff1b step1\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u7ed3\u6784\uff08\u5373\u89e3\u7684\u7ed3\u6784\uff09 \u8bbe\u5e8f\u5217 X=\\{ x_1, x_2, x_3, \\dots ,x_m \\} X=\\{ x_1, x_2, x_3, \\dots ,x_m \\} \u548c Y=\\{ x_1, y_2, y_3, \\dots ,y_n \\} Y=\\{ x_1, y_2, y_3, \\dots ,y_n \\} \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u4e3a Z=\\{z_1, z_2, z_3, \\dots ,z_k\\} Z=\\{z_1, z_2, z_3, \\dots ,z_k\\} \uff0c\u5219 \u82e5 x_m = y_n x_m = y_n \uff0c\u5219 z_k = x_m = y_n z_k = x_m = y_n \uff0c\u4e14 Z_{k-1} Z_{k-1} \u662f X_{m-1} X_{m-1} \u548c Y_{n-1} Y_{n-1} \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u82e5 x_m \\ne y_n x_m \\ne y_n \uff0c\u4e14 z_k \\ne x_m z_k \\ne x_m \uff0c\u5219 Z Z \u662f X_{m-1} X_{m-1} \u548c Y Y \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u82e5 x_m \\ne y_n x_m \\ne y_n \uff0c\u4e14 z_k \\ne y_n z_k \\ne y_n \uff0c\u5219 Z Z \u662f X X \u548c Y_{n-1} Y_{n-1} \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u7531\u6b64\u53ef\u89c1\uff1a\u4e24\u4e2a\u5e8f\u5217\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u8574\u542b\u7740\u8fd9\u4e24\u4e2a\u5e8f\u5217\u7684**\u524d\u7f00**\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff0c\u7531\u6b64**\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u95ee\u9898**\u5177\u6709**\u6700\u4f18\u5b50\u7ed3\u6784**\u6027\u8d28\u3002 step2\u5b50\u95ee\u9898\u7684\u9012\u5f52\u7ed3\u6784\uff08\u5373\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\uff09 \u5efa\u7acb\u5b50\u95ee\u9898\u6700\u4f18\u503c\u7684\u9012\u5f52\u5173\u7cfb\uff1a\u7528 c[i][j] c[i][j] \u8bb0\u5f55\u5e8f\u5217 X_i X_i \u548c Y_j Y_j \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5176\u4e2d X_i= \\{ x_1, x_2, \\dots , x_i \\} X_i= \\{ x_1, x_2, \\dots , x_i \\} \uff1b Y_j = \\{ y_1, y_2, \\dots , y_j \\} Y_j = \\{ y_1, y_2, \\dots , y_j \\} \u3002\u5f53 i=0 i=0 \u6216 j=0 j=0 \u65f6\uff0c\u7a7a\u5e8f\u5217\u662f X_i X_i \u548c Y_j Y_j \u7684**\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217**\uff0c\u6545\u6b64\u65f6 c[i][j]=0 c[i][j]=0 \uff08\u9012\u5f52\u5173\u7cfb\u7684base case\uff09\uff0c\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u6709\u6700\u4f18\u5b50\u7ed3\u6784\u7684\u6027\u8d28\u53ef\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\u5982\u4e0b\uff1a $$ c[i][j]= \\begin{cases} 0 & i=0, j=0 \\ c[i-1][j-1] + 1 & i,j \\gt 0 ; x_i = y_j \\ \\max{ c[i][j-1], c[i-1][j] } i,j \\gt 0 ;x_i \\ne y_j \\end{cases} $$ step3\u8ba1\u7b97\u6700\u4f18\u503c step4\u6784\u9020\u6700\u4f18\u89e3 Q&A Q:\u8ba1\u7b97**\u5b50\u95ee\u9898\u7684\u89e3**\uff0c\u5373\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217**\u5bf9**\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff0c\u7a0b\u5e8f\u4e2d\u9700\u8981\u5c06\u6240\u6709\u7684\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217\u7684**\u7ec4\u5408**\u60c5\u51b5\u90fd**\u679a\u4e3e**\u51fa\u6765\uff0c\u90a3\u4e00\u5171\u6709\u591a\u5c11\u79cd\u7ec4\u5408\u60c5\u51b5\u5462\uff1f\u8fd9\u8fd8\u771f\u662f\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\u3002 A:\u4e0a\u9762\u7684\u60f3\u6cd5\u662f\u9519\u8bef\u7684\uff0c\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u5b83\u5e76\u4e0d\u9700\u8981\u5c06\u5e8f\u5217A\u7684\u6240\u6709\u5b50\u5e8f\u5217\u548c\u5e8f\u5217B\u7684\u6240\u6709\u5b50\u5e8f\u5217\u8fdb\u884c\u7ec4\u5408\uff0c\u770b\u4e86\u5b83\u7684**\u9012\u5f52\u5173\u7cfb**\u4e0e\u7a0b\u5e8f\u7684\u5b9e\u73b0\uff0c\u5b83\u5e76\u6ca1\u6709\u679a\u4e3e\u51fa\u4e24\u4e2a\u5e8f\u5217\u6240\u6709\u7684\u5b50\u5e8f\u5217\u7684\u7ec4\u5408\uff1b \u4ece**\u9012\u5f52\u5173\u7cfb**\u6765\u770b\uff0c\u5b83\u662f\u4ece\u4e24\u4e2a\u5e8f\u5217\u672b\u7aef\u5373\u6700\u540e\u4e00\u4e2a\u5143\u7d20\u5f00\u59cb\uff0c\u6bcf\u6b21\u5265\u79bb\u4e00\u4e2a\u5b57\u7b26\uff1b \u4ece\u52a8\u6001\u89c4\u5212\u7684\u7a0b\u5e8f\u5b9e\u73b0\u6765\u770b\uff0c\u7531\u4e8e\u5b83\u662f\u4ece\u53f3\u81f3\u5de6\uff08\u81ea\u5e95\u5411\u4e0a\uff09\u5730\u5e94\u7528\u9012\u5f52\u5173\u7cfb\uff0c\u6240\u4ee5\u5b83\u662f\u5e8f\u5217\u7684\u7b2c\u4e00\u4e2a\u5b57\u7b26\u5f00\u59cb\uff0c\u6bcf\u6b21\u6dfb\u52a0\u4e00\u4e2a\u5b57\u7b26\uff0c\u76f4\u81f3\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5e8f\u5217\uff1b \u53e6\u5916\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u662f\uff0c\u5982\u4f55\u5c06\u4fdd\u5b58\u5b50\u95ee\u9898\u7684\u89e3\uff0c\u4f7f\u7528\u4ec0\u4e48\u6837\u7684\u6570\u636e\u7ed3\u6784\uff1f\u7531\u4e8e\u5b50\u95ee\u9898\u7684\u89e3\u662f\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217\u7684\u7ec4\u5408\uff0c\u4e00\u4e2a\u4e8c\u7ef4\u8868\u662f\u53ef\u4ee5\u6ee1\u8db3\u5b83\u7684\u9700\u6c42\u7684\uff0c\u5373\u4f7f\u7528\u4e00\u4e2a**\u4e8c\u7ef4\u8868**\u6765\u5c06**\u5b50\u95ee\u9898\u7684\u89e3**\u4fdd\u5b58\u8d77\u6765\uff0c\u4e8c\u7ef4\u8868\u7684\u7684\u7b2c\u4e00\u7ef4\u662f\u7b2c\u4e00\u4e2a\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u7b2c\u4e8c\u7ef4\u662f\u7b2c\u4e8c\u4e2a\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u4e8c\u7ef4\u8868\u8bb0\u5f55\u7684\u662f\u8fd9\u79cd\u7ec4\u5408\u4e0b\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff1b \u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u90fd\u662f\u5178\u578b\u7684\u5e94\u7528\u52a8\u6001\u89c4\u5212\u662f\u5426\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u7684\uff0c\u6211\u89c9\u5f97\u6211\u4e0d\u80fd\u591f\u4ec5\u4ec5\u5c40\u9650\u4e8e\u5b83\u4eec\uff0c\u800c\u5e94\u8be5\u5c06\u601d\u7ef4\u6253\u5f00\uff1a - \u9012\u5f52\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u8fd9\u662f\u89e3\u51b3\u95ee\u9898\u7684\u6839\u672c\u6240\u5728 - \u89e3\u7684\u8868\u793a\u4e0e\u8bb0\u5f55\uff1b\u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u90fd\u662f\u4f7f\u7528\u4e8c\u7ef4\u8868\uff0c\u5176\u5b9e\u5b83\u662f\u7531\u95ee\u9898\u800c\u51b3\u5b9a\u7684\uff0c\u5e76\u4e0d\u4e00\u5b9a\u662f\u6bcf\u79cd\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u90fd\u9700\u8981\u4f7f\u7528\uff0c\u6211\u60f3\u5bf9\u4e8e\u4e00\u4e9b\u66f4\u52a0\u590d\u6742\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u4f7f\u7528\u4e09\u7ef4\u8868\u7b49\uff0c\u6bd4\u5982\u6c42\u4e09\u4e2a\u5b57\u7b26\u4e32\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff1b \u5176\u5b9e\u7ecf\u8fc7\u4e0a\u8ff0\u5206\u6790\u53ef\u4ee5\u770b\u51fa\uff0c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5230\u4e86\u6700\u540e\u8fdb\u884c\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5b9e\u9645\u4e0a\u662f\u586b\u8868\uff1b \u95ee\u9898\uff1a\u6700\u5927\u5b57\u6bb5\u548c \u53c2\u8003\uff1a \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\uff1a\u86ee\u529b\u3001\u9012\u5f52\u53ca\u52a8\u6001\u89c4\u5212 \u95ee\u9898\u63cf\u8ff0 \u7ed9\u7684 n n \u4e2a\u6574\u6570\u7ec4\u6210\u7684\u5e8f\u5217 a_1, a_2, \\dots , a_n a_1, a_2, \\dots , a_n \uff0c\u6c42\u8be5\u5e8f\u5217\u5f62\u5982 \\sum_{k=i}^j {a_k} \\sum_{k=i}^j {a_k} \u7684\u5b50\u6bb5\u548c\u7684\u6700\u5927\u503c\u3002\u5f53\u6240\u6709\u6574\u6570\u5747\u4e3a\u8d1f\u6574\u6570\u65f6\uff0c\u5b9a\u4e49\u5176\u6700\u5927\u5b50\u6bb5\u548c\u4e3a0\u3002\u4f9d\u6b21\u5b9a\u4e49\uff0c\u6240\u6c42\u7684\u6700\u4f18\u503c\u4e3a\uff1a $$ \\max{ 0, \\max \\limits_{1 \\le i \\le j \\le n} \\sum_{k=i}^j a_k } $$ SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5b50\u5e8f\u5217\u662f\u8fde\u7eed\u7684\uff0c\u8fd9\u610f\u5473\u4e2d\uff0c\u5728\u9047\u5230\u4e00\u4e2a\u65b0\u7684\u5143\u7d20\u7684\u65f6\u5019\uff0c\u5fc5\u987b\u8981\u5c06\u5b83\u52a0\u5165\u5230\u5b50\u5e8f\u5217\u4e2d\uff0c\u4f46\u662f\u53ef\u9009\u7684\u662f\u5c06\u4e4b\u524d\u7684\u5b50\u5e8f\u5217\u7ed9\u629b\u5f03\u6389\uff1b O(n^3) O(n^3) \u89e3\u51b3\u7b97\u6cd5 \u601d\u60f3 \uff1a\u4ece\u5e8f\u5217\u9996\u5143\u7d20\u5f00\u59cb\u7a77\u4e3e\u6240\u6709\u53ef\u80fd\u7684\u5b50\u5e8f\u5217\u3002 #include <iostream> using namespace std ; int MaxSubsequenceSum ( const int array [], int n ) { int tempSum , maxSum ; maxSum = 0 ; for ( int i = 0 ; i < n ; i ++ ) // \u5b50\u5e8f\u5217\u8d77\u59cb\u4f4d\u7f6e { for ( int j = i ; j < n ; j ++ ) // \u5b50\u5e8f\u5217\u7ec8\u6b62\u4f4d\u7f6e { tempSum = 0 ; for ( int k = i ; k < j ; k ++ ) // \u5b50\u5e8f\u5217\u904d\u5386\u6c42\u548c tempSum += array [ k ]; if ( tempSum > maxSum ) // \u66f4\u65b0\u6700\u5927\u548c\u503c maxSum = tempSum ; } } return maxSum ; } int main () { const int a [] = { 4 , -3 , 5 , -2 , -1 , 2 , 6 , -2 }; int maxSubSum = MaxSubsequenceSum ( a , 8 ); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl ; system ( \"pause\" ); return 0 ; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3a CSDN\u535a\u4e3b \u300c SanFanCSgo \u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY - SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1a https : //blog.csdn.net/weixin_40170902/article/details/80585218 O(n^2) O(n^2) \u89e3\u51b3\u7b97\u6cd5 \u601d\u60f3\uff1a\u76f4\u63a5\u5728\u5212\u5b9a\u5b50\u5e8f\u5217\u65f6\u7d2f\u52a0\u5143\u7d20\u503c\uff0c\u51cf\u5c11\u4e00\u5c42\u5faa\u73af\u3002 #include <iostream> using namespace std ; int MaxSubsequenceSum ( const int array [], int n ) { int tempSum , maxSum ; maxSum = 0 ; for ( int i = 0 ; i < n ; i ++ ) { tempSum = 0 ; for ( int j = i ; j < n ; j ++ ) { tempSum += array [ j ]; if ( tempSum > maxSum ) maxSum = tempSum ; } } return maxSum ; } int main () { const int a [] = { 4 , -3 , 5 , -2 , -1 , 2 , 6 , -2 }; int maxSubSum = MaxSubsequenceSum ( a , 8 ); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl ; system ( \"pause\" ); return 0 ; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3a CSDN\u535a\u4e3b \u300c SanFanCSgo \u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY - SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1a https : //blog.csdn.net/weixin_40170902/article/details/80585218 O(n \\log n) O(n \\log n) \u89e3\u51b3\u7b97\u6cd5-\u4e8c\u5206\u6cd5 O(n) O(n) \u7b97\u6cd5-\u52a8\u6001\u89c4\u5212\u6cd5 \u82e5\u8bb0 b[j]=\\max \\limits_{1 \\le i \\le j} \\{ \\sum_{k=i}^{j} a[k] \\}, 1 \\le j \\le n b[j]=\\max \\limits_{1 \\le i \\le j} \\{ \\sum_{k=i}^{j} a[k] \\}, 1 \\le j \\le n \uff0c\u5219\u6240\u6c42\u7684\u6700\u5927\u5b50\u6bb5\u548c\u4e3a $$ \\max \\limits_{1 \\le i \\le j \\le n} \\sum_{k=i}^j a[k] = \\max \\limits_{1 \\le j \\le n} \\max \\limits_{1 \\le i \\le j } \\sum_{k=i}^j a[k] = \\max \\limits_{1 \\le j \\le n} b[j] $$ \u7531 b[j] b[j] \u7684\u5b9a\u4e49\u53ef\u77e5\uff0c\u5f53 b[j-1] \\gt 0 \u65f6\uff0c \u65f6\uff0c b[j] = b[j-1] + a[j]b[j] = b[j-1] + a[j] b[j-1] \\gt 0 <span><span class=\"MathJax_Preview\">\u65f6\uff0c</span><script type=\"math/tex\">\u65f6\uff0c b[j] = b[j-1] + a[j]b[j] = b[j-1] + a[j] \uff0c\u5426\u5219 b[j] = a[j] b[j] = a[j] \u3002\u7531\u6b64\u53ef\u77e5\u8ba1\u7b97 b[j] b[j] \u7684\u52a8\u6001\u89c4\u5212\u9012\u5f52\u5f0f\uff1a $$ b[j] = \\max{b[j-1] + a[j], a[j] }, 1 \\le j \\le n $$ SUMMARY : \u9012\u5f52\u5173\u7cfb b[j] = b[j-1] + a[j] b[j] = b[j-1] + a[j] \u4e0e\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u975e\u5e38\u7c7b\u4f3c\uff1b b[j] b[j] \u7684\u8ba1\u7b97\u4ec5\u4ec5\u4f9d\u8d56\u4e8e b[j-1] b[j-1] \uff0c\u5c31\u5982 Fibonacci sequence \u7684\u8ba1\u7b97\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u524d\u4e24\u9879\u4e00\u6837\uff1b SUMMARY : \u6b63\u5982\u5728 \u753b\u89e3\u7b97\u6cd5\uff1a53. \u6700\u5927\u5b50\u5e8f\u548c \u4e2d\u6240\u8bf4\u7684\uff1a $b[j-1] \\gt 0 $ \u8bf4\u660e b[j-1] b[j-1] \u5bf9\u7ed3\u679c\u6709\u589e\u76ca\u6548\u679c\uff0c\u5219 b[j-1] b[j-1] \u4fdd\u7559\u5e76\u52a0\u4e0a\u5f53\u524d\u904d\u5386\u6570\u5b57 \u5982\u679c b[j-1] \\le 0 b[j-1] \\le 0 \u5219\u8bf4\u660e\u5b83\u5bf9\u7ed3\u679c\u5e76\u6ca1\u6709\u589e\u76ca\uff0c\u9700\u8981\u820d\u5f03\uff0c \u5219 b[j] b[j] \u76f4\u63a5\u66f4\u65b0\u4e3a\u5f53\u524d\u904d\u5386\u6570\u5b57 \u636e\u6b64\uff0c\u53ef\u8bbe\u8ba1\u51fa\u6c42\u6700\u5927\u5b50\u6bb5\u548c\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5982\u4e0b\uff1a int MaxSum ( int n , int * a ){ int sum = 0 , b = 0 ; int start = 0 , end = 0 ; //\u8bb0\u5f55\u4e0b\u5b50\u5e8f\u5217\u7684\u8d77\u59cb\u548c\u7ec8\u6b62\u4f4d\u7f6e for ( int i = 1 ; i <= n ; ++ i ){ if ( b > 0 ) b += a [ i ]; else { b = a [ i ]; start = i ; } if ( b > sum ) { sum = b ; end = i ; } } } \u5728 \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\uff1a\u86ee\u529b\u3001\u9012\u5f52\u53ca\u52a8\u6001\u89c4\u5212 \u4e2d\u7ed9\u51fa\u7684\u7a0b\u5e8f\u662f\u8fd9\u6837\u7684\uff1a #include <iostream> using namespace std ; int MaxSubsequenceSum ( const int A [], int n ) { int tempSum = 0 ; int maxSum = 0 ; for ( int j = 0 ; j < n ; j ++ ) // \u5b50\u95ee\u9898\u540e\u8fb9\u754c { tempSum = ( tempSum + A [ j ]) > A [ j ] ? ( tempSum + A [ j ]) : A [ j ]; if ( tempSum > maxSum ) // \u66f4\u65b0\u6700\u5927\u548c maxSum = tempSum ; } return maxSum ; } int main () { const int a [] = { 4 , -3 , 5 , -2 , -1 , 2 , 6 , -2 }; int maxSubSum = MaxSubsequenceSum ( a , 8 ); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl ; system ( \"pause\" ); return 0 ; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3a CSDN\u535a\u4e3b \u300c SanFanCSgo \u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY - SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1a https : //blog.csdn.net/weixin_40170902/article/details/80585218 \u8fd9\u79cd\u5b9e\u73b0\u548c\u4e0a\u9762\u7684\u90a3\u79cd\u5b9e\u73b0\u662f\u5b8c\u5168\u4e0d\u540c\u7684\uff1b summary \u4e0a\u8ff0\u7684\u6240\u6709\u7a0b\u5e8f\u90fd\u662f\u5bf9\u89e3\u51b3\u95ee\u9898\u7684\u6570\u5b66\u516c\u5f0f\u7684\u63cf\u8ff0\uff0c\u6240\u4ee5\u5728\u7b97\u6cd5\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u5c31\u662f\uff1a \u4ece\u6570\u5b66\u516c\u5f0f\u5230\u7a0b\u5e8f","title":"VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#_1","text":"\u5728\u300a\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790\u300b\u7b2c\u56db\u7248\u738b\u6653\u4e1c\u8457\u7684chapter 3 dynamic programming\u4e2d\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\u4f5c\u4e3a\u4f8b\u5b50\u6765\u8bb2\u8ff0\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\uff0c\u4eca\u5929\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\u770b\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u8fd9\u4e09\u4e2a\u95ee\u9898\u5176\u5b9e\u662f\u5b58\u5728\u7740\u8fd9\u6837\u7684\u5171\u6027\uff1a\u5bf9**\u5e8f\u5217**\u8fdb\u884c\u64cd\u4f5c\uff1a \u77e9\u9635\u8fde\u4e58\u95ee\u9898\u662f\u5bf9**\u77e9\u9635\u94fe**\u8fdb\u884c\u64cd\u4f5c\uff08\u5c06\u77e9\u9635\u94fe\u5206\u5272\u4e3a\u5b50\u94fe\uff09 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u662f\u5bf9\u4e24\u4e2a\u5e8f\u5217\u8fdb\u884c\u64cd\u4f5c\uff08\u524d\u7f00\uff09 \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\u662f\u5bf9\u4e00\u4e2a\u6570\u4e32\u8fdb\u884c\u64cd\u4f5c \u8fd9\u4e09\u4e2a\u95ee\u9898\u7684\u5206\u6790\u5982\u4e0b\u3002","title":"\u524d\u8a00"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#_2","text":"","title":"\u95ee\u9898\uff1a\u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#_3","text":"\u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u6b21\u5e8f\u95ee\u9898**\u89e3\u51b3\u7684\u662f\u591a\u4e2a\u77e9\u9635\u8fde\u4e58\u6b21\u6570\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u77e9\u9635\u770b\u505a\u662f\u4e00\u4e2a**\u77e9\u9635\u94fe \uff1b\u5bf9\u5b83\u8fdb\u884c\u9012\u5f52\u5c31\u53ef\u4ee5\u5c06\u539f\u957f\u7684\u77e9\u9635\u94fe\u5207\u5272\u4e3a\u4e24\u4e2a**\u5b50\u94fe**\uff0c\u7136\u540e\u5c06\u4e24\u4e2a**\u5b50\u94fe**\u76f8\u4e58\uff1b\u4e0d\u540c\u7684\u5207\u5272\u65b9\u6848\u5bfc\u81f4**\u5b50\u94fe**\u7684\u957f\u5ea6\u95ee\u9898\uff0c\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u94fe\u5b83\u4eec\u7684\u4e58\u6cd5\u6b21\u6570\u4e5f\u662f\u4e0d\u540c\u7684\uff1b\u53ef\u4ee5\u6839\u636e\u5b83\u5f97\u5230\u66f4\u957f\u7684**\u77e9\u9635\u94fe**\u7684\u4e58\u6cd5\u6b21\u6570\uff1b","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#step1","text":"\u5c06**\u77e9\u9635\u94fe**\u8bb0\u4e3a A_iA_{i+1} \\dots A_j A_iA_{i+1} \\dots A_j \uff0c\u7b80\u8bb0\u4e3a A[i:j] A[i:j] \u3002 \u8003\u5bdf\u8ba1\u7b97 A[1:n] A[1:n] \u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898\uff1a\u8bbe\u8fd9\u4e2a\u8ba1\u7b97\u6b21\u5e8f\u5728 A_k A_k \u548c A_{k+1} A_{k+1} \u4e4b\u95f4\u5c06**\u77e9\u9635\u94fe**\u65ad\u5f00\uff0c 1 \\le k \\lt n 1 \\le k \\lt n \uff0c\u5219\u5176\u76f8\u5e94\u7684\u5b8c\u5168\u52a0\u62ec\u53f7\u65b9\u5f0f\u4e3a$ (A_1 \\dots A_{k}) (A_{k+1} \\ldots A_n) \u3002\u4f9d\u7167\u6b64\u6b21\u5e8f\uff0c\u5148\u8ba1\u7b97 \u3002\u4f9d\u7167\u6b64\u6b21\u5e8f\uff0c\u5148\u8ba1\u7b97 A[1:k] \u548c \u548c A[k+1, n] \uff0c\u7136\u540e\u5c06\u8ba1\u7b97\u7ed3\u679c\u76f8\u4e58\u5f97\u5230 \uff0c\u7136\u540e\u5c06\u8ba1\u7b97\u7ed3\u679c\u76f8\u4e58\u5f97\u5230 A[1:n]$\u3002 \u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u5173\u952e\u7279\u5f81\u662f\uff1a\u8ba1\u7b97 A[1:n] A[1:n] \u6700\u4f18\u6b21\u5e8f\u6240\u5305\u542b\u7684\u8ba1\u7b97\u77e9\u9635\u5b50\u94fe A[1:k] A[1:k] \u548c A[k+1, n] A[k+1, n] \u7684\u6b21\u5e8f\u4e5f\u662f\u6700\u4f18\u7684\u3002\u56e0\u6b64\uff0c\u77e9\u9635\u8fde\u4e58\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898\u7684\u6700\u4f18\u89e3\u8574\u542b\u7740\u5176\u5b50\u95ee\u9898\u7684\u6700\u4f18\u89e3\uff0c\u8fd9\u79cd\u6027\u8d28\u79f0\u4e3a**\u6700\u4f18\u5b50\u7ed3\u6784\u6027\u8d28**\u3002","title":"step1\u5206\u6790\u6700\u4f18\u89e3\u7684\u7ed3\u6784"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#step2","text":"\u8bbe\u8ba1**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u7684\u7b2c2\u6b65\u662f\u9012\u5f52\u5730\u5b9a\u4e49**\u6700\u4f18\u503c**\u3002\u8bbe A[i:j], 1 \\le i\\le j \\le n A[i:j], 1 \\le i\\le j \\le n \uff0c\u6240\u9700\u7684\u6700\u5c11\u4e58\u6cd5\u6b21\u6570\u662f m[i][j] m[i][j] \uff0c\u5219\u539f\u95ee\u9898\u7684**\u6700\u4f18\u503c**\u4e3a m[1][n] m[1][n] \u3002 $$ m[i][j] = \\begin{cases} 0, & i=j \\ \\min\\limits_{i \\le k \\lt j} { m[i][k] + m[k+i][j] + p_{i-1}p_k p_j } & i \\lt j \\end{cases} $$ m[i][j] m[i][j] \u7ed9\u51fa\u4e86\u6700\u4f18\u89e3\uff0c\u540c\u65f6\u8fd8\u786e\u5b9a\u4e86 A[i:j] A[i:j] \u7684\u6700\u4f18\u6b21\u5e8f\u4e2d\u7684\u65ad\u5f00\u4f4d\u7f6e k k \uff0c\u82e5\u5c06\u5bf9\u5e94\u4e8e m[i][j] m[i][j] \u7684\u65ad\u5f00\u4f4d\u7f6e k k \u8bb0\u4e3a s[i][j] s[i][j] \uff0c\u5728\u8ba1\u7b97\u51fa\u6700\u4f18\u503c m[i][j] m[i][j] \u540e\uff0c\u53ef\u9012\u5f52\u5730\u7531 s[i][j] s[i][j] \u6784\u9020\u51fa\u76f8\u5e94\u7684\u6700\u4f18\u89e3\u3002 SUMMARY : i=j i=j \u662f\u9012\u5f52\u5173\u7cfb\u4e2d\u7684base case\uff0c\u662f\u9012\u5f52\u7684\u7ec8\u6b62\u6761\u4ef6\uff1b \u5728\u77e9\u9635\u8fde\u4e58\u95ee\u9898\u4e2d\uff0c\u5982\u4f55\u6765\u5b9a\u4e49\u548c\u4fdd\u5b58\u5b50\u95ee\u9898\u7684\u89e3\u5462\uff1f\u4f7f\u7528\u4e8c\u7ef4\u8868\u6765\u4fdd\u5b58\u7684\u662f\u4e0d\u540c\u957f\u5ea6\u7684\u77e9\u9635\u94fe\u7684\u4e58\u6cd5\u6b21\u6570\uff0c\u4e8c\u7ef4\u8868\u7684\u7b2c\u4e00\u7ef4\u662f\u77e9\u9635\u94fe\u7684\u8d77\u59cb\u4f4d\u7f6e\uff0c\u7b2c\u4e8c\u7ef4\u662f\u77e9\u9635\u94fe\u7684\u7ec8\u6b62\u4f4d\u7f6e\uff1b","title":"step2\u5efa\u7acb\u9012\u5f52\u5173\u7cfb"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#step3","text":"\u8f93\u5165\u53c2\u6570 p_0, p_1, \\dots , p_n p_0, p_1, \\dots , p_n \u5b58\u50a8\u4e8e\u6570\u7ec4 p p \u4e2d\uff0c\u7b97\u6cd5\u9664\u4e86\u8f93\u51fa\u6700\u4f18\u503c\u6570\u7ec4 m \u5916\uff0c\u8fd8\u8f93\u51fa\u8bb0\u5f55\u6700\u4f18\u65ad\u5f00\u4f4d\u7f6e\u7684\u6570\u7ec4 s \u3002 void MatrixChain ( int * p , int n , int ** m , int ** s ){ for ( int i = 0 ; i < n ; ++ i ) m [ i ][ i ] = 0 ; int chain_length = 2 ; //\u77e9\u9635\u94fe\u7684\u957f\u5ea6 for (; chain_length <= n ; ++ chain_length ){ for ( int i = 0 ; i < n - chain_length ; ++ i ){ j = i + chain_length -1 ; //\u5b50\u94fe\u7684\u7ec8\u6b62\u4f4d\u7f6e // \u4e0b\u9762\u4f7f\u7528\u6253\u64c2\u53f0\u7684\u65b9\u5f0f\u9009\u51fa\u6700\u4f18\u89e3 m [ i ][ j ] = m [ i + 1 ][ j ] + p [ i -1 ] * p [ i ] * p [ j ]; s [ i ][ j ] = i ; for ( int k = i + 1 ; k < j ; ++ k ){ int t = m [ i ][ k ] + m [ k + 1 ][ j ] + p [ k -1 ] * p [ k ] * p [ j ]; if ( t < m [ i ][ j ]){ m [ i ][ j ] = t ; s [ i ][ j ] = k ; } } } } } \u65f6\u95f4\u590d\u6742\u5ea6\uff1a O(n^3) O(n^3) \u7a7a\u95f4\u590d\u6742\u5ea6\uff1a O(n^2) O(n^2)","title":"step3\u8ba1\u7b97\u6700\u4f18\u503c"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#step4","text":"s[i][j] s[i][j] \u8868\u793a\u7684\u662f\u8ba1\u7b97 A[i:j] A[i:j] \u7684\u6700\u4f73\u65ad\u5f00\u4f4d\u7f6e\uff1b\u6240\u4ee5\u77e9\u9635\u94fe A[1:n] A[1:n] \u7684\u6700\u4f73\u65ad\u5f00\u4f4d\u7f6e\u4e3a s[1][n] s[1][n] \uff0c\u4ece\u5b83\u53ef\u4ee5\u5f97\u5230\u4e24\u4e2a\u5b50\u94fe A[1:s[1][n]] A[1:s[1][n]] \u3001 A[s[1][n] + 1:n] A[s[1][n] + 1:n] \uff0c\u800c\u8fd9\u4e24\u4e2a\u5b50\u94fe\u7684\u65ad\u5f00\u4f4d\u7f6e\u53c8\u53ef\u4ee5\u901a\u8fc7\u67e5\u8868\u5f97\u5230\uff0c\u663e\u7136\u8fd9\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5982\u4e0b\u51fd\u6570\u53ef\u4ee5\u5b9e\u73b0\u5c06\u89e3\u8f93\u51fa\uff1a void TraceBack ( int i , int j , int ** s ){ if ( i == j ) return ; TraceBack ( i , s [ i ][ j ], s ); TraceBack ( s [ i ][ j ] + 1 , j , s ); cout << \"Multiply A\" << i << \",\" << s [ i ][ j ]; cout << \"and A\" << ( s [ i ][ j ] + 1 ) << \",\" << j << endl ; } SUMMARY : \u4e8c\u53c9\u6811\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22","title":"step4\u6784\u9020\u6700\u4f18\u89e3"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#_4","text":"","title":"\u95ee\u9898\uff1a\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#_5","text":"\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217**\u89e3\u51b3\u7684\u662f**\u4e24\u4e2a**\u5e8f\u5217\u7684\u95ee\u9898\uff0c\u5bf9\u5b83\u8fdb\u884c**\u9012\u5f52**\u5c31\u53ef\u4ee5\u5c06\u539f\u95ee\u9898reduce\u5230\u4e24\u4e2a\u5b50\u5e8f\u5217\uff1b\u7531\u4e8e**\u52a8\u6001\u89c4\u5212\u7b97\u6cd5**\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u81ea\u5e95\u5411\u4e0a\u5730\u8fd0\u7528**\u9012\u5f52\u5173\u7cfb \uff0c\u6240\u4ee5\u5b83\u9700\u8981\u9996\u5148\u8ba1\u7b97\u5b50\u95ee\u9898\uff0c\u7136\u540e\u7531\u5b50\u95ee\u9898\u7684\u89e3**\u63a8\u5bfc**\u51fa\u66f4\u5927\u7684\u95ee\u9898\u7684\u89e3\uff1b","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#step1_1","text":"\u8bbe\u5e8f\u5217 X=\\{ x_1, x_2, x_3, \\dots ,x_m \\} X=\\{ x_1, x_2, x_3, \\dots ,x_m \\} \u548c Y=\\{ x_1, y_2, y_3, \\dots ,y_n \\} Y=\\{ x_1, y_2, y_3, \\dots ,y_n \\} \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u4e3a Z=\\{z_1, z_2, z_3, \\dots ,z_k\\} Z=\\{z_1, z_2, z_3, \\dots ,z_k\\} \uff0c\u5219 \u82e5 x_m = y_n x_m = y_n \uff0c\u5219 z_k = x_m = y_n z_k = x_m = y_n \uff0c\u4e14 Z_{k-1} Z_{k-1} \u662f X_{m-1} X_{m-1} \u548c Y_{n-1} Y_{n-1} \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u82e5 x_m \\ne y_n x_m \\ne y_n \uff0c\u4e14 z_k \\ne x_m z_k \\ne x_m \uff0c\u5219 Z Z \u662f X_{m-1} X_{m-1} \u548c Y Y \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u82e5 x_m \\ne y_n x_m \\ne y_n \uff0c\u4e14 z_k \\ne y_n z_k \\ne y_n \uff0c\u5219 Z Z \u662f X X \u548c Y_{n-1} Y_{n-1} \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u7531\u6b64\u53ef\u89c1\uff1a\u4e24\u4e2a\u5e8f\u5217\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u8574\u542b\u7740\u8fd9\u4e24\u4e2a\u5e8f\u5217\u7684**\u524d\u7f00**\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff0c\u7531\u6b64**\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u95ee\u9898**\u5177\u6709**\u6700\u4f18\u5b50\u7ed3\u6784**\u6027\u8d28\u3002","title":"step1\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u7ed3\u6784\uff08\u5373\u89e3\u7684\u7ed3\u6784\uff09"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#step2_1","text":"\u5efa\u7acb\u5b50\u95ee\u9898\u6700\u4f18\u503c\u7684\u9012\u5f52\u5173\u7cfb\uff1a\u7528 c[i][j] c[i][j] \u8bb0\u5f55\u5e8f\u5217 X_i X_i \u548c Y_j Y_j \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5176\u4e2d X_i= \\{ x_1, x_2, \\dots , x_i \\} X_i= \\{ x_1, x_2, \\dots , x_i \\} \uff1b Y_j = \\{ y_1, y_2, \\dots , y_j \\} Y_j = \\{ y_1, y_2, \\dots , y_j \\} \u3002\u5f53 i=0 i=0 \u6216 j=0 j=0 \u65f6\uff0c\u7a7a\u5e8f\u5217\u662f X_i X_i \u548c Y_j Y_j \u7684**\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217**\uff0c\u6545\u6b64\u65f6 c[i][j]=0 c[i][j]=0 \uff08\u9012\u5f52\u5173\u7cfb\u7684base case\uff09\uff0c\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u6709\u6700\u4f18\u5b50\u7ed3\u6784\u7684\u6027\u8d28\u53ef\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\u5982\u4e0b\uff1a $$ c[i][j]= \\begin{cases} 0 & i=0, j=0 \\ c[i-1][j-1] + 1 & i,j \\gt 0 ; x_i = y_j \\ \\max{ c[i][j-1], c[i-1][j] } i,j \\gt 0 ;x_i \\ne y_j \\end{cases} $$","title":"step2\u5b50\u95ee\u9898\u7684\u9012\u5f52\u7ed3\u6784\uff08\u5373\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\uff09"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#step3_1","text":"","title":"step3\u8ba1\u7b97\u6700\u4f18\u503c"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#step4_1","text":"","title":"step4\u6784\u9020\u6700\u4f18\u89e3"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#qa","text":"Q:\u8ba1\u7b97**\u5b50\u95ee\u9898\u7684\u89e3**\uff0c\u5373\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217**\u5bf9**\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff0c\u7a0b\u5e8f\u4e2d\u9700\u8981\u5c06\u6240\u6709\u7684\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217\u7684**\u7ec4\u5408**\u60c5\u51b5\u90fd**\u679a\u4e3e**\u51fa\u6765\uff0c\u90a3\u4e00\u5171\u6709\u591a\u5c11\u79cd\u7ec4\u5408\u60c5\u51b5\u5462\uff1f\u8fd9\u8fd8\u771f\u662f\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\u3002 A:\u4e0a\u9762\u7684\u60f3\u6cd5\u662f\u9519\u8bef\u7684\uff0c\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u5b83\u5e76\u4e0d\u9700\u8981\u5c06\u5e8f\u5217A\u7684\u6240\u6709\u5b50\u5e8f\u5217\u548c\u5e8f\u5217B\u7684\u6240\u6709\u5b50\u5e8f\u5217\u8fdb\u884c\u7ec4\u5408\uff0c\u770b\u4e86\u5b83\u7684**\u9012\u5f52\u5173\u7cfb**\u4e0e\u7a0b\u5e8f\u7684\u5b9e\u73b0\uff0c\u5b83\u5e76\u6ca1\u6709\u679a\u4e3e\u51fa\u4e24\u4e2a\u5e8f\u5217\u6240\u6709\u7684\u5b50\u5e8f\u5217\u7684\u7ec4\u5408\uff1b \u4ece**\u9012\u5f52\u5173\u7cfb**\u6765\u770b\uff0c\u5b83\u662f\u4ece\u4e24\u4e2a\u5e8f\u5217\u672b\u7aef\u5373\u6700\u540e\u4e00\u4e2a\u5143\u7d20\u5f00\u59cb\uff0c\u6bcf\u6b21\u5265\u79bb\u4e00\u4e2a\u5b57\u7b26\uff1b \u4ece\u52a8\u6001\u89c4\u5212\u7684\u7a0b\u5e8f\u5b9e\u73b0\u6765\u770b\uff0c\u7531\u4e8e\u5b83\u662f\u4ece\u53f3\u81f3\u5de6\uff08\u81ea\u5e95\u5411\u4e0a\uff09\u5730\u5e94\u7528\u9012\u5f52\u5173\u7cfb\uff0c\u6240\u4ee5\u5b83\u662f\u5e8f\u5217\u7684\u7b2c\u4e00\u4e2a\u5b57\u7b26\u5f00\u59cb\uff0c\u6bcf\u6b21\u6dfb\u52a0\u4e00\u4e2a\u5b57\u7b26\uff0c\u76f4\u81f3\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5e8f\u5217\uff1b \u53e6\u5916\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u662f\uff0c\u5982\u4f55\u5c06\u4fdd\u5b58\u5b50\u95ee\u9898\u7684\u89e3\uff0c\u4f7f\u7528\u4ec0\u4e48\u6837\u7684\u6570\u636e\u7ed3\u6784\uff1f\u7531\u4e8e\u5b50\u95ee\u9898\u7684\u89e3\u662f\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217\u7684\u7ec4\u5408\uff0c\u4e00\u4e2a\u4e8c\u7ef4\u8868\u662f\u53ef\u4ee5\u6ee1\u8db3\u5b83\u7684\u9700\u6c42\u7684\uff0c\u5373\u4f7f\u7528\u4e00\u4e2a**\u4e8c\u7ef4\u8868**\u6765\u5c06**\u5b50\u95ee\u9898\u7684\u89e3**\u4fdd\u5b58\u8d77\u6765\uff0c\u4e8c\u7ef4\u8868\u7684\u7684\u7b2c\u4e00\u7ef4\u662f\u7b2c\u4e00\u4e2a\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u7b2c\u4e8c\u7ef4\u662f\u7b2c\u4e8c\u4e2a\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u4e8c\u7ef4\u8868\u8bb0\u5f55\u7684\u662f\u8fd9\u79cd\u7ec4\u5408\u4e0b\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff1b \u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u90fd\u662f\u5178\u578b\u7684\u5e94\u7528\u52a8\u6001\u89c4\u5212\u662f\u5426\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u7684\uff0c\u6211\u89c9\u5f97\u6211\u4e0d\u80fd\u591f\u4ec5\u4ec5\u5c40\u9650\u4e8e\u5b83\u4eec\uff0c\u800c\u5e94\u8be5\u5c06\u601d\u7ef4\u6253\u5f00\uff1a - \u9012\u5f52\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u8fd9\u662f\u89e3\u51b3\u95ee\u9898\u7684\u6839\u672c\u6240\u5728 - \u89e3\u7684\u8868\u793a\u4e0e\u8bb0\u5f55\uff1b\u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u90fd\u662f\u4f7f\u7528\u4e8c\u7ef4\u8868\uff0c\u5176\u5b9e\u5b83\u662f\u7531\u95ee\u9898\u800c\u51b3\u5b9a\u7684\uff0c\u5e76\u4e0d\u4e00\u5b9a\u662f\u6bcf\u79cd\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u90fd\u9700\u8981\u4f7f\u7528\uff0c\u6211\u60f3\u5bf9\u4e8e\u4e00\u4e9b\u66f4\u52a0\u590d\u6742\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u4f7f\u7528\u4e09\u7ef4\u8868\u7b49\uff0c\u6bd4\u5982\u6c42\u4e09\u4e2a\u5b57\u7b26\u4e32\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff1b \u5176\u5b9e\u7ecf\u8fc7\u4e0a\u8ff0\u5206\u6790\u53ef\u4ee5\u770b\u51fa\uff0c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5230\u4e86\u6700\u540e\u8fdb\u884c\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5b9e\u9645\u4e0a\u662f\u586b\u8868\uff1b","title":"Q&amp;A"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#_6","text":"\u53c2\u8003\uff1a \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\uff1a\u86ee\u529b\u3001\u9012\u5f52\u53ca\u52a8\u6001\u89c4\u5212","title":"\u95ee\u9898\uff1a\u6700\u5927\u5b57\u6bb5\u548c"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#_7","text":"\u7ed9\u7684 n n \u4e2a\u6574\u6570\u7ec4\u6210\u7684\u5e8f\u5217 a_1, a_2, \\dots , a_n a_1, a_2, \\dots , a_n \uff0c\u6c42\u8be5\u5e8f\u5217\u5f62\u5982 \\sum_{k=i}^j {a_k} \\sum_{k=i}^j {a_k} \u7684\u5b50\u6bb5\u548c\u7684\u6700\u5927\u503c\u3002\u5f53\u6240\u6709\u6574\u6570\u5747\u4e3a\u8d1f\u6574\u6570\u65f6\uff0c\u5b9a\u4e49\u5176\u6700\u5927\u5b50\u6bb5\u548c\u4e3a0\u3002\u4f9d\u6b21\u5b9a\u4e49\uff0c\u6240\u6c42\u7684\u6700\u4f18\u503c\u4e3a\uff1a $$ \\max{ 0, \\max \\limits_{1 \\le i \\le j \\le n} \\sum_{k=i}^j a_k } $$ SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5b50\u5e8f\u5217\u662f\u8fde\u7eed\u7684\uff0c\u8fd9\u610f\u5473\u4e2d\uff0c\u5728\u9047\u5230\u4e00\u4e2a\u65b0\u7684\u5143\u7d20\u7684\u65f6\u5019\uff0c\u5fc5\u987b\u8981\u5c06\u5b83\u52a0\u5165\u5230\u5b50\u5e8f\u5217\u4e2d\uff0c\u4f46\u662f\u53ef\u9009\u7684\u662f\u5c06\u4e4b\u524d\u7684\u5b50\u5e8f\u5217\u7ed9\u629b\u5f03\u6389\uff1b","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#on3on3","text":"\u601d\u60f3 \uff1a\u4ece\u5e8f\u5217\u9996\u5143\u7d20\u5f00\u59cb\u7a77\u4e3e\u6240\u6709\u53ef\u80fd\u7684\u5b50\u5e8f\u5217\u3002 #include <iostream> using namespace std ; int MaxSubsequenceSum ( const int array [], int n ) { int tempSum , maxSum ; maxSum = 0 ; for ( int i = 0 ; i < n ; i ++ ) // \u5b50\u5e8f\u5217\u8d77\u59cb\u4f4d\u7f6e { for ( int j = i ; j < n ; j ++ ) // \u5b50\u5e8f\u5217\u7ec8\u6b62\u4f4d\u7f6e { tempSum = 0 ; for ( int k = i ; k < j ; k ++ ) // \u5b50\u5e8f\u5217\u904d\u5386\u6c42\u548c tempSum += array [ k ]; if ( tempSum > maxSum ) // \u66f4\u65b0\u6700\u5927\u548c\u503c maxSum = tempSum ; } } return maxSum ; } int main () { const int a [] = { 4 , -3 , 5 , -2 , -1 , 2 , 6 , -2 }; int maxSubSum = MaxSubsequenceSum ( a , 8 ); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl ; system ( \"pause\" ); return 0 ; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3a CSDN\u535a\u4e3b \u300c SanFanCSgo \u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY - SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1a https : //blog.csdn.net/weixin_40170902/article/details/80585218","title":"O(n^3)O(n^3)\u89e3\u51b3\u7b97\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#on2on2","text":"\u601d\u60f3\uff1a\u76f4\u63a5\u5728\u5212\u5b9a\u5b50\u5e8f\u5217\u65f6\u7d2f\u52a0\u5143\u7d20\u503c\uff0c\u51cf\u5c11\u4e00\u5c42\u5faa\u73af\u3002 #include <iostream> using namespace std ; int MaxSubsequenceSum ( const int array [], int n ) { int tempSum , maxSum ; maxSum = 0 ; for ( int i = 0 ; i < n ; i ++ ) { tempSum = 0 ; for ( int j = i ; j < n ; j ++ ) { tempSum += array [ j ]; if ( tempSum > maxSum ) maxSum = tempSum ; } } return maxSum ; } int main () { const int a [] = { 4 , -3 , 5 , -2 , -1 , 2 , 6 , -2 }; int maxSubSum = MaxSubsequenceSum ( a , 8 ); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl ; system ( \"pause\" ); return 0 ; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3a CSDN\u535a\u4e3b \u300c SanFanCSgo \u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY - SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1a https : //blog.csdn.net/weixin_40170902/article/details/80585218","title":"O(n^2)O(n^2)\u89e3\u51b3\u7b97\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#on#log#non#log#n-","text":"","title":"O(n \\log n)O(n \\log n)\u89e3\u51b3\u7b97\u6cd5-\u4e8c\u5206\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#onon-","text":"\u82e5\u8bb0 b[j]=\\max \\limits_{1 \\le i \\le j} \\{ \\sum_{k=i}^{j} a[k] \\}, 1 \\le j \\le n b[j]=\\max \\limits_{1 \\le i \\le j} \\{ \\sum_{k=i}^{j} a[k] \\}, 1 \\le j \\le n \uff0c\u5219\u6240\u6c42\u7684\u6700\u5927\u5b50\u6bb5\u548c\u4e3a $$ \\max \\limits_{1 \\le i \\le j \\le n} \\sum_{k=i}^j a[k] = \\max \\limits_{1 \\le j \\le n} \\max \\limits_{1 \\le i \\le j } \\sum_{k=i}^j a[k] = \\max \\limits_{1 \\le j \\le n} b[j] $$ \u7531 b[j] b[j] \u7684\u5b9a\u4e49\u53ef\u77e5\uff0c\u5f53 b[j-1] \\gt 0 \u65f6\uff0c \u65f6\uff0c b[j] = b[j-1] + a[j]b[j] = b[j-1] + a[j] b[j-1] \\gt 0 <span><span class=\"MathJax_Preview\">\u65f6\uff0c</span><script type=\"math/tex\">\u65f6\uff0c b[j] = b[j-1] + a[j]b[j] = b[j-1] + a[j] \uff0c\u5426\u5219 b[j] = a[j] b[j] = a[j] \u3002\u7531\u6b64\u53ef\u77e5\u8ba1\u7b97 b[j] b[j] \u7684\u52a8\u6001\u89c4\u5212\u9012\u5f52\u5f0f\uff1a $$ b[j] = \\max{b[j-1] + a[j], a[j] }, 1 \\le j \\le n $$ SUMMARY : \u9012\u5f52\u5173\u7cfb b[j] = b[j-1] + a[j] b[j] = b[j-1] + a[j] \u4e0e\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u975e\u5e38\u7c7b\u4f3c\uff1b b[j] b[j] \u7684\u8ba1\u7b97\u4ec5\u4ec5\u4f9d\u8d56\u4e8e b[j-1] b[j-1] \uff0c\u5c31\u5982 Fibonacci sequence \u7684\u8ba1\u7b97\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u524d\u4e24\u9879\u4e00\u6837\uff1b SUMMARY : \u6b63\u5982\u5728 \u753b\u89e3\u7b97\u6cd5\uff1a53. \u6700\u5927\u5b50\u5e8f\u548c \u4e2d\u6240\u8bf4\u7684\uff1a $b[j-1] \\gt 0 $ \u8bf4\u660e b[j-1] b[j-1] \u5bf9\u7ed3\u679c\u6709\u589e\u76ca\u6548\u679c\uff0c\u5219 b[j-1] b[j-1] \u4fdd\u7559\u5e76\u52a0\u4e0a\u5f53\u524d\u904d\u5386\u6570\u5b57 \u5982\u679c b[j-1] \\le 0 b[j-1] \\le 0 \u5219\u8bf4\u660e\u5b83\u5bf9\u7ed3\u679c\u5e76\u6ca1\u6709\u589e\u76ca\uff0c\u9700\u8981\u820d\u5f03\uff0c \u5219 b[j] b[j] \u76f4\u63a5\u66f4\u65b0\u4e3a\u5f53\u524d\u904d\u5386\u6570\u5b57 \u636e\u6b64\uff0c\u53ef\u8bbe\u8ba1\u51fa\u6c42\u6700\u5927\u5b50\u6bb5\u548c\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5982\u4e0b\uff1a int MaxSum ( int n , int * a ){ int sum = 0 , b = 0 ; int start = 0 , end = 0 ; //\u8bb0\u5f55\u4e0b\u5b50\u5e8f\u5217\u7684\u8d77\u59cb\u548c\u7ec8\u6b62\u4f4d\u7f6e for ( int i = 1 ; i <= n ; ++ i ){ if ( b > 0 ) b += a [ i ]; else { b = a [ i ]; start = i ; } if ( b > sum ) { sum = b ; end = i ; } } } \u5728 \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\uff1a\u86ee\u529b\u3001\u9012\u5f52\u53ca\u52a8\u6001\u89c4\u5212 \u4e2d\u7ed9\u51fa\u7684\u7a0b\u5e8f\u662f\u8fd9\u6837\u7684\uff1a #include <iostream> using namespace std ; int MaxSubsequenceSum ( const int A [], int n ) { int tempSum = 0 ; int maxSum = 0 ; for ( int j = 0 ; j < n ; j ++ ) // \u5b50\u95ee\u9898\u540e\u8fb9\u754c { tempSum = ( tempSum + A [ j ]) > A [ j ] ? ( tempSum + A [ j ]) : A [ j ]; if ( tempSum > maxSum ) // \u66f4\u65b0\u6700\u5927\u548c maxSum = tempSum ; } return maxSum ; } int main () { const int a [] = { 4 , -3 , 5 , -2 , -1 , 2 , 6 , -2 }; int maxSubSum = MaxSubsequenceSum ( a , 8 ); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl ; system ( \"pause\" ); return 0 ; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3a CSDN\u535a\u4e3b \u300c SanFanCSgo \u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY - SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1a https : //blog.csdn.net/weixin_40170902/article/details/80585218 \u8fd9\u79cd\u5b9e\u73b0\u548c\u4e0a\u9762\u7684\u90a3\u79cd\u5b9e\u73b0\u662f\u5b8c\u5168\u4e0d\u540c\u7684\uff1b","title":"O(n)O(n)\u7b97\u6cd5-\u52a8\u6001\u89c4\u5212\u6cd5"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Dynamic-Programming/VS-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-VS-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E9%97%AE%E9%A2%98-VS-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/#summary","text":"\u4e0a\u8ff0\u7684\u6240\u6709\u7a0b\u5e8f\u90fd\u662f\u5bf9\u89e3\u51b3\u95ee\u9898\u7684\u6570\u5b66\u516c\u5f0f\u7684\u63cf\u8ff0\uff0c\u6240\u4ee5\u5728\u7b97\u6cd5\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u5c31\u662f\uff1a \u4ece\u6570\u5b66\u516c\u5f0f\u5230\u7a0b\u5e8f","title":"summary"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Greedy-algorithm/Greedy-algorithm/","text":"Greedy algorithm","title":"Greedy-algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Greedy-algorithm/Greedy-algorithm/#greedy#algorithm","text":"","title":"Greedy algorithm"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Greedy-algorithm/Huffman-Coding/","text":"Huffman Coding | Greedy Algo-3 Prefix Codes , means the codes (bit sequences) are assigned in such a way that the code assigned to one character is not the prefix of code assigned to any other character. This is how Huffman Coding makes sure that there is no ambiguity when decoding the generated bitstream. Let us understand prefix codes with a counter example. Let there be four characters a , b , c and d , and their corresponding variable length codes be 00 , 01 , 0 and 1 . This coding leads to ambiguity because code assigned to c is the prefix of codes assigned to a and b . If the compressed bit stream is 0001 , the de-compressed output may be \u201ccccd\u201d or \u201cccb\u201d or \u201cacd\u201d or \u201cab\u201d.See this for applications of Huffman Coding. There are mainly two major parts in Huffman Coding 1) Build a Huffman Tree from input characters. 2) Traverse the Huffman Tree and assign codes to characters. Steps to build Huffman Tree Input is an array of unique characters along with their frequency of occurrences and output is Huffman Tree. 1. Create a leaf node for each unique character and build a min heap of all leaf nodes (Min Heap is used as a priority queue . The value of frequency field is used to compare two nodes in min heap . Initially, the least frequent character is at root) 2. Extract two nodes with the minimum frequency from the min heap. 3. Create a new internal node with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap. 4. Repeat steps #2 and #3 until the heap contains only one node. The remaining node is the root node and the tree is complete. Let us understand the algorithm with an example: // C program for Huffman Coding #include <stdio.h> #include <stdlib.h> // This constant can be avoided by explicitly // calculating height of Huffman Tree #define MAX_TREE_HT 100 // A Huffman tree node struct MinHeapNode { // One of the input characters char data ; // Frequency of the character unsigned freq ; // Left and right child of this node struct MinHeapNode * left , * right ; }; // A Min Heap: Collection of // min-heap (or Huffman tree) nodes struct MinHeap { // Current size of min heap unsigned size ; // capacity of min heap unsigned capacity ; // Array of minheap node pointers struct MinHeapNode ** array ; }; // A utility function allocate a new // min heap node with given character // and frequency of the character struct MinHeapNode * newNode ( char data , unsigned freq ) { struct MinHeapNode * temp = ( struct MinHeapNode * ) malloc ( sizeof ( struct MinHeapNode )); temp -> left = temp -> right = NULL ; temp -> data = data ; temp -> freq = freq ; return temp ; } // A utility function to create // a min heap of given capacity struct MinHeap * createMinHeap ( unsigned capacity ) { struct MinHeap * minHeap = ( struct MinHeap * ) malloc ( sizeof ( struct MinHeap )); // current size is 0 minHeap -> size = 0 ; minHeap -> capacity = capacity ; minHeap -> array = ( struct MinHeapNode ** ) malloc ( minHeap -> capacity * sizeof ( struct MinHeapNode * )); return minHeap ; } // A utility function to // swap two min heap nodes void swapMinHeapNode ( struct MinHeapNode ** a , struct MinHeapNode ** b ) { struct MinHeapNode * t = * a ; * a = * b ; * b = t ; } // The standard minHeapify function. void minHeapify ( struct MinHeap * minHeap , int idx ) { int smallest = idx ; int left = 2 * idx + 1 ; int right = 2 * idx + 2 ; if ( left < minHeap -> size && minHeap -> array [ left ] -> freq < minHeap -> array [ smallest ] -> freq ) smallest = left ; if ( right < minHeap -> size && minHeap -> array [ right ] -> freq < minHeap -> array [ smallest ] -> freq ) smallest = right ; if ( smallest != idx ) { swapMinHeapNode ( & minHeap -> array [ smallest ], & minHeap -> array [ idx ]); minHeapify ( minHeap , smallest ); } } // A utility function to check // if size of heap is 1 or not int isSizeOne ( struct MinHeap * minHeap ) { return ( minHeap -> size == 1 ); } // A standard function to extract // minimum value node from heap struct MinHeapNode * extractMin ( struct MinHeap * minHeap ) { struct MinHeapNode * temp = minHeap -> array [ 0 ]; minHeap -> array [ 0 ] = minHeap -> array [ minHeap -> size - 1 ]; -- minHeap -> size ; minHeapify ( minHeap , 0 ); return temp ; } // A utility function to insert // a new node to Min Heap void insertMinHeap ( struct MinHeap * minHeap , struct MinHeapNode * minHeapNode ) { ++ minHeap -> size ; int i = minHeap -> size - 1 ; while ( i && minHeapNode -> freq < minHeap -> array [( i - 1 ) / 2 ] -> freq ) { minHeap -> array [ i ] = minHeap -> array [( i - 1 ) / 2 ]; i = ( i - 1 ) / 2 ; } minHeap -> array [ i ] = minHeapNode ; } // A standard function to build min heap void buildMinHeap ( struct MinHeap * minHeap ) { int n = minHeap -> size - 1 ; int i ; for ( i = ( n - 1 ) / 2 ; i >= 0 ; -- i ) minHeapify ( minHeap , i ); } // A utility function to print an array of size n void printArr ( int arr [], int n ) { int i ; for ( i = 0 ; i < n ; ++ i ) printf ( \"%d\" , arr [ i ]); printf ( \" \\n \" ); } // Utility function to check if this node is leaf int isLeaf ( struct MinHeapNode * root ) { return ! ( root -> left ) && ! ( root -> right ); } // Creates a min heap of capacity // equal to size and inserts all character of // data[] in min heap. Initially size of // min heap is equal to capacity struct MinHeap * createAndBuildMinHeap ( char data [], int freq [], int size ) { struct MinHeap * minHeap = createMinHeap ( size ); for ( int i = 0 ; i < size ; ++ i ) minHeap -> array [ i ] = newNode ( data [ i ], freq [ i ]); minHeap -> size = size ; buildMinHeap ( minHeap ); return minHeap ; } // The main function that builds Huffman tree struct MinHeapNode * buildHuffmanTree ( char data [], int freq [], int size ) { struct MinHeapNode * left , * right , * top ; // Step 1: Create a min heap of capacity // equal to size. Initially, there are // modes equal to size. struct MinHeap * minHeap = createAndBuildMinHeap ( data , freq , size ); // Iterate while size of heap doesn't become 1 while ( ! isSizeOne ( minHeap )) { // Step 2: Extract the two minimum // freq items from min heap left = extractMin ( minHeap ); right = extractMin ( minHeap ); // Step 3: Create a new internal // node with frequency equal to the // sum of the two nodes frequencies. // Make the two extracted node as // left and right children of this new node. // Add this node to the min heap // '$' is a special value for internal nodes, not used top = newNode ( '$' , left -> freq + right -> freq ); top -> left = left ; top -> right = right ; insertMinHeap ( minHeap , top ); } // Step 4: The remaining node is the // root node and the tree is complete. return extractMin ( minHeap ); } // Prints huffman codes from the root of Huffman Tree. // It uses arr[] to store codes void printCodes ( struct MinHeapNode * root , int arr [], int top ) { // Assign 0 to left edge and recur if ( root -> left ) { arr [ top ] = 0 ; printCodes ( root -> left , arr , top + 1 ); } // Assign 1 to right edge and recur if ( root -> right ) { arr [ top ] = 1 ; printCodes ( root -> right , arr , top + 1 ); } // If this is a leaf node, then // it contains one of the input // characters, print the character // and its code from arr[] if ( isLeaf ( root )) { printf ( \"%c: \" , root -> data ); printArr ( arr , top ); } } // The main function that builds a // Huffman Tree and print codes by traversing // the built Huffman Tree void HuffmanCodes ( char data [], int freq [], int size ) { // Construct Huffman Tree struct MinHeapNode * root = buildHuffmanTree ( data , freq , size ); // Print Huffman codes using // the Huffman tree built above int arr [ MAX_TREE_HT ], top = 0 ; printCodes ( root , arr , top ); } // Driver program to test above functions int main () { char arr [] = { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' }; int freq [] = { 5 , 9 , 12 , 13 , 16 , 45 }; int size = sizeof ( arr ) / sizeof ( arr [ 0 ]); HuffmanCodes ( arr , freq , size ); return 0 ; } // C++ program for Huffman Coding #include <iostream> #include <cstdlib> using namespace std ; // This constant can be avoided by explicitly // calculating height of Huffman Tree #define MAX_TREE_HT 100 // A Huffman tree node struct MinHeapNode { // One of the input characters char data ; // Frequency of the character unsigned freq ; // Left and right child of this node struct MinHeapNode * left , * right ; }; // A Min Heap: Collection of // min-heap (or Huffman tree) nodes struct MinHeap { // Current size of min heap unsigned size ; // capacity of min heap unsigned capacity ; // Attay of minheap node pointers struct MinHeapNode ** array ; }; // A utility function allocate a new // min heap node with given character // and frequency of the character struct MinHeapNode * newNode ( char data , unsigned freq ) { struct MinHeapNode * temp = ( struct MinHeapNode * ) malloc ( sizeof ( struct MinHeapNode )); temp -> left = temp -> right = NULL ; temp -> data = data ; temp -> freq = freq ; return temp ; } // A utility function to create // a min heap of given capacity struct MinHeap * createMinHeap ( unsigned capacity ) { struct MinHeap * minHeap = ( struct MinHeap * ) malloc ( sizeof ( struct MinHeap )); // current size is 0 minHeap -> size = 0 ; minHeap -> capacity = capacity ; minHeap -> array = ( struct MinHeapNode ** ) malloc ( minHeap -> capacity * sizeof ( struct MinHeapNode * )); return minHeap ; } // A utility function to // swap two min heap nodes void swapMinHeapNode ( struct MinHeapNode ** a , struct MinHeapNode ** b ) { struct MinHeapNode * t = * a ; * a = * b ; * b = t ; } // The standard minHeapify function. void minHeapify ( struct MinHeap * minHeap , int idx ) { int smallest = idx ; int left = 2 * idx + 1 ; int right = 2 * idx + 2 ; if ( left < minHeap -> size && minHeap -> array [ left ] -> freq < minHeap -> array [ smallest ] -> freq ) smallest = left ; if ( right < minHeap -> size && minHeap -> array [ right ] -> freq < minHeap -> array [ smallest ] -> freq ) smallest = right ; if ( smallest != idx ) { swapMinHeapNode ( & minHeap -> array [ smallest ], & minHeap -> array [ idx ]); minHeapify ( minHeap , smallest ); } } // A utility function to check // if size of heap is 1 or not int isSizeOne ( struct MinHeap * minHeap ) { return ( minHeap -> size == 1 ); } // A standard function to extract // minimum value node from heap struct MinHeapNode * extractMin ( struct MinHeap * minHeap ) { struct MinHeapNode * temp = minHeap -> array [ 0 ]; minHeap -> array [ 0 ] = minHeap -> array [ minHeap -> size - 1 ]; -- minHeap -> size ; minHeapify ( minHeap , 0 ); return temp ; } // A utility function to insert // a new node to Min Heap void insertMinHeap ( struct MinHeap * minHeap , struct MinHeapNode * minHeapNode ) { ++ minHeap -> size ; int i = minHeap -> size - 1 ; while ( i && minHeapNode -> freq < minHeap -> array [( i - 1 ) / 2 ] -> freq ) { minHeap -> array [ i ] = minHeap -> array [( i - 1 ) / 2 ]; i = ( i - 1 ) / 2 ; } minHeap -> array [ i ] = minHeapNode ; } // A standard function to build min heap void buildMinHeap ( struct MinHeap * minHeap ) { int n = minHeap -> size - 1 ; int i ; for ( i = ( n - 1 ) / 2 ; i >= 0 ; -- i ) minHeapify ( minHeap , i ); } // A utility function to print an array of size n void printArr ( int arr [], int n ) { int i ; for ( i = 0 ; i < n ; ++ i ) cout << arr [ i ]; cout << \" \\n \" ; } // Utility function to check if this node is leaf int isLeaf ( struct MinHeapNode * root ) { return ! ( root -> left ) && ! ( root -> right ); } // Creates a min heap of capacity // equal to size and inserts all character of // data[] in min heap. Initially size of // min heap is equal to capacity struct MinHeap * createAndBuildMinHeap ( char data [], int freq [], int size ) { struct MinHeap * minHeap = createMinHeap ( size ); for ( int i = 0 ; i < size ; ++ i ) minHeap -> array [ i ] = newNode ( data [ i ], freq [ i ]); minHeap -> size = size ; buildMinHeap ( minHeap ); return minHeap ; } // The main function that builds Huffman tree struct MinHeapNode * buildHuffmanTree ( char data [], int freq [], int size ) { struct MinHeapNode * left , * right , * top ; // Step 1: Create a min heap of capacity // equal to size. Initially, there are // modes equal to size. struct MinHeap * minHeap = createAndBuildMinHeap ( data , freq , size ); // Iterate while size of heap doesn't become 1 while ( ! isSizeOne ( minHeap )) { // Step 2: Extract the two minimum // freq items from min heap left = extractMin ( minHeap ); right = extractMin ( minHeap ); // Step 3: Create a new internal // node with frequency equal to the // sum of the two nodes frequencies. // Make the two extracted node as // left and right children of this new node. // Add this node to the min heap // '$' is a special value for internal nodes, not used top = newNode ( '$' , left -> freq + right -> freq ); top -> left = left ; top -> right = right ; insertMinHeap ( minHeap , top ); } // Step 4: The remaining node is the // root node and the tree is complete. return extractMin ( minHeap ); } // Prints huffman codes from the root of Huffman Tree. // It uses arr[] to store codes void printCodes ( struct MinHeapNode * root , int arr [], int top ) { // Assign 0 to left edge and recur if ( root -> left ) { arr [ top ] = 0 ; printCodes ( root -> left , arr , top + 1 ); } // Assign 1 to right edge and recur if ( root -> right ) { arr [ top ] = 1 ; printCodes ( root -> right , arr , top + 1 ); } // If this is a leaf node, then // it contains one of the input // characters, print the character // and its code from arr[] if ( isLeaf ( root )) { cout << root -> data << \": \" ; printArr ( arr , top ); } } // The main function that builds a // Huffman Tree and print codes by traversing // the built Huffman Tree void HuffmanCodes ( char data [], int freq [], int size ) { // Construct Huffman Tree struct MinHeapNode * root = buildHuffmanTree ( data , freq , size ); // Print Huffman codes using // the Huffman tree built above int arr [ MAX_TREE_HT ], top = 0 ; printCodes ( root , arr , top ); } // Driver program to test above functions int main () { char arr [] = { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' }; int freq [] = { 5 , 9 , 12 , 13 , 16 , 45 }; int size = sizeof ( arr ) / sizeof ( arr [ 0 ]); HuffmanCodes ( arr , freq , size ); return 0 ; } Huffman-tree-vs-trie","title":"Huffman-Coding"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Greedy-algorithm/Huffman-Coding/#huffman#coding#greedy#algo-3","text":"Prefix Codes , means the codes (bit sequences) are assigned in such a way that the code assigned to one character is not the prefix of code assigned to any other character. This is how Huffman Coding makes sure that there is no ambiguity when decoding the generated bitstream. Let us understand prefix codes with a counter example. Let there be four characters a , b , c and d , and their corresponding variable length codes be 00 , 01 , 0 and 1 . This coding leads to ambiguity because code assigned to c is the prefix of codes assigned to a and b . If the compressed bit stream is 0001 , the de-compressed output may be \u201ccccd\u201d or \u201cccb\u201d or \u201cacd\u201d or \u201cab\u201d.See this for applications of Huffman Coding. There are mainly two major parts in Huffman Coding 1) Build a Huffman Tree from input characters. 2) Traverse the Huffman Tree and assign codes to characters.","title":"Huffman Coding | Greedy Algo-3"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Greedy-algorithm/Huffman-Coding/#steps#to#build#huffman#tree","text":"Input is an array of unique characters along with their frequency of occurrences and output is Huffman Tree. 1. Create a leaf node for each unique character and build a min heap of all leaf nodes (Min Heap is used as a priority queue . The value of frequency field is used to compare two nodes in min heap . Initially, the least frequent character is at root) 2. Extract two nodes with the minimum frequency from the min heap. 3. Create a new internal node with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap. 4. Repeat steps #2 and #3 until the heap contains only one node. The remaining node is the root node and the tree is complete. Let us understand the algorithm with an example: // C program for Huffman Coding #include <stdio.h> #include <stdlib.h> // This constant can be avoided by explicitly // calculating height of Huffman Tree #define MAX_TREE_HT 100 // A Huffman tree node struct MinHeapNode { // One of the input characters char data ; // Frequency of the character unsigned freq ; // Left and right child of this node struct MinHeapNode * left , * right ; }; // A Min Heap: Collection of // min-heap (or Huffman tree) nodes struct MinHeap { // Current size of min heap unsigned size ; // capacity of min heap unsigned capacity ; // Array of minheap node pointers struct MinHeapNode ** array ; }; // A utility function allocate a new // min heap node with given character // and frequency of the character struct MinHeapNode * newNode ( char data , unsigned freq ) { struct MinHeapNode * temp = ( struct MinHeapNode * ) malloc ( sizeof ( struct MinHeapNode )); temp -> left = temp -> right = NULL ; temp -> data = data ; temp -> freq = freq ; return temp ; } // A utility function to create // a min heap of given capacity struct MinHeap * createMinHeap ( unsigned capacity ) { struct MinHeap * minHeap = ( struct MinHeap * ) malloc ( sizeof ( struct MinHeap )); // current size is 0 minHeap -> size = 0 ; minHeap -> capacity = capacity ; minHeap -> array = ( struct MinHeapNode ** ) malloc ( minHeap -> capacity * sizeof ( struct MinHeapNode * )); return minHeap ; } // A utility function to // swap two min heap nodes void swapMinHeapNode ( struct MinHeapNode ** a , struct MinHeapNode ** b ) { struct MinHeapNode * t = * a ; * a = * b ; * b = t ; } // The standard minHeapify function. void minHeapify ( struct MinHeap * minHeap , int idx ) { int smallest = idx ; int left = 2 * idx + 1 ; int right = 2 * idx + 2 ; if ( left < minHeap -> size && minHeap -> array [ left ] -> freq < minHeap -> array [ smallest ] -> freq ) smallest = left ; if ( right < minHeap -> size && minHeap -> array [ right ] -> freq < minHeap -> array [ smallest ] -> freq ) smallest = right ; if ( smallest != idx ) { swapMinHeapNode ( & minHeap -> array [ smallest ], & minHeap -> array [ idx ]); minHeapify ( minHeap , smallest ); } } // A utility function to check // if size of heap is 1 or not int isSizeOne ( struct MinHeap * minHeap ) { return ( minHeap -> size == 1 ); } // A standard function to extract // minimum value node from heap struct MinHeapNode * extractMin ( struct MinHeap * minHeap ) { struct MinHeapNode * temp = minHeap -> array [ 0 ]; minHeap -> array [ 0 ] = minHeap -> array [ minHeap -> size - 1 ]; -- minHeap -> size ; minHeapify ( minHeap , 0 ); return temp ; } // A utility function to insert // a new node to Min Heap void insertMinHeap ( struct MinHeap * minHeap , struct MinHeapNode * minHeapNode ) { ++ minHeap -> size ; int i = minHeap -> size - 1 ; while ( i && minHeapNode -> freq < minHeap -> array [( i - 1 ) / 2 ] -> freq ) { minHeap -> array [ i ] = minHeap -> array [( i - 1 ) / 2 ]; i = ( i - 1 ) / 2 ; } minHeap -> array [ i ] = minHeapNode ; } // A standard function to build min heap void buildMinHeap ( struct MinHeap * minHeap ) { int n = minHeap -> size - 1 ; int i ; for ( i = ( n - 1 ) / 2 ; i >= 0 ; -- i ) minHeapify ( minHeap , i ); } // A utility function to print an array of size n void printArr ( int arr [], int n ) { int i ; for ( i = 0 ; i < n ; ++ i ) printf ( \"%d\" , arr [ i ]); printf ( \" \\n \" ); } // Utility function to check if this node is leaf int isLeaf ( struct MinHeapNode * root ) { return ! ( root -> left ) && ! ( root -> right ); } // Creates a min heap of capacity // equal to size and inserts all character of // data[] in min heap. Initially size of // min heap is equal to capacity struct MinHeap * createAndBuildMinHeap ( char data [], int freq [], int size ) { struct MinHeap * minHeap = createMinHeap ( size ); for ( int i = 0 ; i < size ; ++ i ) minHeap -> array [ i ] = newNode ( data [ i ], freq [ i ]); minHeap -> size = size ; buildMinHeap ( minHeap ); return minHeap ; } // The main function that builds Huffman tree struct MinHeapNode * buildHuffmanTree ( char data [], int freq [], int size ) { struct MinHeapNode * left , * right , * top ; // Step 1: Create a min heap of capacity // equal to size. Initially, there are // modes equal to size. struct MinHeap * minHeap = createAndBuildMinHeap ( data , freq , size ); // Iterate while size of heap doesn't become 1 while ( ! isSizeOne ( minHeap )) { // Step 2: Extract the two minimum // freq items from min heap left = extractMin ( minHeap ); right = extractMin ( minHeap ); // Step 3: Create a new internal // node with frequency equal to the // sum of the two nodes frequencies. // Make the two extracted node as // left and right children of this new node. // Add this node to the min heap // '$' is a special value for internal nodes, not used top = newNode ( '$' , left -> freq + right -> freq ); top -> left = left ; top -> right = right ; insertMinHeap ( minHeap , top ); } // Step 4: The remaining node is the // root node and the tree is complete. return extractMin ( minHeap ); } // Prints huffman codes from the root of Huffman Tree. // It uses arr[] to store codes void printCodes ( struct MinHeapNode * root , int arr [], int top ) { // Assign 0 to left edge and recur if ( root -> left ) { arr [ top ] = 0 ; printCodes ( root -> left , arr , top + 1 ); } // Assign 1 to right edge and recur if ( root -> right ) { arr [ top ] = 1 ; printCodes ( root -> right , arr , top + 1 ); } // If this is a leaf node, then // it contains one of the input // characters, print the character // and its code from arr[] if ( isLeaf ( root )) { printf ( \"%c: \" , root -> data ); printArr ( arr , top ); } } // The main function that builds a // Huffman Tree and print codes by traversing // the built Huffman Tree void HuffmanCodes ( char data [], int freq [], int size ) { // Construct Huffman Tree struct MinHeapNode * root = buildHuffmanTree ( data , freq , size ); // Print Huffman codes using // the Huffman tree built above int arr [ MAX_TREE_HT ], top = 0 ; printCodes ( root , arr , top ); } // Driver program to test above functions int main () { char arr [] = { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' }; int freq [] = { 5 , 9 , 12 , 13 , 16 , 45 }; int size = sizeof ( arr ) / sizeof ( arr [ 0 ]); HuffmanCodes ( arr , freq , size ); return 0 ; } // C++ program for Huffman Coding #include <iostream> #include <cstdlib> using namespace std ; // This constant can be avoided by explicitly // calculating height of Huffman Tree #define MAX_TREE_HT 100 // A Huffman tree node struct MinHeapNode { // One of the input characters char data ; // Frequency of the character unsigned freq ; // Left and right child of this node struct MinHeapNode * left , * right ; }; // A Min Heap: Collection of // min-heap (or Huffman tree) nodes struct MinHeap { // Current size of min heap unsigned size ; // capacity of min heap unsigned capacity ; // Attay of minheap node pointers struct MinHeapNode ** array ; }; // A utility function allocate a new // min heap node with given character // and frequency of the character struct MinHeapNode * newNode ( char data , unsigned freq ) { struct MinHeapNode * temp = ( struct MinHeapNode * ) malloc ( sizeof ( struct MinHeapNode )); temp -> left = temp -> right = NULL ; temp -> data = data ; temp -> freq = freq ; return temp ; } // A utility function to create // a min heap of given capacity struct MinHeap * createMinHeap ( unsigned capacity ) { struct MinHeap * minHeap = ( struct MinHeap * ) malloc ( sizeof ( struct MinHeap )); // current size is 0 minHeap -> size = 0 ; minHeap -> capacity = capacity ; minHeap -> array = ( struct MinHeapNode ** ) malloc ( minHeap -> capacity * sizeof ( struct MinHeapNode * )); return minHeap ; } // A utility function to // swap two min heap nodes void swapMinHeapNode ( struct MinHeapNode ** a , struct MinHeapNode ** b ) { struct MinHeapNode * t = * a ; * a = * b ; * b = t ; } // The standard minHeapify function. void minHeapify ( struct MinHeap * minHeap , int idx ) { int smallest = idx ; int left = 2 * idx + 1 ; int right = 2 * idx + 2 ; if ( left < minHeap -> size && minHeap -> array [ left ] -> freq < minHeap -> array [ smallest ] -> freq ) smallest = left ; if ( right < minHeap -> size && minHeap -> array [ right ] -> freq < minHeap -> array [ smallest ] -> freq ) smallest = right ; if ( smallest != idx ) { swapMinHeapNode ( & minHeap -> array [ smallest ], & minHeap -> array [ idx ]); minHeapify ( minHeap , smallest ); } } // A utility function to check // if size of heap is 1 or not int isSizeOne ( struct MinHeap * minHeap ) { return ( minHeap -> size == 1 ); } // A standard function to extract // minimum value node from heap struct MinHeapNode * extractMin ( struct MinHeap * minHeap ) { struct MinHeapNode * temp = minHeap -> array [ 0 ]; minHeap -> array [ 0 ] = minHeap -> array [ minHeap -> size - 1 ]; -- minHeap -> size ; minHeapify ( minHeap , 0 ); return temp ; } // A utility function to insert // a new node to Min Heap void insertMinHeap ( struct MinHeap * minHeap , struct MinHeapNode * minHeapNode ) { ++ minHeap -> size ; int i = minHeap -> size - 1 ; while ( i && minHeapNode -> freq < minHeap -> array [( i - 1 ) / 2 ] -> freq ) { minHeap -> array [ i ] = minHeap -> array [( i - 1 ) / 2 ]; i = ( i - 1 ) / 2 ; } minHeap -> array [ i ] = minHeapNode ; } // A standard function to build min heap void buildMinHeap ( struct MinHeap * minHeap ) { int n = minHeap -> size - 1 ; int i ; for ( i = ( n - 1 ) / 2 ; i >= 0 ; -- i ) minHeapify ( minHeap , i ); } // A utility function to print an array of size n void printArr ( int arr [], int n ) { int i ; for ( i = 0 ; i < n ; ++ i ) cout << arr [ i ]; cout << \" \\n \" ; } // Utility function to check if this node is leaf int isLeaf ( struct MinHeapNode * root ) { return ! ( root -> left ) && ! ( root -> right ); } // Creates a min heap of capacity // equal to size and inserts all character of // data[] in min heap. Initially size of // min heap is equal to capacity struct MinHeap * createAndBuildMinHeap ( char data [], int freq [], int size ) { struct MinHeap * minHeap = createMinHeap ( size ); for ( int i = 0 ; i < size ; ++ i ) minHeap -> array [ i ] = newNode ( data [ i ], freq [ i ]); minHeap -> size = size ; buildMinHeap ( minHeap ); return minHeap ; } // The main function that builds Huffman tree struct MinHeapNode * buildHuffmanTree ( char data [], int freq [], int size ) { struct MinHeapNode * left , * right , * top ; // Step 1: Create a min heap of capacity // equal to size. Initially, there are // modes equal to size. struct MinHeap * minHeap = createAndBuildMinHeap ( data , freq , size ); // Iterate while size of heap doesn't become 1 while ( ! isSizeOne ( minHeap )) { // Step 2: Extract the two minimum // freq items from min heap left = extractMin ( minHeap ); right = extractMin ( minHeap ); // Step 3: Create a new internal // node with frequency equal to the // sum of the two nodes frequencies. // Make the two extracted node as // left and right children of this new node. // Add this node to the min heap // '$' is a special value for internal nodes, not used top = newNode ( '$' , left -> freq + right -> freq ); top -> left = left ; top -> right = right ; insertMinHeap ( minHeap , top ); } // Step 4: The remaining node is the // root node and the tree is complete. return extractMin ( minHeap ); } // Prints huffman codes from the root of Huffman Tree. // It uses arr[] to store codes void printCodes ( struct MinHeapNode * root , int arr [], int top ) { // Assign 0 to left edge and recur if ( root -> left ) { arr [ top ] = 0 ; printCodes ( root -> left , arr , top + 1 ); } // Assign 1 to right edge and recur if ( root -> right ) { arr [ top ] = 1 ; printCodes ( root -> right , arr , top + 1 ); } // If this is a leaf node, then // it contains one of the input // characters, print the character // and its code from arr[] if ( isLeaf ( root )) { cout << root -> data << \": \" ; printArr ( arr , top ); } } // The main function that builds a // Huffman Tree and print codes by traversing // the built Huffman Tree void HuffmanCodes ( char data [], int freq [], int size ) { // Construct Huffman Tree struct MinHeapNode * root = buildHuffmanTree ( data , freq , size ); // Print Huffman codes using // the Huffman tree built above int arr [ MAX_TREE_HT ], top = 0 ; printCodes ( root , arr , top ); } // Driver program to test above functions int main () { char arr [] = { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' }; int freq [] = { 5 , 9 , 12 , 13 , 16 , 45 }; int size = sizeof ( arr ) / sizeof ( arr [ 0 ]); HuffmanCodes ( arr , freq , size ); return 0 ; }","title":"Steps to build Huffman Tree"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Greedy-algorithm/Huffman-Coding/#huffman-tree-vs-trie","text":"","title":"Huffman-tree-vs-trie"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/02-Dynamic-Programming%26Greedy-Algorithm/Greedy-algorithm/Reading-list/","text":"","title":"Reading-list"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/Heuristic/Heuristic%28computer-science%29/","text":"Heuristic (computer science) Heuristic (computer science)","title":"Heuristic(computer-science)"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/Heuristic/Heuristic%28computer-science%29/#heuristic#computer#science","text":"","title":"Heuristic (computer science)"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/Heuristic/Heuristic-algorithm-reading-list/","text":"Heuristic algorithms","title":"Heuristic-algorithm-reading-list"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/Heuristic/Heuristic-algorithm-reading-list/#heuristic#algorithms","text":"","title":"Heuristic algorithms"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/Heuristic/Heuristic-algorithms/","text":"Heuristic algorithms Heuristic algorithms","title":"Heuristic-algorithms"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/Heuristic/Heuristic-algorithms/#heuristic#algorithms","text":"","title":"Heuristic algorithms"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/Statistical-algorithms/HyperLogLog/","text":"HyperLogLog https://en.wikipedia.org/wiki/Count-distinct_problem","title":"HyperLogLog"},{"location":"Relation-structure-computation/Computation/Algorithm/Paradigm/Statistical-algorithms/HyperLogLog/#hyperloglog","text":"https://en.wikipedia.org/wiki/Count-distinct_problem","title":"HyperLogLog"},{"location":"Relation-structure-computation/Computation/Computation-direction/","text":"\u8ba1\u7b97\u7684\u65b9\u5411 \u6cbf\u7740\u5173\u7cfb\u3001\u7ed3\u6784\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u4e00\u822c\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4e24\u4e2a\u4e0d\u540c\u7684\u65b9\u5411\uff1a \u6bd4\u5982\uff1a 1) \u81ea\u5e95\u5411\u4e0a \u4e0e \u81ea\u9876\u5411\u4e0b 2) \u6b63\u5411\u4f20\u64ad \u4e0e \u53cd\u5411\u4f20\u64ad 3) \u9012\u5f52\u662f\u81ea\u9876\u5411\u4e0b\u3001\u52a8\u6001\u89c4\u5212\u662f\u81ea\u5e95\u5411\u4e0a 4) backprop\u662f\u81ea\u9876\u5411\u4e0b 5) parsing: Recursive descent parser: https://en.wikipedia.org/wiki/Recursive_descent_parser Top-down parsing: https://en.wikipedia.org/wiki/Top-down_parsing Bottom-up parsing: https://en.wikipedia.org/wiki/Bottom-up_parsing LR parser: https://en.wikipedia.org/wiki/LR_parser \u4e0a\u8ff0\u8fd9\u4e9b\u90fd\u4f53\u73b0\u4e86\u8ba1\u7b97\u7684\u65b9\u5411\u3002","title":"\u8ba1\u7b97\u7684\u65b9\u5411"},{"location":"Relation-structure-computation/Computation/Computation-direction/#_1","text":"\u6cbf\u7740\u5173\u7cfb\u3001\u7ed3\u6784\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u4e00\u822c\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4e24\u4e2a\u4e0d\u540c\u7684\u65b9\u5411\uff1a \u6bd4\u5982\uff1a 1) \u81ea\u5e95\u5411\u4e0a \u4e0e \u81ea\u9876\u5411\u4e0b 2) \u6b63\u5411\u4f20\u64ad \u4e0e \u53cd\u5411\u4f20\u64ad 3) \u9012\u5f52\u662f\u81ea\u9876\u5411\u4e0b\u3001\u52a8\u6001\u89c4\u5212\u662f\u81ea\u5e95\u5411\u4e0a 4) backprop\u662f\u81ea\u9876\u5411\u4e0b 5) parsing: Recursive descent parser: https://en.wikipedia.org/wiki/Recursive_descent_parser Top-down parsing: https://en.wikipedia.org/wiki/Top-down_parsing Bottom-up parsing: https://en.wikipedia.org/wiki/Bottom-up_parsing LR parser: https://en.wikipedia.org/wiki/LR_parser \u4e0a\u8ff0\u8fd9\u4e9b\u90fd\u4f53\u73b0\u4e86\u8ba1\u7b97\u7684\u65b9\u5411\u3002","title":"\u8ba1\u7b97\u7684\u65b9\u5411"},{"location":"Relation-structure-computation/Computation/Computation-direction/FIFO-and-FILO/","text":"FIFO and FILO wikipedia FIFO (computing and electronics)","title":"FIFO and FILO"},{"location":"Relation-structure-computation/Computation/Computation-direction/FIFO-and-FILO/#fifo#and#filo","text":"","title":"FIFO and FILO"},{"location":"Relation-structure-computation/Computation/Computation-direction/FIFO-and-FILO/#wikipedia#fifo#computing#and#electronics","text":"","title":"wikipedia FIFO (computing and electronics)"},{"location":"Relation-structure-computation/Computation/Computation-direction/Top-down-and-bottom-up/","text":"Top-down and bottom-up wikipedia Top-down and bottom-up design NOTE: \u662f\u5728\u9605\u8bfb wikipedia Law of Demeter \u65f6\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u8fd9\u7bc7\u6587\u7ae0\u7684\u94fe\u63a5\u3002","title":"Top-down and bottom-up"},{"location":"Relation-structure-computation/Computation/Computation-direction/Top-down-and-bottom-up/#top-down#and#bottom-up","text":"","title":"Top-down and bottom-up"},{"location":"Relation-structure-computation/Computation/Computation-direction/Top-down-and-bottom-up/#wikipedia#top-down#and#bottom-up#design","text":"NOTE: \u662f\u5728\u9605\u8bfb wikipedia Law of Demeter \u65f6\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u8fd9\u7bc7\u6587\u7ae0\u7684\u94fe\u63a5\u3002","title":"wikipedia Top-down and bottom-up design"},{"location":"Relation-structure-computation/Computation/Computation-on-structure/","text":"Computation on structure \u672c\u8282\u6807\u9898\u7684structure\uff0c\u53ef\u4ee5\u6307abstract structure\uff0c\u4e5f\u53ef\u4ee5\u6307data structure\u3001stream\u3002 \u672c\u6587\u8ba8\u8bba\u5bf9structure\u7684computation\uff0c\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u8868\u793a\u7684\u662f\u5bf9structure\u7684\u64cd\u4f5c: 1) Add: \u589e 2) Delete: \u5220 3) Update: \u6539 4) Query: \u67e5 5) Traverse: \u904d\u5386 6) Swap: \u4ea4\u6362 Add add\u7684\u65b9\u5f0f\u975e\u5e38\u591a: 1) insert 2) expand/rewrite Traverse Traverse\u662f\u4e00\u79cd\u57fa\u7840\u7684\u64cd\u4f5c\uff0c\u5f88\u591a\u5176\u4ed6\u7684operation\u3001algorithm\u90fd\u662f\u5efa\u7acb\u5728traverse\u7684\u57fa\u7840\u4e4b\u4e0a\u7684\u3002 \u5982\u4f55\u5b9e\u73b0\uff1f Traverse\u5bf9\u5e94\u7684computation\u662frepetition\uff0c\u56e0\u6b64\u6709\u4e24\u79cd\u5b9e\u73b0traverse\u7684\u65b9\u5f0f: 1) iteration 2) recursion \u4e0b\u9762\u662f\u4e00\u4e9b\u76f8\u5173\u5185\u5bb9: Pointer: \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\Pointer-and-array \u7ae0\u8282\u3002 Iterator: \u53c2\u89c1 Relation-structure-computation\\Computation\\Repetition\\Iteration \u7ae0\u8282\u3002 Iterator pattern: \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Object-oriented-programming\\Design-pattern\\Behavioral-pattern\\Iterator-pattern\\Iterator-pattern \u7ae0\u8282\u3002 Visitor pattern: \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Object-oriented-programming\\Design-pattern\\Behavioral-pattern\\Visitor-pattern\\ \u7ae0\u8282 Database Cursor : \u53c2\u89c1\u5de5\u7a0bDB Traverse and functional programming apply Traverse\u662ffunctional programming\u4e2dapply\u7684\u57fa\u7840\u3002 \u5728traverse\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5f80\u5f80\u5bf9structure\u4e2d\u7684node\u540c\u65f6\u6267\u884c\u540c\u4e00\u4e2acomputation\uff0c\u8fd9\u662f\u975e\u5e38\u591a\u7684algorithm\u7684\u6a21\u5f0f\u3002\u6211\u89c9\u5f97functional programming\u5c31\u662f\u57fa\u4e8e\u8fd9\u79cd\u6a21\u5f0f\u3002\u540e\u7eed\u6211\u4eec\u5c06\u6b64\u6210\u4e3a\"\u5bf9structure\u987a\u5e8f\u6267\u884c\u67d0\u4e2acomputation\"\uff0c\u6216\u8005\u662f apply \u6a21\u578b\uff0c\u611f\u89c9apply\u6a21\u578b\u662f\u66f4\u52a0\u7b80\u5355\u7684\u3002 Traverse on nonlinear structure \u8fd9\u79cdstructure\u4e0d\u4e00\u5b9a\u662f\u7ebf\u6027\u7ed3\u6784(sequence)\uff0c\u53ea\u8981\u80fd\u591f\u6309\u7167\u67d0\u79cd\u987a\u5e8f\u8bbf\u95ee\u5176node\u5373\u53ef\u3002 \u5bf9tree\u3001graph\u7b49nonlinear structure\u8fdb\u884ctraverse\uff0c\u5982\u679c\u4f7f\u7528sequencial algorithm\uff0c\u5219\u8fd9\u4e2a\u8fc7\u7a0b\u5176\u5b9e\u5c31\u76f8\u5f53\u4e8eflat\uff0c\u5bf9tree\u3001graph\u7b49\u975e\u7ebf\u6027\u7ed3\u6784\u8fdb\u884c\u987a\u5e8f\u904d\u5386\u7684\u8fc7\u7a0b\uff0c\u5176\u5b9e\u5c31\u662f\u5c06\u5b83\u8f6c\u6362\u4e3a\u4e00\u4e2a**\u7ebf\u6027**\u7684\u7ed3\u6784\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u79f0\u4e3a**flat**\u3002**\u987a\u5e8f\u904d\u5386**\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662f\u5f88\u591aalgorithm\u7684\u57fa\u7840\u3002\u6bd4\u5982graph\u7684\u904d\u5386\u662f\u5f88\u591agraph algorithm\u7684\u57fa\u7840\u3002 List comprehension and generator List comprehension https://infogalactic.com/info/List_comprehension Generator https://infogalactic.com/info/Generator_(computer_programming)#C.2B.2B","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Computation-on-structure/#computation#on#structure","text":"\u672c\u8282\u6807\u9898\u7684structure\uff0c\u53ef\u4ee5\u6307abstract structure\uff0c\u4e5f\u53ef\u4ee5\u6307data structure\u3001stream\u3002 \u672c\u6587\u8ba8\u8bba\u5bf9structure\u7684computation\uff0c\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u8868\u793a\u7684\u662f\u5bf9structure\u7684\u64cd\u4f5c: 1) Add: \u589e 2) Delete: \u5220 3) Update: \u6539 4) Query: \u67e5 5) Traverse: \u904d\u5386 6) Swap: \u4ea4\u6362","title":"Computation on structure"},{"location":"Relation-structure-computation/Computation/Computation-on-structure/#add","text":"add\u7684\u65b9\u5f0f\u975e\u5e38\u591a: 1) insert 2) expand/rewrite","title":"Add"},{"location":"Relation-structure-computation/Computation/Computation-on-structure/#traverse","text":"Traverse\u662f\u4e00\u79cd\u57fa\u7840\u7684\u64cd\u4f5c\uff0c\u5f88\u591a\u5176\u4ed6\u7684operation\u3001algorithm\u90fd\u662f\u5efa\u7acb\u5728traverse\u7684\u57fa\u7840\u4e4b\u4e0a\u7684\u3002","title":"Traverse"},{"location":"Relation-structure-computation/Computation/Computation-on-structure/#_1","text":"Traverse\u5bf9\u5e94\u7684computation\u662frepetition\uff0c\u56e0\u6b64\u6709\u4e24\u79cd\u5b9e\u73b0traverse\u7684\u65b9\u5f0f: 1) iteration 2) recursion \u4e0b\u9762\u662f\u4e00\u4e9b\u76f8\u5173\u5185\u5bb9: Pointer: \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\Pointer-and-array \u7ae0\u8282\u3002 Iterator: \u53c2\u89c1 Relation-structure-computation\\Computation\\Repetition\\Iteration \u7ae0\u8282\u3002 Iterator pattern: \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Object-oriented-programming\\Design-pattern\\Behavioral-pattern\\Iterator-pattern\\Iterator-pattern \u7ae0\u8282\u3002 Visitor pattern: \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Object-oriented-programming\\Design-pattern\\Behavioral-pattern\\Visitor-pattern\\ \u7ae0\u8282 Database Cursor : \u53c2\u89c1\u5de5\u7a0bDB","title":"\u5982\u4f55\u5b9e\u73b0\uff1f"},{"location":"Relation-structure-computation/Computation/Computation-on-structure/#traverse#and#functional#programming#apply","text":"Traverse\u662ffunctional programming\u4e2dapply\u7684\u57fa\u7840\u3002 \u5728traverse\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5f80\u5f80\u5bf9structure\u4e2d\u7684node\u540c\u65f6\u6267\u884c\u540c\u4e00\u4e2acomputation\uff0c\u8fd9\u662f\u975e\u5e38\u591a\u7684algorithm\u7684\u6a21\u5f0f\u3002\u6211\u89c9\u5f97functional programming\u5c31\u662f\u57fa\u4e8e\u8fd9\u79cd\u6a21\u5f0f\u3002\u540e\u7eed\u6211\u4eec\u5c06\u6b64\u6210\u4e3a\"\u5bf9structure\u987a\u5e8f\u6267\u884c\u67d0\u4e2acomputation\"\uff0c\u6216\u8005\u662f apply \u6a21\u578b\uff0c\u611f\u89c9apply\u6a21\u578b\u662f\u66f4\u52a0\u7b80\u5355\u7684\u3002","title":"Traverse and functional programming apply"},{"location":"Relation-structure-computation/Computation/Computation-on-structure/#traverse#on#nonlinear#structure","text":"\u8fd9\u79cdstructure\u4e0d\u4e00\u5b9a\u662f\u7ebf\u6027\u7ed3\u6784(sequence)\uff0c\u53ea\u8981\u80fd\u591f\u6309\u7167\u67d0\u79cd\u987a\u5e8f\u8bbf\u95ee\u5176node\u5373\u53ef\u3002 \u5bf9tree\u3001graph\u7b49nonlinear structure\u8fdb\u884ctraverse\uff0c\u5982\u679c\u4f7f\u7528sequencial algorithm\uff0c\u5219\u8fd9\u4e2a\u8fc7\u7a0b\u5176\u5b9e\u5c31\u76f8\u5f53\u4e8eflat\uff0c\u5bf9tree\u3001graph\u7b49\u975e\u7ebf\u6027\u7ed3\u6784\u8fdb\u884c\u987a\u5e8f\u904d\u5386\u7684\u8fc7\u7a0b\uff0c\u5176\u5b9e\u5c31\u662f\u5c06\u5b83\u8f6c\u6362\u4e3a\u4e00\u4e2a**\u7ebf\u6027**\u7684\u7ed3\u6784\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u79f0\u4e3a**flat**\u3002**\u987a\u5e8f\u904d\u5386**\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662f\u5f88\u591aalgorithm\u7684\u57fa\u7840\u3002\u6bd4\u5982graph\u7684\u904d\u5386\u662f\u5f88\u591agraph algorithm\u7684\u57fa\u7840\u3002","title":"Traverse on nonlinear structure"},{"location":"Relation-structure-computation/Computation/Computation-on-structure/#list#comprehension#and#generator","text":"List comprehension https://infogalactic.com/info/List_comprehension Generator https://infogalactic.com/info/Generator_(computer_programming)#C.2B.2B","title":"List comprehension and generator"},{"location":"Relation-structure-computation/Computation/Computer-algebra/","text":"\u5173\u4e8e\u672c\u7ae0 \u79fb\u5230\u4e86\u5de5\u7a0bLanguage\u7684 Formal-language-processing \u7ae0\u8282\u4e2d\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Computer-algebra/#_1","text":"\u79fb\u5230\u4e86\u5de5\u7a0bLanguage\u7684 Formal-language-processing \u7ae0\u8282\u4e2d\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Make-it-computational/","text":"Make it computational \u672c\u7ae0\u6807\u9898\u7684\u542b\u4e49\u662f: \u4f7f\u4e4b\u53ef\u8ba1\u7b97\u3002 \u672c\u7ae0\u63a2\u8ba8\u7684\u8bdd\u9898\u662f: \u5404\u79cd\u5404\u6837\u7684\u95ee\u9898\uff0c\u5982\u679c\u8981\u4f7f\u7528computer\u6765\u8fdb\u884c\u89e3\u51b3\uff0c\u90a3\u8981\u5982\u4f55\u8fdb\u884c\u8bbe\u8ba1\u5462\uff1f\u6709\u54ea\u4e9b\u6307\u5bfc\u601d\u60f3\u5462\uff1f \u4e0b\u9762\u662f\u4e00\u4e9b\u91cd\u8981\u7684\u601d\u8def\u3001\u6307\u5bfc\u601d\u60f3\uff1a 1) \u5982\u4f55\u6765\u8868\u793a( representation )\uff1f \u5728\u5e94\u7528\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5bfb\u627e\u5408\u9002\u7684representation\uff0c\u5bf9\u4e8e\u89e3\u51b3\u95ee\u9898\u81f3\u5173\u91cd\u8981(\u5728computer science\u4e2d\uff0crepresentation\u662f\u4e00\u4e2a\u975e\u5e38\u6838\u5fc3\u7684\u95ee\u9898)\u3002 \u5728 Structuralization-and-formalization \u7ae0\u8282\u5bf9\u6b64\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002 2) \u5982\u4f55\u8ba9\u5b83**\u6709\u5e8f**? \u5bf9\u4e8e\u7814\u7a76\u7684\u95ee\u9898\uff0c\u627e\u5230**\u5408\u9002**\u7684representation\u3001\u4f7f\u4e4b**\u6709\u5e8f**\u624d\u80fd\u591f\u4f7f\u5f97\u5bf9\u5b83**\u8ba1\u7b97**\u79f0\u4e3a\u53ef\u80fd\uff0c\u5373**make it computational**\u3002\u8fd9\u662f\u6211\u4eec\u89e3\u51b3\u4e00\u4e2a\u95ee\u9898\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u8fd9\u662fsoftware engineer\u9700\u8981\u601d\u8003\u7684\u95ee\u9898\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Make-it-computational/#make#it#computational","text":"\u672c\u7ae0\u6807\u9898\u7684\u542b\u4e49\u662f: \u4f7f\u4e4b\u53ef\u8ba1\u7b97\u3002 \u672c\u7ae0\u63a2\u8ba8\u7684\u8bdd\u9898\u662f: \u5404\u79cd\u5404\u6837\u7684\u95ee\u9898\uff0c\u5982\u679c\u8981\u4f7f\u7528computer\u6765\u8fdb\u884c\u89e3\u51b3\uff0c\u90a3\u8981\u5982\u4f55\u8fdb\u884c\u8bbe\u8ba1\u5462\uff1f\u6709\u54ea\u4e9b\u6307\u5bfc\u601d\u60f3\u5462\uff1f \u4e0b\u9762\u662f\u4e00\u4e9b\u91cd\u8981\u7684\u601d\u8def\u3001\u6307\u5bfc\u601d\u60f3\uff1a 1) \u5982\u4f55\u6765\u8868\u793a( representation )\uff1f \u5728\u5e94\u7528\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5bfb\u627e\u5408\u9002\u7684representation\uff0c\u5bf9\u4e8e\u89e3\u51b3\u95ee\u9898\u81f3\u5173\u91cd\u8981(\u5728computer science\u4e2d\uff0crepresentation\u662f\u4e00\u4e2a\u975e\u5e38\u6838\u5fc3\u7684\u95ee\u9898)\u3002 \u5728 Structuralization-and-formalization \u7ae0\u8282\u5bf9\u6b64\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002 2) \u5982\u4f55\u8ba9\u5b83**\u6709\u5e8f**? \u5bf9\u4e8e\u7814\u7a76\u7684\u95ee\u9898\uff0c\u627e\u5230**\u5408\u9002**\u7684representation\u3001\u4f7f\u4e4b**\u6709\u5e8f**\u624d\u80fd\u591f\u4f7f\u5f97\u5bf9\u5b83**\u8ba1\u7b97**\u79f0\u4e3a\u53ef\u80fd\uff0c\u5373**make it computational**\u3002\u8fd9\u662f\u6211\u4eec\u89e3\u51b3\u4e00\u4e2a\u95ee\u9898\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u8fd9\u662fsoftware engineer\u9700\u8981\u601d\u8003\u7684\u95ee\u9898\u3002","title":"Make it computational"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/","text":"Ordering Thoughts Thought: chao(\u6df7\u4e71\u7684\u3001\u6df7\u6c8c\u7684) \"chao\"\u8fd9\u4e2a\u6b64\u662f\u7ecf\u5e38\u770b\u5230\u7684\uff0c\u5b83\u8868\u793a\u7684\u662funcomputational\u3002 \u8fd9\u662f\u6211\u5728 \u9605\u8bfb wikipedia Leslie Lamport \u65f6\uff0c\u53d1\u73b0\u7684: Leslie Lamport was the winner of the 2013 Turing Award [ 4] for imposing clear, well-defined coherence on the seemingly chaotic behavior of distributed computing systems, in which several autonomous computers communicate with each other by passing messages. \u5728\u6211\u78b0\u5230\u7684\u5f88\u591a\u9519\u8bef\u4e2d\uff0c\u5176\u5b9e\u90fd\u53ef\u4ee5\u5f52\u5165\u5230\u8fd9\u4e00\u7c7b\u4e2d Thought1 \u5bf9\u4e8e\u65e0\u5e8f\u3001\u6df7\u4e71\u3001\u4e0d\u53ef\u91cd\u590d\u3001\u968f\u673a\u3001\u4e8c\u4e49\u6027\uff0c\u8fd9\u4e9b\u90fd\u662f\u4e0d\u53ef\u8ba1\u7b97\u7684\uff0c\u8fd9\u4e9b\u662f\u9700\u8981\u88ab\u514b\u670d\u7684\uff0c\u9700\u8981programmer\u8fdb\u884c\u663e\u5f0f\u7684\u63a7\u5236\u3002\u53ea\u6709**\u6709\u5e8f**\u624d\u80fd\u591f\u5b9e\u73b0computation\uff0c\u624d\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u6027\u3002 NOTE: \"\u663e\u5f0f\u7684\u63a7\u5236\"\u53ef\u4ee5\u4ececontrol-theory\u6765\u8fdb\u884c\u5206\u6790\uff0c\u89d2\u5ea6\u662f: \u52a0\u5165control\u4ee5\u4f7f\u4e4b\u6709\u5e8f\uff0c\u4ece\u800c\u662fcomputational\u7684\u3002 Thought2 Order\u662f\u4e00\u79cdabstract relation\uff0c\u901a\u8fc7order\u8fd9\u4e2arelation\u6765\u6784\u9020\u51faabstract structure\uff0c\u7136\u540e\u5bf9 \u8fd9\u4e2aabstract structure\u8fdb\u884c\u8ba1\u7b97\uff0c\u4ece\u800c\u5b9e\u73b0computation\u3002\u5176\u5b9eordering\u4e5f\u662f\u5c5e\u4e8e\"structuralization\"\u7684\u3002 \u6bd4\u5982stream\u7b49\u7ed3\u6784\uff0c\u5b83\u5929\u7136\u5c31\u662f\u6709\u5e8f\u7684\u3002 Thought3 \u8fd9\u5176\u5b9e\u662fcontrol theory\u7684\u7406\u8bba\uff0c\u5bf9\u4e8e\u65e0\u5e8f\u7684\uff0c\u5fc5\u987b\u8981\u8fdb\u884ccontrol\uff0c\u5426\u5219\u7cfb\u7edf\u662f\u4e0d\u53ef\u63a7\u7684\u3001\u5b58\u5728\u9519\u8bef\u7684\u3002 Example \u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50: 1) TensorFlow tf.control_dependencies \u5728\u9605\u8bfbTensorFlow whitepaper\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0: Our implementation also sometimes inserts control dependencies to enforce orderings between otherwise independent operations as a way of, for example, controlling the peak memory usage. \u7531\u4e8ecomputational graph\u662f\u53ef\u80fd\u5b58\u5728\u6b67\u4e49\u7684\uff0c\u5373\u5b83\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u5730\u8868\u8fbedependency\u5173\u7cfb\uff0c\u56e0\u6b64TF\u7684 tf.control_dependencies \u5c31\u663e\u5f0f\u5730\u5bf9dependency\u5173\u7cfb\u8fdb\u884c\u63a7\u5236\uff0c\u4ece\u800c\u4f7f\u5b83\u5177\u5907\u4e86\u51c6\u786e\u7684ordering\u3002 \u53c2\u89c1\u5de5\u7a0bmachine-learning\u3002 2) memory barrier \u53c2\u89c1\u5de5\u7a0bparallel-computing 3) Ordering in distributed system \u53c2\u89c1\u5de5\u7a0bparallel-computing 4) type ordering \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Type-system\\Type-relation \u7ae0\u8282\u3002 5) ordering and object lifetime \u53c2\u89c1\u5de5\u7a0bprogramming language\u7684 Ordering-and-object-lifetime \u7ae0\u8282\u3002 Order theory \u5173\u4e8eorder theory\uff0c\u53c2\u89c1 Relation-structure-computation\\Relation\\Order-theory \u7ae0\u8282\u3002 Ordering\u7684\u7ef4\u5ea6 \u57fa\u4e8e\u7a7a\u95f4\u7ef4\u5ea6 \u57fa\u4e8e\u65f6\u95f4\u7ef4\u5ea6 \u65f6\u95f4\u662f\u4e00\u79cd\u975e\u5e38\u6709\u6548\u7684order\u7ef4\u5ea6\uff0c\u5728\u8bf8\u591a\u9886\u57df\u4e2d\u90fd\u6709\u7740\u57fa\u4e8etime\u7684ordering: 1) \u5de5\u7a0bparallel-computing\u7684 Concurrent-computing\\Concurrency-control\\Time-based-concurrency-control \u7ae0\u8282\u3002 2) Lamport timestamp\uff0c\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684\u76f8\u5173\u7ae0\u8282","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#ordering","text":"","title":"Ordering"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#thoughts","text":"","title":"Thoughts"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#thought#chao","text":"\"chao\"\u8fd9\u4e2a\u6b64\u662f\u7ecf\u5e38\u770b\u5230\u7684\uff0c\u5b83\u8868\u793a\u7684\u662funcomputational\u3002 \u8fd9\u662f\u6211\u5728 \u9605\u8bfb wikipedia Leslie Lamport \u65f6\uff0c\u53d1\u73b0\u7684: Leslie Lamport was the winner of the 2013 Turing Award [ 4] for imposing clear, well-defined coherence on the seemingly chaotic behavior of distributed computing systems, in which several autonomous computers communicate with each other by passing messages. \u5728\u6211\u78b0\u5230\u7684\u5f88\u591a\u9519\u8bef\u4e2d\uff0c\u5176\u5b9e\u90fd\u53ef\u4ee5\u5f52\u5165\u5230\u8fd9\u4e00\u7c7b\u4e2d","title":"Thought: chao(\u6df7\u4e71\u7684\u3001\u6df7\u6c8c\u7684)"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#thought1","text":"\u5bf9\u4e8e\u65e0\u5e8f\u3001\u6df7\u4e71\u3001\u4e0d\u53ef\u91cd\u590d\u3001\u968f\u673a\u3001\u4e8c\u4e49\u6027\uff0c\u8fd9\u4e9b\u90fd\u662f\u4e0d\u53ef\u8ba1\u7b97\u7684\uff0c\u8fd9\u4e9b\u662f\u9700\u8981\u88ab\u514b\u670d\u7684\uff0c\u9700\u8981programmer\u8fdb\u884c\u663e\u5f0f\u7684\u63a7\u5236\u3002\u53ea\u6709**\u6709\u5e8f**\u624d\u80fd\u591f\u5b9e\u73b0computation\uff0c\u624d\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u6027\u3002 NOTE: \"\u663e\u5f0f\u7684\u63a7\u5236\"\u53ef\u4ee5\u4ececontrol-theory\u6765\u8fdb\u884c\u5206\u6790\uff0c\u89d2\u5ea6\u662f: \u52a0\u5165control\u4ee5\u4f7f\u4e4b\u6709\u5e8f\uff0c\u4ece\u800c\u662fcomputational\u7684\u3002","title":"Thought1"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#thought2","text":"Order\u662f\u4e00\u79cdabstract relation\uff0c\u901a\u8fc7order\u8fd9\u4e2arelation\u6765\u6784\u9020\u51faabstract structure\uff0c\u7136\u540e\u5bf9 \u8fd9\u4e2aabstract structure\u8fdb\u884c\u8ba1\u7b97\uff0c\u4ece\u800c\u5b9e\u73b0computation\u3002\u5176\u5b9eordering\u4e5f\u662f\u5c5e\u4e8e\"structuralization\"\u7684\u3002 \u6bd4\u5982stream\u7b49\u7ed3\u6784\uff0c\u5b83\u5929\u7136\u5c31\u662f\u6709\u5e8f\u7684\u3002","title":"Thought2"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#thought3","text":"\u8fd9\u5176\u5b9e\u662fcontrol theory\u7684\u7406\u8bba\uff0c\u5bf9\u4e8e\u65e0\u5e8f\u7684\uff0c\u5fc5\u987b\u8981\u8fdb\u884ccontrol\uff0c\u5426\u5219\u7cfb\u7edf\u662f\u4e0d\u53ef\u63a7\u7684\u3001\u5b58\u5728\u9519\u8bef\u7684\u3002","title":"Thought3"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#example","text":"\u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50: 1) TensorFlow tf.control_dependencies \u5728\u9605\u8bfbTensorFlow whitepaper\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0: Our implementation also sometimes inserts control dependencies to enforce orderings between otherwise independent operations as a way of, for example, controlling the peak memory usage. \u7531\u4e8ecomputational graph\u662f\u53ef\u80fd\u5b58\u5728\u6b67\u4e49\u7684\uff0c\u5373\u5b83\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u5730\u8868\u8fbedependency\u5173\u7cfb\uff0c\u56e0\u6b64TF\u7684 tf.control_dependencies \u5c31\u663e\u5f0f\u5730\u5bf9dependency\u5173\u7cfb\u8fdb\u884c\u63a7\u5236\uff0c\u4ece\u800c\u4f7f\u5b83\u5177\u5907\u4e86\u51c6\u786e\u7684ordering\u3002 \u53c2\u89c1\u5de5\u7a0bmachine-learning\u3002 2) memory barrier \u53c2\u89c1\u5de5\u7a0bparallel-computing 3) Ordering in distributed system \u53c2\u89c1\u5de5\u7a0bparallel-computing 4) type ordering \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Type-system\\Type-relation \u7ae0\u8282\u3002 5) ordering and object lifetime \u53c2\u89c1\u5de5\u7a0bprogramming language\u7684 Ordering-and-object-lifetime \u7ae0\u8282\u3002","title":"Example"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#order#theory","text":"\u5173\u4e8eorder theory\uff0c\u53c2\u89c1 Relation-structure-computation\\Relation\\Order-theory \u7ae0\u8282\u3002","title":"Order theory"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#ordering_1","text":"","title":"Ordering\u7684\u7ef4\u5ea6"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#_1","text":"","title":"\u57fa\u4e8e\u7a7a\u95f4\u7ef4\u5ea6"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Ordering/#_2","text":"\u65f6\u95f4\u662f\u4e00\u79cd\u975e\u5e38\u6709\u6548\u7684order\u7ef4\u5ea6\uff0c\u5728\u8bf8\u591a\u9886\u57df\u4e2d\u90fd\u6709\u7740\u57fa\u4e8etime\u7684ordering: 1) \u5de5\u7a0bparallel-computing\u7684 Concurrent-computing\\Concurrency-control\\Time-based-concurrency-control \u7ae0\u8282\u3002 2) Lamport timestamp\uff0c\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684\u76f8\u5173\u7ae0\u8282","title":"\u57fa\u4e8e\u65f6\u95f4\u7ef4\u5ea6"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/","text":"\u7ed3\u6784\u5316(structuralization) \u548c \u5f62\u5f0f\u5316(formalization) \u7ed3\u6784\u5316(structuralization) \u5728\u5f53\u6211\u4eec\u4f7f\u7528**relation**\u6765\u63cf\u8ff0\u4e8b\u7269\u7684\u65f6\u5019\uff08\u5373\u6309\u7167\u524d\u9762\u63cf\u8ff0\u7684node\u3001orderd-pair\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u7ec4\u7ec7\uff09\uff0c\u6211\u4eec\u4f1a\u53d1\u73b0\u5b83\u4eec\u4f1a\u5f62\u6210\u4e00\u5b9a\u7684structure\uff0c\u6bd4\u5982graph\u3001tree\u3001chain\uff0c\u6211\u4eec\u5efa\u8fd9\u79cd\u601d\u7ef4\u79f0\u4e3a\u201c \u7ed3\u6784\u5316\u601d\u7ef4 \u201d\uff0c\u5b83\u5176\u5b9e\u5c31\u662f\" \u8ba1\u7b97\u601d\u7ef4 \"(\u53c2\u89c1 Relation-structure-computation\\Computation \u7ae0\u8282)\u3002 \u6211\u4eec\u5c06\u6839\u636erelation\u5f97\u51fa\u5176**structure**\u3001 \u7ed3\u6784\u5316\u8868\u793a**\u7684\u8fc7\u7a0b\u79f0\u4e3a**\u7ed3\u6784\u5316 / \u5f62\u5f0f\u5316 \u3002\u53ea\u6709**\u7ed3\u6784\u5316**/ \u5f62\u5f0f\u5316**\u540e\u624d\u80fd\u591f\u5b9e\u73b0**computation \u3002 \u6211\u89c9\u5f97**\u8ba1\u7b97\u673a\u79d1\u5b66**\u662f\u9700\u8981\u8fd9\u79cd\u601d\u7ef4\u7684\uff0c\u53ea\u6709**\u7ed3\u6784\u5316**\u4e86\u4e4b\u540e\uff0c\u8ba1\u7b97\u673a\u624d\u80fd\u591f\u5bf9\u5176\u8fdb\u884c**\u8868\u793a**\uff08representation\uff09\u3001\u8fdb\u800c\u8fdb\u884c**\u8ba1\u7b97**\uff08computation\uff09\uff1b\u8fd9\u91cc\u6240\u8bf4\u7684**\u7ed3\u6784\u5316**\u5982\u679c\u5f80\u66f4\u9ad8\u5c42\u9762\u6765\u601d\u8003\u7684\u8bdd\uff0c\u5176\u5b9e\u662f\uff1a \u5f62\u5f0f\u5316 \uff0c\u53ea\u6709**\u5f62\u5f0f\u5316**\u540e\u624d\u80fd\u591f\u4f7f\u7528\u8ba1\u7b97\u673a\u7b97\u6cd5\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u6216\u8005\u66f4\u52a0\u901a\u4fd7\u5730\u6765\u8bf4\uff1a**\u7ed3\u6784\u5316**\u662f**\u5f62\u5f0f\u5316**\u7684\u4e00\u79cd\u3002 Examples Structureed data \u7ed3\u6784\u5316\u6570\u636e SQL http://en.wikipedia.org/wiki/SQL File format \u63cf\u8ff0\u6587\u4ef6\u7684\u7ed3\u6784 \u4ece\"\u8bed\u8a00\"\u7684\u89d2\u5ea6\u6765\u770b\u5f85\u7ed3\u6784\u5316 NOTE: \u4ece\u8bed\u8a00\u7684\u89d2\u5ea6\u6765\u770b\u5f85\u7ed3\u6784\u5316 \u5728\u6587\u7ae0 Language.md \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053**\u4e00\u5207\u201c\u63cf\u8ff0\u201d\u90fd\u662f\u8bed\u8a00**\u3002\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684language\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u5728computer science\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u91c7\u7528computer\u80fd\u591f\u7406\u89e3\u7684\u8bed\u8a00\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u8fd9\u5c31\u662f\u672c\u8282\u63d0\u51fa\u7684\" \u7ed3\u6784\u5316\u8868\u793a/\u8bed\u8a00 \"\uff0c\u5728\u4e0a\u4e00\u8282\u63d0\u51fa\u7684**\u7ed3\u6784\u5316\u601d\u7ef4**\uff0c\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u3001\u521b\u9020\u9002\u5408\u4e8e\u95ee\u9898\u7684representation\u3002 \u7ed3\u6784\u5316\u8868\u793a/\u8bed\u8a00**\u662f\u4e00\u79cd**\u8ba1\u7b97\u673a\u8bed\u8a00 \uff0c\u7ed3\u6784\u5316\u8868\u793a\u540e\uff0c\u624d\u80fd\u591f\u8fdb\u884c**computation**\u3002 Abstract structure \u7ed3\u6784\uff0c\u53ef\u4ee5\u662f**\u6709\u5f62**\u7684\u7ed3\u6784\uff0c\u4e5f\u53ef\u4ee5\u662f**\u65e0\u5f62**\u7684\u3001 \u903b\u8f91**\u7684\u7ed3\u6784\uff0c\u8fd9\u5c31\u662f**abstract structure \u3002\u663e\u7136\uff0c\u6211\u4eec**\u7ed3\u6784\u5316**\u5f97\u5230\u7684\u662f**abstract structure**\u3002 \u5728wikipedia Language of mathematics \u4e2d\uff0c\u6709\u5bf9abstract structure\u7684\u63cf\u8ff0: Mathematics describes abstract structures : on the other hand, there are areas of pure mathematics which deal with abstract structures , which have no known physical counterparts at all. However, it is difficult to give any categorical examples here, as even the most abstract structures can be co-opted as models in some branch of physics (see Calabi-Yau spaces and string theory ). Examples \u4e0b\u9762\u7ed3\u5408\u5177\u4f53\u7684\u4f8b\u5b50\u6765\u5bf9\u4e0a\u8ff0\u89c2\u70b9\u8fdb\u884c\u8bf4\u660e\u3002\u4e0d\u540c\u7684\u9886\u57df\u6709\u7740\u5404\u81ea\u7684representation\u3002 Example: Linguistics \u5728\u8bed\u8a00\u5b66\u4e2d\u4f7f\u7528 Grammar \u3001 Syntax \u6765\u8868\u793a\u8bed\u8a00\u7684\u7ed3\u6784\uff0c\u6700\u6700\u5178\u578b\u7684\u5c31\u662f Phrase structure grammar \u3002 regular language\u662flinear structure\uff0ccontext free language\u662fhierarchy \u7ed3\u6784\u3002\u56e0\u4e3aregular language\u7684grammar\uff0c\u5373regular grammar\u65e0\u6cd5\u8868\u8fbecontaining\u5173\u7cfb\u3002 \u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662fcompile principle\u4e2d\uff0c\u5e7f\u6cdb\u5730\u4f7f\u7528tree\u3001graph\u6765\uff0c\u5bf9\u4e8e\u8bed\u8a00\u8fd9\u79cd\u770b\u4f3c\u975e\u5e38\u7075\u6d3b\u7684\u3001\u65e0\u89c4\u5f8b\u7684\u4e1c\u897f\uff0c\u8fdb\u884c**\u5f62\u5f0f\u5316**\u7684\u63cf\u8ff0\uff0c\u8fd9\u8ba9programming language\u79f0\u4e3a\u4e86\u53ef\u80fd\u3002 Representation of word \u53c2\u89c1\u5de5\u7a0bmachine-learning\u7684 Application\\NLP\\Representation-of-word \u7ae0\u8282 \u3002 Example: Computer algebra \u4ea7\u751f\u5f0f\u3001\u51fd\u6570\u8868\u8fbe\u5f0f\uff08expression\uff09\u90fd\u662f\u6570\u5b66**\u8bed\u8a00**\uff0c\u5b83\u4eec\u63cf\u8ff0\u4e86**\u5173\u7cfb**\u3002 \u5728computer science\u4e2d\uff0c\u6211\u4eec\u77e5\u9053\uff0cgraph\u4e5f\u53ef\u4ee5\u7528\u6765\u63cf\u8ff0**\u5173\u7cfb**\u3002 \u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u6211\u4eec\u5e94\u8be5\u4f7f\u7528discrete relation\u6765\u5206\u6790\u4e8b\u7269\uff0c\u4ece\u800c\u5bf9\u5b83\u4eec\u8fdb\u884c\u63cf\u8ff0\u3001\u8ba1\u7b97\uff1a \u4ea7\u751f\u5f0f\u53ef\u4ee5\u4f7f\u7528tree structure\u6765\u8868\u793a\uff0ctree \u662f\u4e00\u79cd graph \u51fd\u6570\u8868\u8fbe\u5f0f\u53ef\u4ee5\u4f7f\u7528computation graph\u6765\u8868\u793a Computational graph and tree \u4f7f\u7528computational graph\u6765\u8868\u793aexpression\uff0c\u4f7f\u7528tree\u6765\u8868\u793aformal language\u3002\u5b83\u4eec\u90fd\u662f\u4f7f\u7528\u8ba1\u7b97\u673a\u80fd\u591f\u63a5\u53d7\u7684language\u6765\u63cf\u8ff0\u4e8b\u7269\u7684\u5178\u578b\u4f8b\u5b50\uff0c\u5b83\u4eec\u90fd\u662f\u4e00\u79cdlanguage\u3002\u8ba1\u7b97\u673a\u80fd\u591f\u63a5\u53d7\u7684\u8bed\u8a00\uff1astructure\u3002\u6240\u4ee5\uff0c\u7ed3\u6784\u5316\u65b9\u5f0f\uff0c\u5373\u4f7f\u7528\u7ed3\u6784\u5316\u7684\u8bed\u8a00\u8fdb\u884c\u63cf\u8ff0\u662f\u89e3\u51b3\u8ba1\u7b97\u95ee\u9898\u7684\u7b2c\u4e00\u6b65\u3002\u8fd9\u9700\u8981\u548c\u7ed3\u6784\u5316\u601d\u7ef4\u4e00\u8d77\u3002 \u6570\u5b66\u516c\u5f0f\u7684\u7ed3\u6784 \u6570\u5b66\u516c\u5f0f\u7684\u7ed3\u6784\u662f\u5178\u578b\u7684abstract structure\u3002 Example: computational graph of math expression \u4f7f\u7528computational graph\u6765\u63cf\u8ff0math expression\u3002 \u7d20\u6750\uff1a\u7ef4\u57fa\u767e\u79d1 Backpropagation \uff1aforward network\u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\uff1a $$ g(x):=f {L}(W {L}f {L-1}(W {L-1}\\cdots f {1}(W {1}x)\\cdots )) $$ \u7d20\u6750\uff1a machine-learning\\docs\\Theory\\Deep-learning\\Book-deep-learning\\Part-II-Deep-Networks-Modern-Practices\\Model-And-layer-And-computation-And-computational-graph.md Example: recurrence relation recurrence relation \u5176\u5b9e\u6240\u63cf\u8ff0\u7684\u662f\u4e24\u4e2a\u5143\u7d20\u7684\u5173\u7cfb\uff0c\u8fd9\u79cd\u5173\u7cfb\u53ef\u80fd\u662f\u7ebf\u6027\u7684\u3002 recurrence relation \u662f\u975e\u5e38\u9002\u5408\u4e8e\u4f7f\u7528computer algorithm\u6765\u5b9e\u73b0\u7684\uff0c\u56e0\u4e3a\u5b83\u662f\u79bb\u6563\u7684\uff0c\u5b83\u662f\u53ef\u4ee5\u4f7f\u7528one-by-one\u6765\u8ba1\u7b97\u51fa\u6765\u7684\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u8bf4\u5b83\u5177\u6709\u79bb\u6563\u7ed3\u6784\u3002 \u6bd4\u5982 : \u9012\u5f52\u516c\u5f0f Fibonacci number \u7684\u7ed3\u6784\u975e\u5e38\u7c7b\u4f3c\u4e8ebinary tree\u3002 Example: permutation and combination permutation\u548ccombination\u90fd\u662f\u4f7f\u7528\u7684\u4e58\u6cd5\uff0c\u5b83\u4eec\u90fd\u662fnesting\u5173\u7cfb\uff0c\u90fd\u5448\u73b0\u51fa tree \u7ed3\u6784\uff0c\u5728 Relation-structure-computation\\Computation\\Algorithm\\Paradigm\\Backtracking\\Backtrack \u7ae0\u8282\u4e2d\u6709**\u7ec4\u5408\u6811**\u3001**\u6392\u5217\u6811**\u7684\u63cf\u8ff0\u3002 Algebraic structure Mathematical structure In mathematics , a structure is a set endowed with some additional features on the set (e.g., operation , relation , metric , topology ).[ 1] Often, the additional features are attached or related to the set, so as to provide it with some additional meaning or significance. Example: Entity-relation model in DBMS \u4f7f\u7528Entity-relation model\u6765\u63cf\u8ff0\u73b0\u5b9e\u4e16\u754c\uff0c\u4ece\u540e\u4f7f\u7528table\u6765\u8fdb\u884c\u5b58\u50a8\u3002 \u53c2\u89c1: Entity-relation model \u3002 Example: \u8fc7\u7a0b\u7684\u7ed3\u6784 \u4e00\u4e9b\u52a8\u6001\u8fc7\u7a0b\uff0c\u6bd4\u5982\u51fd\u6570\u6267\u884c\u8fc7\u7a0b\u3001\u63a8\u5bfc\u8fc7\u7a0b\u7b49\uff0c\u90fd\u5448\u73b0\u51fa\u4e00\u5b9a\u7684\u7ed3\u6784\uff0c\u672c\u8282\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\uff0c\u663e\u7136\u8fd9\u79cd\u7ed3\u6784\u5c31\u662f\u524d\u9762\u63d0\u5230\u7684\u903b\u8f91\u7ed3\u6784\u3002 Proof\u8fc7\u7a0b\u5448\u73b0\u51falist\u6216tree\u7ed3\u6784 \u5728\u9605\u8bfb Proof theory \u65f6\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd\uff1a Proofs are typically presented as inductively-defined data structures such as plain lists, boxed lists, or trees , which are constructed according to the axioms and rules of inference of the logical system. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\uff0c\u5982\u679c\u5c06\u63a8\u5bfc\u7684\u8fc7\u7a0b\u53ef\u4ee5\u5c55\u793a\u4e3a\u4e00\u79cd\u6570\u636e\u7ed3\u6784\uff0c\u6bd4\u5982\u5217\u8868\u3001\u6811\u3002 Parse tree \u5c31\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff0c\u5728\u81ea\u9876\u5411\u4e0b parsing \u7684\u8fc7\u7a0b\u4e2d\uff0cparser\u4e0d\u65ad\u5730\u4f7f\u7528production\u8fdb\u884c\u63a8\u5bfc\uff08expand\uff09\uff0c\u6700\u7ec8\u751f\u6210\u4e86\u4e00\u68f5parse tree\u3002 Parsing\u8fc7\u7a0b\u4ea7\u751f Parse tree \u5728\u5de5\u7a0b automata-and-formal-language \u7684 Formal-language \u7ae0\u8282\u7684 Summary-of-theory \u6587\u7ae0\u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\u751f\u6210parse tree\u7684\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u8fdb\u884cProof\uff0c\u6240\u4ee5\u5c06\u672c\u8282\u7f6e\u4e8e\u201cProof\u8fc7\u7a0b\u5448\u73b0\u51falist\u6216tree\u7ed3\u6784\u201d\u4e2d\u3002 \u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u5448\u73b0tree\u7ed3\u6784 \u5728 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#structuralization#formalization","text":"","title":"\u7ed3\u6784\u5316(structuralization) \u548c \u5f62\u5f0f\u5316(formalization)"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#structuralization","text":"\u5728\u5f53\u6211\u4eec\u4f7f\u7528**relation**\u6765\u63cf\u8ff0\u4e8b\u7269\u7684\u65f6\u5019\uff08\u5373\u6309\u7167\u524d\u9762\u63cf\u8ff0\u7684node\u3001orderd-pair\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u7ec4\u7ec7\uff09\uff0c\u6211\u4eec\u4f1a\u53d1\u73b0\u5b83\u4eec\u4f1a\u5f62\u6210\u4e00\u5b9a\u7684structure\uff0c\u6bd4\u5982graph\u3001tree\u3001chain\uff0c\u6211\u4eec\u5efa\u8fd9\u79cd\u601d\u7ef4\u79f0\u4e3a\u201c \u7ed3\u6784\u5316\u601d\u7ef4 \u201d\uff0c\u5b83\u5176\u5b9e\u5c31\u662f\" \u8ba1\u7b97\u601d\u7ef4 \"(\u53c2\u89c1 Relation-structure-computation\\Computation \u7ae0\u8282)\u3002 \u6211\u4eec\u5c06\u6839\u636erelation\u5f97\u51fa\u5176**structure**\u3001 \u7ed3\u6784\u5316\u8868\u793a**\u7684\u8fc7\u7a0b\u79f0\u4e3a**\u7ed3\u6784\u5316 / \u5f62\u5f0f\u5316 \u3002\u53ea\u6709**\u7ed3\u6784\u5316**/ \u5f62\u5f0f\u5316**\u540e\u624d\u80fd\u591f\u5b9e\u73b0**computation \u3002 \u6211\u89c9\u5f97**\u8ba1\u7b97\u673a\u79d1\u5b66**\u662f\u9700\u8981\u8fd9\u79cd\u601d\u7ef4\u7684\uff0c\u53ea\u6709**\u7ed3\u6784\u5316**\u4e86\u4e4b\u540e\uff0c\u8ba1\u7b97\u673a\u624d\u80fd\u591f\u5bf9\u5176\u8fdb\u884c**\u8868\u793a**\uff08representation\uff09\u3001\u8fdb\u800c\u8fdb\u884c**\u8ba1\u7b97**\uff08computation\uff09\uff1b\u8fd9\u91cc\u6240\u8bf4\u7684**\u7ed3\u6784\u5316**\u5982\u679c\u5f80\u66f4\u9ad8\u5c42\u9762\u6765\u601d\u8003\u7684\u8bdd\uff0c\u5176\u5b9e\u662f\uff1a \u5f62\u5f0f\u5316 \uff0c\u53ea\u6709**\u5f62\u5f0f\u5316**\u540e\u624d\u80fd\u591f\u4f7f\u7528\u8ba1\u7b97\u673a\u7b97\u6cd5\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u6216\u8005\u66f4\u52a0\u901a\u4fd7\u5730\u6765\u8bf4\uff1a**\u7ed3\u6784\u5316**\u662f**\u5f62\u5f0f\u5316**\u7684\u4e00\u79cd\u3002","title":"\u7ed3\u6784\u5316(structuralization)"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#examples","text":"Structureed data \u7ed3\u6784\u5316\u6570\u636e SQL http://en.wikipedia.org/wiki/SQL File format \u63cf\u8ff0\u6587\u4ef6\u7684\u7ed3\u6784","title":"Examples"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#_1","text":"NOTE: \u4ece\u8bed\u8a00\u7684\u89d2\u5ea6\u6765\u770b\u5f85\u7ed3\u6784\u5316 \u5728\u6587\u7ae0 Language.md \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053**\u4e00\u5207\u201c\u63cf\u8ff0\u201d\u90fd\u662f\u8bed\u8a00**\u3002\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684language\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u5728computer science\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u91c7\u7528computer\u80fd\u591f\u7406\u89e3\u7684\u8bed\u8a00\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u8fd9\u5c31\u662f\u672c\u8282\u63d0\u51fa\u7684\" \u7ed3\u6784\u5316\u8868\u793a/\u8bed\u8a00 \"\uff0c\u5728\u4e0a\u4e00\u8282\u63d0\u51fa\u7684**\u7ed3\u6784\u5316\u601d\u7ef4**\uff0c\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u3001\u521b\u9020\u9002\u5408\u4e8e\u95ee\u9898\u7684representation\u3002 \u7ed3\u6784\u5316\u8868\u793a/\u8bed\u8a00**\u662f\u4e00\u79cd**\u8ba1\u7b97\u673a\u8bed\u8a00 \uff0c\u7ed3\u6784\u5316\u8868\u793a\u540e\uff0c\u624d\u80fd\u591f\u8fdb\u884c**computation**\u3002","title":"\u4ece\"\u8bed\u8a00\"\u7684\u89d2\u5ea6\u6765\u770b\u5f85\u7ed3\u6784\u5316"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#abstract#structure","text":"\u7ed3\u6784\uff0c\u53ef\u4ee5\u662f**\u6709\u5f62**\u7684\u7ed3\u6784\uff0c\u4e5f\u53ef\u4ee5\u662f**\u65e0\u5f62**\u7684\u3001 \u903b\u8f91**\u7684\u7ed3\u6784\uff0c\u8fd9\u5c31\u662f**abstract structure \u3002\u663e\u7136\uff0c\u6211\u4eec**\u7ed3\u6784\u5316**\u5f97\u5230\u7684\u662f**abstract structure**\u3002 \u5728wikipedia Language of mathematics \u4e2d\uff0c\u6709\u5bf9abstract structure\u7684\u63cf\u8ff0: Mathematics describes abstract structures : on the other hand, there are areas of pure mathematics which deal with abstract structures , which have no known physical counterparts at all. However, it is difficult to give any categorical examples here, as even the most abstract structures can be co-opted as models in some branch of physics (see Calabi-Yau spaces and string theory ).","title":"Abstract structure"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#examples_1","text":"\u4e0b\u9762\u7ed3\u5408\u5177\u4f53\u7684\u4f8b\u5b50\u6765\u5bf9\u4e0a\u8ff0\u89c2\u70b9\u8fdb\u884c\u8bf4\u660e\u3002\u4e0d\u540c\u7684\u9886\u57df\u6709\u7740\u5404\u81ea\u7684representation\u3002","title":"Examples"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#example#linguistics","text":"\u5728\u8bed\u8a00\u5b66\u4e2d\u4f7f\u7528 Grammar \u3001 Syntax \u6765\u8868\u793a\u8bed\u8a00\u7684\u7ed3\u6784\uff0c\u6700\u6700\u5178\u578b\u7684\u5c31\u662f Phrase structure grammar \u3002 regular language\u662flinear structure\uff0ccontext free language\u662fhierarchy \u7ed3\u6784\u3002\u56e0\u4e3aregular language\u7684grammar\uff0c\u5373regular grammar\u65e0\u6cd5\u8868\u8fbecontaining\u5173\u7cfb\u3002 \u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662fcompile principle\u4e2d\uff0c\u5e7f\u6cdb\u5730\u4f7f\u7528tree\u3001graph\u6765\uff0c\u5bf9\u4e8e\u8bed\u8a00\u8fd9\u79cd\u770b\u4f3c\u975e\u5e38\u7075\u6d3b\u7684\u3001\u65e0\u89c4\u5f8b\u7684\u4e1c\u897f\uff0c\u8fdb\u884c**\u5f62\u5f0f\u5316**\u7684\u63cf\u8ff0\uff0c\u8fd9\u8ba9programming language\u79f0\u4e3a\u4e86\u53ef\u80fd\u3002","title":"Example: Linguistics"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#representation#of#word","text":"\u53c2\u89c1\u5de5\u7a0bmachine-learning\u7684 Application\\NLP\\Representation-of-word \u7ae0\u8282 \u3002","title":"Representation of word"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#example#computer#algebra","text":"\u4ea7\u751f\u5f0f\u3001\u51fd\u6570\u8868\u8fbe\u5f0f\uff08expression\uff09\u90fd\u662f\u6570\u5b66**\u8bed\u8a00**\uff0c\u5b83\u4eec\u63cf\u8ff0\u4e86**\u5173\u7cfb**\u3002 \u5728computer science\u4e2d\uff0c\u6211\u4eec\u77e5\u9053\uff0cgraph\u4e5f\u53ef\u4ee5\u7528\u6765\u63cf\u8ff0**\u5173\u7cfb**\u3002 \u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u6211\u4eec\u5e94\u8be5\u4f7f\u7528discrete relation\u6765\u5206\u6790\u4e8b\u7269\uff0c\u4ece\u800c\u5bf9\u5b83\u4eec\u8fdb\u884c\u63cf\u8ff0\u3001\u8ba1\u7b97\uff1a \u4ea7\u751f\u5f0f\u53ef\u4ee5\u4f7f\u7528tree structure\u6765\u8868\u793a\uff0ctree \u662f\u4e00\u79cd graph \u51fd\u6570\u8868\u8fbe\u5f0f\u53ef\u4ee5\u4f7f\u7528computation graph\u6765\u8868\u793a","title":"Example: Computer algebra"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#computational#graph#and#tree","text":"\u4f7f\u7528computational graph\u6765\u8868\u793aexpression\uff0c\u4f7f\u7528tree\u6765\u8868\u793aformal language\u3002\u5b83\u4eec\u90fd\u662f\u4f7f\u7528\u8ba1\u7b97\u673a\u80fd\u591f\u63a5\u53d7\u7684language\u6765\u63cf\u8ff0\u4e8b\u7269\u7684\u5178\u578b\u4f8b\u5b50\uff0c\u5b83\u4eec\u90fd\u662f\u4e00\u79cdlanguage\u3002\u8ba1\u7b97\u673a\u80fd\u591f\u63a5\u53d7\u7684\u8bed\u8a00\uff1astructure\u3002\u6240\u4ee5\uff0c\u7ed3\u6784\u5316\u65b9\u5f0f\uff0c\u5373\u4f7f\u7528\u7ed3\u6784\u5316\u7684\u8bed\u8a00\u8fdb\u884c\u63cf\u8ff0\u662f\u89e3\u51b3\u8ba1\u7b97\u95ee\u9898\u7684\u7b2c\u4e00\u6b65\u3002\u8fd9\u9700\u8981\u548c\u7ed3\u6784\u5316\u601d\u7ef4\u4e00\u8d77\u3002","title":"Computational graph and tree"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#_2","text":"\u6570\u5b66\u516c\u5f0f\u7684\u7ed3\u6784\u662f\u5178\u578b\u7684abstract structure\u3002","title":"\u6570\u5b66\u516c\u5f0f\u7684\u7ed3\u6784"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#example#computational#graph#of#math#expression","text":"\u4f7f\u7528computational graph\u6765\u63cf\u8ff0math expression\u3002 \u7d20\u6750\uff1a\u7ef4\u57fa\u767e\u79d1 Backpropagation \uff1aforward network\u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\uff1a $$ g(x):=f {L}(W {L}f {L-1}(W {L-1}\\cdots f {1}(W {1}x)\\cdots )) $$ \u7d20\u6750\uff1a machine-learning\\docs\\Theory\\Deep-learning\\Book-deep-learning\\Part-II-Deep-Networks-Modern-Practices\\Model-And-layer-And-computation-And-computational-graph.md","title":"Example: computational graph of math expression"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#example#recurrence#relation","text":"recurrence relation \u5176\u5b9e\u6240\u63cf\u8ff0\u7684\u662f\u4e24\u4e2a\u5143\u7d20\u7684\u5173\u7cfb\uff0c\u8fd9\u79cd\u5173\u7cfb\u53ef\u80fd\u662f\u7ebf\u6027\u7684\u3002 recurrence relation \u662f\u975e\u5e38\u9002\u5408\u4e8e\u4f7f\u7528computer algorithm\u6765\u5b9e\u73b0\u7684\uff0c\u56e0\u4e3a\u5b83\u662f\u79bb\u6563\u7684\uff0c\u5b83\u662f\u53ef\u4ee5\u4f7f\u7528one-by-one\u6765\u8ba1\u7b97\u51fa\u6765\u7684\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u8bf4\u5b83\u5177\u6709\u79bb\u6563\u7ed3\u6784\u3002 \u6bd4\u5982 : \u9012\u5f52\u516c\u5f0f Fibonacci number \u7684\u7ed3\u6784\u975e\u5e38\u7c7b\u4f3c\u4e8ebinary tree\u3002","title":"Example: recurrence relation"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#example#permutation#and#combination","text":"permutation\u548ccombination\u90fd\u662f\u4f7f\u7528\u7684\u4e58\u6cd5\uff0c\u5b83\u4eec\u90fd\u662fnesting\u5173\u7cfb\uff0c\u90fd\u5448\u73b0\u51fa tree \u7ed3\u6784\uff0c\u5728 Relation-structure-computation\\Computation\\Algorithm\\Paradigm\\Backtracking\\Backtrack \u7ae0\u8282\u4e2d\u6709**\u7ec4\u5408\u6811**\u3001**\u6392\u5217\u6811**\u7684\u63cf\u8ff0\u3002","title":"Example: permutation and combination"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#algebraic#structure","text":"","title":"Algebraic structure"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#mathematical#structure","text":"In mathematics , a structure is a set endowed with some additional features on the set (e.g., operation , relation , metric , topology ).[ 1] Often, the additional features are attached or related to the set, so as to provide it with some additional meaning or significance.","title":"Mathematical structure"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#example#entity-relation#model#in#dbms","text":"\u4f7f\u7528Entity-relation model\u6765\u63cf\u8ff0\u73b0\u5b9e\u4e16\u754c\uff0c\u4ece\u540e\u4f7f\u7528table\u6765\u8fdb\u884c\u5b58\u50a8\u3002 \u53c2\u89c1: Entity-relation model \u3002","title":"Example: Entity-relation model in DBMS"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#example","text":"\u4e00\u4e9b\u52a8\u6001\u8fc7\u7a0b\uff0c\u6bd4\u5982\u51fd\u6570\u6267\u884c\u8fc7\u7a0b\u3001\u63a8\u5bfc\u8fc7\u7a0b\u7b49\uff0c\u90fd\u5448\u73b0\u51fa\u4e00\u5b9a\u7684\u7ed3\u6784\uff0c\u672c\u8282\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\uff0c\u663e\u7136\u8fd9\u79cd\u7ed3\u6784\u5c31\u662f\u524d\u9762\u63d0\u5230\u7684\u903b\u8f91\u7ed3\u6784\u3002","title":"Example: \u8fc7\u7a0b\u7684\u7ed3\u6784"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#prooflisttree","text":"\u5728\u9605\u8bfb Proof theory \u65f6\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd\uff1a Proofs are typically presented as inductively-defined data structures such as plain lists, boxed lists, or trees , which are constructed according to the axioms and rules of inference of the logical system. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\uff0c\u5982\u679c\u5c06\u63a8\u5bfc\u7684\u8fc7\u7a0b\u53ef\u4ee5\u5c55\u793a\u4e3a\u4e00\u79cd\u6570\u636e\u7ed3\u6784\uff0c\u6bd4\u5982\u5217\u8868\u3001\u6811\u3002 Parse tree \u5c31\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff0c\u5728\u81ea\u9876\u5411\u4e0b parsing \u7684\u8fc7\u7a0b\u4e2d\uff0cparser\u4e0d\u65ad\u5730\u4f7f\u7528production\u8fdb\u884c\u63a8\u5bfc\uff08expand\uff09\uff0c\u6700\u7ec8\u751f\u6210\u4e86\u4e00\u68f5parse tree\u3002","title":"Proof\u8fc7\u7a0b\u5448\u73b0\u51falist\u6216tree\u7ed3\u6784"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#parsingparse#tree","text":"\u5728\u5de5\u7a0b automata-and-formal-language \u7684 Formal-language \u7ae0\u8282\u7684 Summary-of-theory \u6587\u7ae0\u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\u751f\u6210parse tree\u7684\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u8fdb\u884cProof\uff0c\u6240\u4ee5\u5c06\u672c\u8282\u7f6e\u4e8e\u201cProof\u8fc7\u7a0b\u5448\u73b0\u51falist\u6216tree\u7ed3\u6784\u201d\u4e2d\u3002","title":"Parsing\u8fc7\u7a0b\u4ea7\u751fParse tree"},{"location":"Relation-structure-computation/Computation/Make-it-computational/Structuralization-and-formalization/#tree","text":"\u5728 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u5448\u73b0tree\u7ed3\u6784"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/","text":"Relation-based algorithm model \u201cRelation-based algorithm model\u201d\u5373\u201c\u57fa\u4e8erelation\u7684algorithm model\u201d\uff0c\u7b80\u5355\u800c\u8a00\u662f\uff1a\u57fa\u4e8erelation\u6765\u8bbe\u8ba1algorithm\u3002\u4e0b\u9762\u662f\u5bf9\u5b83\u7684\u6982\u62ec\u63cf\u8ff0\uff1a \u6cbf\u7740structure\u3001relation\u9010\u4e2a\u8fdb\u884c\u5904\u7406\uff0c\u91c7\u7528one-by-one computation\uff08one node by another node\uff09\u3002 \u524d\u63d0 relation\u9700\u8981\u5177\u5907transitive\u6027\u8d28\u3002 algorithm\u6b65\u9aa4 step 1 \u5206\u6790relation\u3002 \u5bf9\u4e8ediscrete objects\uff0c\u5206\u6790\u5176\u7ed3\u6784\uff0c\u627e\u51farelation\uff0c\u7ed9\u51faformal definition\uff0c\u540e\u9762\u6211\u4eec\u5c31\u53ef\u4ee5\u5f00\u59cb\u8bbe\u8ba1algorithm\uff0c\u5bf9\u5176\u8fdb\u884c\u8ba1\u7b97\u3002 step 2 rewrite/expand operation \u6839\u636erelation\u8fdb\u884crewrite/expand\uff0crelation\u5c31\u76f8\u5f53\u4e8erewrite rule\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u9700\u8981one-by-one\u6267\u884c\u3002 \u6bd4\u5982\uff1a \u5728parsing\u4e2d\uff0c\u662f\u6839\u636eproduction\u8fdb\u884cexpand\uff0cproduction\u6240\u8868\u8fbe\u7684\u662f\u5305\u542b\u5173\u7cfb \u5728graph\u4e2d\uff0c\u5219\u662f\u6839\u636e\u76f8\u90bb\u5173\u7cfb\u6765\u8fdb\u884cexpand\u7684 Function composition \u5728backtracing\u4e2d\uff0c\u5f80\u5f80\u662f\u6839\u636enesting\u5173\u7cfb\u8fdb\u884cexpand Implementation: recursion and iteration \u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662f\uff0c\u53ef\u4ee5\u4f7f\u7528recursion \u548c iteration\u6765\u5b9e\u73b0relation-based algorithm\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\u3002 \u4f7f\u7528\u540c\u4e00\u4e2arelation\u3001transitive relation\u5f62\u6210\u7684structure\uff0c\u5f80\u5f80\u662frecursive\u7684\uff0c\u662f\u53ef\u4ee5\u7ed9\u51fa\u9012\u5f52\u5b9a\u4e49\u4e86\uff0c\u53ef\u4ee5\u4f7f\u7528structural recursion\u6765\u8fdb\u884c\u89e3\u51b3\u3002 \u4e0b\u9762\u662f\u6211\u5728\u5b66\u4e60backtracing\u7b97\u6cd5\u7684\u5b9e\u73b0\u65f6\uff0c\u6240\u603b\u7ed3\u7684\uff1a structure\u662f\u4f7f\u7528\u540c\u4e00\u79cdrelation\u5f62\u6210\u7684\uff08\u6bd4\u5982 backtracing\u4e2d\u7684nesting relation\uff09\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528\u540c\u4e00\u79cdcomputation \u5bf9\u4e8e\u540c\u4e00\u79cdcomputation\u7684\u91cd\u590d\u6267\u884c\uff0c\u6211\u4eec\u5f80\u5f80\u53ef\u4ee5\u4f7f\u7528 recursion \u548c iteration \u6765\u5b9e\u73b0 \u4e0b\u9762\u63cf\u8ff0\u57fa\u4e8erecursion\u7684\u5b9e\u73b0\uff1a \u4f7f\u7528**\u9012\u5f52\u51fd\u6570**\u53ef\u4ee5\u5b9e\u73b0\u6cbf\u7740**\u7ed3\u6784**\u91cd\u590d\u6267\u884c\u76f8\u540c\u7684\u64cd\u4f5c\uff08computation\uff09\uff0c\u8fdb\u800c\u9010\u6b65\u6784\u5efa\u51fa\u5b8c\u6574\u7684**structure**\uff08\u8fd9\u5176\u5b9e\u5c31\u662f\u524d\u9762\u63cf\u8ff0\u7684expand\u6a21\u578b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6b64\u5904\u7684structure\u53ef\u80fd\u662f\u865a\u62df\u7684\uff0c\u4e0d\u4e00\u5b9a\u8981\u663e\u5f0f\u5730\u6784\u9020\u51fa\u6765\uff0c\u6216\u8005\u8bf4\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5448\u73b0\u51fa\u4e00\u5b9a\u7684structure\uff09\uff1b\u56e0\u6b64\uff0c\u4e00\u822c\u5c06\u7ed3\u6784\u7684 node\u3001level \u4f5c\u4e3a\u9012\u5f52\u51fd\u6570\u7684\u5165\u53c2\uff08\u8fd9\u5176\u5b9e\u662fone-by-one computation model\uff0c\u5373 \u9010\u6b65\u6784\u5efa\u5b8c\u6574\u7684\u7ed3\u6784\uff09\uff0c\u5f53node\u3001level \u8fbe\u5230\u5b8c\u6574\u7ed3\u6784\u65f6\uff0c\u8868\u793a\u5df2\u7ecf\u6784\u9020\u51fa\u4e86\u5b8c\u6574\u7684\u89e3\uff0c\u5219\u53ef\u4ee5\u7ec8\u6b62\u9012\u5f52\u4e86\uff0c\u5373\u7ec8\u6b62expand\u4e86\u3002\u4e0b\u9762\u662f\u8fd9\u4e2a\u8fc7\u7a0b\u7684\u5c55\u793a expand backtrace(level) \u8fc7\u6e21\u5230\u4e0b\u4e00level: backtrace(level + 1) backtrace(node) \u8fc7\u6e21\u5230\u4e0b\u4e00node: backtrace( next_node ) \u9700\u8981\u7ed3\u5408backtracing\u6765\u7406\u89e3\u4e0a\u9762\u7684\u5185\u5bb9\u3002 \u53c2\u89c1\uff1a \u5173\u4e8erelation\u3001transitive relation\uff0c\u53c2\u89c1 Relation-structure-computation\\Relation\\Relation \u7ae0\u8282 \u5173\u4e8ebacktracing\uff0c\u53c2\u89c1 Relation-structure-computation\\Computation\\Algorithm\\Paradigm\\Backtracking\\Backtrack Optional sub structure \u6700\u4f18\u5b50\u7ed3\u6784\uff0c\u5b83\u662f\u975e\u5e38\u5178\u578b\u7684\u53ef\u4ee5\u4f7f\u7528relation-based algorithm\u89e3\u51b3\u7684\u95ee\u9898\u3002 Application Graph search algorithm \u53c2\u89c1 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Search-algorithm \u3002 CFG CFG\u7684parsing Combinatorial optimization Combinatorial-optimization\uff0c\u53c2\u89c1 Relation-structure-computation\\Computation\\Algorithm\\Application\\Optimization\\Combinatorial-optimization","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#relation-based#algorithm#model","text":"\u201cRelation-based algorithm model\u201d\u5373\u201c\u57fa\u4e8erelation\u7684algorithm model\u201d\uff0c\u7b80\u5355\u800c\u8a00\u662f\uff1a\u57fa\u4e8erelation\u6765\u8bbe\u8ba1algorithm\u3002\u4e0b\u9762\u662f\u5bf9\u5b83\u7684\u6982\u62ec\u63cf\u8ff0\uff1a \u6cbf\u7740structure\u3001relation\u9010\u4e2a\u8fdb\u884c\u5904\u7406\uff0c\u91c7\u7528one-by-one computation\uff08one node by another node\uff09\u3002","title":"Relation-based algorithm model"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#_1","text":"relation\u9700\u8981\u5177\u5907transitive\u6027\u8d28\u3002","title":"\u524d\u63d0"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#algorithm","text":"","title":"algorithm\u6b65\u9aa4"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#step#1","text":"\u5206\u6790relation\u3002 \u5bf9\u4e8ediscrete objects\uff0c\u5206\u6790\u5176\u7ed3\u6784\uff0c\u627e\u51farelation\uff0c\u7ed9\u51faformal definition\uff0c\u540e\u9762\u6211\u4eec\u5c31\u53ef\u4ee5\u5f00\u59cb\u8bbe\u8ba1algorithm\uff0c\u5bf9\u5176\u8fdb\u884c\u8ba1\u7b97\u3002","title":"step 1"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#step#2","text":"rewrite/expand operation \u6839\u636erelation\u8fdb\u884crewrite/expand\uff0crelation\u5c31\u76f8\u5f53\u4e8erewrite rule\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u9700\u8981one-by-one\u6267\u884c\u3002 \u6bd4\u5982\uff1a \u5728parsing\u4e2d\uff0c\u662f\u6839\u636eproduction\u8fdb\u884cexpand\uff0cproduction\u6240\u8868\u8fbe\u7684\u662f\u5305\u542b\u5173\u7cfb \u5728graph\u4e2d\uff0c\u5219\u662f\u6839\u636e\u76f8\u90bb\u5173\u7cfb\u6765\u8fdb\u884cexpand\u7684 Function composition \u5728backtracing\u4e2d\uff0c\u5f80\u5f80\u662f\u6839\u636enesting\u5173\u7cfb\u8fdb\u884cexpand","title":"step 2"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#implementation#recursion#and#iteration","text":"\u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662f\uff0c\u53ef\u4ee5\u4f7f\u7528recursion \u548c iteration\u6765\u5b9e\u73b0relation-based algorithm\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\u3002 \u4f7f\u7528\u540c\u4e00\u4e2arelation\u3001transitive relation\u5f62\u6210\u7684structure\uff0c\u5f80\u5f80\u662frecursive\u7684\uff0c\u662f\u53ef\u4ee5\u7ed9\u51fa\u9012\u5f52\u5b9a\u4e49\u4e86\uff0c\u53ef\u4ee5\u4f7f\u7528structural recursion\u6765\u8fdb\u884c\u89e3\u51b3\u3002 \u4e0b\u9762\u662f\u6211\u5728\u5b66\u4e60backtracing\u7b97\u6cd5\u7684\u5b9e\u73b0\u65f6\uff0c\u6240\u603b\u7ed3\u7684\uff1a structure\u662f\u4f7f\u7528\u540c\u4e00\u79cdrelation\u5f62\u6210\u7684\uff08\u6bd4\u5982 backtracing\u4e2d\u7684nesting relation\uff09\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528\u540c\u4e00\u79cdcomputation \u5bf9\u4e8e\u540c\u4e00\u79cdcomputation\u7684\u91cd\u590d\u6267\u884c\uff0c\u6211\u4eec\u5f80\u5f80\u53ef\u4ee5\u4f7f\u7528 recursion \u548c iteration \u6765\u5b9e\u73b0 \u4e0b\u9762\u63cf\u8ff0\u57fa\u4e8erecursion\u7684\u5b9e\u73b0\uff1a \u4f7f\u7528**\u9012\u5f52\u51fd\u6570**\u53ef\u4ee5\u5b9e\u73b0\u6cbf\u7740**\u7ed3\u6784**\u91cd\u590d\u6267\u884c\u76f8\u540c\u7684\u64cd\u4f5c\uff08computation\uff09\uff0c\u8fdb\u800c\u9010\u6b65\u6784\u5efa\u51fa\u5b8c\u6574\u7684**structure**\uff08\u8fd9\u5176\u5b9e\u5c31\u662f\u524d\u9762\u63cf\u8ff0\u7684expand\u6a21\u578b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6b64\u5904\u7684structure\u53ef\u80fd\u662f\u865a\u62df\u7684\uff0c\u4e0d\u4e00\u5b9a\u8981\u663e\u5f0f\u5730\u6784\u9020\u51fa\u6765\uff0c\u6216\u8005\u8bf4\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5448\u73b0\u51fa\u4e00\u5b9a\u7684structure\uff09\uff1b\u56e0\u6b64\uff0c\u4e00\u822c\u5c06\u7ed3\u6784\u7684 node\u3001level \u4f5c\u4e3a\u9012\u5f52\u51fd\u6570\u7684\u5165\u53c2\uff08\u8fd9\u5176\u5b9e\u662fone-by-one computation model\uff0c\u5373 \u9010\u6b65\u6784\u5efa\u5b8c\u6574\u7684\u7ed3\u6784\uff09\uff0c\u5f53node\u3001level \u8fbe\u5230\u5b8c\u6574\u7ed3\u6784\u65f6\uff0c\u8868\u793a\u5df2\u7ecf\u6784\u9020\u51fa\u4e86\u5b8c\u6574\u7684\u89e3\uff0c\u5219\u53ef\u4ee5\u7ec8\u6b62\u9012\u5f52\u4e86\uff0c\u5373\u7ec8\u6b62expand\u4e86\u3002\u4e0b\u9762\u662f\u8fd9\u4e2a\u8fc7\u7a0b\u7684\u5c55\u793a expand backtrace(level) \u8fc7\u6e21\u5230\u4e0b\u4e00level: backtrace(level + 1) backtrace(node) \u8fc7\u6e21\u5230\u4e0b\u4e00node: backtrace( next_node ) \u9700\u8981\u7ed3\u5408backtracing\u6765\u7406\u89e3\u4e0a\u9762\u7684\u5185\u5bb9\u3002 \u53c2\u89c1\uff1a \u5173\u4e8erelation\u3001transitive relation\uff0c\u53c2\u89c1 Relation-structure-computation\\Relation\\Relation \u7ae0\u8282 \u5173\u4e8ebacktracing\uff0c\u53c2\u89c1 Relation-structure-computation\\Computation\\Algorithm\\Paradigm\\Backtracking\\Backtrack","title":"Implementation: recursion and iteration"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#optional#sub#structure","text":"\u6700\u4f18\u5b50\u7ed3\u6784\uff0c\u5b83\u662f\u975e\u5e38\u5178\u578b\u7684\u53ef\u4ee5\u4f7f\u7528relation-based algorithm\u89e3\u51b3\u7684\u95ee\u9898\u3002","title":"Optional sub structure"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#application","text":"","title":"Application"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#graph#search#algorithm","text":"\u53c2\u89c1 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Search-algorithm \u3002","title":"Graph search algorithm"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#cfg","text":"CFG\u7684parsing","title":"CFG"},{"location":"Relation-structure-computation/Computation/Relation-based-algorithm-model/#combinatorial#optimization","text":"Combinatorial-optimization\uff0c\u53c2\u89c1 Relation-structure-computation\\Computation\\Algorithm\\Application\\Optimization\\Combinatorial-optimization","title":"Combinatorial optimization"},{"location":"Relation-structure-computation/Computation/Repetition/","text":"\u5173\u4e8e\u672c\u7ae0 \u5728\u524d\u9762\u7ae0\u8282\u5df2\u7ecf\u4ecb\u7ecd\u4e86repetition\uff0c\u672c\u7ae0\u5bf9\u5b83\u8fdb\u884c\u8be6\u7ec6\u63cf\u8ff0\u3002 \u4e0b\u9762\u662f\u5b9e\u73b0repetition(\"\u91cd\u590d\u6267\u884c\u67d0\u4e2acomputation\")\u7684\u4e24\u79cd\u65b9\u5f0f\uff1a iteration recursion \u65e0\u5904\u4e0d\u5728\u7684repetition Traverse on structure \u53c2\u89c1 Relation-structure-computation\\Structure\\Computation-on-structure \u7ae0\u8282\u3002 Recursion and functional programming \u5728functional programming\u4e00\u822c\u4f7f\u7528recursion\u6765\u5b9e\u73b0repetition\uff0c\u5173\u4e8e\u6b64\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u6709\u63cf\u8ff0: infogalactic Recursion (computer science) Some functional programming languages do not define any looping constructs but rely solely on recursion to repeatedly call code. infogalactic Functional programming Iteration (looping) in functional languages is usually accomplished via recursion . Recursive functions invoke themselves, allowing an operation to be performed over and over until the base case is reached. Iteration VS recursion \u4e24\u79cd\u90fd\u80fd\u591f\u5b9e\u73b0repetition\u3002 Iteration\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8eIteration statements NOTE: \u5173\u4e8eIteration statements\uff0c\u53c2\u89c1: 1) C++: cppreference Iteration statements 2) Python: 8.3. The for statement \u00b6 8.2. The while statement \u00b6 Recursion\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8erecursive function \u5f53\u6240\u5904\u7684\u73af\u5883\u65e0\u6cd5\u4f7f\u7528Iteration statements\u7684\u65f6\u5019\uff0c\u5219\u53ea\u80fd\u591f\u901a\u8fc7Recursion\u6765\u5b9e\u73b0repetition: 1) \u6709\u7684programming language\u5e76\u6ca1\u6709\u63d0\u4f9biteration statement\uff0c\u6bd4\u5982Lisp\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u53ea\u80fd\u4f7f\u7528recursion\u6765\u5b9e\u73b0repetition\u3002 2) C++ TMP","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Repetition/#_1","text":"\u5728\u524d\u9762\u7ae0\u8282\u5df2\u7ecf\u4ecb\u7ecd\u4e86repetition\uff0c\u672c\u7ae0\u5bf9\u5b83\u8fdb\u884c\u8be6\u7ec6\u63cf\u8ff0\u3002 \u4e0b\u9762\u662f\u5b9e\u73b0repetition(\"\u91cd\u590d\u6267\u884c\u67d0\u4e2acomputation\")\u7684\u4e24\u79cd\u65b9\u5f0f\uff1a iteration recursion","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Repetition/#repetition","text":"","title":"\u65e0\u5904\u4e0d\u5728\u7684repetition"},{"location":"Relation-structure-computation/Computation/Repetition/#traverse#on#structure","text":"\u53c2\u89c1 Relation-structure-computation\\Structure\\Computation-on-structure \u7ae0\u8282\u3002","title":"Traverse on structure"},{"location":"Relation-structure-computation/Computation/Repetition/#recursion#and#functional#programming","text":"\u5728functional programming\u4e00\u822c\u4f7f\u7528recursion\u6765\u5b9e\u73b0repetition\uff0c\u5173\u4e8e\u6b64\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u6709\u63cf\u8ff0: infogalactic Recursion (computer science) Some functional programming languages do not define any looping constructs but rely solely on recursion to repeatedly call code. infogalactic Functional programming Iteration (looping) in functional languages is usually accomplished via recursion . Recursive functions invoke themselves, allowing an operation to be performed over and over until the base case is reached.","title":"Recursion and functional programming"},{"location":"Relation-structure-computation/Computation/Repetition/#iteration#vs#recursion","text":"\u4e24\u79cd\u90fd\u80fd\u591f\u5b9e\u73b0repetition\u3002 Iteration\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8eIteration statements NOTE: \u5173\u4e8eIteration statements\uff0c\u53c2\u89c1: 1) C++: cppreference Iteration statements 2) Python: 8.3. The for statement \u00b6 8.2. The while statement \u00b6 Recursion\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8erecursive function \u5f53\u6240\u5904\u7684\u73af\u5883\u65e0\u6cd5\u4f7f\u7528Iteration statements\u7684\u65f6\u5019\uff0c\u5219\u53ea\u80fd\u591f\u901a\u8fc7Recursion\u6765\u5b9e\u73b0repetition: 1) \u6709\u7684programming language\u5e76\u6ca1\u6709\u63d0\u4f9biteration statement\uff0c\u6bd4\u5982Lisp\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u53ea\u80fd\u4f7f\u7528recursion\u6765\u5b9e\u73b0repetition\u3002 2) C++ TMP","title":"Iteration VS recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0induction\u3001deduction\u3001recursion\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u672c\u7ae0\u7684\u5185\u5bb9\u4e3b\u8981\u6765\u81ea\u4e8e\u7ef4\u57fa\u767e\u79d1\u3002\u5728 Discrete Mathematics and Its Applications \u7684 chapter 5 \u4e2d\u5bf9\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/#_1","text":"\u672c\u7ae0\u63cf\u8ff0induction\u3001deduction\u3001recursion\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u672c\u7ae0\u7684\u5185\u5bb9\u4e3b\u8981\u6765\u81ea\u4e8e\u7ef4\u57fa\u767e\u79d1\u3002\u5728 Discrete Mathematics and Its Applications \u7684 chapter 5 \u4e2d\u5bf9\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/","text":"Summary \u524d\u8a00 \u5728\u524d\u9762\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u4e00\u7cfb\u5217\u6982\u5ff5\uff0c\u5b83\u4eec\u4e4b\u95f4\u662f\u6709\u7740\u4e00\u5b9a\u7684\u5173\u8054\u7684\uff0c\u672c\u6587\u5c31\u5bf9\u5b83\u4eec\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8fdb\u884c\u603b\u7ed3\uff0c\u4e0b\u9762\u662f\u4e2d\u6587\u90e8\u5206\u3002 \u6b63\u6587 \u65e0\u8bba\u662fMathematical induction\uff0c\u8fd8\u662fStructural induction\uff0c\u5b83\u4eec\u672c\u8d28\u4e0a\u90fd\u662f proof method \uff08\u8bc1\u660e\u65b9\u6cd5\uff09\uff0c\u5b83\u63cf\u8ff0\u7684\u662f\u4e00\u79cd\u63a8\u5e7f\u3002 Recursive definition \uff08\u4e5f\u53eb\u505a**inductive definition**\uff09\u6b63\u5982\u5176\u540d\uff0c\u5b83\u662fdefinition\uff0c\u662fspecification\uff0c\u6240\u4ee5\u9700\u8981\u975e\u5e38\u4e25\u683c\uff0c\u5b83\u4e0d\u6d89\u53ca\u5b9e\u73b0\u3002 \u9012\u5f52 \u5f52\u7eb3 Recursion Induction Corecursion Coinduction Structural recursion Structural induction \u5173\u4e8eStructural recursion\u548cStructural induction\uff0c\u53c2\u89c1 Structural induction \u3002 Corecursion \u3001 Induction \u3001iteration\u7684\u65b9\u5411\u76f8\u540c\uff0c\u90fd\u662f\u81ea\u5e95\u5411\u4e0a\uff1b Recursion \u3001 Structural recursion \u7684\u65b9\u5411\u76f8\u540c\uff0c\u90fd\u662f\u81ea\u9876\u5411\u4e0b\uff1b Recursion VS corecursion recursion \u548c corecursion \u7684\u8ba1\u7b97\u65b9\u5411\u662f\u76f8\u53cd\uff1a\u5bf9\u4e8e\u4e00\u4e2a recurrence relations \uff0c\u5982*n! := n \u00d7 (n - 1)!*.\uff0crecursion\u662f\u4ece\u5de6\u81f3\u53f3\uff0c\u4f46\u662fcorecursion\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u4f46\u662f\u80fd\u591f\u6b8a\u9014\u540c\u5f52 recursion works analytically VS corecursion works synthetically recursion top-down VS corecursion bottom-up recursion reduce VS corecursion produce \u5728 Tree traversal \u4e2d\u6709\u5982\u4e0b\u63cf\u8ff0\uff1a Depth-first search is easily implemented via a stack , including recursively (via the call stack ), while breadth-first search is easily implemented via a queue , including corecursively . Recursion VS induction \u65e0\u8bba\u662finduction\u8fd8\u662frecursion\uff0c\u90fd\u9700\u8981base case\u3002 Induction\u672c\u8d28\u4e0a\u90fd\u662f proof method \u3002 \u5728\u7ef4\u57fa\u767e\u79d1 Structural induction \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A structurally recursive function uses the same idea to define a recursive function: \"base cases\" handle each minimal structure and a rule for recursion. Structural recursion is usually proved correct by structural induction; \u53e6\u5916\u53c2\u89c1\uff1a https://www.cs.cmu.edu/~rwh/introsml/techniques/indrec.htm \uff0c\u975e\u5e38\u597d\u7684\u4e00\u7bc7\u6587\u7ae0\u3002 Structural recursion versus generative recursion \u5728 Recursion (computer science) \u4e2d\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u6765\u63cf\u8ff0Structural versus generative recursion\u3002 How does structural recursion differ from generative recursion? The description of generative recursion in Wikipedia is clear to me, but I'm confused about the concept of structural recursion. Can someone explain if a function calculating nth Fibonacci number and a function calculating factorial from 1 to N will be structural or generative? COMMENTS My two pennies: Fib is generative recursive using that definition because the data is \"generated\" as it goes along. Whereas, according to the article, structural recursion is about traversing an [existing] graph. The article goes on to state that an crucial distinction is that structural recursion can be proven to terminate through structural induction .. \u2013 user166390 A The key difference between structural and generative recursion is where a recursive procedure gets the data that it works on and how it processes that data\uff08 recursive procedure gets the data that it works on\u5176\u5b9e\u5c31\u662frecursion function\u7684**\u5165\u53c2**\uff09. Specifically, for structural recursion , a recursive call is made on a subset of the original input data. Whereas for generative recursion , a recursive call is made on data that was constructed/calculated from the original input data. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u603b\u7ed3\u5730\u662f\u975e\u5e38\u597d\u7684\uff1b For example, if you wanted to count the number of elements in a linked list, you could do the following: int NumberOfNodes ( ListNode * node ) { if ( node == nullptr ) return 0 ; return 1 + NumberOfNodes ( node -> next ); } Here, the recursive call to NumberOfNodes is being made on node->next , which is a piece of the original input which already existed. In this case, the recursion works by breaking down the input into smaller pieces, then recursing on the smaller pieces. Similarly, this code to search a BST for a value would be structural recursion, because the recursive calls are to subparts of the original input: TreeNode * Find ( TreeNode * root , DataType value ) { if ( root == nullptr ) return nullptr ; if ( value < root -> value ) return Find ( root -> left , value ); else return Find ( root -> right , value ); The term \"structural recursion\" comes from the fact that these structures (lists, BSTs, etc.) can be defined recursively: A list is either nothing, or a cell followed by a list. A binary tree is either nothing, or a node with two binary trees as children. When doing structural recursion, you are \"undoing\" the operation from which these structures are built out of one another. For example, the NumberOfNodes function \"undoes\" the construction of taking a node and prepending it to an existing list. The Find operator \"undoes\" the operation of gluing a node to two other trees. Therefore, it's easy to see why these functions have to terminate - eventually, you \"undo\" all of the operations that went in to building up the object in the first place, and the recursion stops. On the other hand, consider Quicksort , which does the following: Pick a pivot. Create three new lists: one of all elements less than the pivot, one of all elements greater than the pivot, and one of all elements equal to the pivot. Recursively sort the first and second of these lists. Concatenate the list of smaller, equal, and larger values. Here, the recursive calls are being made on smaller arrays that weren't part of the original input - the lists had to be created from the data. (Typically, an implementation would reuse space for these lists, but those sublists weren't guaranteed to exist directly within the input). This distinction is blurry\uff08\u6a21\u7cca\u7684\uff09 when it comes to natural numbers . Usually, natural numbers are recursively defined as follows: 0 is a natural number. If n is a natural number, n + 1 is a natural number. Nothing else is a natural number. Under this definition, the number n is a \"part\" of n + 1. Therefore, this recursive code to compute n! is structural recursion: int Factorial ( int n ) { if ( n == 0 ) return 1 ; return n * Factorial ( n - 1 ); } This is structural recursion, because the argument n - 1 was a \"part\" of the original input n. Similarly, by this definition, computing the nth Fibonacci number recursively counts as structural recursion: int Fibonacci ( int n ) { if ( n <= 1 ) return n ; return Fibonacci ( n - 1 ) + Fibonacci ( n - 2 ); } This is considered structural recursion because n - 1 is a part of n (formed by \"undoing\" the +1) and n - 2 is a part of n - 1 (again formed by \"undoing\" the +1). On the other hand, this code to compute gcd would be considered generative recursion, rather than structural recursion: int gcd ( int a , int b ) { if ( b == 0 ) return a ; return gcd ( b , a % b ); } The reasoning is that since a % b is \"computed\" from a and b , rather than formed by \"undoing\" some number of +1 operations, the data is generated. The reason that generative recursion is different from structural recursion is that there's no guarantee that it terminates. For example, think about this function: int BadTimes ( int a , int b ) { if ( a == 0 && b == 0 ) return 0 ; return BadTimes ( a * 2 , b - 1 ); } This generative recursive function never terminates: a keeps getting bigger even though b keeps getting smaller. Honestly, I've never heard of this distinction before and I teach courses in discrete math and programming. I wouldn't worry too much about it unless someone is requiring you to know the difference. Hope this helps!","title":"Summary"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/#summary","text":"","title":"Summary"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/#_1","text":"\u5728\u524d\u9762\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u4e00\u7cfb\u5217\u6982\u5ff5\uff0c\u5b83\u4eec\u4e4b\u95f4\u662f\u6709\u7740\u4e00\u5b9a\u7684\u5173\u8054\u7684\uff0c\u672c\u6587\u5c31\u5bf9\u5b83\u4eec\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8fdb\u884c\u603b\u7ed3\uff0c\u4e0b\u9762\u662f\u4e2d\u6587\u90e8\u5206\u3002","title":"\u524d\u8a00"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/#_2","text":"\u65e0\u8bba\u662fMathematical induction\uff0c\u8fd8\u662fStructural induction\uff0c\u5b83\u4eec\u672c\u8d28\u4e0a\u90fd\u662f proof method \uff08\u8bc1\u660e\u65b9\u6cd5\uff09\uff0c\u5b83\u63cf\u8ff0\u7684\u662f\u4e00\u79cd\u63a8\u5e7f\u3002 Recursive definition \uff08\u4e5f\u53eb\u505a**inductive definition**\uff09\u6b63\u5982\u5176\u540d\uff0c\u5b83\u662fdefinition\uff0c\u662fspecification\uff0c\u6240\u4ee5\u9700\u8981\u975e\u5e38\u4e25\u683c\uff0c\u5b83\u4e0d\u6d89\u53ca\u5b9e\u73b0\u3002 \u9012\u5f52 \u5f52\u7eb3 Recursion Induction Corecursion Coinduction Structural recursion Structural induction \u5173\u4e8eStructural recursion\u548cStructural induction\uff0c\u53c2\u89c1 Structural induction \u3002 Corecursion \u3001 Induction \u3001iteration\u7684\u65b9\u5411\u76f8\u540c\uff0c\u90fd\u662f\u81ea\u5e95\u5411\u4e0a\uff1b Recursion \u3001 Structural recursion \u7684\u65b9\u5411\u76f8\u540c\uff0c\u90fd\u662f\u81ea\u9876\u5411\u4e0b\uff1b","title":"\u6b63\u6587"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/#recursion#vs#corecursion","text":"recursion \u548c corecursion \u7684\u8ba1\u7b97\u65b9\u5411\u662f\u76f8\u53cd\uff1a\u5bf9\u4e8e\u4e00\u4e2a recurrence relations \uff0c\u5982*n! := n \u00d7 (n - 1)!*.\uff0crecursion\u662f\u4ece\u5de6\u81f3\u53f3\uff0c\u4f46\u662fcorecursion\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u4f46\u662f\u80fd\u591f\u6b8a\u9014\u540c\u5f52 recursion works analytically VS corecursion works synthetically recursion top-down VS corecursion bottom-up recursion reduce VS corecursion produce \u5728 Tree traversal \u4e2d\u6709\u5982\u4e0b\u63cf\u8ff0\uff1a Depth-first search is easily implemented via a stack , including recursively (via the call stack ), while breadth-first search is easily implemented via a queue , including corecursively .","title":"Recursion VS corecursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/#recursion#vs#induction","text":"\u65e0\u8bba\u662finduction\u8fd8\u662frecursion\uff0c\u90fd\u9700\u8981base case\u3002 Induction\u672c\u8d28\u4e0a\u90fd\u662f proof method \u3002 \u5728\u7ef4\u57fa\u767e\u79d1 Structural induction \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A structurally recursive function uses the same idea to define a recursive function: \"base cases\" handle each minimal structure and a rule for recursion. Structural recursion is usually proved correct by structural induction; \u53e6\u5916\u53c2\u89c1\uff1a https://www.cs.cmu.edu/~rwh/introsml/techniques/indrec.htm \uff0c\u975e\u5e38\u597d\u7684\u4e00\u7bc7\u6587\u7ae0\u3002","title":"Recursion VS induction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/#structural#recursion#versus#generative#recursion","text":"\u5728 Recursion (computer science) \u4e2d\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u6765\u63cf\u8ff0Structural versus generative recursion\u3002","title":"Structural recursion versus generative recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/#how#does#structural#recursion#differ#from#generative#recursion","text":"The description of generative recursion in Wikipedia is clear to me, but I'm confused about the concept of structural recursion. Can someone explain if a function calculating nth Fibonacci number and a function calculating factorial from 1 to N will be structural or generative? COMMENTS My two pennies: Fib is generative recursive using that definition because the data is \"generated\" as it goes along. Whereas, according to the article, structural recursion is about traversing an [existing] graph. The article goes on to state that an crucial distinction is that structural recursion can be proven to terminate through structural induction .. \u2013 user166390","title":"How does structural recursion differ from generative recursion?"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Summary/#a","text":"The key difference between structural and generative recursion is where a recursive procedure gets the data that it works on and how it processes that data\uff08 recursive procedure gets the data that it works on\u5176\u5b9e\u5c31\u662frecursion function\u7684**\u5165\u53c2**\uff09. Specifically, for structural recursion , a recursive call is made on a subset of the original input data. Whereas for generative recursion , a recursive call is made on data that was constructed/calculated from the original input data. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u603b\u7ed3\u5730\u662f\u975e\u5e38\u597d\u7684\uff1b For example, if you wanted to count the number of elements in a linked list, you could do the following: int NumberOfNodes ( ListNode * node ) { if ( node == nullptr ) return 0 ; return 1 + NumberOfNodes ( node -> next ); } Here, the recursive call to NumberOfNodes is being made on node->next , which is a piece of the original input which already existed. In this case, the recursion works by breaking down the input into smaller pieces, then recursing on the smaller pieces. Similarly, this code to search a BST for a value would be structural recursion, because the recursive calls are to subparts of the original input: TreeNode * Find ( TreeNode * root , DataType value ) { if ( root == nullptr ) return nullptr ; if ( value < root -> value ) return Find ( root -> left , value ); else return Find ( root -> right , value ); The term \"structural recursion\" comes from the fact that these structures (lists, BSTs, etc.) can be defined recursively: A list is either nothing, or a cell followed by a list. A binary tree is either nothing, or a node with two binary trees as children. When doing structural recursion, you are \"undoing\" the operation from which these structures are built out of one another. For example, the NumberOfNodes function \"undoes\" the construction of taking a node and prepending it to an existing list. The Find operator \"undoes\" the operation of gluing a node to two other trees. Therefore, it's easy to see why these functions have to terminate - eventually, you \"undo\" all of the operations that went in to building up the object in the first place, and the recursion stops. On the other hand, consider Quicksort , which does the following: Pick a pivot. Create three new lists: one of all elements less than the pivot, one of all elements greater than the pivot, and one of all elements equal to the pivot. Recursively sort the first and second of these lists. Concatenate the list of smaller, equal, and larger values. Here, the recursive calls are being made on smaller arrays that weren't part of the original input - the lists had to be created from the data. (Typically, an implementation would reuse space for these lists, but those sublists weren't guaranteed to exist directly within the input). This distinction is blurry\uff08\u6a21\u7cca\u7684\uff09 when it comes to natural numbers . Usually, natural numbers are recursively defined as follows: 0 is a natural number. If n is a natural number, n + 1 is a natural number. Nothing else is a natural number. Under this definition, the number n is a \"part\" of n + 1. Therefore, this recursive code to compute n! is structural recursion: int Factorial ( int n ) { if ( n == 0 ) return 1 ; return n * Factorial ( n - 1 ); } This is structural recursion, because the argument n - 1 was a \"part\" of the original input n. Similarly, by this definition, computing the nth Fibonacci number recursively counts as structural recursion: int Fibonacci ( int n ) { if ( n <= 1 ) return n ; return Fibonacci ( n - 1 ) + Fibonacci ( n - 2 ); } This is considered structural recursion because n - 1 is a part of n (formed by \"undoing\" the +1) and n - 2 is a part of n - 1 (again formed by \"undoing\" the +1). On the other hand, this code to compute gcd would be considered generative recursion, rather than structural recursion: int gcd ( int a , int b ) { if ( b == 0 ) return a ; return gcd ( b , a % b ); } The reasoning is that since a % b is \"computed\" from a and b , rather than formed by \"undoing\" some number of +1 operations, the data is generated. The reason that generative recursion is different from structural recursion is that there's no guarantee that it terminates. For example, think about this function: int BadTimes ( int a , int b ) { if ( a == 0 && b == 0 ) return 0 ; return BadTimes ( a * 2 , b - 1 ); } This generative recursive function never terminates: a keeps getting bigger even though b keeps getting smaller. Honestly, I've never heard of this distinction before and I teach courses in discrete math and programming. I wouldn't worry too much about it unless someone is requiring you to know the difference. Hope this helps!","title":"A"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0induction\uff08\u5f52\u7eb3\uff09\u548cdeduction\uff08\u6f14\u7ece\uff09\u7684\u5185\u5bb9\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/#_1","text":"\u672c\u7ae0\u63cf\u8ff0induction\uff08\u5f52\u7eb3\uff09\u548cdeduction\uff08\u6f14\u7ece\uff09\u7684\u5185\u5bb9\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Coinduction/","text":"Coinduction \u201ccoinduction\u201d\u5373\u201c\u5171\u5f52\u7eb3\u201d\u3002\u5728\u9605\u8bfb Stream (computing) \u65f6\u78b0\u5230\u7684\u8fd9\u4e2a\u6982\u5ff5\u3002 \u7ef4\u57fa\u767e\u79d1 Coinduction \u672c\u6587\u6ca1\u6709\u7406\u89e3\u3002","title":"Coinduction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Coinduction/#coinduction","text":"\u201ccoinduction\u201d\u5373\u201c\u5171\u5f52\u7eb3\u201d\u3002\u5728\u9605\u8bfb Stream (computing) \u65f6\u78b0\u5230\u7684\u8fd9\u4e2a\u6982\u5ff5\u3002","title":"Coinduction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Coinduction/#coinduction_1","text":"\u672c\u6587\u6ca1\u6709\u7406\u89e3\u3002","title":"\u7ef4\u57fa\u767e\u79d1Coinduction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction-and-deduction/","text":"Induction & Deduction \u5728\u9605\u8bfb\u4e2d\u7ecf\u5e38\u78b0\u5230\u8fd9\u4e24\u4e2a\u8bcd\u8bed\uff0c\u201cinduction\u201d\u5373\u201c\u5f52\u7eb3\u201d\uff0c\u201cdeduction\u201d\u5373\u201c\u6f14\u7ece\u201d\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u603b\u7ed3\u3002 Deduction & Induction In logic, we often refer to the two broad methods of reasoning as the deductive and inductive approaches. \u4e24\u8005\u90fd\u662f\u201c\u63a8\u7406\u201d\u7684\u65b9\u6cd5\u3002 Deductive reasoning works from the more general to the more specific. Sometimes this is informally called a \u201ctop-down\u201d approach. Inductive reasoning works the other way, moving from specific observations to broader generalizations and theories. Informally, we sometimes call this a \u201cbottom up\u201d approach (please note that it\u2019s \u201cbottom up\u201d and not \u201cbottom**s** up\u201d which is the kind of thing the bartender says to customers when he\u2019s trying to close for the night!).","title":"Induction-and-deduction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction-and-deduction/#induction#deduction","text":"\u5728\u9605\u8bfb\u4e2d\u7ecf\u5e38\u78b0\u5230\u8fd9\u4e24\u4e2a\u8bcd\u8bed\uff0c\u201cinduction\u201d\u5373\u201c\u5f52\u7eb3\u201d\uff0c\u201cdeduction\u201d\u5373\u201c\u6f14\u7ece\u201d\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u603b\u7ed3\u3002","title":"Induction &amp; Deduction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction-and-deduction/#deduction#induction","text":"In logic, we often refer to the two broad methods of reasoning as the deductive and inductive approaches. \u4e24\u8005\u90fd\u662f\u201c\u63a8\u7406\u201d\u7684\u65b9\u6cd5\u3002 Deductive reasoning works from the more general to the more specific. Sometimes this is informally called a \u201ctop-down\u201d approach. Inductive reasoning works the other way, moving from specific observations to broader generalizations and theories. Informally, we sometimes call this a \u201cbottom up\u201d approach (please note that it\u2019s \u201cbottom up\u201d and not \u201cbottom**s** up\u201d which is the kind of thing the bartender says to customers when he\u2019s trying to close for the night!).","title":"Deduction &amp; Induction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction-and-deduction/#_1","text":"","title":""},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction/","text":"Induction \"induction\"\u5373\u5f52\u7eb3\uff0c\u6211\u89c9\u5f97\u5b83\u6240\u63cf\u8ff0\u7684\u662f\u4e00\u79cd\u63a8\u5e7f\u3002\u672c\u6587\u5bf9\u5b83\u8fdb\u884c\u5206\u6790\u3002 \u7ef4\u57fa\u767e\u79d1 Mathematical induction \u6570\u5b66\u5f52\u7eb3\u6cd5 Mathematical induction is a mathematical proof technique. It is essentially used to prove that a property P ( n ) holds for every natural number n , i.e. for n = 0, 1, 2, 3, and so on. Metaphors can be informally used to understand the concept of mathematical induction, such as the metaphor of falling dominoes\uff08\u591a\u7c73\u8bfa\u9aa8\u724c\uff09 or climbing a ladder\uff08\u68af\u5b50\uff09. The method of induction requires two cases to be proved. The first case, called the base case (or, sometimes, the basis ), proves that the property holds for the number 0. The second case, called the induction step , proves that if the property holds for one natural number n , then it holds for the next natural number n+1 . These two steps establish the property P(n) for every natural number n=0,1,2,3... The method can be extended to prove statements about more general well-founded structures, such as trees ; this generalization, known as structural induction , is used in mathematical logic and computer science . Mathematical induction in this extended sense is closely related to recursion . Mathematical induction, in some form, is the foundation of all correctness proofs for computer programs . \u5bf9\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u800c\u8a00\uff0c\u78b0\u5230\u7684\u66f4\u591a\u7684\u662f structural induction \u3002 \u7ef4\u57fa\u767e\u79d1 Structural induction NOTE: \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7\u6587\u7ae0\u603b\u7ed3\u7684\u975e\u5e38\u597d Structural induction is a proof method that is used in mathematical logic (e.g., in the proof of \u0141o\u015b' theorem ), computer science , graph theory , and some other mathematical fields. It is a generalization of mathematical induction over natural numbers and can be further generalized to arbitrary Noetherian induction . Structural recursion is a recursion method bearing the same relationship to structural induction as ordinary recursion bears to ordinary mathematical induction . NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5c06\u8868\u8fbe\u7684\u542b\u4e49\u662f\uff1astructural recursion \u548c structural induction \u7684\u5173\u7cfb\u4e0erecursion \u548c mathematical induction \u7684\u5173\u7cfb \u76f8\u540c\u3002 Structural induction is used to prove that some proposition P ( x ) holds for all x of some sort of recursively defined structure, such as formulas , lists , or trees . A well-founded partial order is defined on the structures (\"subformula\" for formulas, \"sublist\" for lists, and \"subtree\" for trees). The structural induction proof is a proof that the proposition holds for all the minimal structures and that if it holds for the immediate substructures of a certain structure S , then it must hold for S also. (Formally speaking, this then satisfies the premises of an axiom of well-founded induction , which asserts that these two conditions are sufficient for the proposition to hold for all x .) A structurally recursive function uses the same idea to define a recursive function: \"base cases\" handle each minimal structure and a rule for recursion. Structural recursion is usually proved correct by structural induction ; in particularly easy cases, the inductive step is often left out. The length and ++ functions in the example below are structurally recursive. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86structurally recursive function\u7684\u5b9e\u73b0\u601d\u8def\u3002 NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e5f\u8bf4\u660e\u4e86recursion\u548cinduction\u4e4b\u95f4\u7684\u5173\u7cfb\uff1arecursion\u7528\u4e8e\u8ba1\u7b97\u673a\u5b9e\u73b0\uff0cinduction\u7528\u4e8e\u6570\u5b66\u8bc1\u660e\u3002 \u7ef4\u57fa\u767e\u79d1 Transfinite induction \u201ctransfinite induction\u201d\u5373\u201c\u65e0\u9650\u5f52\u7eb3\u201d\u3002","title":"Induction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction/#induction","text":"\"induction\"\u5373\u5f52\u7eb3\uff0c\u6211\u89c9\u5f97\u5b83\u6240\u63cf\u8ff0\u7684\u662f\u4e00\u79cd\u63a8\u5e7f\u3002\u672c\u6587\u5bf9\u5b83\u8fdb\u884c\u5206\u6790\u3002","title":"Induction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction/#mathematical#induction","text":"\u6570\u5b66\u5f52\u7eb3\u6cd5 Mathematical induction is a mathematical proof technique. It is essentially used to prove that a property P ( n ) holds for every natural number n , i.e. for n = 0, 1, 2, 3, and so on. Metaphors can be informally used to understand the concept of mathematical induction, such as the metaphor of falling dominoes\uff08\u591a\u7c73\u8bfa\u9aa8\u724c\uff09 or climbing a ladder\uff08\u68af\u5b50\uff09. The method of induction requires two cases to be proved. The first case, called the base case (or, sometimes, the basis ), proves that the property holds for the number 0. The second case, called the induction step , proves that if the property holds for one natural number n , then it holds for the next natural number n+1 . These two steps establish the property P(n) for every natural number n=0,1,2,3... The method can be extended to prove statements about more general well-founded structures, such as trees ; this generalization, known as structural induction , is used in mathematical logic and computer science . Mathematical induction in this extended sense is closely related to recursion . Mathematical induction, in some form, is the foundation of all correctness proofs for computer programs . \u5bf9\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u800c\u8a00\uff0c\u78b0\u5230\u7684\u66f4\u591a\u7684\u662f structural induction \u3002","title":"\u7ef4\u57fa\u767e\u79d1Mathematical induction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction/#structural#induction","text":"NOTE: \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7\u6587\u7ae0\u603b\u7ed3\u7684\u975e\u5e38\u597d Structural induction is a proof method that is used in mathematical logic (e.g., in the proof of \u0141o\u015b' theorem ), computer science , graph theory , and some other mathematical fields. It is a generalization of mathematical induction over natural numbers and can be further generalized to arbitrary Noetherian induction . Structural recursion is a recursion method bearing the same relationship to structural induction as ordinary recursion bears to ordinary mathematical induction . NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5c06\u8868\u8fbe\u7684\u542b\u4e49\u662f\uff1astructural recursion \u548c structural induction \u7684\u5173\u7cfb\u4e0erecursion \u548c mathematical induction \u7684\u5173\u7cfb \u76f8\u540c\u3002 Structural induction is used to prove that some proposition P ( x ) holds for all x of some sort of recursively defined structure, such as formulas , lists , or trees . A well-founded partial order is defined on the structures (\"subformula\" for formulas, \"sublist\" for lists, and \"subtree\" for trees). The structural induction proof is a proof that the proposition holds for all the minimal structures and that if it holds for the immediate substructures of a certain structure S , then it must hold for S also. (Formally speaking, this then satisfies the premises of an axiom of well-founded induction , which asserts that these two conditions are sufficient for the proposition to hold for all x .) A structurally recursive function uses the same idea to define a recursive function: \"base cases\" handle each minimal structure and a rule for recursion. Structural recursion is usually proved correct by structural induction ; in particularly easy cases, the inductive step is often left out. The length and ++ functions in the example below are structurally recursive. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86structurally recursive function\u7684\u5b9e\u73b0\u601d\u8def\u3002 NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e5f\u8bf4\u660e\u4e86recursion\u548cinduction\u4e4b\u95f4\u7684\u5173\u7cfb\uff1arecursion\u7528\u4e8e\u8ba1\u7b97\u673a\u5b9e\u73b0\uff0cinduction\u7528\u4e8e\u6570\u5b66\u8bc1\u660e\u3002","title":"\u7ef4\u57fa\u767e\u79d1Structural induction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Induction-and-deduction/Induction/#transfinite#induction","text":"\u201ctransfinite induction\u201d\u5373\u201c\u65e0\u9650\u5f52\u7eb3\u201d\u3002","title":"\u7ef4\u57fa\u767e\u79d1Transfinite induction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/","text":"\u5173\u4e8e\u672c\u7ae0 \u9012\u5f52\u662f\u4e00\u79cd\u4f18\u826f\u7684\u7279\u6027\uff0c\u5728\u5404\u4e2a\u5b66\u79d1\u4e2d\u90fd\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6982\u5ff5\u3002\u5728\u591a\u4e2a\u5de5\u7a0b\u4e2d\u90fd\u4f1a\u6d89\u53ca\u5230\u9012\u5f52\uff0c\u6240\u4ee5\u672c\u7ae0\u5bf9\u9012\u5f52\u5b9a\u4e49\u53ca\u9012\u5f52\u8fdb\u884c\u8bba\u8ff0\u3002\u672c\u7ae0\u5148\u8bba\u8ff0\u201d Recursive-definition \u201c\uff0c\u7136\u540e\u8bba\u8ff0\u66f4\u52a0\u62bd\u8c61\u7684\u201d Recursion \u201c\uff0c\u56e0\u4e3a\u524d\u8005\u76f8\u5bf9\u800c\u6765\u8bf4\u66f4\u52a0\u6613\u4e8e\u7406\u89e3\u3002\u6709\u4e86\u8fd9\u4e9b\u8ba4\u77e5\uff0c\u6211\u4eec\u518d\u6765\u5b66\u4e60recursion in computer science\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/#_1","text":"\u9012\u5f52\u662f\u4e00\u79cd\u4f18\u826f\u7684\u7279\u6027\uff0c\u5728\u5404\u4e2a\u5b66\u79d1\u4e2d\u90fd\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6982\u5ff5\u3002\u5728\u591a\u4e2a\u5de5\u7a0b\u4e2d\u90fd\u4f1a\u6d89\u53ca\u5230\u9012\u5f52\uff0c\u6240\u4ee5\u672c\u7ae0\u5bf9\u9012\u5f52\u5b9a\u4e49\u53ca\u9012\u5f52\u8fdb\u884c\u8bba\u8ff0\u3002\u672c\u7ae0\u5148\u8bba\u8ff0\u201d Recursive-definition \u201c\uff0c\u7136\u540e\u8bba\u8ff0\u66f4\u52a0\u62bd\u8c61\u7684\u201d Recursion \u201c\uff0c\u56e0\u4e3a\u524d\u8005\u76f8\u5bf9\u800c\u6765\u8bf4\u66f4\u52a0\u6613\u4e8e\u7406\u89e3\u3002\u6709\u4e86\u8fd9\u4e9b\u8ba4\u77e5\uff0c\u6211\u4eec\u518d\u6765\u5b66\u4e60recursion in computer science\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/","text":"Recurrence relation \u201crecurrence relation\u201d\u5373\u201c\u9012\u5f52\u5173\u7cfb\u201d\uff0c\u201c\u9012\u5f52\u65b9\u7a0b\u201d\uff0c\u5b83\u662f\u4e00\u4e2a\u6570\u5b66\u6982\u5ff5\u3002\u5728 Discrete Mathematics and Its Applications \u7684chapter 4 advanced counting techniques\u4e2d\u5bf9\u5b83\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 \u201crecurrence relation\u201d\u6240\u63cf\u8ff0\u7684relation\u662f \u79bb\u6563 \u7684\u3002 \u7ef4\u57fa\u767e\u79d1 Recurrence relation In mathematics , a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given; each further term of the sequence or array is defined as a function of the preceding terms. NOTE: \u663e\u7136\uff0crecurrence relation\u662f\u4e00\u4e2a\u7684 recursive definition \u3002 The term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation . However, \"difference equation\" is frequently used to refer to any recurrence relation. Definition NOTE: \u539f\u6587\u7684\u8fd9\u4e00\u6bb5\u975e\u5e38\u96be\u61c2 Examples Factorial The factorial is defined by the recurrence relation $ n!=n(n-1)!\\quad {\\text{for}}\\quad n>0, $ and the initial condition $ 0!=1. $ Logistic map Fibonacci numbers Solving NOTE: \u7ed9\u5b9a\u4e00\u4e2arecurrence relation\uff0c\u5982\u4f55\u6c42\u89e3\u51fa\u5b83\u7684\u901a\u7528\u8868\u8fbe\u5f0f\uff0c\u8fd9\u662f\u672c\u8282\u6240\u8ba8\u8bba\u7684\u95ee\u9898\u3002 Applications Computer science Recurrence relations are also of fundamental importance in analysis of algorithms . TODO: \u6dfb\u52a0\u94fe\u63a5\u5230\u5de5\u7a0b algorithm \u7684analysis of algorithm\u7ae0\u8282\u3002","title":"Recurrence-relation"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#recurrence#relation","text":"\u201crecurrence relation\u201d\u5373\u201c\u9012\u5f52\u5173\u7cfb\u201d\uff0c\u201c\u9012\u5f52\u65b9\u7a0b\u201d\uff0c\u5b83\u662f\u4e00\u4e2a\u6570\u5b66\u6982\u5ff5\u3002\u5728 Discrete Mathematics and Its Applications \u7684chapter 4 advanced counting techniques\u4e2d\u5bf9\u5b83\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 \u201crecurrence relation\u201d\u6240\u63cf\u8ff0\u7684relation\u662f \u79bb\u6563 \u7684\u3002","title":"Recurrence relation"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#recurrence#relation_1","text":"In mathematics , a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given; each further term of the sequence or array is defined as a function of the preceding terms. NOTE: \u663e\u7136\uff0crecurrence relation\u662f\u4e00\u4e2a\u7684 recursive definition \u3002 The term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation . However, \"difference equation\" is frequently used to refer to any recurrence relation.","title":"\u7ef4\u57fa\u767e\u79d1Recurrence relation"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#definition","text":"NOTE: \u539f\u6587\u7684\u8fd9\u4e00\u6bb5\u975e\u5e38\u96be\u61c2","title":"Definition"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#examples","text":"","title":"Examples"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#factorial","text":"The factorial is defined by the recurrence relation $ n!=n(n-1)!\\quad {\\text{for}}\\quad n>0, $ and the initial condition $ 0!=1. $","title":"Factorial"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#logistic#map","text":"","title":"Logistic map"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#fibonacci#numbers","text":"","title":"Fibonacci numbers"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#solving","text":"NOTE: \u7ed9\u5b9a\u4e00\u4e2arecurrence relation\uff0c\u5982\u4f55\u6c42\u89e3\u51fa\u5b83\u7684\u901a\u7528\u8868\u8fbe\u5f0f\uff0c\u8fd9\u662f\u672c\u8282\u6240\u8ba8\u8bba\u7684\u95ee\u9898\u3002","title":"Solving"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#applications","text":"","title":"Applications"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recurrence-relation/#computer#science","text":"Recurrence relations are also of fundamental importance in analysis of algorithms . TODO: \u6dfb\u52a0\u94fe\u63a5\u5230\u5de5\u7a0b algorithm \u7684analysis of algorithm\u7ae0\u8282\u3002","title":"Computer science"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/","text":"Recursion \u672c\u6587\u63cf\u8ff0Recursion\uff0c\u5176\u5b9e\u4e0a\u4e00\u7bc7 Recursive-definition \u4e2d\u7684\u5185\u5bb9\u662f\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u7ef4\u57fa\u767e\u79d1 Recursion Recursion (adjective: recursive ) occurs when a thing is defined in terms of itself or of its type. NOTE: \u5176\u5b9e\u4e0a\u8ff0\u5b9a\u4e49\u5c31\u662f\u7684\u542b\u4e49\u5176\u5b9e\u5c31\u662f recursive definition \u3002\u5728\u7ef4\u57fa\u767e\u79d1 Recursive acronym \u4e2d\u4f7f\u7528 \u201crefers to itself\u201d \u6765\u8868\u8fbe\u8fd9\u79cd\u542b\u4e49\u3002 Recursion is used in a variety of disciplines ranging from linguistics to logic . The most common application of recursion is in mathematics and computer science , where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no loop or infinite chain of references can occur. NOTE: \u5982\u679c\u51fa\u73b0loop\u6216\u8005infinite chain\uff0c\u5219\u7a0b\u5e8f\u5c31\u4f1a\u51fa\u73b0\u6b7b\u5faa\u73af\uff1b Formal definitions In mathematics and computer science, a class of objects or methods exhibits recursive behavior when it can be defined by two properties: A simple base case (or cases)\u2014a terminating scenario that does not use recursion to produce an answer A set of rules that reduces \uff08\u8fd9\u4e2a\u8bcd\u7528\u5f97\u975e\u5e38\u597d\uff09 all other cases toward the base case NOTE: \u201creduce\u201d\u8bf4\u660erecursion\u662f\u81ea\u9876\u5411\u4e0b\u7684\u3002 The Fibonacci sequence is a classic example of recursion: $ {\\text{Fib}}(0)=0{\\text{ as base case 1,}} $ $ {\\text{Fib}}(1)=1{\\text{ as base case 2,}} $ $ {\\text{For all integers }}n>1,~{\\text{ Fib}}(n):={\\text{Fib}}(n-1)+{\\text{Fib}}(n-2). $ Many mathematical axioms\uff08\u516c\u7406\uff09 are based upon recursive rules . For example, the formal definition of the natural numbers by the Peano axioms can be described as: 0 is a natural number, and each natural number has a successor, which is also a natural number. By this base case and recursive rule, one can generate the set of all natural numbers. Recursively defined mathematical objects include functions , sets , and especially fractals . NOTE: \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5e94\u8be5\u5bf9recursive definition\u654f\u611f\u3002 In mathematics Recursively defined sets \u53c2\u89c1\u6587\u7ae0 Recursive definition \u3002 Example: the natural numbers Example: Proof procedure Functional recursion \u9012\u5f52\u51fd\u6570 A function may be recursively defined in terms of itself. A familiar example is the Fibonacci number sequence: F ( n ) = F ( n \u2212 1) + F ( n \u2212 2). For such a definition to be useful, it must be reducible to non-recursively defined values: in this case F (0) = 0 and F (1) = 1. A famous recursive function is the Ackermann function , which, unlike the Fibonacci sequence, cannot be expressed without recursion. Finite subdivision rules Main article: Finite subdivision rule The recursion theorem \u9012\u5f52\u5b9a\u7406 In set theory , this is a theorem guaranteeing that recursively defined functions exist. Given a set X , an element a of X and a function $ f:X\\rightarrow X $, the theorem states that there is a unique function $ F:\\mathbb {N} \\rightarrow X $ (where $ \\mathbb {N} $ denotes the set of natural numbers including zero) such that $ F(0)=a $ $ F(n+1)=f(F(n)) $ for any natural number n . NOTE: \u5e76\u6ca1\u6709\u641e\u61c2 In computer science \u53c2\u89c1\u5de5\u7a0b\u6587\u7ae0 Recursion-in-computer-science\\Recursion(computer-science).md \u3002","title":"Recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#recursion","text":"\u672c\u6587\u63cf\u8ff0Recursion\uff0c\u5176\u5b9e\u4e0a\u4e00\u7bc7 Recursive-definition \u4e2d\u7684\u5185\u5bb9\u662f\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u7684\u3002","title":"Recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#recursion_1","text":"Recursion (adjective: recursive ) occurs when a thing is defined in terms of itself or of its type. NOTE: \u5176\u5b9e\u4e0a\u8ff0\u5b9a\u4e49\u5c31\u662f\u7684\u542b\u4e49\u5176\u5b9e\u5c31\u662f recursive definition \u3002\u5728\u7ef4\u57fa\u767e\u79d1 Recursive acronym \u4e2d\u4f7f\u7528 \u201crefers to itself\u201d \u6765\u8868\u8fbe\u8fd9\u79cd\u542b\u4e49\u3002 Recursion is used in a variety of disciplines ranging from linguistics to logic . The most common application of recursion is in mathematics and computer science , where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no loop or infinite chain of references can occur. NOTE: \u5982\u679c\u51fa\u73b0loop\u6216\u8005infinite chain\uff0c\u5219\u7a0b\u5e8f\u5c31\u4f1a\u51fa\u73b0\u6b7b\u5faa\u73af\uff1b","title":"\u7ef4\u57fa\u767e\u79d1Recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#formal#definitions","text":"In mathematics and computer science, a class of objects or methods exhibits recursive behavior when it can be defined by two properties: A simple base case (or cases)\u2014a terminating scenario that does not use recursion to produce an answer A set of rules that reduces \uff08\u8fd9\u4e2a\u8bcd\u7528\u5f97\u975e\u5e38\u597d\uff09 all other cases toward the base case NOTE: \u201creduce\u201d\u8bf4\u660erecursion\u662f\u81ea\u9876\u5411\u4e0b\u7684\u3002 The Fibonacci sequence is a classic example of recursion: $ {\\text{Fib}}(0)=0{\\text{ as base case 1,}} $ $ {\\text{Fib}}(1)=1{\\text{ as base case 2,}} $ $ {\\text{For all integers }}n>1,~{\\text{ Fib}}(n):={\\text{Fib}}(n-1)+{\\text{Fib}}(n-2). $ Many mathematical axioms\uff08\u516c\u7406\uff09 are based upon recursive rules . For example, the formal definition of the natural numbers by the Peano axioms can be described as: 0 is a natural number, and each natural number has a successor, which is also a natural number. By this base case and recursive rule, one can generate the set of all natural numbers. Recursively defined mathematical objects include functions , sets , and especially fractals . NOTE: \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5e94\u8be5\u5bf9recursive definition\u654f\u611f\u3002","title":"Formal definitions"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#in#mathematics","text":"","title":"In mathematics"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#recursively#defined#sets","text":"\u53c2\u89c1\u6587\u7ae0 Recursive definition \u3002","title":"Recursively defined sets"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#example#the#natural#numbers","text":"","title":"Example: the natural numbers"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#example#proof#procedure","text":"","title":"Example: Proof procedure"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#functional#recursion","text":"\u9012\u5f52\u51fd\u6570 A function may be recursively defined in terms of itself. A familiar example is the Fibonacci number sequence: F ( n ) = F ( n \u2212 1) + F ( n \u2212 2). For such a definition to be useful, it must be reducible to non-recursively defined values: in this case F (0) = 0 and F (1) = 1. A famous recursive function is the Ackermann function , which, unlike the Fibonacci sequence, cannot be expressed without recursion.","title":"Functional recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#finite#subdivision#rules","text":"Main article: Finite subdivision rule","title":"Finite subdivision rules"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#the#recursion#theorem","text":"\u9012\u5f52\u5b9a\u7406 In set theory , this is a theorem guaranteeing that recursively defined functions exist. Given a set X , an element a of X and a function $ f:X\\rightarrow X $, the theorem states that there is a unique function $ F:\\mathbb {N} \\rightarrow X $ (where $ \\mathbb {N} $ denotes the set of natural numbers including zero) such that $ F(0)=a $ $ F(n+1)=f(F(n)) $ for any natural number n . NOTE: \u5e76\u6ca1\u6709\u641e\u61c2","title":"The recursion theorem"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion/#in#computer#science","text":"\u53c2\u89c1\u5de5\u7a0b\u6587\u7ae0 Recursion-in-computer-science\\Recursion(computer-science).md \u3002","title":"In computer science"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/","text":"Recursive definition \u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u5bf9\u4e8edefinition\uff08\u5b9a\u4e49\uff09\u8fd9\u4e2a\u8bcd\u80af\u5b9a\u4e0d\u4f1a\u964c\u751f\uff0c\u56e0\u4e3a\u6211\u4eec\u6bcf\u5929\u90fd\u5728\u201c\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u201d\u3001\u201c\u5b9a\u4e49\u4e00\u4e2a\u7c7b\u201d\u3002\u7ef4\u57fa\u767e\u79d1\u7684 definition \u5185\u5bb9\u6bd4\u8f83\u6df1\u5965\uff0c\u672c\u6587\u5c06\u7b80\u5355\u5730\u6765\u8bf4definition\uff0c\u5b9a\u4e49\u5c31\u662f\u5728\u524d\u6587\u4e2d\u6240\u63d0\u53ca\u7684\u201c\u63cf\u8ff0\u201d\uff0c\u4e0d\u8fc7\u5b9a\u4e49\u6709\u7740\u66f4\u591a\u9650\u5236\u3002\u672c\u6587\u91cd\u70b9\u7b80\u8ff0\u7684\u662f Recursive definition \uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c recursion \u65e0\u5904\u4e0d\u5728\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5e94\u8be5\u5bf9\u5b83\u4fdd\u6301\u654f\u611f\uff0c\u5bf9\u4e8e\u6240\u6709\u5177\u5907 recursion \u7279\u6027\u7684\uff08\u5305\u62ec\u7c7b\u578b\u3001\u8fc7\u7a0b\u7b49\uff09\uff0c\u90fd\u80fd\u591f\u7ed9\u51fa\u5176 Recursive definition \u3002 wikipedia Recursive definition In mathematics and computer science , a recursive definition , or inductive definition \uff08\u5f52\u7eb3\u5b9a\u4e49\uff09, is used to define the elements in a set in terms of other elements in the set ( Aczel 1977:740ff). Some examples of recursively-definable objects include factorials , natural numbers , Fibonacci numbers , and the Cantor ternary set . NOTE: \u4e0a\u8ff0\u5bf9recursive definition\u7684\u63cf\u8ff0\u4f7f\u7528\u7684\u662f\u6570\u5b66\u4e2d\u7684set\u7684\u6982\u5ff5\uff0c\u6570\u5b66\u4e2d\u7684set\u8868\u793a\u6240\u6709\u5177\u5907\u67d0\u4e00\u7279\u6027\u7684object\u7684\u96c6\u5408\uff0c\u6bd4\u5982\u6211\u4eec\u53ef\u4ee5\u5c06\u6240\u6709\u5177\u5907\u76f8\u540ctype\u7684object\u653e\u5230\u4e00\u4e2aset\u4e2d\u3002\u4e0b\u9762\u6211\u5c06\u5b83\u8f6c\u6362\u4e3asoftware engineer\u66f4\u52a0\u719f\u6089\u7684type\uff08\u7c7b\u578b\uff09\u7684\u6982\u5ff5\uff1a \u89c4\u5b9a\u5c5e\u4e8e\u540c\u4e00\u4e2aset\u7684\u6240\u6709\u5143\u7d20\u90fd\u662f\u5177\u6709\u76f8\u540c\u7c7b\u578b\u7684\u5143\u7d20\uff0c\u5219recursively-definable object\u662f\u7531\u76f8\u540c\u7c7b\u578b\u7684\u5176\u4ed6object\uff08\u5373\u8fd9\u4e2a\u96c6\u5408\u4e2d\u7684\u5176\u4ed6\u5143\u7d20\uff09\u6765\u8fdb\u884c\u5b9a\u4e49\u7684\uff08\u6784\u6210\uff09\u3002 \u53ef\u4ee5\u770b\u5230\uff0c\u5f53\u6211\u4eec\u5c06set\u7406\u89e3\u4e3atype\u540e\uff0c\u539f\u6765\u7684\u63cf\u8ff0\u5c31\u53d8\u6210\u4e86software engineer\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u4e86\u3002 \u4e0a\u8ff0\u5b9a\u4e49\u662f\u975e\u5e38\u4e25\u8c28\u7684\uff0c\u4f7f\u7528\u4e86\u6570\u5b66\u4e2d\u7684 set \u7684\u6982\u5ff5\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u4f7f\u7528\u7684\u662f\u4e00\u79cd\u6570\u5b66\u8bed\u8a00\u3002\u5728\u5176\u4ed6\u6587\u7ae0\u4e2d\u90fd\u53ef\u4ee5\u770b\u5230\u5b8c\u5168\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684recursive definition\uff0c\u6bd4\u5982\uff1a \u5728\u7ef4\u57fa\u767e\u79d1 Recursion \u4e2d\uff1a Recursion (adjective: recursive ) occurs when a thing is defined in terms of itself or of its type. \u5728\u7ef4\u57fa\u767e\u79d1 Recursive acronym \u4e2d\uff1a A recursive acronym is an acronym that refers to itself . \u663e\u7136\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7684\u63cf\u8ff0\u662f\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u5982\u4f55\u7406\u89e3recursively defined function\uff1f \u663e\u7136\uff0c\u4e00\u4e2arecursively defined function\u5c31\u662f\u201c defined in terms of itself or of its type \u201d A recursive definition of a function defines values of the function for some inputs in terms of the values of the same function for other (usually smaller) inputs. For example, the factorial function n ! is defined by the rules 0! = 1. ( n + 1)! = ( n + 1)\u00b7 n !. NOTE: \u539f\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e24\u79cd**recursive definition**\uff1arecursively defined functions and recursive defined objects Form of recursive definitions Most recursive definitions have two foundations: a base case (basis) and an inductive clause . NOTE: \u201cinductive clause\u201d\u7684\u542b\u4e49\u662f\u201c\u5f52\u7eb3\u5b50\u53e5\u201d\uff0c\u5173\u4e8e\u201cinductive \u201d\uff0c\u53c2\u89c1 Induction \u3002 That recursive definitions are valid \u2013 meaning that a recursive definition identifies a unique function \u2013 is a theorem of set theory known as the recursion theorem , the proof of which is non-trivial. Where the domain of the function is the natural numbers , sufficient conditions for the definition to be valid are that the value of f(0) (i.e., base case) is given, and that for n > 0, an algorithm is given for determining f(n) in terms of f(0) , f(1) , ... , f(n-1) (i.e., inductive clause). More generally, recursive definitions of functions can be made whenever the domain is a well-ordered set , using the principle of transfinite recursion . The formal criteria for what constitutes a valid recursive definition are more complex for the general case. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5e76\u6ca1\u6709\u7406\u89e3\u3002 \u603b\u7ed3 \u6309\u7167\u539f\u6587\u7b2c\u4e00\u6bb5\u4e2d\u5bf9recursive definition\u7684\u63cf\u8ff0\uff0c\u53d1\u73b0\u5176\u5b9e\u5b83\u975e\u5e38\u7c7b\u4f3c\u4e8einduction\uff08\u5f52\u7eb3\u6cd5\uff09\uff0c\u6211\u4eec\u77e5\u9053\uff0cinduction\u662fbottom-up\u7684\uff0c\u5c24\u5176\u662f natural numbers \u7684\u4f8b\u5b50\u3002\u800c\u6211\u5bf9recursion\u7684\u60ef\u5e38\u5370\u8c61\u662f\u5b83\u662ftop-down\u3002\u8fd9\u4e24\u8005\u4e0d\u662f\u77db\u76fe\u5417\uff1f \u201crecursive definition\u201d\u662f\u4e00\u79cd\u63cf\u8ff0\u65b9\u5f0f\uff0c\u5b83\u4e0d\u6d89\u53ca\u5b9e\u73b0\u7684\u95ee\u9898\uff0c\u5b83\u7684\u63cf\u8ff0\u53ef\u4ee5\u662f\u7c7b\u4f3cinduction\u7684\u81ea\u5e95\u5411\u4e0a\uff0c\u4e5f\u53ef\u4ee5\u662f\u81ea\u9876\u5411\u4e0b\uff0c\u6b63\u5982\u539f\u6587\u7b2c\u4e00\u6bb5\u6240\u8ff0\u7684\uff1a recursive definition \u4e5f\u53ef\u4ee5\u53eb\u505a**inductive definition**\u3002\u201crecursive definition\u201d\u672c\u8d28\u5728\u4e8e\u201c defined in terms of itself or of its type \u201d\uff0c\u6b63\u5982\u672c\u6587\u7b2c\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684\uff1a In mathematics and computer science , a recursive definition , or inductive definition \uff08\u5f52\u7eb3\u5b9a\u4e49\uff09, is used to define the elements in a set in terms of other elements in the set \u5173\u4e8e\u201crecursive definition\u201d\u7684\u4e00\u4e2a\u4f8b\u5b50\u5c31\u662f Recursive grammar \uff0c\u6bd4\u5982\u5728\u7ef4\u57fa\u767e\u79d1 Recursion (computer science) \u7684 Recursive data types \u6bb5\u4e2d\u7ed9\u51fa\u4e86\u8fd9\u6837\u7684\u4e00\u4e2a\u4f8b\u5b50\uff1a <expr> ::= <number> | (<expr> * <expr>) | (<expr> + <expr>) \u53ef\u4ee5\u770b\u5230\uff0c <expr> \u7684\u5b9a\u4e49\u4e2d\u5c31\u5305\u542b\u4e86\"itself\"\u3002 \u201c\u6211\u5bf9recursion\u7684\u60ef\u5e38\u5370\u8c61\u662f\u5b83\u662ftop-down\u201d\u662f\u6e90\u4e8e\u6211\u662f\u4ecerecursion function\u7684\u89d2\u5ea6\u6765\u770b\u5f85\u7684\u3002\u663e\u7136\uff0c\u8fd9\u662f\u63cf\u8ff0\u4e0e\u5b9e\u73b0\u7684\u5dee\u5f02\u3002\u5728\u539f\u6587\u7684 Form of recursive definitions \u4e2d\u7ed9\u51fa\u4e86recursive definition\u6240\u5bf9\u5e94\u7684function\uff0c\u663e\u7136\u5bf9\u4e8e\u8fd9\u4e2afunction\u7684\u5b9e\u73b0\uff0c\u6211\u4eec\u53ef\u4ee5\u91c7\u7528\u7684\u5b9e\u73b0\u65b9\u5f0f\u6709\u4e24\u79cd\uff1a \u81ea\u5e95\u5411\u4e0a \u81ea\u9876\u5411\u4e0b Examples of recursive definitions Recursive grammar \u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Recursive grammar \u9f99\u4e66 2.2.1 Definition of Grammars Recursive definition in computer science \u6709\u592a\u591a\u592a\u591a\u7684\u7b97\u6cd5\u3001\u7ed3\u6784\u90fd\u662f\u53ef\u4ee5\u4f7f\u7528recursive definition\u7684\uff0c\u80fd\u591frecursive definition\u7684\uff0c\u79f0\u5b83\u5177\u5907\u9012\u5f52\u6027\u3002 sub structure \u4e00\u822c\u5e26\u6709\u201csub\u201d\u7684\u90fd\u662f\u53ef\u4ee5\u8fdb\u884crecursive definition\u7684\uff0c\u5b83\u662f\u4e00\u79cd\u5178\u578b\u7684**containing\u5173\u7cfb**\uff0c\u5b83\u662f\u53ef\u4ee5\u4f7f\u7528CFG\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\u3002\u4e0b\u9762\u662f\u5178\u578b\u7684sub structure\uff1a subtree\uff0csublist\uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Structural induction \u4e2d\u6240\u63d0\u53ca\u7684\uff1a recursively defined structure, such as formulas , lists , or trees \u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50\uff1a Subobjects\uff08\u53c2\u89c1cppreference Object#Subobjects \uff09 \u7ef4\u57fa\u767e\u79d1 Optimal substructure pointer of pointer \u65e0\u8bba\u662f\u5728computer science\u8fd8\u662f\u5728\u5b9e\u9645\u751f\u6d3b\u4e2d\uff0c\u90fd\u5b58\u5728\u7740\u5927\u91cf\u7684\u7c7b\u4f3c\u4e8epointer of pointer\u7684\u4f8b\u5b50\uff1a pointer of pointer array of array\uff08\u5176\u5b9e\u662f\u4e00\u79cdsub structure\uff0c\u4e00\u4e2aarray\u53ef\u4ee5\u770b\u505a\u662f\u7531\u591a\u4e2asub array \u7ec4\u6210\uff09 mother of mother \uff08\u5bb6\u65cfhierarchy\uff09 \u5b83\u4eec\u90fd\u662f\u90fd\u662f\u975e\u5e38\u5178\u578b\u7684recursion definition\u3002\u4e0b\u9762\u5bf9array of array\u5373multiple-dimensional array\u8fdb\u884c\u8bf4\u660e\u3002 multiple-dimensional array \u5728\u9605\u8bfb https://stackoverflow.com/a/4810676 \u65f6\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Multidimensional arrays are often referred to as \"arrays of arrays\" \u663e\u7136\uff0c\u8fd9\u662f\u5178\u578b\u7684recursive definition\u3002 \u4e0e\u6b64\u7c7b\u4f3c\u7684\u8fd8\u6709\uff1amultiple-level pointer\u3002 multiple-dimensional array\u548cmultiple-level pointer\u53ef\u4ee5\u770b\u505a\u662f\u5177\u5907nesting\u5173\u7cfb\uff08\u53c2\u89c1\u5de5\u7a0b data-structure \u7684 Graph\\Tree\\Tree-structure.md \u7ae0\u8282\uff09\u3002 multiple-dimensional array\u548cmultiple-level pointer\u53ef\u4ee5\u9012\u5f52\u7684\u8fdb\u884c\u5904\u7406\uff0c\u5728\u6587\u7ae0 https://stackoverflow.com/a/5580952 \u4e2d\u7ed9\u51fa\u4e86\u793a\u4f8b\u7a0b\u5e8f\u3002 Recursive definition and closure \u6309\u7167recursively defined sets\u7684\u6982\u5ff5\uff0c\u8fd9\u4e2aset\u5728\u6784\u9020\u65b0\u5143\u7d20\u7684operation\u4e0b\u662fclosed\u7684\uff08\u53c2\u89c1 Closure (mathematics) \uff09\u3002 Recursive definition and structure Recursive data types \u4e2d\u4e13\u95e8\u63cf\u8ff0\u53ef\u4ee5\u4f7f\u7528recursive definition\u7684structure\uff0c\u8fd9\u5728 data-structure \u4e2d\u4f1a\u8fdb\u884c\u8ba8\u8bba\u3002 \u53ef\u4ee5\u4f7f\u7528recursive definition\u8fdb\u884c\u5b9a\u4e49\u7684structure\uff0c\u90fd\u5177\u5907\u9012\u5f52\u7279\u6027\u3002","title":"Recursive-Definition"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#recursive#definition","text":"\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u5bf9\u4e8edefinition\uff08\u5b9a\u4e49\uff09\u8fd9\u4e2a\u8bcd\u80af\u5b9a\u4e0d\u4f1a\u964c\u751f\uff0c\u56e0\u4e3a\u6211\u4eec\u6bcf\u5929\u90fd\u5728\u201c\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u201d\u3001\u201c\u5b9a\u4e49\u4e00\u4e2a\u7c7b\u201d\u3002\u7ef4\u57fa\u767e\u79d1\u7684 definition \u5185\u5bb9\u6bd4\u8f83\u6df1\u5965\uff0c\u672c\u6587\u5c06\u7b80\u5355\u5730\u6765\u8bf4definition\uff0c\u5b9a\u4e49\u5c31\u662f\u5728\u524d\u6587\u4e2d\u6240\u63d0\u53ca\u7684\u201c\u63cf\u8ff0\u201d\uff0c\u4e0d\u8fc7\u5b9a\u4e49\u6709\u7740\u66f4\u591a\u9650\u5236\u3002\u672c\u6587\u91cd\u70b9\u7b80\u8ff0\u7684\u662f Recursive definition \uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c recursion \u65e0\u5904\u4e0d\u5728\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5e94\u8be5\u5bf9\u5b83\u4fdd\u6301\u654f\u611f\uff0c\u5bf9\u4e8e\u6240\u6709\u5177\u5907 recursion \u7279\u6027\u7684\uff08\u5305\u62ec\u7c7b\u578b\u3001\u8fc7\u7a0b\u7b49\uff09\uff0c\u90fd\u80fd\u591f\u7ed9\u51fa\u5176 Recursive definition \u3002","title":"Recursive definition"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#wikipedia#recursive#definition","text":"In mathematics and computer science , a recursive definition , or inductive definition \uff08\u5f52\u7eb3\u5b9a\u4e49\uff09, is used to define the elements in a set in terms of other elements in the set ( Aczel 1977:740ff). Some examples of recursively-definable objects include factorials , natural numbers , Fibonacci numbers , and the Cantor ternary set . NOTE: \u4e0a\u8ff0\u5bf9recursive definition\u7684\u63cf\u8ff0\u4f7f\u7528\u7684\u662f\u6570\u5b66\u4e2d\u7684set\u7684\u6982\u5ff5\uff0c\u6570\u5b66\u4e2d\u7684set\u8868\u793a\u6240\u6709\u5177\u5907\u67d0\u4e00\u7279\u6027\u7684object\u7684\u96c6\u5408\uff0c\u6bd4\u5982\u6211\u4eec\u53ef\u4ee5\u5c06\u6240\u6709\u5177\u5907\u76f8\u540ctype\u7684object\u653e\u5230\u4e00\u4e2aset\u4e2d\u3002\u4e0b\u9762\u6211\u5c06\u5b83\u8f6c\u6362\u4e3asoftware engineer\u66f4\u52a0\u719f\u6089\u7684type\uff08\u7c7b\u578b\uff09\u7684\u6982\u5ff5\uff1a \u89c4\u5b9a\u5c5e\u4e8e\u540c\u4e00\u4e2aset\u7684\u6240\u6709\u5143\u7d20\u90fd\u662f\u5177\u6709\u76f8\u540c\u7c7b\u578b\u7684\u5143\u7d20\uff0c\u5219recursively-definable object\u662f\u7531\u76f8\u540c\u7c7b\u578b\u7684\u5176\u4ed6object\uff08\u5373\u8fd9\u4e2a\u96c6\u5408\u4e2d\u7684\u5176\u4ed6\u5143\u7d20\uff09\u6765\u8fdb\u884c\u5b9a\u4e49\u7684\uff08\u6784\u6210\uff09\u3002 \u53ef\u4ee5\u770b\u5230\uff0c\u5f53\u6211\u4eec\u5c06set\u7406\u89e3\u4e3atype\u540e\uff0c\u539f\u6765\u7684\u63cf\u8ff0\u5c31\u53d8\u6210\u4e86software engineer\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u4e86\u3002 \u4e0a\u8ff0\u5b9a\u4e49\u662f\u975e\u5e38\u4e25\u8c28\u7684\uff0c\u4f7f\u7528\u4e86\u6570\u5b66\u4e2d\u7684 set \u7684\u6982\u5ff5\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u4f7f\u7528\u7684\u662f\u4e00\u79cd\u6570\u5b66\u8bed\u8a00\u3002\u5728\u5176\u4ed6\u6587\u7ae0\u4e2d\u90fd\u53ef\u4ee5\u770b\u5230\u5b8c\u5168\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684recursive definition\uff0c\u6bd4\u5982\uff1a \u5728\u7ef4\u57fa\u767e\u79d1 Recursion \u4e2d\uff1a Recursion (adjective: recursive ) occurs when a thing is defined in terms of itself or of its type. \u5728\u7ef4\u57fa\u767e\u79d1 Recursive acronym \u4e2d\uff1a A recursive acronym is an acronym that refers to itself . \u663e\u7136\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7684\u63cf\u8ff0\u662f\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u5982\u4f55\u7406\u89e3recursively defined function\uff1f \u663e\u7136\uff0c\u4e00\u4e2arecursively defined function\u5c31\u662f\u201c defined in terms of itself or of its type \u201d A recursive definition of a function defines values of the function for some inputs in terms of the values of the same function for other (usually smaller) inputs. For example, the factorial function n ! is defined by the rules 0! = 1. ( n + 1)! = ( n + 1)\u00b7 n !. NOTE: \u539f\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e24\u79cd**recursive definition**\uff1arecursively defined functions and recursive defined objects","title":"wikipedia Recursive definition"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#form#of#recursive#definitions","text":"Most recursive definitions have two foundations: a base case (basis) and an inductive clause . NOTE: \u201cinductive clause\u201d\u7684\u542b\u4e49\u662f\u201c\u5f52\u7eb3\u5b50\u53e5\u201d\uff0c\u5173\u4e8e\u201cinductive \u201d\uff0c\u53c2\u89c1 Induction \u3002 That recursive definitions are valid \u2013 meaning that a recursive definition identifies a unique function \u2013 is a theorem of set theory known as the recursion theorem , the proof of which is non-trivial. Where the domain of the function is the natural numbers , sufficient conditions for the definition to be valid are that the value of f(0) (i.e., base case) is given, and that for n > 0, an algorithm is given for determining f(n) in terms of f(0) , f(1) , ... , f(n-1) (i.e., inductive clause). More generally, recursive definitions of functions can be made whenever the domain is a well-ordered set , using the principle of transfinite recursion . The formal criteria for what constitutes a valid recursive definition are more complex for the general case. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5e76\u6ca1\u6709\u7406\u89e3\u3002","title":"Form of recursive definitions"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#_1","text":"\u6309\u7167\u539f\u6587\u7b2c\u4e00\u6bb5\u4e2d\u5bf9recursive definition\u7684\u63cf\u8ff0\uff0c\u53d1\u73b0\u5176\u5b9e\u5b83\u975e\u5e38\u7c7b\u4f3c\u4e8einduction\uff08\u5f52\u7eb3\u6cd5\uff09\uff0c\u6211\u4eec\u77e5\u9053\uff0cinduction\u662fbottom-up\u7684\uff0c\u5c24\u5176\u662f natural numbers \u7684\u4f8b\u5b50\u3002\u800c\u6211\u5bf9recursion\u7684\u60ef\u5e38\u5370\u8c61\u662f\u5b83\u662ftop-down\u3002\u8fd9\u4e24\u8005\u4e0d\u662f\u77db\u76fe\u5417\uff1f \u201crecursive definition\u201d\u662f\u4e00\u79cd\u63cf\u8ff0\u65b9\u5f0f\uff0c\u5b83\u4e0d\u6d89\u53ca\u5b9e\u73b0\u7684\u95ee\u9898\uff0c\u5b83\u7684\u63cf\u8ff0\u53ef\u4ee5\u662f\u7c7b\u4f3cinduction\u7684\u81ea\u5e95\u5411\u4e0a\uff0c\u4e5f\u53ef\u4ee5\u662f\u81ea\u9876\u5411\u4e0b\uff0c\u6b63\u5982\u539f\u6587\u7b2c\u4e00\u6bb5\u6240\u8ff0\u7684\uff1a recursive definition \u4e5f\u53ef\u4ee5\u53eb\u505a**inductive definition**\u3002\u201crecursive definition\u201d\u672c\u8d28\u5728\u4e8e\u201c defined in terms of itself or of its type \u201d\uff0c\u6b63\u5982\u672c\u6587\u7b2c\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684\uff1a In mathematics and computer science , a recursive definition , or inductive definition \uff08\u5f52\u7eb3\u5b9a\u4e49\uff09, is used to define the elements in a set in terms of other elements in the set \u5173\u4e8e\u201crecursive definition\u201d\u7684\u4e00\u4e2a\u4f8b\u5b50\u5c31\u662f Recursive grammar \uff0c\u6bd4\u5982\u5728\u7ef4\u57fa\u767e\u79d1 Recursion (computer science) \u7684 Recursive data types \u6bb5\u4e2d\u7ed9\u51fa\u4e86\u8fd9\u6837\u7684\u4e00\u4e2a\u4f8b\u5b50\uff1a <expr> ::= <number> | (<expr> * <expr>) | (<expr> + <expr>) \u53ef\u4ee5\u770b\u5230\uff0c <expr> \u7684\u5b9a\u4e49\u4e2d\u5c31\u5305\u542b\u4e86\"itself\"\u3002 \u201c\u6211\u5bf9recursion\u7684\u60ef\u5e38\u5370\u8c61\u662f\u5b83\u662ftop-down\u201d\u662f\u6e90\u4e8e\u6211\u662f\u4ecerecursion function\u7684\u89d2\u5ea6\u6765\u770b\u5f85\u7684\u3002\u663e\u7136\uff0c\u8fd9\u662f\u63cf\u8ff0\u4e0e\u5b9e\u73b0\u7684\u5dee\u5f02\u3002\u5728\u539f\u6587\u7684 Form of recursive definitions \u4e2d\u7ed9\u51fa\u4e86recursive definition\u6240\u5bf9\u5e94\u7684function\uff0c\u663e\u7136\u5bf9\u4e8e\u8fd9\u4e2afunction\u7684\u5b9e\u73b0\uff0c\u6211\u4eec\u53ef\u4ee5\u91c7\u7528\u7684\u5b9e\u73b0\u65b9\u5f0f\u6709\u4e24\u79cd\uff1a \u81ea\u5e95\u5411\u4e0a \u81ea\u9876\u5411\u4e0b","title":"\u603b\u7ed3"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#examples#of#recursive#definitions","text":"","title":"Examples of recursive definitions"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#recursive#grammar","text":"\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Recursive grammar \u9f99\u4e66 2.2.1 Definition of Grammars","title":"Recursive grammar"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#recursive#definition#in#computer#science","text":"\u6709\u592a\u591a\u592a\u591a\u7684\u7b97\u6cd5\u3001\u7ed3\u6784\u90fd\u662f\u53ef\u4ee5\u4f7f\u7528recursive definition\u7684\uff0c\u80fd\u591frecursive definition\u7684\uff0c\u79f0\u5b83\u5177\u5907\u9012\u5f52\u6027\u3002","title":"Recursive definition in computer science"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#sub#structure","text":"\u4e00\u822c\u5e26\u6709\u201csub\u201d\u7684\u90fd\u662f\u53ef\u4ee5\u8fdb\u884crecursive definition\u7684\uff0c\u5b83\u662f\u4e00\u79cd\u5178\u578b\u7684**containing\u5173\u7cfb**\uff0c\u5b83\u662f\u53ef\u4ee5\u4f7f\u7528CFG\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\u3002\u4e0b\u9762\u662f\u5178\u578b\u7684sub structure\uff1a subtree\uff0csublist\uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Structural induction \u4e2d\u6240\u63d0\u53ca\u7684\uff1a recursively defined structure, such as formulas , lists , or trees \u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50\uff1a Subobjects\uff08\u53c2\u89c1cppreference Object#Subobjects \uff09 \u7ef4\u57fa\u767e\u79d1 Optimal substructure","title":"sub structure"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#pointer#of#pointer","text":"\u65e0\u8bba\u662f\u5728computer science\u8fd8\u662f\u5728\u5b9e\u9645\u751f\u6d3b\u4e2d\uff0c\u90fd\u5b58\u5728\u7740\u5927\u91cf\u7684\u7c7b\u4f3c\u4e8epointer of pointer\u7684\u4f8b\u5b50\uff1a pointer of pointer array of array\uff08\u5176\u5b9e\u662f\u4e00\u79cdsub structure\uff0c\u4e00\u4e2aarray\u53ef\u4ee5\u770b\u505a\u662f\u7531\u591a\u4e2asub array \u7ec4\u6210\uff09 mother of mother \uff08\u5bb6\u65cfhierarchy\uff09 \u5b83\u4eec\u90fd\u662f\u90fd\u662f\u975e\u5e38\u5178\u578b\u7684recursion definition\u3002\u4e0b\u9762\u5bf9array of array\u5373multiple-dimensional array\u8fdb\u884c\u8bf4\u660e\u3002 multiple-dimensional array \u5728\u9605\u8bfb https://stackoverflow.com/a/4810676 \u65f6\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Multidimensional arrays are often referred to as \"arrays of arrays\" \u663e\u7136\uff0c\u8fd9\u662f\u5178\u578b\u7684recursive definition\u3002 \u4e0e\u6b64\u7c7b\u4f3c\u7684\u8fd8\u6709\uff1amultiple-level pointer\u3002 multiple-dimensional array\u548cmultiple-level pointer\u53ef\u4ee5\u770b\u505a\u662f\u5177\u5907nesting\u5173\u7cfb\uff08\u53c2\u89c1\u5de5\u7a0b data-structure \u7684 Graph\\Tree\\Tree-structure.md \u7ae0\u8282\uff09\u3002 multiple-dimensional array\u548cmultiple-level pointer\u53ef\u4ee5\u9012\u5f52\u7684\u8fdb\u884c\u5904\u7406\uff0c\u5728\u6587\u7ae0 https://stackoverflow.com/a/5580952 \u4e2d\u7ed9\u51fa\u4e86\u793a\u4f8b\u7a0b\u5e8f\u3002","title":"pointer of pointer"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#recursive#definition#and#closure","text":"\u6309\u7167recursively defined sets\u7684\u6982\u5ff5\uff0c\u8fd9\u4e2aset\u5728\u6784\u9020\u65b0\u5143\u7d20\u7684operation\u4e0b\u662fclosed\u7684\uff08\u53c2\u89c1 Closure (mathematics) \uff09\u3002","title":"Recursive definition and closure"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursive-definition/#recursive#definition#and#structure","text":"Recursive data types \u4e2d\u4e13\u95e8\u63cf\u8ff0\u53ef\u4ee5\u4f7f\u7528recursive definition\u7684structure\uff0c\u8fd9\u5728 data-structure \u4e2d\u4f1a\u8fdb\u884c\u8ba8\u8bba\u3002 \u53ef\u4ee5\u4f7f\u7528recursive definition\u8fdb\u884c\u5b9a\u4e49\u7684structure\uff0c\u90fd\u5177\u5907\u9012\u5f52\u7279\u6027\u3002","title":"Recursive definition and structure"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/","text":"\u9012\u5f52\u51fd\u6570\u7684\u8868\u793a\u4e0e\u5206\u6790 \u9012\u5f52\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u7684\u8868\u793a \u5bf9\u9012\u5f52\u51fd\u6570\u8fdb\u884c\u590d\u6742\u5ea6\u5206\u6790\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u56e0\u4e3a\u6240\u6709\u4f7f\u7528\u9012\u5f52\u7684\u51fd\u6570\u90fd\u6d89\u53ca\u8fd9\u4e2a\u95ee\u9898\uff1b\u5176\u5b9e\u590d\u6742\u5ea6\u5206\u6790\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u7edf\u8ba1\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u6b21\u6570\u3001\u6267\u884c\u6df1\u5ea6\u7b49\u95ee\u9898\uff0c\u6240\u4ee5\u5982\u679c\u5bf9\u9012\u5f52\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6709\u4e00\u4e2a\u76f4\u89c2\uff0c\u51c6\u786e\u5730\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5206\u6790\u5176\u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u5ea6\u4e5f\u4f1a\u975e\u5e38\u5bb9\u6613\uff0c\u76ee\u524d\u6d41\u884c\u7684\u8868\u793a\u65b9\u6cd5\u662f\uff1a\u9012\u5f52\u8c03\u7528\u6811\uff0c\u5982\u4e0b\u662f\u4e00\u4e9b\u4f7f\u7528\u9012\u5f52\u8c03\u7528\u6811\u6765\u8868\u793a\uff1a Write a program to print all permutations of a given string \u6590\u6ce2\u90a3\u5951\u6570 \u90a3\u5982\u4f55\u6765\u7406\u89e3**\u9012\u5f52\u8c03\u7528\u6811**\u5462\uff1f\u5b9e\u9645\u4e0a\uff0c \u9012\u5f52\u51fd\u6570**\u7684\u6267\u884c\u8fc7\u7a0b\u5e76\u4e0d\u4f1a\u663e\u793a\u7684\u6784\u9020\u51fa\u4e00\u4e2a**\u9012\u5f52\u8c03\u7528\u6811 \uff0c\u5b83\u53ea\u662f\u903b\u8f91\u4e0a\u5f62\u6210\u4e86\u4e00\u4e2a\u6811\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u6790\uff1a \u6211\u4eec\u77e5\u9053\uff0c\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6240\u4f7f\u7528\u7684\u662f Call stack \uff0c\u6bcf\u4e00\u6b21\u7684\u51fd\u6570\u8c03\u7528\u90fd\u4f1a\u5728 Call stack \u4e0apush\u4e00\u4e2a stack frame \uff08\u53c2\u89c1 Call stack \uff09\uff1b\u9012\u5f52\u51fd\u6570\u4e00\u76f4\u6267\u884c\u7684\u662f\u540c\u4e00\u4e2a\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u7684 Call stack \u4e2d\u7684**stack frame**\u7684\u6267\u884c\u903b\u8f91\u662f\u76f8\u540c\u7684\uff08\u5165\u53c2\u53ef\u80fd\u4e0d\u540c\uff09\uff1b\u5728\u9012\u5f52\u51fd\u6570\u6267\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u5c31\u5f80 Call stack \u4e0apush\uff08\u5165\u6808\uff09\u4e00\u4e2a stack frame \uff0c\u76f4\u5230\u67d0\u4e2a\u9012\u5f52\u51fd\u6570\u6267\u884c\u5230\u4e86base case\uff0c\u5219\u5b83\u4f1areturn\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\u5b83\u7684 **stack frame**\u4f1apop\uff08\u51fa\u6808\uff09\uff0c\u5219\u63a7\u5236\u4f1a\u8fd4\u56de\u5230\u8c03\u7528\u5b83\u7684\u51fd\u6570\uff1b\u663e\u7136\uff0c\u524d\u9762\u6240\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5bf9\u5e94\u8fd9 \u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386 \uff0c\u6240\u4ee5\u6211\u4eec\u8bf4\uff1a\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u5bf9\u9012\u5f52\u8c03\u7528\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002 SUMMARY : \u4e0a\u9762\u6240\u63cf\u8ff0\u7684\uff1a\u51fd\u6570\u8c03\u7528-\u5165\u6808\uff0c\u51fd\u6570\u8fd4\u56de-\u51fa\u6808\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8e\u62ec\u53f7\u5339\u914d\u7b97\u6cd5\u4e2d\u7684\u6b63\u62ec\u53f7\u5165\u6808\uff0c\u53cd\u62ec\u53f7\u51fa\u6808\uff1b \u603b\u7684\u6765\u8bf4\uff0c\u9012\u5f52\u8c03\u7528\u6570\u662f\u5bf9\u9012\u5f52\u51fd\u6570\u7684 Call stack \u7684\u53ef\u89c6\u5316\u5206\u6790\uff1b \u5176\u5b9e\u6211\u4eec\u662f\u5b8c\u5168\u53ef\u4ee5\u6839\u636e**\u9012\u5f52\u51fd\u6570**\u753b\u51fa\u5bf9\u5e94\u7684**\u9012\u5f52\u8c03\u7528\u6811**\u7684\u3002\u6bd4\u5982 perm \u51fd\u6570\u5c31\u662f\u5178\u578b\u7684**\u6392\u5217\u6811**\uff0c\u4e8c\u5206\u641c\u7d22\u3001quike sort\u7b49\u5c31\u662f\u5178\u578b\u7684**\u4e8c\u53c9\u6811**\u3002 Matrix Chain Multiplication | DP-8 \u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u6027\u5206\u6790 Analysis of Algorithm | Set 4 (Solving Recurrences) Lecture 20: Recursion Trees and the Master Method","title":"\u9012\u5f52\u51fd\u6570\u7684\u8868\u793a\u4e0e\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/#_1","text":"","title":"\u9012\u5f52\u51fd\u6570\u7684\u8868\u793a\u4e0e\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/#_2","text":"\u5bf9\u9012\u5f52\u51fd\u6570\u8fdb\u884c\u590d\u6742\u5ea6\u5206\u6790\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u56e0\u4e3a\u6240\u6709\u4f7f\u7528\u9012\u5f52\u7684\u51fd\u6570\u90fd\u6d89\u53ca\u8fd9\u4e2a\u95ee\u9898\uff1b\u5176\u5b9e\u590d\u6742\u5ea6\u5206\u6790\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u7edf\u8ba1\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u6b21\u6570\u3001\u6267\u884c\u6df1\u5ea6\u7b49\u95ee\u9898\uff0c\u6240\u4ee5\u5982\u679c\u5bf9\u9012\u5f52\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6709\u4e00\u4e2a\u76f4\u89c2\uff0c\u51c6\u786e\u5730\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5206\u6790\u5176\u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u5ea6\u4e5f\u4f1a\u975e\u5e38\u5bb9\u6613\uff0c\u76ee\u524d\u6d41\u884c\u7684\u8868\u793a\u65b9\u6cd5\u662f\uff1a\u9012\u5f52\u8c03\u7528\u6811\uff0c\u5982\u4e0b\u662f\u4e00\u4e9b\u4f7f\u7528\u9012\u5f52\u8c03\u7528\u6811\u6765\u8868\u793a\uff1a","title":"\u9012\u5f52\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u7684\u8868\u793a"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/#write#a#program#to#print#all#permutations#of#a#given#string","text":"","title":"Write a program to print all permutations of a given string"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/#_3","text":"\u90a3\u5982\u4f55\u6765\u7406\u89e3**\u9012\u5f52\u8c03\u7528\u6811**\u5462\uff1f\u5b9e\u9645\u4e0a\uff0c \u9012\u5f52\u51fd\u6570**\u7684\u6267\u884c\u8fc7\u7a0b\u5e76\u4e0d\u4f1a\u663e\u793a\u7684\u6784\u9020\u51fa\u4e00\u4e2a**\u9012\u5f52\u8c03\u7528\u6811 \uff0c\u5b83\u53ea\u662f\u903b\u8f91\u4e0a\u5f62\u6210\u4e86\u4e00\u4e2a\u6811\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u6790\uff1a \u6211\u4eec\u77e5\u9053\uff0c\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6240\u4f7f\u7528\u7684\u662f Call stack \uff0c\u6bcf\u4e00\u6b21\u7684\u51fd\u6570\u8c03\u7528\u90fd\u4f1a\u5728 Call stack \u4e0apush\u4e00\u4e2a stack frame \uff08\u53c2\u89c1 Call stack \uff09\uff1b\u9012\u5f52\u51fd\u6570\u4e00\u76f4\u6267\u884c\u7684\u662f\u540c\u4e00\u4e2a\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u7684 Call stack \u4e2d\u7684**stack frame**\u7684\u6267\u884c\u903b\u8f91\u662f\u76f8\u540c\u7684\uff08\u5165\u53c2\u53ef\u80fd\u4e0d\u540c\uff09\uff1b\u5728\u9012\u5f52\u51fd\u6570\u6267\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u5c31\u5f80 Call stack \u4e0apush\uff08\u5165\u6808\uff09\u4e00\u4e2a stack frame \uff0c\u76f4\u5230\u67d0\u4e2a\u9012\u5f52\u51fd\u6570\u6267\u884c\u5230\u4e86base case\uff0c\u5219\u5b83\u4f1areturn\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\u5b83\u7684 **stack frame**\u4f1apop\uff08\u51fa\u6808\uff09\uff0c\u5219\u63a7\u5236\u4f1a\u8fd4\u56de\u5230\u8c03\u7528\u5b83\u7684\u51fd\u6570\uff1b\u663e\u7136\uff0c\u524d\u9762\u6240\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5bf9\u5e94\u8fd9 \u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386 \uff0c\u6240\u4ee5\u6211\u4eec\u8bf4\uff1a\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u5bf9\u9012\u5f52\u8c03\u7528\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002 SUMMARY : \u4e0a\u9762\u6240\u63cf\u8ff0\u7684\uff1a\u51fd\u6570\u8c03\u7528-\u5165\u6808\uff0c\u51fd\u6570\u8fd4\u56de-\u51fa\u6808\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8e\u62ec\u53f7\u5339\u914d\u7b97\u6cd5\u4e2d\u7684\u6b63\u62ec\u53f7\u5165\u6808\uff0c\u53cd\u62ec\u53f7\u51fa\u6808\uff1b \u603b\u7684\u6765\u8bf4\uff0c\u9012\u5f52\u8c03\u7528\u6570\u662f\u5bf9\u9012\u5f52\u51fd\u6570\u7684 Call stack \u7684\u53ef\u89c6\u5316\u5206\u6790\uff1b \u5176\u5b9e\u6211\u4eec\u662f\u5b8c\u5168\u53ef\u4ee5\u6839\u636e**\u9012\u5f52\u51fd\u6570**\u753b\u51fa\u5bf9\u5e94\u7684**\u9012\u5f52\u8c03\u7528\u6811**\u7684\u3002\u6bd4\u5982 perm \u51fd\u6570\u5c31\u662f\u5178\u578b\u7684**\u6392\u5217\u6811**\uff0c\u4e8c\u5206\u641c\u7d22\u3001quike sort\u7b49\u5c31\u662f\u5178\u578b\u7684**\u4e8c\u53c9\u6811**\u3002","title":"\u6590\u6ce2\u90a3\u5951\u6570"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/#matrix#chain#multiplication#dp-8","text":"","title":"Matrix Chain Multiplication | DP-8"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/#_4","text":"","title":"\u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u6027\u5206\u6790"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/#analysis#of#algorithm#set#4#solving#recurrences","text":"","title":"Analysis of Algorithm | Set 4 (Solving Recurrences)"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Analysis-and-representation-of-recursion/#lecture#20#recursion#trees#and#the#master#method","text":"","title":"Lecture 20: Recursion Trees and the Master Method"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Corecursion/","text":"Corecursion \u5171\u9012\u5f52 In computer science , corecursion is a type of operation that is dual to recursion . Whereas recursion works analytically , starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically , starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is generative recursion which may lack a definite\uff08\u786e\u5207\u7684\uff09 \"direction\" inherent in corecursion and recursion. NOTE: recursion \u548c corecursion \u7684\u8ba1\u7b97\u65b9\u5411\u662f\u76f8\u53cd\uff1a\u5bf9\u4e8e\u4e00\u4e2a recurrence relations \uff0c\u5982*n! := n \u00d7 (n - 1)! .\uff0crecursion\u662f\u4ece\u5de6\u81f3\u53f3\uff0c\u4f46\u662fcorecursion\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u4f46\u662f\u80fd\u591f\u6b8a\u9014\u540c\u5f52 - recursion works **analytically* VS corecursion works synthetically - recursion top-down VS corecursion bottom-up - recursion reduce VS corecursion produce Where recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams , so long as it can be produced from simple data ( base cases ) in a sequence of finite steps. Where recursion may not terminate, never reaching a base state , corecursion starts from a base state , and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non- productive . Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial\uff08\u9636\u4e58\uff09. Corecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation , to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming , where corecursion and codata allow total languages to work with infinite data structures. NOTE : python\u7684generator\u5c31\u662fCorecursion\u7684\u6700\u597d\u7684\u4f8b\u5b50\u3002 Examples Corecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables , these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition. Factorial A classic example of recursion is computing the factorial \uff08\u9636\u4e58\uff09, which is defined recursively by 0! := 1 and n! := n \u00d7 (n - 1)! . To recursively compute its result on a given input, a recursive function calls (a copy of) itself with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the base case has been reached. Thus a call stack develops in the process. For example, to compute fac(3) , this recursively calls in turn fac(2) , fac(1) , fac(0) (\"winding up\" the stack), at which point recursion terminates with fac(0) = 1 , and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame fac(3) that uses the result of fac(2) = 2 to calculate the final result as 3 \u00d7 2 = 3 \u00d7 fac(2) =: fac(3) and finally return fac(3) = 6 . In this example a function returns a single value. This stack unwinding can be explicated, defining the factorial corecursively , as an iterator , where one starts with the case of $ 1=:0! $, then from this starting value constructs factorial values for increasing numbers 1, 2, 3... as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it backwards as $ n!\\times (n+1)=:(n+1)! $. The corecursive algorithm thus defined produces a stream of all factorials. This may be concretely implemented as a generator . Symbolically, noting that computing next factorial value requires keeping track of both n and f (a previous factorial value), this can be represented as: $ n,f=(0,1):(n+1,f\\times (n+1)) $ In Python, a recursive factorial function can be defined as: def factorial ( n ): if n == 0 : return 1 else : return n * factorial ( n - 1 ) This could then be called for example as factorial(5) to compute 5! . A corresponding corecursive generator can be defined as: def factorials (): n , f = 0 , 1 while True : yield f n , f = n + 1 , f * ( n + 1 ) This generates an infinite stream of factorials in order; a finite portion of it can be produced by: def n_factorials ( k ): n , f = 0 , 1 while n <= k : yield f n , f = n + 1 , f * ( n + 1 ) This could then be called to produce the factorials up to 5! via: for f in n_factorials ( 5 ): print ( f ) If we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function, def nth_factorial ( k ): n , f = 0 , 1 while n < k : n , f = n + 1 , f * ( n + 1 ) yield f As can be readily seen here, this is practically equivalent (just by substituting return for the only yield there) to the accumulator argument technique for tail recursion , unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable. \u56e0\u6b64\uff0c\u53ef\u4ee5\u8fd9\u6837\u8bf4\uff0c\u534f\u9012\u5f52\u7684\u6982\u5ff5\u662f\u901a\u8fc7\u9012\u5f52\u5b9a\u4e49\u6765\u89e3\u91ca\u8fed\u4ee3\u8ba1\u7b97\u8fc7\u7a0b\u7684\u4f53\u73b0\u3002 Fibonacci sequence In the same way, the Fibonacci sequence can be represented as: $ a,b=(0,1):(b,a+b) $ Note that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the $ (b,-) $ corresponding to shift forward by one step, and the $ (-,a+b) $ corresponding to computing the next term. This can then be implemented as follows (using parallel assignment ): def fibonacci_sequence (): a , b = 0 , 1 while True : yield a a , b = b , a + b In Haskell, map fst ( ( \\ ( a , b ) -> ( b , a + b )) ` iterate ` ( 0 , 1 ) ) Tree traversal Tree traversal via a depth-first approach is a classic example of recursion . Dually, breadth-first traversal can very naturally be implemented via corecursion . Without using recursion or corecursion specifically, one may traverse a tree by starting at the root node , placing its child nodes in a data structure , then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure.[ b] If the data structure is a stack (LIFO), this yields depth-first traversal , and if the data structure is a queue (FIFO), this yields breadth-first traversal . NOTE: \u5728\u4e0d\u4f7f\u7528recursion\u6216\u8005corecursion\u7684\u65f6\u5019\uff0c\u6211\u4eec\u5982\u679c\u60f3\u8981\u904d\u5386\u4e00\u68f5\u6811\uff0c\u5219\u9700\u8981\u501f\u52a9\u4e8e\u4e00\u4e2adata structure\u6765\u5b9e\u73b0\uff1b Using recursion , a (post-order)[ c] depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) \u2013 the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions ) acts as the stack that is iterated over. Using corecursion , a breadth-first traversal can be implemented by starting at the root node, outputting its value,[ d] then breadth-first traversing the subtrees \u2013 i.e., passing on the whole list of subtrees to the next step (not a single subtree, as in the recursive approach) \u2013 at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc.[ e] In this case the generator function , indeed the output sequence itself, acts as the queue . As in the factorial example (above), where the auxiliary information of the index (which step one was at, n ) was pushed forward, in addition to the actual output of n !, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically: $ v,t=([],[FullTree]):(RootValues(t),ChildTrees(t)) $","title":"[Corecursion](https://en.wikipedia.org/wiki/Corecursion)"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Corecursion/#corecursion","text":"\u5171\u9012\u5f52 In computer science , corecursion is a type of operation that is dual to recursion . Whereas recursion works analytically , starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically , starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is generative recursion which may lack a definite\uff08\u786e\u5207\u7684\uff09 \"direction\" inherent in corecursion and recursion. NOTE: recursion \u548c corecursion \u7684\u8ba1\u7b97\u65b9\u5411\u662f\u76f8\u53cd\uff1a\u5bf9\u4e8e\u4e00\u4e2a recurrence relations \uff0c\u5982*n! := n \u00d7 (n - 1)! .\uff0crecursion\u662f\u4ece\u5de6\u81f3\u53f3\uff0c\u4f46\u662fcorecursion\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u4f46\u662f\u80fd\u591f\u6b8a\u9014\u540c\u5f52 - recursion works **analytically* VS corecursion works synthetically - recursion top-down VS corecursion bottom-up - recursion reduce VS corecursion produce Where recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams , so long as it can be produced from simple data ( base cases ) in a sequence of finite steps. Where recursion may not terminate, never reaching a base state , corecursion starts from a base state , and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non- productive . Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial\uff08\u9636\u4e58\uff09. Corecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation , to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming , where corecursion and codata allow total languages to work with infinite data structures. NOTE : python\u7684generator\u5c31\u662fCorecursion\u7684\u6700\u597d\u7684\u4f8b\u5b50\u3002","title":"Corecursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Corecursion/#examples","text":"Corecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables , these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.","title":"Examples"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Corecursion/#factorial","text":"A classic example of recursion is computing the factorial \uff08\u9636\u4e58\uff09, which is defined recursively by 0! := 1 and n! := n \u00d7 (n - 1)! . To recursively compute its result on a given input, a recursive function calls (a copy of) itself with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the base case has been reached. Thus a call stack develops in the process. For example, to compute fac(3) , this recursively calls in turn fac(2) , fac(1) , fac(0) (\"winding up\" the stack), at which point recursion terminates with fac(0) = 1 , and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame fac(3) that uses the result of fac(2) = 2 to calculate the final result as 3 \u00d7 2 = 3 \u00d7 fac(2) =: fac(3) and finally return fac(3) = 6 . In this example a function returns a single value. This stack unwinding can be explicated, defining the factorial corecursively , as an iterator , where one starts with the case of $ 1=:0! $, then from this starting value constructs factorial values for increasing numbers 1, 2, 3... as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it backwards as $ n!\\times (n+1)=:(n+1)! $. The corecursive algorithm thus defined produces a stream of all factorials. This may be concretely implemented as a generator . Symbolically, noting that computing next factorial value requires keeping track of both n and f (a previous factorial value), this can be represented as: $ n,f=(0,1):(n+1,f\\times (n+1)) $ In Python, a recursive factorial function can be defined as: def factorial ( n ): if n == 0 : return 1 else : return n * factorial ( n - 1 ) This could then be called for example as factorial(5) to compute 5! . A corresponding corecursive generator can be defined as: def factorials (): n , f = 0 , 1 while True : yield f n , f = n + 1 , f * ( n + 1 ) This generates an infinite stream of factorials in order; a finite portion of it can be produced by: def n_factorials ( k ): n , f = 0 , 1 while n <= k : yield f n , f = n + 1 , f * ( n + 1 ) This could then be called to produce the factorials up to 5! via: for f in n_factorials ( 5 ): print ( f ) If we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function, def nth_factorial ( k ): n , f = 0 , 1 while n < k : n , f = n + 1 , f * ( n + 1 ) yield f As can be readily seen here, this is practically equivalent (just by substituting return for the only yield there) to the accumulator argument technique for tail recursion , unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable. \u56e0\u6b64\uff0c\u53ef\u4ee5\u8fd9\u6837\u8bf4\uff0c\u534f\u9012\u5f52\u7684\u6982\u5ff5\u662f\u901a\u8fc7\u9012\u5f52\u5b9a\u4e49\u6765\u89e3\u91ca\u8fed\u4ee3\u8ba1\u7b97\u8fc7\u7a0b\u7684\u4f53\u73b0\u3002","title":"Factorial"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Corecursion/#fibonacci#sequence","text":"In the same way, the Fibonacci sequence can be represented as: $ a,b=(0,1):(b,a+b) $ Note that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the $ (b,-) $ corresponding to shift forward by one step, and the $ (-,a+b) $ corresponding to computing the next term. This can then be implemented as follows (using parallel assignment ): def fibonacci_sequence (): a , b = 0 , 1 while True : yield a a , b = b , a + b In Haskell, map fst ( ( \\ ( a , b ) -> ( b , a + b )) ` iterate ` ( 0 , 1 ) )","title":"Fibonacci sequence"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Corecursion/#tree#traversal","text":"Tree traversal via a depth-first approach is a classic example of recursion . Dually, breadth-first traversal can very naturally be implemented via corecursion . Without using recursion or corecursion specifically, one may traverse a tree by starting at the root node , placing its child nodes in a data structure , then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure.[ b] If the data structure is a stack (LIFO), this yields depth-first traversal , and if the data structure is a queue (FIFO), this yields breadth-first traversal . NOTE: \u5728\u4e0d\u4f7f\u7528recursion\u6216\u8005corecursion\u7684\u65f6\u5019\uff0c\u6211\u4eec\u5982\u679c\u60f3\u8981\u904d\u5386\u4e00\u68f5\u6811\uff0c\u5219\u9700\u8981\u501f\u52a9\u4e8e\u4e00\u4e2adata structure\u6765\u5b9e\u73b0\uff1b Using recursion , a (post-order)[ c] depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) \u2013 the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions ) acts as the stack that is iterated over. Using corecursion , a breadth-first traversal can be implemented by starting at the root node, outputting its value,[ d] then breadth-first traversing the subtrees \u2013 i.e., passing on the whole list of subtrees to the next step (not a single subtree, as in the recursive approach) \u2013 at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc.[ e] In this case the generator function , indeed the output sequence itself, acts as the queue . As in the factorial example (above), where the auxiliary information of the index (which step one was at, n ) was pushed forward, in addition to the actual output of n !, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically: $ v,t=([],[FullTree]):(RootValues(t),ChildTrees(t)) $","title":"Tree traversal"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/","text":"Recursion (computer science) \u5728 Recursion \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u63a5\u89e6\u4e86recursion \u7684\u6982\u5ff5\uff0c\u672c\u6587\u5c06\u5206\u6790computer science\u4e2d\u7684recursion\u3002 \u7ef4\u57fa\u767e\u79d1 Recursion (computer science) Recursion in computer science is a method of solving a problem where the solution depends on solutions to smaller instances of the same problem (as opposed to iteration ). The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science. The power of recursion evidently lies in the possibility of defining an infinite\uff08\u65e0\u9650\u7684\uff09 set of objects by a finite\uff08\u6709\u9650\u7684\uff09 statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions. \u2014\u2009 Niklaus Wirth , Algorithms + Data Structures = Programs , 1976 NOTE: \u7ef4\u57fa\u767e\u79d1 Recursion \u662f\u4ece\u5b9a\u4e49\uff08\u5982\u4f55\u8fdb\u884c\u9012\u5f52\u5b9a\u4e49\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Recursive definition \uff09\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0\u9012\u5f52\uff0c\u7ef4\u57fa\u767e\u79d1 Recursion (computer science) \u4e2d\uff0c\u662f\u4ece\u89e3\u51b3\u95ee\u9898\u7684\u89d2\u5ea6\uff08\u5982\u4f55\u7f16\u7801\u5b9e\u73b0\uff09\u6765\u63cf\u8ff0\u9012\u5f52\uff0c\u6b63\u5982\u5b83\u7684\u5f00\u5934\u6240\u8ff0\uff1a This article is about recursive approaches to solving problems. NOTE: \u4ecefinite\uff08\u6709\u9650\u7684\uff09 statement\u5230 infinite\uff08\u65e0\u9650\u7684\uff09\uff0c\u8fd9\u6b63\u662frecursion\u7684\u5f3a\u5927\u6240\u5728\uff1b Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages do not define any looping constructs but rely solely on recursion to repeatedly call code. Computability theory proves that these recursive-only languages are Turing complete ; they are as computationally powerful as Turing complete imperative languages, meaning they can solve the same kinds of problems as imperative languages even without iterative control structures such as while and for . Recursive functions and algorithms A common computer programming tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method ; when combined with a lookup table that stores the results of solving sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization . NOTE: TODO \u9700\u8981\u6dfb\u52a0\u5de5\u7a0b algorithm \u4e2d\uff0c\u5173\u4e8erecursion\u548cdynamic programming\u7684\u6bd4\u8f83\u3002 A recursive function definition has one or more base cases , meaning input(s) for which the function produces a result trivially (without recurring), and one or more recursive cases , meaning input(s) for which the program recurs\uff08\u9012\u5f52\uff0c\u91cd\u73b0\uff0c\u91cd\u590d\uff09 (calls itself). For example, the factorial function can be defined recursively by the equations 0! = 1 and, for all n > 0, n ! = n ( n \u2212 1)!. Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the \"terminating case\". The job of the recursive cases can be seen as breaking down complex inputs into simpler ones. In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached. (Functions that are not intended to terminate under normal circumstances\u2014for example, some system and server processes \u2014are an exception to this.) Neglecting to write a base case, or testing for it incorrectly, can cause an infinite loop . For some functions (such as one that computes the series for e = 1/0! + 1/1! + \u00bd! + \u2153! + ...) there is not an obvious base case implied by the input data; for these one may add a parameter (such as the number of terms to be added, in our series example) to provide a 'stopping criterion' that establishes the base case . Such an example is more naturally treated by co-recursion , where successive terms in the output are the partial sums; this can be converted to a recursion by using the indexing parameter to say \"compute the *n*th term (*n*th partial sum)\". NOTE: \u5728\u9012\u5f52\u51fd\u6570\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5165\u53c2\uff0c\u8fd9\u4e2a\u5165\u53c2\u5c31\u8868\u793a\u505c\u6b62\u6761\u4ef6\uff1b\u5173\u4e8eco-recursion\uff0c\u53c2\u89c1 Corecursion \u3002 Recursive data types Many computer programs must process or generate an arbitrarily large quantity of data . Recursion is one technique for representing data whose exact size the programmer does not know: the programmer can specify this data with a self-referential definition. There are two types of self-referential definitions : inductive and coinductive definitions. Further information: Algebraic data type Inductively defined data Main article: Recursive data type Similarly recursive definitions are often used to model the structure of expressions and statements in programming languages. Language designers often express grammars in a syntax such as Backus\u2013Naur form ; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition: <expr> ::= <number> | (<expr> * <expr>) | (<expr> + <expr>) This says that an expression is either a number, a product of two expressions, or a sum of two expressions. By recursively referring to expressions in the second and third lines, the grammar permits arbitrarily complex arithmetic expressions such as (5 * ((3 * 6) + 8)) , with more than one product or sum operation in a single expression. Coinductively defined data and corecursion Main articles: Coinduction and Corecursion Types of recursion Single recursion and multiple recursion Recursion that only contains a single self-reference is known as single recursion , while recursion that contains multiple self-references is known as multiple recursion . Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include tree traversal , such as in a depth-first search . Single recursion is often much more efficient than multiple recursion , and can generally be replaced by an iterative computation , running in linear time and requiring constant space. Multiple recursion , by contrast, may require exponential time and space, and is more fundamentally recursive, not being able to be replaced by iteration without an explicit stack . NOTE: \u901a\u8fc7Fibonacci\u548ctree traversal \u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u9a8c\u8bc1\u4e0a\u9762\u8fd9\u6bb5\u8bdd NOTE: Multiple recursion\u7684\u590d\u6742\u6027 Multiple recursion can sometimes be converted to single recursion (and, if desired, thence to iteration). For example, while computing the Fibonacci sequence naively is multiple iteration, as each value requires two previous values , it can be computed by single recursion by passing two successive values as parameters. This is more naturally framed as corecursion , building up from the initial values , tracking at each step two successive values \u2013 see corecursion: examples . A more sophisticated example is using a threaded binary tree , which allows iterative tree traversal, rather than multiple recursion. NOTE : Fibonacci \u51fd\u6570\u7684\u8868\u8fbe\u5f0f\u4e2d\u5305\u542b\u4e86\u4e24\u4e2a\u9012\u5f52\u8c03\u7528\uff0c\u4f46\u662f\u6b63\u5982\u4e0a\u9762\u6240\u8bf4\u7684\uff1a it can be computed by single recursion by passing two successive values as parameters\uff1b\u8fd9\u6837\u5c31\u53ef\u4ee5\u5c06\u5b83\u8f6c\u6362\u4e3a\u4e00\u4e2atail recursion\u4e86\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u6d88\u9664\u6389tail recursion\uff0c\u4f7f\u7528iterative\u65b9\u6cd5\u6765\u5b9e\u73b0\u4e86\uff1b Indirect recursion Main article: Mutual recursion Most basic examples of recursion , and most of the examples presented here, demonstrate direct recursion , in which a function calls itself. Indirect recursion occurs when a function is called not by itself but by another function that it called (either directly or indirectly). For example, if f calls f, that is direct recursion, but if f calls g which calls f, then that is indirect recursion of f. Chains of three or more functions are possible; for example, function 1 calls function 2, function 2 calls function 3, and function 3 calls function 1 again. Indirect recursion is also called mutual recursion , which is a more symmetric term, though this is simply a difference of emphasis, not a different notion. That is, if f calls g and then g calls f, which in turn calls g again, from the point of view of f alone, f is indirectly recursing, while from the point of view of g alone, it is indirectly recursing, while from the point of view of both, f and g are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions. Anonymous recursion Main article: Anonymous recursion Recursion is usually done by explicitly calling a function by name. However, recursion can also be done via implicitly calling a function based on the current context, which is particularly useful for anonymous functions , and is known as anonymous recursion . Structural versus generative recursion See also: Structural recursion Some authors classify recursion as either \" structural \" or \" generative \". The distinction is related to where a recursive procedure gets the data that it works on, and how it processes that data: [Functions that consume structured data ] typically decompose their arguments into their immediate structural components and then process those components. If one of the immediate components belongs to the same class of data as the input, the function is recursive. For that reason, we refer to these functions as (STRUCTURALLY) RECURSIVE FUNCTIONS. \u2014\u2009Felleisen, Findler, Flatt, and Krishnaurthi, How to Design Programs , 2001[ 4] Thus, the defining characteristic of a structurally recursive function is that the argument to each recursive call is the content of a field of the original input. Structural recursion includes nearly all tree traversals , including XML processing , binary tree creation and search , etc. By considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion . Generative recursion is the alternative: Many well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it. HtDP ( How to Design Programs ) refers to this kind as generative recursion . Examples of generative recursion include: gcd , quicksort , binary search , mergesort , Newton's method , fractals , and adaptive integration . \u2014\u2009Matthias Felleisen, Advanced Functional Programming , 2002[ 5] This distinction is important in proving termination of a function. All structurally recursive functions on finite ( inductively defined ) data structures can easily be shown to terminate, via structural induction : intuitively, each recursive call receives a smaller piece of input data, until a base case is reached. Generatively recursive functions , in contrast, do not necessarily feed smaller input to their recursive calls, so proof of their termination is not necessarily as simple, and avoiding infinite loops requires greater care. These generatively recursive functions can often be interpreted as corecursive functions \u2013 each step generates the new data, such as successive approximation in Newton's method \u2013 and terminating this corecursion requires that the data eventually satisfy some condition, which is not necessarily guaranteed. In terms of loop variants , structural recursion is when there is an obvious loop variant, namely size or complexity, which starts off finite and decreases at each recursive step. By contrast, generative recursion is when there is not such an obvious loop variant , and termination depends on a function, such as \"error of approximation\" that does not necessarily decrease to zero, and thus termination is not guaranteed without further analysis. Recursive programs Recursive procedures Factorial Greatest common divisor The Euclidean algorithm , which computes the greatest common divisor of two integers, can be written recursively. Function definition*:* $ \\gcd(x,y)={\\begin{cases}x&{\\mbox{if }}y=0\\\\gcd(y,\\operatorname {remainder} (x,y))&{\\mbox{if }}y>0\\\\end{cases}} $ Pseudocode (recursive): function gcd is: input: integer x, integer y such that x > 0 and y >= 0 1. if y is 0, return x 2. otherwise, return [ gcd( y, (remainder of x/y) ) ] end gcd Recurrence relation for greatest common divisor, where $ x\\%y $ expresses the remainder of $ x/y $: $ \\gcd(x,y)=\\gcd(y,x\\%y) $ if $ y\\neq 0 $ $ \\gcd(x,0)=x $ Computing the recurrence relation for x = 27 and y = 9: gcd(27, 9) = gcd(9, 27% 9) = gcd(9, 0) = 9 The recursive program above is tail-recursive ; it is equivalent to an iterative algorithm , and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls . Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls. By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack. Pseudocode (iterative): function gcd is: input: integer x, integer y such that x >= y and y >= 0 1. create new variable called remainder 2. begin loop 1. if y is zero, exit loop 2. set remainder to the remainder of x/y 3. set x to y 4. set y to remainder 5. repeat loop 3. return x end gcd The iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps. Towers of Hanoi Main article: Towers of Hanoi The Towers of Hanoi is a mathematical puzzle whose solution illustrates recursion.[ 6] [ 7] There are three pegs which can hold stacks of disks of different diameters. A larger disk may never be stacked on top of a smaller. Starting with n disks on one peg, they must be moved to another peg one at a time. What is the smallest number of steps to move the stack? Function definition : $ \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\end{cases}} $ Recurrence relation for hanoi : $ h_{n}=2h_{n-1}+1 $ $ h_{1}=1 $ Computing the recurrence relation for n = 4: hanoi(4) = 2*hanoi(3) + 1 = 2*(2*hanoi(2) + 1) + 1 = 2*(2*(2*hanoi(1) + 1) + 1) + 1 = 2*(2*(2*1 + 1) + 1) + 1 = 2*(2*(3) + 1) + 1 = 2*(7) + 1 = 15 Example implementations: Binary search The binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass. The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for. Recursion is used in this algorithm because with each pass a new array is created by cutting the old one in half. The binary search procedure is then called recursively, this time on the new (and smaller) array. Typically the array's size is adjusted by manipulating a beginning and ending index. The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass. Example implementation of binary search in C: /* Call binary_search with proper initial conditions. INPUT: data is an array of integers SORTED in ASCENDING order, toFind is the integer to search for, count is the total number of elements in the array OUTPUT: result of binary_search */ int search ( int * data , int toFind , int count ) { // Start = 0 (beginning index) // End = count - 1 (top index) return binary_search ( data , toFind , 0 , count -1 ); } /* Binary Search Algorithm. INPUT: data is a array of integers SORTED in ASCENDING order, toFind is the integer to search for, start is the minimum array index, end is the maximum array index OUTPUT: position of the integer toFind within array data, -1 if not found */ int binary_search ( int * data , int toFind , int start , int end ) { //Get the midpoint. int mid = start + ( end - start ) / 2 ; //Integer division //Stop condition. if ( start > end ) return -1 ; else if ( data [ mid ] == toFind ) //Found? return mid ; else if ( data [ mid ] > toFind ) //Data is greater than toFind, search lower half return binary_search ( data , toFind , start , mid -1 ); else //Data is less than toFind, search upper half return binary_search ( data , toFind , mid + 1 , end ); } Recursive data structures (structural recursion) Main article: Recursive data type An important application of recursion in computer science is in defining dynamic data structures such as lists and trees . Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time. \" Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.\"[ 9] The examples in this section illustrate what is known as \" structural recursion \". This term refers to the fact that the recursive procedures are acting on data that is defined recursively. As long as a programmer derives the template from a data definition, functions employ structural recursion. That is, the recursions in a function's body consume some immediate piece of a given compound value.[ 5] Linked lists Main article: Linked list Below is a C definition of a linked list node structure. Notice especially how the node is defined in terms of itself. The \"next\" element of struct node is a pointer to another struct node , effectively creating a list type. struct node { int data ; // some integer data struct node * next ; // pointer to another struct node }; Because the struct node data structure is defined recursively , procedures that operate on it can be implemented naturally as recursive procedures . The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL). For each node it prints the data element (an integer). In the C implementation, the list remains unchanged by the list_print procedure. void list_print ( struct node * list ) { if ( list != NULL ) // base case { printf ( \"%d \" , list -> data ); // print integer data followed by a space list_print ( list -> next ); // recursive call on the next node } } Binary trees Main article: Binary tree Below is a simple definition for a binary tree node. Like the node for linked lists, it is defined in terms of itself, recursively. There are two self-referential pointers: left (pointing to the left sub-tree) and right (pointing to the right sub-tree). struct node { int data ; // some integer data struct node * left ; // pointer to the left subtree struct node * right ; // point to the right subtree }; Operations on the tree can be implemented using recursion. Note that because there are two self-referencing pointers (left and right), tree operations may require two recursive calls: // Test if tree_node contains i; return 1 if so, 0 if not. int tree_contains ( struct node * tree_node , int i ) { if ( tree_node == NULL ) return 0 ; // base case else if ( tree_node -> data == i ) return 1 ; else return tree_contains ( tree_node -> left , i ) || tree_contains ( tree_node -> right , i ); } At most two recursive calls will be made for any given call to tree_contains as defined above. // Inorder traversal: void tree_print ( struct node * tree_node ) { if ( tree_node != NULL ) { // base case tree_print ( tree_node -> left ); // go left printf ( \"%d \" , tree_node -> data ); // print the integer followed by a space tree_print ( tree_node -> right ); // go right } } 5 3 7 2 4 6 8 P(5) P(3) P(2) P(NULL) return printf(2) P(NULL) return printf(3) p(4) P(NULL) return printf(4) P(NULL) return \u5199\u9012\u5f52\u51fd\u6570\u7684\u6838\u5fc3\u5728\u4e8e\u628a\u6211\u9012\u5f52\u7684\u672c\u8d28\uff1a\u81ea\u9876\u5411\u4e0b\uff0c\u53ea\u6709\u5b50\u95ee\u9898\u90fd\u89e3\u4e86\uff0c\u624d\u80fd\u591f\u89e3\u4e0a\u4e00\u5c42\u7684\u95ee\u9898\u3002\u4f7f\u7528\u7cfb\u7edf\u5806\u6808\u6765\u5b9e\u73b0\u8be5\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u7cfb\u7edf\u5806\u6808\u80fd\u591f\u4e0d\u65ad\u5730\u6309\u7167\u9012\u5f52\u7684\u987a\u5e8f\u8fdb\u884c\u5165\u6808\uff0c\u76f4\u5230\u8fbe\u5230\u6700\u5c0f\u7684\u5b50\u95ee\u9898\uff0c\u4ece\u800c\u5c06\u5b50\u95ee\u9898\u89e3\u51b3\uff0c\u7136\u540e\u51fa\u6808\uff0c\u7136\u540e\u89e3\u51b3\u4e0a\u4e00\u5c42\u5b50\u95ee\u9898\uff0c\u76f4\u81f3\u6700\u9876\u5c42\u7684\u95ee\u9898\u89e3\u51b3\u4e86\uff1b \u6bcf\u4e2a\u9012\u5f52\u8c03\u7528\u90fd\u662f\u4e00\u6761\u76f4\u7ebf\uff0c\u5982\u679c\u9012\u5f52\u51fd\u6570\u4e2d\uff0c\u51fa\u73b0\u4e86\u4e24\u6b21\u9012\u5f52\u8c03\u7528\uff0c\u5219\u5c31\u662f\u4e24\u6761\u7ebf\u4e86\uff1b \u5bf9\u4e8e\u6709\u9012\u5f52\u6027\u8d28\u7684\u95ee\u9898\u6216\u8005\u7ed3\u6784\uff0c\u6211\u4eec\u53ef\u4ee5\u6309\u7167\u4e0a\u8ff0\u9012\u5f52\u7684\u601d\u60f3\u53bb\u7f16\u5199\u7a0b\u5e8f\u3002\u5f53\u8c08\u53ca\u9700\u8981\u9a8c\u8bc1\u6216\u8005\u6a21\u62df\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u4ece\u76f8\u53cd\u7684\u65b9\u5411\u6765\u89e3\u51b3\u95ee\u9898\u4e86\uff0c\u6211\u4eec\u9700\u8981\u4ece\u5e95\u5411\u4e0a\u6765\u6267\u884c\u3002 Implementation issues In actual implementation, rather than a pure recursive function (single check for base case, otherwise recursive step), a number of modifications may be made, for purposes of clarity or efficiency. These include: Wrapper function (at top) Short-circuiting\uff08\u7b80\u5316\uff09 the base case , aka \"Arm's-length recursion\" (at bottom) Hybrid algorithm (at bottom) \u2013 switching to a different algorithm once data is small enough On the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this. Recursion versus iteration Recursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit call stack , while iteration can be replaced with tail recursion . Which approach is preferable depends on the problem under consideration and the language used. In imperative programming , iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion . By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead. Implementing an algorithm using iteration may not be easily achievable. For example, a factorial function may be implemented iteratively in C by assigning to an loop index variable and accumulator variable, rather than by passing arguments and returning values by recursion: unsigned int factorial ( unsigned int n ) { unsigned int product = 1 ; // empty product is 1 while ( n ) { product *= n ; -- n ; } return product ; } Multiply recursive problems Multiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search ; though both recursive and iterative methods are used,[ 19] they contrast with list traversal and linear search in a list, which is a singly recursive and thus naturally iterative method. Other examples include divide-and-conquer algorithms such as Quicksort , and functions such as the Ackermann function . All of these algorithms can be implemented iteratively with the help of an explicit stack , but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution. Refactoring recursion Recursive algorithms can be replaced with non-recursive counterparts.[ 20] . One method for replacing recursive algorithms is to simulate them using heap memory in place of stack memory .[ 21] An alternative is to develop a replacement algorithm entirely based on non-recursive methods, which can be challenging.[ 22] For example, recursive algorithms for matching wildcards , such as Rich Salz ' wildmat algorithm,[ 23] were once typical. Non-recursive algorithms for the same purpose, such as the Krauss matching wildcards algorithm , have been developed to avoid the drawbacks of recursion[ 24] and have improved only gradually based on techniques such as collecting tests and profiling performance.[ 25] Tail-recursive functions Tail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive. In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes. With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls , a tail-recursive function such as gcd will execute using constant space . Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops. Tail recursion : //INPUT: Integers x, y such that x >= y and y >= 0 int gcd ( int x , int y ) { if ( y == 0 ) return x ; else return gcd ( y , x % y ); } Augmenting recursion: //INPUT: n is an Integer such that n >= 0 int fact ( int n ) { if ( n == 0 ) return 1 ; else return n * fact ( n - 1 ); } The significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack ; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time. Time-efficiency of recursive algorithms The time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation . They can (usually) then be simplified into a single Big-O term. Shortcut rule (master theorem) Main article: Master theorem (analysis of algorithms) If the time-complexity of the function is in the form $ T(n)=a\\cdot T(n/b)+f(n) $ Then the Big O of the time-complexity is thus: If $ f(n)=O(n^{\\log _{b}a-\\epsilon }) $ for some constant $ \\epsilon >0 $, then $ T(n)=\\Theta (n^{\\log _{b}a}) $ If $ f(n)=\\Theta (n^{\\log _{b}a}) $, then $ T(n)=\\Theta (n^{\\log _{b}a}\\log n) $ If $ f(n)=\\Omega (n^{\\log _{b}a+\\epsilon }) $ for some constant $ \\epsilon >0 $, and if $ a\\cdot f(n/b)\\leq c\\cdot f(n) $ for some constant c < 1 and all sufficiently large n , then $ T(n)=\\Theta (f(n)) $ where a represents the number of recursive calls at each level of recursion, b represents by what factor smaller the input is for the next level of recursion (i.e. the number of pieces you divide the problem into), and f \u2009( n ) represents the work the function does independent of any recursion (e.g. partitioning, recombining) at each level of recursion.","title":"Recursion (computer science)"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#recursion#computer#science","text":"\u5728 Recursion \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u63a5\u89e6\u4e86recursion \u7684\u6982\u5ff5\uff0c\u672c\u6587\u5c06\u5206\u6790computer science\u4e2d\u7684recursion\u3002","title":"Recursion (computer science)"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#recursion#computer#science_1","text":"Recursion in computer science is a method of solving a problem where the solution depends on solutions to smaller instances of the same problem (as opposed to iteration ). The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science. The power of recursion evidently lies in the possibility of defining an infinite\uff08\u65e0\u9650\u7684\uff09 set of objects by a finite\uff08\u6709\u9650\u7684\uff09 statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions. \u2014\u2009 Niklaus Wirth , Algorithms + Data Structures = Programs , 1976 NOTE: \u7ef4\u57fa\u767e\u79d1 Recursion \u662f\u4ece\u5b9a\u4e49\uff08\u5982\u4f55\u8fdb\u884c\u9012\u5f52\u5b9a\u4e49\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Recursive definition \uff09\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0\u9012\u5f52\uff0c\u7ef4\u57fa\u767e\u79d1 Recursion (computer science) \u4e2d\uff0c\u662f\u4ece\u89e3\u51b3\u95ee\u9898\u7684\u89d2\u5ea6\uff08\u5982\u4f55\u7f16\u7801\u5b9e\u73b0\uff09\u6765\u63cf\u8ff0\u9012\u5f52\uff0c\u6b63\u5982\u5b83\u7684\u5f00\u5934\u6240\u8ff0\uff1a This article is about recursive approaches to solving problems. NOTE: \u4ecefinite\uff08\u6709\u9650\u7684\uff09 statement\u5230 infinite\uff08\u65e0\u9650\u7684\uff09\uff0c\u8fd9\u6b63\u662frecursion\u7684\u5f3a\u5927\u6240\u5728\uff1b Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages do not define any looping constructs but rely solely on recursion to repeatedly call code. Computability theory proves that these recursive-only languages are Turing complete ; they are as computationally powerful as Turing complete imperative languages, meaning they can solve the same kinds of problems as imperative languages even without iterative control structures such as while and for .","title":"\u7ef4\u57fa\u767e\u79d1Recursion (computer science)"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#recursive#functions#and#algorithms","text":"A common computer programming tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method ; when combined with a lookup table that stores the results of solving sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization . NOTE: TODO \u9700\u8981\u6dfb\u52a0\u5de5\u7a0b algorithm \u4e2d\uff0c\u5173\u4e8erecursion\u548cdynamic programming\u7684\u6bd4\u8f83\u3002 A recursive function definition has one or more base cases , meaning input(s) for which the function produces a result trivially (without recurring), and one or more recursive cases , meaning input(s) for which the program recurs\uff08\u9012\u5f52\uff0c\u91cd\u73b0\uff0c\u91cd\u590d\uff09 (calls itself). For example, the factorial function can be defined recursively by the equations 0! = 1 and, for all n > 0, n ! = n ( n \u2212 1)!. Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the \"terminating case\". The job of the recursive cases can be seen as breaking down complex inputs into simpler ones. In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached. (Functions that are not intended to terminate under normal circumstances\u2014for example, some system and server processes \u2014are an exception to this.) Neglecting to write a base case, or testing for it incorrectly, can cause an infinite loop . For some functions (such as one that computes the series for e = 1/0! + 1/1! + \u00bd! + \u2153! + ...) there is not an obvious base case implied by the input data; for these one may add a parameter (such as the number of terms to be added, in our series example) to provide a 'stopping criterion' that establishes the base case . Such an example is more naturally treated by co-recursion , where successive terms in the output are the partial sums; this can be converted to a recursion by using the indexing parameter to say \"compute the *n*th term (*n*th partial sum)\". NOTE: \u5728\u9012\u5f52\u51fd\u6570\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5165\u53c2\uff0c\u8fd9\u4e2a\u5165\u53c2\u5c31\u8868\u793a\u505c\u6b62\u6761\u4ef6\uff1b\u5173\u4e8eco-recursion\uff0c\u53c2\u89c1 Corecursion \u3002","title":"Recursive functions and algorithms"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#recursive#data#types","text":"Many computer programs must process or generate an arbitrarily large quantity of data . Recursion is one technique for representing data whose exact size the programmer does not know: the programmer can specify this data with a self-referential definition. There are two types of self-referential definitions : inductive and coinductive definitions. Further information: Algebraic data type","title":"Recursive data types"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#inductively#defined#data","text":"Main article: Recursive data type Similarly recursive definitions are often used to model the structure of expressions and statements in programming languages. Language designers often express grammars in a syntax such as Backus\u2013Naur form ; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition: <expr> ::= <number> | (<expr> * <expr>) | (<expr> + <expr>) This says that an expression is either a number, a product of two expressions, or a sum of two expressions. By recursively referring to expressions in the second and third lines, the grammar permits arbitrarily complex arithmetic expressions such as (5 * ((3 * 6) + 8)) , with more than one product or sum operation in a single expression.","title":"Inductively defined data"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#coinductively#defined#data#and#corecursion","text":"Main articles: Coinduction and Corecursion","title":"Coinductively defined data and corecursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#types#of#recursion","text":"","title":"Types of recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#single#recursion#and#multiple#recursion","text":"Recursion that only contains a single self-reference is known as single recursion , while recursion that contains multiple self-references is known as multiple recursion . Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include tree traversal , such as in a depth-first search . Single recursion is often much more efficient than multiple recursion , and can generally be replaced by an iterative computation , running in linear time and requiring constant space. Multiple recursion , by contrast, may require exponential time and space, and is more fundamentally recursive, not being able to be replaced by iteration without an explicit stack . NOTE: \u901a\u8fc7Fibonacci\u548ctree traversal \u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u9a8c\u8bc1\u4e0a\u9762\u8fd9\u6bb5\u8bdd NOTE: Multiple recursion\u7684\u590d\u6742\u6027 Multiple recursion can sometimes be converted to single recursion (and, if desired, thence to iteration). For example, while computing the Fibonacci sequence naively is multiple iteration, as each value requires two previous values , it can be computed by single recursion by passing two successive values as parameters. This is more naturally framed as corecursion , building up from the initial values , tracking at each step two successive values \u2013 see corecursion: examples . A more sophisticated example is using a threaded binary tree , which allows iterative tree traversal, rather than multiple recursion. NOTE : Fibonacci \u51fd\u6570\u7684\u8868\u8fbe\u5f0f\u4e2d\u5305\u542b\u4e86\u4e24\u4e2a\u9012\u5f52\u8c03\u7528\uff0c\u4f46\u662f\u6b63\u5982\u4e0a\u9762\u6240\u8bf4\u7684\uff1a it can be computed by single recursion by passing two successive values as parameters\uff1b\u8fd9\u6837\u5c31\u53ef\u4ee5\u5c06\u5b83\u8f6c\u6362\u4e3a\u4e00\u4e2atail recursion\u4e86\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u6d88\u9664\u6389tail recursion\uff0c\u4f7f\u7528iterative\u65b9\u6cd5\u6765\u5b9e\u73b0\u4e86\uff1b","title":"Single recursion and multiple recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#indirect#recursion","text":"Main article: Mutual recursion Most basic examples of recursion , and most of the examples presented here, demonstrate direct recursion , in which a function calls itself. Indirect recursion occurs when a function is called not by itself but by another function that it called (either directly or indirectly). For example, if f calls f, that is direct recursion, but if f calls g which calls f, then that is indirect recursion of f. Chains of three or more functions are possible; for example, function 1 calls function 2, function 2 calls function 3, and function 3 calls function 1 again. Indirect recursion is also called mutual recursion , which is a more symmetric term, though this is simply a difference of emphasis, not a different notion. That is, if f calls g and then g calls f, which in turn calls g again, from the point of view of f alone, f is indirectly recursing, while from the point of view of g alone, it is indirectly recursing, while from the point of view of both, f and g are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.","title":"Indirect recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#anonymous#recursion","text":"Main article: Anonymous recursion Recursion is usually done by explicitly calling a function by name. However, recursion can also be done via implicitly calling a function based on the current context, which is particularly useful for anonymous functions , and is known as anonymous recursion .","title":"Anonymous recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#structural#versus#generative#recursion","text":"See also: Structural recursion Some authors classify recursion as either \" structural \" or \" generative \". The distinction is related to where a recursive procedure gets the data that it works on, and how it processes that data: [Functions that consume structured data ] typically decompose their arguments into their immediate structural components and then process those components. If one of the immediate components belongs to the same class of data as the input, the function is recursive. For that reason, we refer to these functions as (STRUCTURALLY) RECURSIVE FUNCTIONS. \u2014\u2009Felleisen, Findler, Flatt, and Krishnaurthi, How to Design Programs , 2001[ 4] Thus, the defining characteristic of a structurally recursive function is that the argument to each recursive call is the content of a field of the original input. Structural recursion includes nearly all tree traversals , including XML processing , binary tree creation and search , etc. By considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion . Generative recursion is the alternative: Many well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it. HtDP ( How to Design Programs ) refers to this kind as generative recursion . Examples of generative recursion include: gcd , quicksort , binary search , mergesort , Newton's method , fractals , and adaptive integration . \u2014\u2009Matthias Felleisen, Advanced Functional Programming , 2002[ 5] This distinction is important in proving termination of a function. All structurally recursive functions on finite ( inductively defined ) data structures can easily be shown to terminate, via structural induction : intuitively, each recursive call receives a smaller piece of input data, until a base case is reached. Generatively recursive functions , in contrast, do not necessarily feed smaller input to their recursive calls, so proof of their termination is not necessarily as simple, and avoiding infinite loops requires greater care. These generatively recursive functions can often be interpreted as corecursive functions \u2013 each step generates the new data, such as successive approximation in Newton's method \u2013 and terminating this corecursion requires that the data eventually satisfy some condition, which is not necessarily guaranteed. In terms of loop variants , structural recursion is when there is an obvious loop variant, namely size or complexity, which starts off finite and decreases at each recursive step. By contrast, generative recursion is when there is not such an obvious loop variant , and termination depends on a function, such as \"error of approximation\" that does not necessarily decrease to zero, and thus termination is not guaranteed without further analysis.","title":"Structural versus generative recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#recursive#programs","text":"","title":"Recursive programs"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#recursive#procedures","text":"","title":"Recursive procedures"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#factorial","text":"","title":"Factorial"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#greatest#common#divisor","text":"The Euclidean algorithm , which computes the greatest common divisor of two integers, can be written recursively. Function definition*:* $ \\gcd(x,y)={\\begin{cases}x&{\\mbox{if }}y=0\\\\gcd(y,\\operatorname {remainder} (x,y))&{\\mbox{if }}y>0\\\\end{cases}} $ Pseudocode (recursive): function gcd is: input: integer x, integer y such that x > 0 and y >= 0 1. if y is 0, return x 2. otherwise, return [ gcd( y, (remainder of x/y) ) ] end gcd Recurrence relation for greatest common divisor, where $ x\\%y $ expresses the remainder of $ x/y $: $ \\gcd(x,y)=\\gcd(y,x\\%y) $ if $ y\\neq 0 $ $ \\gcd(x,0)=x $ Computing the recurrence relation for x = 27 and y = 9: gcd(27, 9) = gcd(9, 27% 9) = gcd(9, 0) = 9 The recursive program above is tail-recursive ; it is equivalent to an iterative algorithm , and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls . Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls. By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack. Pseudocode (iterative): function gcd is: input: integer x, integer y such that x >= y and y >= 0 1. create new variable called remainder 2. begin loop 1. if y is zero, exit loop 2. set remainder to the remainder of x/y 3. set x to y 4. set y to remainder 5. repeat loop 3. return x end gcd The iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps.","title":"Greatest common divisor"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#towers#of#hanoi","text":"Main article: Towers of Hanoi The Towers of Hanoi is a mathematical puzzle whose solution illustrates recursion.[ 6] [ 7] There are three pegs which can hold stacks of disks of different diameters. A larger disk may never be stacked on top of a smaller. Starting with n disks on one peg, they must be moved to another peg one at a time. What is the smallest number of steps to move the stack? Function definition : $ \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\end{cases}} $ Recurrence relation for hanoi : $ h_{n}=2h_{n-1}+1 $ $ h_{1}=1 $ Computing the recurrence relation for n = 4: hanoi(4) = 2*hanoi(3) + 1 = 2*(2*hanoi(2) + 1) + 1 = 2*(2*(2*hanoi(1) + 1) + 1) + 1 = 2*(2*(2*1 + 1) + 1) + 1 = 2*(2*(3) + 1) + 1 = 2*(7) + 1 = 15 Example implementations:","title":"Towers of Hanoi"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#binary#search","text":"The binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass. The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for. Recursion is used in this algorithm because with each pass a new array is created by cutting the old one in half. The binary search procedure is then called recursively, this time on the new (and smaller) array. Typically the array's size is adjusted by manipulating a beginning and ending index. The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass. Example implementation of binary search in C: /* Call binary_search with proper initial conditions. INPUT: data is an array of integers SORTED in ASCENDING order, toFind is the integer to search for, count is the total number of elements in the array OUTPUT: result of binary_search */ int search ( int * data , int toFind , int count ) { // Start = 0 (beginning index) // End = count - 1 (top index) return binary_search ( data , toFind , 0 , count -1 ); } /* Binary Search Algorithm. INPUT: data is a array of integers SORTED in ASCENDING order, toFind is the integer to search for, start is the minimum array index, end is the maximum array index OUTPUT: position of the integer toFind within array data, -1 if not found */ int binary_search ( int * data , int toFind , int start , int end ) { //Get the midpoint. int mid = start + ( end - start ) / 2 ; //Integer division //Stop condition. if ( start > end ) return -1 ; else if ( data [ mid ] == toFind ) //Found? return mid ; else if ( data [ mid ] > toFind ) //Data is greater than toFind, search lower half return binary_search ( data , toFind , start , mid -1 ); else //Data is less than toFind, search upper half return binary_search ( data , toFind , mid + 1 , end ); }","title":"Binary search"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#recursive#data#structures#structural#recursion","text":"Main article: Recursive data type An important application of recursion in computer science is in defining dynamic data structures such as lists and trees . Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time. \" Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.\"[ 9] The examples in this section illustrate what is known as \" structural recursion \". This term refers to the fact that the recursive procedures are acting on data that is defined recursively. As long as a programmer derives the template from a data definition, functions employ structural recursion. That is, the recursions in a function's body consume some immediate piece of a given compound value.[ 5]","title":"Recursive data structures (structural recursion)"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#linked#lists","text":"Main article: Linked list Below is a C definition of a linked list node structure. Notice especially how the node is defined in terms of itself. The \"next\" element of struct node is a pointer to another struct node , effectively creating a list type. struct node { int data ; // some integer data struct node * next ; // pointer to another struct node }; Because the struct node data structure is defined recursively , procedures that operate on it can be implemented naturally as recursive procedures . The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL). For each node it prints the data element (an integer). In the C implementation, the list remains unchanged by the list_print procedure. void list_print ( struct node * list ) { if ( list != NULL ) // base case { printf ( \"%d \" , list -> data ); // print integer data followed by a space list_print ( list -> next ); // recursive call on the next node } }","title":"Linked lists"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#binary#trees","text":"Main article: Binary tree Below is a simple definition for a binary tree node. Like the node for linked lists, it is defined in terms of itself, recursively. There are two self-referential pointers: left (pointing to the left sub-tree) and right (pointing to the right sub-tree). struct node { int data ; // some integer data struct node * left ; // pointer to the left subtree struct node * right ; // point to the right subtree }; Operations on the tree can be implemented using recursion. Note that because there are two self-referencing pointers (left and right), tree operations may require two recursive calls: // Test if tree_node contains i; return 1 if so, 0 if not. int tree_contains ( struct node * tree_node , int i ) { if ( tree_node == NULL ) return 0 ; // base case else if ( tree_node -> data == i ) return 1 ; else return tree_contains ( tree_node -> left , i ) || tree_contains ( tree_node -> right , i ); } At most two recursive calls will be made for any given call to tree_contains as defined above. // Inorder traversal: void tree_print ( struct node * tree_node ) { if ( tree_node != NULL ) { // base case tree_print ( tree_node -> left ); // go left printf ( \"%d \" , tree_node -> data ); // print the integer followed by a space tree_print ( tree_node -> right ); // go right } } 5 3 7 2 4 6 8 P(5) P(3) P(2) P(NULL) return printf(2) P(NULL) return printf(3) p(4) P(NULL) return printf(4) P(NULL) return \u5199\u9012\u5f52\u51fd\u6570\u7684\u6838\u5fc3\u5728\u4e8e\u628a\u6211\u9012\u5f52\u7684\u672c\u8d28\uff1a\u81ea\u9876\u5411\u4e0b\uff0c\u53ea\u6709\u5b50\u95ee\u9898\u90fd\u89e3\u4e86\uff0c\u624d\u80fd\u591f\u89e3\u4e0a\u4e00\u5c42\u7684\u95ee\u9898\u3002\u4f7f\u7528\u7cfb\u7edf\u5806\u6808\u6765\u5b9e\u73b0\u8be5\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u7cfb\u7edf\u5806\u6808\u80fd\u591f\u4e0d\u65ad\u5730\u6309\u7167\u9012\u5f52\u7684\u987a\u5e8f\u8fdb\u884c\u5165\u6808\uff0c\u76f4\u5230\u8fbe\u5230\u6700\u5c0f\u7684\u5b50\u95ee\u9898\uff0c\u4ece\u800c\u5c06\u5b50\u95ee\u9898\u89e3\u51b3\uff0c\u7136\u540e\u51fa\u6808\uff0c\u7136\u540e\u89e3\u51b3\u4e0a\u4e00\u5c42\u5b50\u95ee\u9898\uff0c\u76f4\u81f3\u6700\u9876\u5c42\u7684\u95ee\u9898\u89e3\u51b3\u4e86\uff1b \u6bcf\u4e2a\u9012\u5f52\u8c03\u7528\u90fd\u662f\u4e00\u6761\u76f4\u7ebf\uff0c\u5982\u679c\u9012\u5f52\u51fd\u6570\u4e2d\uff0c\u51fa\u73b0\u4e86\u4e24\u6b21\u9012\u5f52\u8c03\u7528\uff0c\u5219\u5c31\u662f\u4e24\u6761\u7ebf\u4e86\uff1b \u5bf9\u4e8e\u6709\u9012\u5f52\u6027\u8d28\u7684\u95ee\u9898\u6216\u8005\u7ed3\u6784\uff0c\u6211\u4eec\u53ef\u4ee5\u6309\u7167\u4e0a\u8ff0\u9012\u5f52\u7684\u601d\u60f3\u53bb\u7f16\u5199\u7a0b\u5e8f\u3002\u5f53\u8c08\u53ca\u9700\u8981\u9a8c\u8bc1\u6216\u8005\u6a21\u62df\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u4ece\u76f8\u53cd\u7684\u65b9\u5411\u6765\u89e3\u51b3\u95ee\u9898\u4e86\uff0c\u6211\u4eec\u9700\u8981\u4ece\u5e95\u5411\u4e0a\u6765\u6267\u884c\u3002","title":"Binary trees"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#implementation#issues","text":"In actual implementation, rather than a pure recursive function (single check for base case, otherwise recursive step), a number of modifications may be made, for purposes of clarity or efficiency. These include: Wrapper function (at top) Short-circuiting\uff08\u7b80\u5316\uff09 the base case , aka \"Arm's-length recursion\" (at bottom) Hybrid algorithm (at bottom) \u2013 switching to a different algorithm once data is small enough On the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this.","title":"Implementation issues"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#recursion#versus#iteration","text":"Recursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit call stack , while iteration can be replaced with tail recursion . Which approach is preferable depends on the problem under consideration and the language used. In imperative programming , iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion . By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead. Implementing an algorithm using iteration may not be easily achievable. For example, a factorial function may be implemented iteratively in C by assigning to an loop index variable and accumulator variable, rather than by passing arguments and returning values by recursion: unsigned int factorial ( unsigned int n ) { unsigned int product = 1 ; // empty product is 1 while ( n ) { product *= n ; -- n ; } return product ; }","title":"Recursion versus iteration"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#multiply#recursive#problems","text":"Multiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search ; though both recursive and iterative methods are used,[ 19] they contrast with list traversal and linear search in a list, which is a singly recursive and thus naturally iterative method. Other examples include divide-and-conquer algorithms such as Quicksort , and functions such as the Ackermann function . All of these algorithms can be implemented iteratively with the help of an explicit stack , but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution.","title":"Multiply recursive problems"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#refactoring#recursion","text":"Recursive algorithms can be replaced with non-recursive counterparts.[ 20] . One method for replacing recursive algorithms is to simulate them using heap memory in place of stack memory .[ 21] An alternative is to develop a replacement algorithm entirely based on non-recursive methods, which can be challenging.[ 22] For example, recursive algorithms for matching wildcards , such as Rich Salz ' wildmat algorithm,[ 23] were once typical. Non-recursive algorithms for the same purpose, such as the Krauss matching wildcards algorithm , have been developed to avoid the drawbacks of recursion[ 24] and have improved only gradually based on techniques such as collecting tests and profiling performance.[ 25]","title":"Refactoring recursion"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#tail-recursive#functions","text":"Tail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive. In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes. With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls , a tail-recursive function such as gcd will execute using constant space . Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops. Tail recursion : //INPUT: Integers x, y such that x >= y and y >= 0 int gcd ( int x , int y ) { if ( y == 0 ) return x ; else return gcd ( y , x % y ); } Augmenting recursion: //INPUT: n is an Integer such that n >= 0 int fact ( int n ) { if ( n == 0 ) return 1 ; else return n * fact ( n - 1 ); } The significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack ; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time.","title":"Tail-recursive functions"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#time-efficiency#of#recursive#algorithms","text":"The time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation . They can (usually) then be simplified into a single Big-O term.","title":"Time-efficiency of recursive algorithms"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion%28computer-science%29/#shortcut#rule#master#theorem","text":"Main article: Master theorem (analysis of algorithms) If the time-complexity of the function is in the form $ T(n)=a\\cdot T(n/b)+f(n) $ Then the Big O of the time-complexity is thus: If $ f(n)=O(n^{\\log _{b}a-\\epsilon }) $ for some constant $ \\epsilon >0 $, then $ T(n)=\\Theta (n^{\\log _{b}a}) $ If $ f(n)=\\Theta (n^{\\log _{b}a}) $, then $ T(n)=\\Theta (n^{\\log _{b}a}\\log n) $ If $ f(n)=\\Omega (n^{\\log _{b}a+\\epsilon }) $ for some constant $ \\epsilon >0 $, and if $ a\\cdot f(n/b)\\leq c\\cdot f(n) $ for some constant c < 1 and all sufficiently large n , then $ T(n)=\\Theta (f(n)) $ where a represents the number of recursive calls at each level of recursion, b represents by what factor smaller the input is for the next level of recursion (i.e. the number of pieces you divide the problem into), and f \u2009( n ) represents the work the function does independent of any recursion (e.g. partitioning, recombining) at each level of recursion.","title":"Shortcut rule (master theorem)"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/","text":"Recursion and iteration Recursion \u548c iteration \u662f\u4e24\u79cd**\u5b9e\u73b0**\u65b9\u5f0f\uff0crecursion\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u81ea\u9876\u5411\u4e0b\u7684\u4f7f\u7528 \u9012\u5f52\u5173\u7cfb \uff0citeration\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u81ea\u5e95\u5411\u4e0a\u5730\u4f7f\u7528 \u9012\u5f52\u5173\u7cfb \uff08dynamic programming\u3001greedy algorithm\u90fd\u662f\u57fa\u4e8eiteration\u7684\uff09\u3002\u672c\u6587\u5c31\u5bf9\u4e24\u8005\u8fdb\u884c\u63a2\u8ba8\u3002 Recursion VS iteration \u4ece\u8fd0\u884c\u6210\u672c\u6bd4\u8f83\uff1a\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Recursion (computer science) \u7b2c\u4e09\u6bb5\u3002 \u4e24\u8005\u7684\u76f8\u540c\u70b9\uff1a\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Recursion (computer science) \u7b2c\u4e8c\u6bb5\uff1a\u90fd\u80fd\u591f\u5b9e\u73b0\u201crepeatedly call code\u201d\u3002 \u7ef4\u57fa\u767e\u79d1 Iteration \uff0c\u5176\u4e2d\u7684 Relationship with recursion \u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002 Recursin to iteration \u5c3e\u9012\u5f52\u6d88\u9664 \u5c3e\u9012\u5f52\u51fd\u6570\u53ef\u4ee5\u65e0\u9700\u501f\u52a9data structure\u5c31\u53ef\u4ee5\u6d88\u9664\uff0c\u5982fibnacci\u3002 \u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Tail call \u3002 using user stack to replace the call stack of recursion function \u5176\u4ed6\u7684\u9012\u5f52\u51fd\u6570\u5982tree\u904d\u5386\u51fd\u6570\uff0c\u4e0d\u662f\u5c3e\u9012\u5f52\uff0c\u9700\u8981\u501f\u52a9\u4e8edata structure\u624d\u80fd\u591f\u6d88\u9664\u3002 \u5173\u4e8e\u6b64\u7684\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f Tree traversal \uff0c\u4ee5\u4e0b\u662f\u4ece\u8fd9\u6458\u6284\u7684code\uff1a recursion\u7684\u5b9e\u73b0 preorder(node) if (node == null) return visit(node) preorder(node.left) preorder(node.right) iteration\u7684\u5b9e\u73b0 iterativePreorder ( node ) if ( node == null ) return s \u2190 empty stack s . push ( node ) while ( not s . isEmpty ()) node \u2190 s . pop () visit ( node ) //right child is pushed first so that left is processed first if ( node . right \u2260 null ) s . push ( node . right ) if ( node . left \u2260 null ) s . push ( node . left ) recursion\u7684\u5b9e\u73b0 VS iteration\u7684\u5b9e\u73b0 \u6b63\u5982\u5728\u300a recursion-analysis-and-representation.md \u300b\u4e2d\u6240\u63cf\u8ff0\u7684\uff1a \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6240\u4f7f\u7528\u7684\u662f Call stack \uff0c\u6bcf\u4e00\u6b21\u7684\u51fd\u6570\u8c03\u7528\u90fd\u4f1a\u5728 Call stack \u4e0apush\u4e00\u4e2a stack frame \uff08\u53c2\u89c1 Call stack \uff09\uff1b\u9012\u5f52\u51fd\u6570\u4e00\u76f4\u6267\u884c\u7684\u662f\u540c\u4e00\u4e2a\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u7684 Call stack \u4e2d\u7684**stack frame**\u7684\u6267\u884c\u903b\u8f91\u662f\u76f8\u540c\u7684\uff08\u5165\u53c2\u53ef\u80fd\u4e0d\u540c\uff09\uff1b\u5728\u9012\u5f52\u51fd\u6570\u6267\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u5c31\u5f80 Call stack \u4e0apush\uff08\u5165\u6808\uff09\u4e00\u4e2a stack frame \uff0c\u76f4\u5230\u67d0\u4e2a\u9012\u5f52\u51fd\u6570\u6267\u884c\u5230\u4e86base case\uff0c\u5219\u5b83\u4f1areturn\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\u5b83\u7684 **stack frame**\u4f1apop\uff08\u51fa\u6808\uff09\uff0c\u5219\u63a7\u5236\u4f1a\u8fd4\u56de\u5230\u8c03\u7528\u5b83\u7684\u51fd\u6570\uff1b\u663e\u7136\uff0c\u524d\u9762\u6240\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5bf9\u5e94\u7740 \u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386 \uff0c\u6240\u4ee5\u6211\u4eec\u8bf4\uff1a\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u5bf9\u9012\u5f52\u8c03\u7528\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002 \u5728\u9012\u5f52\u7684\u5b9e\u73b0\u4e2d\uff0c\u51fd\u6570 preorder \u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u4e0d\u65ad\u5730\u5f80 Call stack \u4e2dpush stack frame\uff0c\u8fd9\u4e9bstack frame\u4e2d\u5c31\u5305\u542b\u4e86\u4e00\u4e2a\u4e00\u4e2a\u7684**\u8282\u70b9**\u4fe1\u606f\uff0c\u76f4\u81f3\u9047\u5230base case\uff0c Call stack \u4e2d\u7684stack frame\u624d\u4f1a\u51fa\u6808\uff1b\u73b0\u5728\u53ea\u5206\u6790 preorder \u4e2d\u5173\u4e8e preorder \u7684\u8c03\u7528\uff0c\u5728\u6bcf\u4e00\u6b21\u8c03\u7528\u4e2d\u4f1a\u5148\u6267\u884c preorder(node.left) \uff0c\u8fd9\u8bf4\u660e preorder \u4f1a\u4f18\u5148\u5904\u7406 node.left \uff0c\u800c\u5c06 node.right \u7559\u5728stack frame\u4e2d\uff08\u51fd\u6570\u6682\u65f6\u6ca1\u6709\u6267\u884c\u5230\u8fd9\u91cc\uff0c\u5f85 preorder(node.left) \u8fd4\u56de\u540e\uff0c\u624d\u4f1a\u6267\u884c\u5230\u5b83\uff09\uff0c\u800c\u5f85\u63a7\u5236\u8fd4\u56de\u65f6\u624d\u8fdb\u884c\u5904\u7406\uff1b\u8fd9\u5c31\u662f\u4f7f\u7528call stack\u6765\u4fdd\u5b58deferred node\uff0c\u6b63\u5982 Tree traversal \u4e2d\u6240\u8ff0\uff1a Traversing a tree involves iterating over all nodes in some manner. Because from a given node there is more than one possible next node (it is not a linear data structure), then, assuming sequential computation (not parallel), some nodes must be deferred\u2014stored in some way for later visiting. This is often done via a stack (LIFO) or queue (FIFO). As a tree is a self-referential (recursively defined) data structure, traversal can be defined by recursion or, more subtly, corecursion , in a very natural and clear fashion; in these cases the deferred nodes are stored implicitly in the call stack . \u663e\u7136\uff0c\u5728\u4f7f\u7528iteration\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u7528\u6237\u5b9a\u4e49\u4e00\u4e2astack\u6765\u5145\u5f53calling stack\u5728recursion\u4e2d\u7684\u89d2\u8272\u4e86\uff1a\u5728recursion\u4e2d\uff0c preorder(node.left) \u7684\u6267\u884c\u5728 preorder(node.right) \u4e4b\u524d\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\uff0c\u4f18\u5148\u5904\u7406 node.left \uff0c\u6240\u4ee5\u5728iteration\u7684\u5b9e\u73b0\u4e2d\uff0c\u8981\u5148\u5165\u6808 node.right \uff0c\u518d\u5165\u6808 node.left \uff08\u56e0\u4e3astack\u662f\u540e\u8fdb\u5148\u51fa\uff09\uff0c\u4ece\u800c\u6a21\u62df\u4e86\u4e0a\u8ff0\u7cfb\u7edf\u8c03\u7528\u4e2d\u5c06 node.right \u4fdd\u5b58\u5728call stack\u4e2d\u3002 \u518d\u6765\u770b\u770b**\u4e2d\u5e8f\u904d\u5386** inorder(node) if (node == null) return inorder(node.left) visit(node) inorder(node.right) \u4e0d\u65ad\u5730\u5c06\u5de6\u8282\u70b9\u538b\u5165call stack\u4e2d\uff0c\u77e5\u9053\u5de6\u8282\u70b9\u4e3aNULL\uff0c\u624d\u8fd4\u56de\uff08\u51fa\u6808\uff09\uff0c\u624dvisit\u8282\u70b9\uff1b iterativeInorder(node) s \u2190 empty stack while (not s.isEmpty() or node \u2260 null) if (node \u2260 null) s.push(node) node \u2190 node.left else node \u2190 s.pop() visit(node) node \u2190 node.right \u4e2d\u5e8f\u904d\u5386\u7684recursion\u7248\u672c\u8981\u6bd4\u5148\u5e8f\u904d\u5386\u7684\u590d\u6742\u5730\u591a\u3002\u5176\u5b9e\u5bf9\u4ed6\u7684\u5206\u6790\u8fd8\u662f\u8981\u4ece\u4ececall stack\u5230user stack\u3002\u4e2d\u5e8f\u904d\u5386\u4e2d\uff0c\u5f53\u9012\u5f52\u51fd\u6570\u8fd4\u56de\u5230\u5b83\u7684\u4e3b\u8c03\u51fd\u6570\u7684\u65f6\u5019\uff0c\u8fd8\u9700\u8981\u8bbf\u95ee\u8282\u70b9 node \uff08\u5728call stack\u4e2d\u4fdd\u5b58\u4e86\u8fd9\u4e9b\u4fe1\u606f\uff09\uff0c\u4f46\u662f\u5728\u5148\u5e8f\u904d\u5386\u4e2d\uff0c\u5f53\u9012\u5f52\u51fd\u6570\u8fd4\u56de\u5230\u5b83\u7684\u4e3b\u8c03\u51fd\u6570\u7684\u65f6\u5019\uff0c\u65e0\u9700\u5728\u8bbf\u95ee\u8282\u70b9 node \u4e86\uff1b\u6240\u4ee5\u5728\u5148\u5e8f\u904d\u5386\u7684iteration\u7248\u672c\u4e2d\uff0c node \u8282\u70b9\u5728\u672c\u8f6e\u4f7f\u7528\u4e86\u4e4b\u540e\uff0c\u5c31\u65e0\u9700\u7ee7\u7eed\u7559\u5728user stack\u4e2d\u4e86\uff0c\u6240\u4ee5\u5728\u6bcf\u8f6e\u7684\u5faa\u73af\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u5c31\u5c06\u5b83\u4eceuser stack\u4e2d\u53d6\u51fa\uff1b\u4f46\u662f\u4e2d\u5e8f\u904d\u5386\u7684iteration\u7248\u672c\u4e2d\uff0c\u5c31\u4e0d\u80fd\u591f\u8fd9\u6837\u4e86\uff0c\u53ea\u6709\u5f53node\u6ca1\u6709\u4e86left node\u540e\uff0c\u624d\u80fd\u591f\u5c06\u5b83\u4eceuser stack\u4e2d\u53d6\u51fa\uff1b user stack\u7684\u51fa\u6808\u5bf9\u5e94\u7740call stack\u4e2d\u7684\u4ece\u9012\u5f52\u51fd\u6570\u4e2d\u8fd4\u56de\uff0cuser stack\u4e2d\u8fdb\u6808\u5219\u5bf9\u5e94\u4e2d\u8c03\u7528\u9012\u5f52\u51fd\u6570\uff1b \u5bf9\u6bd4\u4e0a\u8ff0\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u5982\u4e0b\u4e24\u4e2a\u8bed\u53e5\u662f\u5728\u4e00\u8d77\u7684\uff1a node \u2190 s.pop() visit(node) \u5176\u5b9e\u8fd9\u662f\u4e00\u4e2a\u7406\u89e3\u95ee\u9898\u672c\u8d28\u7684\u6240\u5728\uff0c\u65e0\u8bba\u54ea\u79cd\u65b9\u5f0f\uff0c\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\uff0c\u7136\u540e\u8fdb\u884cvisit\uff0c\u4e0d\u540c\u7684\u662f\u6df1\u5ea6\u4f18\u5148\u5148\u5e8f\u904d\u5386\u662f\u5728\u6bcf\u6b21\u5148\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\u8fdb\u884cvisit\u3002\u6df1\u5ea6\u4f18\u5148\u7684\u4e2d\u5e8f\u904d\u5386\u5219\u662f\u5728\u5de6\u5b50\u6811\u90fd\u8bbf\u95ee\u5b8c\u4e86\u540e\u624d\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\u8fdb\u884cvisit\u3002","title":"Recursion and iteration"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/#recursion#and#iteration","text":"Recursion \u548c iteration \u662f\u4e24\u79cd**\u5b9e\u73b0**\u65b9\u5f0f\uff0crecursion\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u81ea\u9876\u5411\u4e0b\u7684\u4f7f\u7528 \u9012\u5f52\u5173\u7cfb \uff0citeration\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u81ea\u5e95\u5411\u4e0a\u5730\u4f7f\u7528 \u9012\u5f52\u5173\u7cfb \uff08dynamic programming\u3001greedy algorithm\u90fd\u662f\u57fa\u4e8eiteration\u7684\uff09\u3002\u672c\u6587\u5c31\u5bf9\u4e24\u8005\u8fdb\u884c\u63a2\u8ba8\u3002","title":"Recursion and iteration"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/#recursion#vs#iteration","text":"\u4ece\u8fd0\u884c\u6210\u672c\u6bd4\u8f83\uff1a\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Recursion (computer science) \u7b2c\u4e09\u6bb5\u3002 \u4e24\u8005\u7684\u76f8\u540c\u70b9\uff1a\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Recursion (computer science) \u7b2c\u4e8c\u6bb5\uff1a\u90fd\u80fd\u591f\u5b9e\u73b0\u201crepeatedly call code\u201d\u3002 \u7ef4\u57fa\u767e\u79d1 Iteration \uff0c\u5176\u4e2d\u7684 Relationship with recursion \u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002","title":"Recursion VS iteration"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/#recursin#to#iteration","text":"","title":"Recursin to iteration"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/#_1","text":"\u5c3e\u9012\u5f52\u51fd\u6570\u53ef\u4ee5\u65e0\u9700\u501f\u52a9data structure\u5c31\u53ef\u4ee5\u6d88\u9664\uff0c\u5982fibnacci\u3002 \u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Tail call \u3002","title":"\u5c3e\u9012\u5f52\u6d88\u9664"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/#using#user#stack#to#replace#the#call#stack#of#recursion#function","text":"\u5176\u4ed6\u7684\u9012\u5f52\u51fd\u6570\u5982tree\u904d\u5386\u51fd\u6570\uff0c\u4e0d\u662f\u5c3e\u9012\u5f52\uff0c\u9700\u8981\u501f\u52a9\u4e8edata structure\u624d\u80fd\u591f\u6d88\u9664\u3002 \u5173\u4e8e\u6b64\u7684\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f Tree traversal \uff0c\u4ee5\u4e0b\u662f\u4ece\u8fd9\u6458\u6284\u7684code\uff1a","title":"using user stack to replace the call stack of recursion function"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/#recursion","text":"preorder(node) if (node == null) return visit(node) preorder(node.left) preorder(node.right)","title":"recursion\u7684\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/#iteration","text":"iterativePreorder ( node ) if ( node == null ) return s \u2190 empty stack s . push ( node ) while ( not s . isEmpty ()) node \u2190 s . pop () visit ( node ) //right child is pushed first so that left is processed first if ( node . right \u2260 null ) s . push ( node . right ) if ( node . left \u2260 null ) s . push ( node . left )","title":"iteration\u7684\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursion-and-iteration/#recursion#vs#iteration_1","text":"\u6b63\u5982\u5728\u300a recursion-analysis-and-representation.md \u300b\u4e2d\u6240\u63cf\u8ff0\u7684\uff1a \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6240\u4f7f\u7528\u7684\u662f Call stack \uff0c\u6bcf\u4e00\u6b21\u7684\u51fd\u6570\u8c03\u7528\u90fd\u4f1a\u5728 Call stack \u4e0apush\u4e00\u4e2a stack frame \uff08\u53c2\u89c1 Call stack \uff09\uff1b\u9012\u5f52\u51fd\u6570\u4e00\u76f4\u6267\u884c\u7684\u662f\u540c\u4e00\u4e2a\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u7684 Call stack \u4e2d\u7684**stack frame**\u7684\u6267\u884c\u903b\u8f91\u662f\u76f8\u540c\u7684\uff08\u5165\u53c2\u53ef\u80fd\u4e0d\u540c\uff09\uff1b\u5728\u9012\u5f52\u51fd\u6570\u6267\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u5c31\u5f80 Call stack \u4e0apush\uff08\u5165\u6808\uff09\u4e00\u4e2a stack frame \uff0c\u76f4\u5230\u67d0\u4e2a\u9012\u5f52\u51fd\u6570\u6267\u884c\u5230\u4e86base case\uff0c\u5219\u5b83\u4f1areturn\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\u5b83\u7684 **stack frame**\u4f1apop\uff08\u51fa\u6808\uff09\uff0c\u5219\u63a7\u5236\u4f1a\u8fd4\u56de\u5230\u8c03\u7528\u5b83\u7684\u51fd\u6570\uff1b\u663e\u7136\uff0c\u524d\u9762\u6240\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5bf9\u5e94\u7740 \u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386 \uff0c\u6240\u4ee5\u6211\u4eec\u8bf4\uff1a\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u5bf9\u9012\u5f52\u8c03\u7528\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002 \u5728\u9012\u5f52\u7684\u5b9e\u73b0\u4e2d\uff0c\u51fd\u6570 preorder \u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u4e0d\u65ad\u5730\u5f80 Call stack \u4e2dpush stack frame\uff0c\u8fd9\u4e9bstack frame\u4e2d\u5c31\u5305\u542b\u4e86\u4e00\u4e2a\u4e00\u4e2a\u7684**\u8282\u70b9**\u4fe1\u606f\uff0c\u76f4\u81f3\u9047\u5230base case\uff0c Call stack \u4e2d\u7684stack frame\u624d\u4f1a\u51fa\u6808\uff1b\u73b0\u5728\u53ea\u5206\u6790 preorder \u4e2d\u5173\u4e8e preorder \u7684\u8c03\u7528\uff0c\u5728\u6bcf\u4e00\u6b21\u8c03\u7528\u4e2d\u4f1a\u5148\u6267\u884c preorder(node.left) \uff0c\u8fd9\u8bf4\u660e preorder \u4f1a\u4f18\u5148\u5904\u7406 node.left \uff0c\u800c\u5c06 node.right \u7559\u5728stack frame\u4e2d\uff08\u51fd\u6570\u6682\u65f6\u6ca1\u6709\u6267\u884c\u5230\u8fd9\u91cc\uff0c\u5f85 preorder(node.left) \u8fd4\u56de\u540e\uff0c\u624d\u4f1a\u6267\u884c\u5230\u5b83\uff09\uff0c\u800c\u5f85\u63a7\u5236\u8fd4\u56de\u65f6\u624d\u8fdb\u884c\u5904\u7406\uff1b\u8fd9\u5c31\u662f\u4f7f\u7528call stack\u6765\u4fdd\u5b58deferred node\uff0c\u6b63\u5982 Tree traversal \u4e2d\u6240\u8ff0\uff1a Traversing a tree involves iterating over all nodes in some manner. Because from a given node there is more than one possible next node (it is not a linear data structure), then, assuming sequential computation (not parallel), some nodes must be deferred\u2014stored in some way for later visiting. This is often done via a stack (LIFO) or queue (FIFO). As a tree is a self-referential (recursively defined) data structure, traversal can be defined by recursion or, more subtly, corecursion , in a very natural and clear fashion; in these cases the deferred nodes are stored implicitly in the call stack . \u663e\u7136\uff0c\u5728\u4f7f\u7528iteration\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u7528\u6237\u5b9a\u4e49\u4e00\u4e2astack\u6765\u5145\u5f53calling stack\u5728recursion\u4e2d\u7684\u89d2\u8272\u4e86\uff1a\u5728recursion\u4e2d\uff0c preorder(node.left) \u7684\u6267\u884c\u5728 preorder(node.right) \u4e4b\u524d\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\uff0c\u4f18\u5148\u5904\u7406 node.left \uff0c\u6240\u4ee5\u5728iteration\u7684\u5b9e\u73b0\u4e2d\uff0c\u8981\u5148\u5165\u6808 node.right \uff0c\u518d\u5165\u6808 node.left \uff08\u56e0\u4e3astack\u662f\u540e\u8fdb\u5148\u51fa\uff09\uff0c\u4ece\u800c\u6a21\u62df\u4e86\u4e0a\u8ff0\u7cfb\u7edf\u8c03\u7528\u4e2d\u5c06 node.right \u4fdd\u5b58\u5728call stack\u4e2d\u3002 \u518d\u6765\u770b\u770b**\u4e2d\u5e8f\u904d\u5386** inorder(node) if (node == null) return inorder(node.left) visit(node) inorder(node.right) \u4e0d\u65ad\u5730\u5c06\u5de6\u8282\u70b9\u538b\u5165call stack\u4e2d\uff0c\u77e5\u9053\u5de6\u8282\u70b9\u4e3aNULL\uff0c\u624d\u8fd4\u56de\uff08\u51fa\u6808\uff09\uff0c\u624dvisit\u8282\u70b9\uff1b iterativeInorder(node) s \u2190 empty stack while (not s.isEmpty() or node \u2260 null) if (node \u2260 null) s.push(node) node \u2190 node.left else node \u2190 s.pop() visit(node) node \u2190 node.right \u4e2d\u5e8f\u904d\u5386\u7684recursion\u7248\u672c\u8981\u6bd4\u5148\u5e8f\u904d\u5386\u7684\u590d\u6742\u5730\u591a\u3002\u5176\u5b9e\u5bf9\u4ed6\u7684\u5206\u6790\u8fd8\u662f\u8981\u4ece\u4ececall stack\u5230user stack\u3002\u4e2d\u5e8f\u904d\u5386\u4e2d\uff0c\u5f53\u9012\u5f52\u51fd\u6570\u8fd4\u56de\u5230\u5b83\u7684\u4e3b\u8c03\u51fd\u6570\u7684\u65f6\u5019\uff0c\u8fd8\u9700\u8981\u8bbf\u95ee\u8282\u70b9 node \uff08\u5728call stack\u4e2d\u4fdd\u5b58\u4e86\u8fd9\u4e9b\u4fe1\u606f\uff09\uff0c\u4f46\u662f\u5728\u5148\u5e8f\u904d\u5386\u4e2d\uff0c\u5f53\u9012\u5f52\u51fd\u6570\u8fd4\u56de\u5230\u5b83\u7684\u4e3b\u8c03\u51fd\u6570\u7684\u65f6\u5019\uff0c\u65e0\u9700\u5728\u8bbf\u95ee\u8282\u70b9 node \u4e86\uff1b\u6240\u4ee5\u5728\u5148\u5e8f\u904d\u5386\u7684iteration\u7248\u672c\u4e2d\uff0c node \u8282\u70b9\u5728\u672c\u8f6e\u4f7f\u7528\u4e86\u4e4b\u540e\uff0c\u5c31\u65e0\u9700\u7ee7\u7eed\u7559\u5728user stack\u4e2d\u4e86\uff0c\u6240\u4ee5\u5728\u6bcf\u8f6e\u7684\u5faa\u73af\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u5c31\u5c06\u5b83\u4eceuser stack\u4e2d\u53d6\u51fa\uff1b\u4f46\u662f\u4e2d\u5e8f\u904d\u5386\u7684iteration\u7248\u672c\u4e2d\uff0c\u5c31\u4e0d\u80fd\u591f\u8fd9\u6837\u4e86\uff0c\u53ea\u6709\u5f53node\u6ca1\u6709\u4e86left node\u540e\uff0c\u624d\u80fd\u591f\u5c06\u5b83\u4eceuser stack\u4e2d\u53d6\u51fa\uff1b user stack\u7684\u51fa\u6808\u5bf9\u5e94\u7740call stack\u4e2d\u7684\u4ece\u9012\u5f52\u51fd\u6570\u4e2d\u8fd4\u56de\uff0cuser stack\u4e2d\u8fdb\u6808\u5219\u5bf9\u5e94\u4e2d\u8c03\u7528\u9012\u5f52\u51fd\u6570\uff1b \u5bf9\u6bd4\u4e0a\u8ff0\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u5982\u4e0b\u4e24\u4e2a\u8bed\u53e5\u662f\u5728\u4e00\u8d77\u7684\uff1a node \u2190 s.pop() visit(node) \u5176\u5b9e\u8fd9\u662f\u4e00\u4e2a\u7406\u89e3\u95ee\u9898\u672c\u8d28\u7684\u6240\u5728\uff0c\u65e0\u8bba\u54ea\u79cd\u65b9\u5f0f\uff0c\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\uff0c\u7136\u540e\u8fdb\u884cvisit\uff0c\u4e0d\u540c\u7684\u662f\u6df1\u5ea6\u4f18\u5148\u5148\u5e8f\u904d\u5386\u662f\u5728\u6bcf\u6b21\u5148\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\u8fdb\u884cvisit\u3002\u6df1\u5ea6\u4f18\u5148\u7684\u4e2d\u5e8f\u904d\u5386\u5219\u662f\u5728\u5de6\u5b50\u6811\u90fd\u8bbf\u95ee\u5b8c\u4e86\u540e\u624d\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\u8fdb\u884cvisit\u3002","title":"recursion\u7684\u5b9e\u73b0 VS iteration\u7684\u5b9e\u73b0"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursive-data-type/","text":"Recursive data type \u672c\u6587\u662f\u7ef4\u57fa\u767e\u79d1 Recursive data type \u7684\u9605\u8bfb\u7b14\u8bb0\u3002 \u5982\u679c\u5c06\u5404\u79cddata structure\u770b\u505a\u662f\u4e00\u79cd\u7c7b\u578b\uff0c\u90a3\u4e9b\u5177\u5907 \u9012\u5f52\u6027 \u7684data structure\uff0c\u5c31\u662f\u672c\u8282\u6807\u9898\u6240\u8ff0\u201c recursive data type \u201d\uff0c\u8fd9\u4e2a\u672f\u8bed\u8868\u8fbe\u4e86\u6570\u636e\u7c7b\u578b\u7684\u9012\u5f52\u6027\u7279\u5f81\u3002\u5bf9\u4e8erecursive data type\uff0c\u90fd\u53ef\u4ee5\u7ed9\u51fa\u5b83\u7684 recursive definition \u3002 \u7ef4\u57fa\u767e\u79d1 Recursive data type \u4e2d\u5173\u4e8e\u5b83\u7684\u63cf\u8ff0\u662f\u4f7f\u7528\u7684\u7ef4\u57fa\u767e\u79d1 Recursive definition \u4e2d\u201crecursively defined set\u201d\u7684\u63cf\u8ff0\u65b9\u5f0f\u3002 \u6700\u6700\u5178\u578b\u7684recursive data type\u5c31\u662f\uff1a list tree \u5728\u7ef4\u57fa\u767e\u79d1 Recursive data type \u7684 Example \u8282\u7ed9\u51fa\u4e86\u4e0a\u8ff0\u4e24\u79cdrecursive data type\u7684 recursive definition \u3002 Recursive data type \u548c Structural induction \u7d27\u5bc6\u76f8\u5173\uff1b","title":"Recursive data type"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Recursive-data-type/#recursive#data#type","text":"\u672c\u6587\u662f\u7ef4\u57fa\u767e\u79d1 Recursive data type \u7684\u9605\u8bfb\u7b14\u8bb0\u3002 \u5982\u679c\u5c06\u5404\u79cddata structure\u770b\u505a\u662f\u4e00\u79cd\u7c7b\u578b\uff0c\u90a3\u4e9b\u5177\u5907 \u9012\u5f52\u6027 \u7684data structure\uff0c\u5c31\u662f\u672c\u8282\u6807\u9898\u6240\u8ff0\u201c recursive data type \u201d\uff0c\u8fd9\u4e2a\u672f\u8bed\u8868\u8fbe\u4e86\u6570\u636e\u7c7b\u578b\u7684\u9012\u5f52\u6027\u7279\u5f81\u3002\u5bf9\u4e8erecursive data type\uff0c\u90fd\u53ef\u4ee5\u7ed9\u51fa\u5b83\u7684 recursive definition \u3002 \u7ef4\u57fa\u767e\u79d1 Recursive data type \u4e2d\u5173\u4e8e\u5b83\u7684\u63cf\u8ff0\u662f\u4f7f\u7528\u7684\u7ef4\u57fa\u767e\u79d1 Recursive definition \u4e2d\u201crecursively defined set\u201d\u7684\u63cf\u8ff0\u65b9\u5f0f\u3002 \u6700\u6700\u5178\u578b\u7684recursive data type\u5c31\u662f\uff1a list tree \u5728\u7ef4\u57fa\u767e\u79d1 Recursive data type \u7684 Example \u8282\u7ed9\u51fa\u4e86\u4e0a\u8ff0\u4e24\u79cdrecursive data type\u7684 recursive definition \u3002 Recursive data type \u548c Structural induction \u7d27\u5bc6\u76f8\u5173\uff1b","title":"Recursive data type"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Term-with-recursion-in-computer-science/","text":"Term with recursion in computer science Recursive grammar Recursion theory\uff08Computability theory\uff09 Recursive language Recursive set Recursively enumerable set","title":"Term with recursion in computer science"},{"location":"Relation-structure-computation/Computation/Repetition/Induction-and-Recursion/Recursion/Recursion-in-computer-science/Term-with-recursion-in-computer-science/#term#with#recursion#in#computer#science","text":"Recursive grammar Recursion theory\uff08Computability theory\uff09 Recursive language Recursive set Recursively enumerable set","title":"Term with recursion in computer science"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/","text":"\u5173\u4e8e\u672c\u7ae0 \u5728 Recursion \u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u63d0\u53ca\u4e86iteration\uff0c\u672c\u7ae0\u8be6\u7ec6\u63cf\u8ff0iteration\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/#_1","text":"\u5728 Recursion \u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u63d0\u53ca\u4e86iteration\uff0c\u672c\u7ae0\u8be6\u7ec6\u63cf\u8ff0iteration\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iteration/","text":"Iteration \u6211\u4eec\u5df2\u7ecf\u7814\u7a76\u4e86 recursion \uff0c\u73b0\u5728\u6765\u770b\u548crecursion\u5bc6\u5207\u76f8\u5173\u7684\u53e6\u5916\u4e00\u79cd\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\uff1aiteration\u3002 \u6211\u89c9\u5f97\u4e4b\u6240\u4ee5recursion\u548citeration\u5982\u6b64\u91cd\u8981\uff0c\u662f\u56e0\u4e3acomputer science\u4e2d\uff0c \u79bb\u6563 \u662f\u5e7f\u6cdb\u5b58\u5728\u7684\uff0c\u5728\u57fa\u4e8e\u8fd9\u4e9b \u79bb\u6563\u7ed3\u6784 \u8fdb\u884c\u8ba1\u7b97\u7684\u65f6\u5019\uff0c\u6211\u4eec\u6700\u6700\u5e38\u7528\u7684\u5c31\u662f recursion \u548citeration\u3002 \u6b63\u5982\u5728\u7ef4\u57fa\u767e\u79d1 Iteration \u4e2d\u6709\u8fd9\u6837\u7684\u603b\u7ed3\uff1a In mathematics and computer science , iteration (along with the related technique of recursion ) is a standard element of algorithms . \u4e0eiteration\u76f8\u5173\u7684\u91cd\u8981\u6982\u5ff5\u6709\uff1a Iterator Iterator pattern Iterative method Iterative deepening depth-first search \u7ef4\u57fa\u767e\u79d1\uff1a Category:Iteration in programming stream\u662f\u4e00\u79cd\u7ebf\u6027\u7ed3\u6784\u3002 \u4e0d\u540c\u7684programming language\uff0c\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u6765\u652f\u6301iteration\u7684\u65b9\u5f0f\u662f\u4e0d\u540c\u7684\uff0cc\u4e2d\u76f4\u63a5\u4f7f\u7528\u88f8\u6307\u9488\u6765\u652f\u6301iteration\uff0cc++\u4e2d\u62bd\u8c61\u51fa\u6765iterator\uff08\u53c2\u89c1 Iterator library \uff09\u3002python\u4e2d\u4f7f\u7528magic function\u6765\u652f\u6301iteration\u3002python\u4e2d\u5e76\u6ca1\u6709\u663e\u5f0f\u5730\u5b9a\u4e49iterator\u7c7b\u578b\u3002 \u5bb9\u5668\u4e0e\u7b97\u6cd5\u7684\u5206\u79bb \u6b63\u5982\u7ef4\u57fa\u767e\u79d1 Iterator pattern \u4e2d\u6240\u603b\u7ed3\u7684\uff1a The iterator pattern decouples algorithms from containers; in some cases, algorithms are necessarily container-specific and thus cannot be decoupled. iteration and algorithm \u5728algorithm\u4e2d\uff0c\u6700\u6700\u5e38\u4f7f\u7528\u7684\u5c31\u662fiteration\u4e86\uff0c\u800citerator pattern\u6b63\u662f\u5c06\u4e24\u8005\u8fdb\u884c\u5206\u79bb\u3002visitor pattern\u611f\u89c9\u5c31\u662f\u8fd0\u7528\u7b97\u6cd5\u3002","title":"Iteration"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iteration/#iteration","text":"\u6211\u4eec\u5df2\u7ecf\u7814\u7a76\u4e86 recursion \uff0c\u73b0\u5728\u6765\u770b\u548crecursion\u5bc6\u5207\u76f8\u5173\u7684\u53e6\u5916\u4e00\u79cd\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\uff1aiteration\u3002 \u6211\u89c9\u5f97\u4e4b\u6240\u4ee5recursion\u548citeration\u5982\u6b64\u91cd\u8981\uff0c\u662f\u56e0\u4e3acomputer science\u4e2d\uff0c \u79bb\u6563 \u662f\u5e7f\u6cdb\u5b58\u5728\u7684\uff0c\u5728\u57fa\u4e8e\u8fd9\u4e9b \u79bb\u6563\u7ed3\u6784 \u8fdb\u884c\u8ba1\u7b97\u7684\u65f6\u5019\uff0c\u6211\u4eec\u6700\u6700\u5e38\u7528\u7684\u5c31\u662f recursion \u548citeration\u3002 \u6b63\u5982\u5728\u7ef4\u57fa\u767e\u79d1 Iteration \u4e2d\u6709\u8fd9\u6837\u7684\u603b\u7ed3\uff1a In mathematics and computer science , iteration (along with the related technique of recursion ) is a standard element of algorithms . \u4e0eiteration\u76f8\u5173\u7684\u91cd\u8981\u6982\u5ff5\u6709\uff1a Iterator Iterator pattern Iterative method Iterative deepening depth-first search \u7ef4\u57fa\u767e\u79d1\uff1a Category:Iteration in programming stream\u662f\u4e00\u79cd\u7ebf\u6027\u7ed3\u6784\u3002 \u4e0d\u540c\u7684programming language\uff0c\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u6765\u652f\u6301iteration\u7684\u65b9\u5f0f\u662f\u4e0d\u540c\u7684\uff0cc\u4e2d\u76f4\u63a5\u4f7f\u7528\u88f8\u6307\u9488\u6765\u652f\u6301iteration\uff0cc++\u4e2d\u62bd\u8c61\u51fa\u6765iterator\uff08\u53c2\u89c1 Iterator library \uff09\u3002python\u4e2d\u4f7f\u7528magic function\u6765\u652f\u6301iteration\u3002python\u4e2d\u5e76\u6ca1\u6709\u663e\u5f0f\u5730\u5b9a\u4e49iterator\u7c7b\u578b\u3002","title":"Iteration"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iteration/#_1","text":"\u6b63\u5982\u7ef4\u57fa\u767e\u79d1 Iterator pattern \u4e2d\u6240\u603b\u7ed3\u7684\uff1a The iterator pattern decouples algorithms from containers; in some cases, algorithms are necessarily container-specific and thus cannot be decoupled.","title":"\u5bb9\u5668\u4e0e\u7b97\u6cd5\u7684\u5206\u79bb"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iteration/#iteration#and#algorithm","text":"\u5728algorithm\u4e2d\uff0c\u6700\u6700\u5e38\u4f7f\u7528\u7684\u5c31\u662fiteration\u4e86\uff0c\u800citerator pattern\u6b63\u662f\u5c06\u4e24\u8005\u8fdb\u884c\u5206\u79bb\u3002visitor pattern\u611f\u89c9\u5c31\u662f\u8fd0\u7528\u7b97\u6cd5\u3002","title":"iteration and algorithm"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/","text":"Iterator Iterator\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6982\u5ff5\u3002 wikipedia Iterator In computer programming , an iterator is an object that enables a programmer to traverse a container , particularly lists . Various types of iterators are often provided via a container's interface . Though the interface and semantics of a given iterator are fixed, iterators are often implemented in terms of the structures underlying a container implementation and are often tightly coupled to the container to enable the operational semantics of the iterator. An iterator performs traversal and also gives access to data elements in a container, but does not itself perform iteration (i.e., not without some significant liberty taken with that concept or with trivial use of the terminology). An iterator is behaviorally similar to a database cursor . NOTE:\u8bf7\u601d\u8003iterator\u548citeration\u4e4b\u95f4\u5173\u7cfb\uff1f\u6211\u4eec\u5f80\u5f80\u662f\u901a\u8fc7 for \uff0c while \u8bed\u53e5\u6765\u5b9e\u73b0iteration\u3002iterator\u5219\u76f8\u5f53\u4e8e\u4e0e\u4e00\u4e2apointer\u3002 Description NOTE: \u539f\u6587\u7684\u8fd9\u4e00\u6bb5\u975e\u5e38\u503c\u5f97\u4e00\u8bfb Internal Iterators Internal iterators are higher order functions (often taking anonymous functions ) such as map , reduce etc., implementing the traversal across a container, applying the given function to every element in turn. NOTE: \u5176\u5b9e\u8fd9\u5c5e\u4e8efunctional programming paradigm\uff0c\u7ed3\u5408python\u7684Functional Programming Modules \u00b6 \u3001Functional Programming HOWTO \u00b6 \u6765\u7406\u89e3\u662f\u975e\u5e38\u597d\u7406\u89e3\u7684\u3002\u6240\u8c13\u7684internal iterator\u662f\u6307\u4f7f\u7528 map \u7b49 higher order function\uff08\u8fd9\u4e9b\u51fd\u6570\u63a5\u6536\u4e00\u4e2a\u51fd\u6570\u4f5c\u4e3a\u53c2\u6570\uff09\uff0c\u8fd9\u4e9b\u51fd\u6570\u5728\u5185\u90e8\u6267\u884citeration\u64cd\u4f5c\uff0c\u6240\u4ee5\u5bf9\u4e8e\u4f7f\u7528\u8005\u800c\u8a00\uff0c\u5b83\u5c31\u65e0\u9700\u663e\u5f0f\u5730\u4f7f\u7528 for \u3001 while \u7b49\u5faa\u73af\u8bed\u53e5\u4e86\uff0c\u8fd9\u5c31\u662f\u6240\u8c13\u7684\u201cInternal Iterators\u201d\u3002 External iterators and the iterator pattern An external iterator may be thought of as a type of pointer that has two primary operations: referencing one particular element in the object collection (called element access ), and modifying itself so it points to the next element (called element traversal ). There must also be a way to create an iterator so it points to some first element as well as some way to determine when the iterator has exhausted all of the elements in the container. Depending on the language and intended use, iterators may also provide additional operations or exhibit different behaviors. NOTE: \u5c06iterator\u6bd4\u4f5cpointer\uff0c\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u3002 The primary purpose of an iterator is to allow a user to process every element of a container while isolating the user from the internal structure of the container. This allows the container to store elements in any manner it wishes while allowing the user to treat it as if it were a simple sequence or list. An iterator class is usually designed in tight coordination with the corresponding container class. Usually, the container provides the methods for creating iterators. NOTE: separation A loop counter is sometimes also referred to as a loop iterator. A loop counter , however, only provides the traversal functionality and not the element access functionality. Implicit iterators Some object-oriented languages such as C# , C++ (later versions), Delphi (later versions), Go , Java (later versions), Lua , Perl , Python , Ruby provide an intrinsic way of iterating through the elements of a container object without the introduction of an explicit iterator object. An actual iterator object may exist in reality, but if it does it is not exposed within the source code of the language. Implicit iterators are often manifested by a \" foreach \" statement (or equivalent), such as in the following Python example: for value in iterable : print ( value ) NOTE: c++\u4e2d\u5c06\u6b64\u79f0\u4e3a range-for Contrasting with indexing NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002 Iterator VS pointer \u6211\u89c9\u5f97\uff0c\u6700\u6700\u80fd\u591f\u8bf4\u660eiterator VS pointer\u7684\u662f Iterator pattern \uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Iterator pattern#Overview \u4e2d\u8bf4\u660e\u4e86\u6211\u4eec\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528iterator pattern\uff0c\u5176\u5b9e\u5c31\u662fiterator pattern\u76f8\u6bd4\u4e8epointer\u7684\u4f18\u52bf\u6240\u5728\uff0citerator\u66f4\u52a0\u5730\u62bd\u8c61\uff0c\u6211\u4eec\u4f9d\u8d56\u4e8e\u62bd\u8c61\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u5177\u4f53\u3002 Application of iterator Generic programming \u53c2\u89c1 Theory\\Programming-paradigm\\Generic-programming \uff0c\u5178\u578b\u4f8b\u5b50\u5c31\u662fSTL\uff0c\u5176\u4e2d\u4f7f\u7528iterator\u6765\u4f5c\u4e3acontainer\u7684\u62bd\u8c61\u63cf\u8ff0\u3002","title":"Iterator"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#iterator","text":"Iterator\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6982\u5ff5\u3002","title":"Iterator"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#wikipedia#iterator","text":"In computer programming , an iterator is an object that enables a programmer to traverse a container , particularly lists . Various types of iterators are often provided via a container's interface . Though the interface and semantics of a given iterator are fixed, iterators are often implemented in terms of the structures underlying a container implementation and are often tightly coupled to the container to enable the operational semantics of the iterator. An iterator performs traversal and also gives access to data elements in a container, but does not itself perform iteration (i.e., not without some significant liberty taken with that concept or with trivial use of the terminology). An iterator is behaviorally similar to a database cursor . NOTE:\u8bf7\u601d\u8003iterator\u548citeration\u4e4b\u95f4\u5173\u7cfb\uff1f\u6211\u4eec\u5f80\u5f80\u662f\u901a\u8fc7 for \uff0c while \u8bed\u53e5\u6765\u5b9e\u73b0iteration\u3002iterator\u5219\u76f8\u5f53\u4e8e\u4e0e\u4e00\u4e2apointer\u3002","title":"wikipedia Iterator"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#description","text":"NOTE: \u539f\u6587\u7684\u8fd9\u4e00\u6bb5\u975e\u5e38\u503c\u5f97\u4e00\u8bfb","title":"Description"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#internal#iterators","text":"Internal iterators are higher order functions (often taking anonymous functions ) such as map , reduce etc., implementing the traversal across a container, applying the given function to every element in turn. NOTE: \u5176\u5b9e\u8fd9\u5c5e\u4e8efunctional programming paradigm\uff0c\u7ed3\u5408python\u7684Functional Programming Modules \u00b6 \u3001Functional Programming HOWTO \u00b6 \u6765\u7406\u89e3\u662f\u975e\u5e38\u597d\u7406\u89e3\u7684\u3002\u6240\u8c13\u7684internal iterator\u662f\u6307\u4f7f\u7528 map \u7b49 higher order function\uff08\u8fd9\u4e9b\u51fd\u6570\u63a5\u6536\u4e00\u4e2a\u51fd\u6570\u4f5c\u4e3a\u53c2\u6570\uff09\uff0c\u8fd9\u4e9b\u51fd\u6570\u5728\u5185\u90e8\u6267\u884citeration\u64cd\u4f5c\uff0c\u6240\u4ee5\u5bf9\u4e8e\u4f7f\u7528\u8005\u800c\u8a00\uff0c\u5b83\u5c31\u65e0\u9700\u663e\u5f0f\u5730\u4f7f\u7528 for \u3001 while \u7b49\u5faa\u73af\u8bed\u53e5\u4e86\uff0c\u8fd9\u5c31\u662f\u6240\u8c13\u7684\u201cInternal Iterators\u201d\u3002","title":"Internal Iterators"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#external#iterators#and#the#iterator#pattern","text":"An external iterator may be thought of as a type of pointer that has two primary operations: referencing one particular element in the object collection (called element access ), and modifying itself so it points to the next element (called element traversal ). There must also be a way to create an iterator so it points to some first element as well as some way to determine when the iterator has exhausted all of the elements in the container. Depending on the language and intended use, iterators may also provide additional operations or exhibit different behaviors. NOTE: \u5c06iterator\u6bd4\u4f5cpointer\uff0c\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u3002 The primary purpose of an iterator is to allow a user to process every element of a container while isolating the user from the internal structure of the container. This allows the container to store elements in any manner it wishes while allowing the user to treat it as if it were a simple sequence or list. An iterator class is usually designed in tight coordination with the corresponding container class. Usually, the container provides the methods for creating iterators. NOTE: separation A loop counter is sometimes also referred to as a loop iterator. A loop counter , however, only provides the traversal functionality and not the element access functionality.","title":"External iterators and the iterator pattern"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#implicit#iterators","text":"Some object-oriented languages such as C# , C++ (later versions), Delphi (later versions), Go , Java (later versions), Lua , Perl , Python , Ruby provide an intrinsic way of iterating through the elements of a container object without the introduction of an explicit iterator object. An actual iterator object may exist in reality, but if it does it is not exposed within the source code of the language. Implicit iterators are often manifested by a \" foreach \" statement (or equivalent), such as in the following Python example: for value in iterable : print ( value ) NOTE: c++\u4e2d\u5c06\u6b64\u79f0\u4e3a range-for","title":"Implicit iterators"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#contrasting#with#indexing","text":"NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002","title":"Contrasting with indexing"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#iterator#vs#pointer","text":"\u6211\u89c9\u5f97\uff0c\u6700\u6700\u80fd\u591f\u8bf4\u660eiterator VS pointer\u7684\u662f Iterator pattern \uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Iterator pattern#Overview \u4e2d\u8bf4\u660e\u4e86\u6211\u4eec\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528iterator pattern\uff0c\u5176\u5b9e\u5c31\u662fiterator pattern\u76f8\u6bd4\u4e8epointer\u7684\u4f18\u52bf\u6240\u5728\uff0citerator\u66f4\u52a0\u5730\u62bd\u8c61\uff0c\u6211\u4eec\u4f9d\u8d56\u4e8e\u62bd\u8c61\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u5177\u4f53\u3002","title":"Iterator VS pointer"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#application#of#iterator","text":"","title":"Application of iterator"},{"location":"Relation-structure-computation/Computation/Repetition/Iteration/Iterator/#generic#programming","text":"\u53c2\u89c1 Theory\\Programming-paradigm\\Generic-programming \uff0c\u5178\u578b\u4f8b\u5b50\u5c31\u662fSTL\uff0c\u5176\u4e2d\u4f7f\u7528iterator\u6765\u4f5c\u4e3acontainer\u7684\u62bd\u8c61\u63cf\u8ff0\u3002","title":"Generic programming"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/","text":"\u5173\u4e8e\u672c\u7ae0 \u5728\u9605\u8bfbautomata theory\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u539f\u6765\u5b83\u662ftheory of computation\u7684\u4e00\u4e2a\u5206\u652f\uff0c\u6240\u4ee5\u6b64\u5904\u5c31\u5c06theory of computation\u4e00\u5e76\u7ed9\u6574\u7406\u4e86\u4e00\u4e0b\u3002 \u9996\u5148\u63cf\u8ff0 theory of computation \uff0c\u7136\u540e\u63cf\u8ff0 model of computation \uff0c\u7136\u540e\u63cf\u8ff0\u5b83\u7684\u5404\u4e2abranch\uff08\u5171\u4e09\u4e2a\uff0c\u8fd9\u4e09\u4e2abranch\u662fsoftware engineer\u7ecf\u5e38\u4f1a\u63a5\u89e6\u5230\u7684\uff09\u3002\u56e0\u4e3a model of computation \u5728\u4e09\u4e2abranch\u4e2d\u90fd\u4f1a\u4f7f\u7528\u5230\uff0c\u6240\u4ee5\u628a\u5b83\u653e\u5230\u524d\u9762\u6765\u8fdb\u884c\u63cf\u8ff0\u3002","title":"index"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/#_1","text":"\u5728\u9605\u8bfbautomata theory\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u539f\u6765\u5b83\u662ftheory of computation\u7684\u4e00\u4e2a\u5206\u652f\uff0c\u6240\u4ee5\u6b64\u5904\u5c31\u5c06theory of computation\u4e00\u5e76\u7ed9\u6574\u7406\u4e86\u4e00\u4e0b\u3002 \u9996\u5148\u63cf\u8ff0 theory of computation \uff0c\u7136\u540e\u63cf\u8ff0 model of computation \uff0c\u7136\u540e\u63cf\u8ff0\u5b83\u7684\u5404\u4e2abranch\uff08\u5171\u4e09\u4e2a\uff0c\u8fd9\u4e09\u4e2abranch\u662fsoftware engineer\u7ecf\u5e38\u4f1a\u63a5\u89e6\u5230\u7684\uff09\u3002\u56e0\u4e3a model of computation \u5728\u4e09\u4e2abranch\u4e2d\u90fd\u4f1a\u4f7f\u7528\u5230\uff0c\u6240\u4ee5\u628a\u5b83\u653e\u5230\u524d\u9762\u6765\u8fdb\u884c\u63cf\u8ff0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Theory-of-computation/","text":"wikipedia Theory of computation In theoretical computer science and mathematics , the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation , using an algorithm . The field is divided into three major branches: automata theory and formal languages computability theory computational complexity theory , which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\". In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation . There are several models in use, but the most commonly examined is the Turing machine . Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible \"reasonable\" model of computation (see Church\u2013Turing thesis ). It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.","title":"Theory-of-computation"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Theory-of-computation/#wikipedia#theory#of#computation","text":"In theoretical computer science and mathematics , the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation , using an algorithm . The field is divided into three major branches: automata theory and formal languages computability theory computational complexity theory , which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\". In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation . There are several models in use, but the most commonly examined is the Turing machine . Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible \"reasonable\" model of computation (see Church\u2013Turing thesis ). It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.","title":"wikipedia Theory of computation"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Abstract-machine/","text":"wikipedia Abstract machine An abstract machine , also called an abstract computer , is a theoretical model of a computer hardware or software system used in automata theory . Abstraction of computing processes is used in both the computer science and computer engineering disciplines and usually assumes a discrete time paradigm .","title":"Abstract-machine"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Abstract-machine/#wikipedia#abstract#machine","text":"An abstract machine , also called an abstract computer , is a theoretical model of a computer hardware or software system used in automata theory . Abstraction of computing processes is used in both the computer science and computer engineering disciplines and usually assumes a discrete time paradigm .","title":"wikipedia Abstract machine"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-software/","text":"Automata software Python python-statemachine automata python automata-from-regex pywonderland Cloujr automat Java stateless4j C++ state How to Code a State Machine in C or C++ State Machine Design in C++ Expressive Code for State Machines in C++ C++ code for state machine RUST regex-automata","title":"Automata-software"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-software/#automata#software","text":"","title":"Automata software"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-software/#python","text":"python-statemachine automata python automata-from-regex pywonderland","title":"Python"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-software/#cloujr","text":"automat","title":"Cloujr"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-software/#java","text":"stateless4j","title":"Java"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-software/#c","text":"state How to Code a State Machine in C or C++ State Machine Design in C++ Expressive Code for State Machines in C++ C++ code for state machine","title":"C++"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-software/#rust","text":"regex-automata","title":"RUST"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/","text":"wikipedia Automata theory TIPS: In Chinese, Automata theory means \u81ea\u52a8\u673a\u7406\u8bba. Automata theory is the study of abstract machines and automata , as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science ). The word automata (the plural of automaton ) comes from the Greek word \u03b1\u1f50\u03c4\u03cc\u03bc\u03b1\u03c4\u03b1, which means \"self-making\". The figure at right illustrates a finite-state machine , which belongs to a well-known type of automaton. This automaton\uff08\u81ea\u52a8\u673a\uff09 consists of states (represented in the figure by circles) and transitions (represented by arrows). As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function , which takes the current state and the recent symbol as its inputs. Automata theory is closely related to formal language theory. An automaton is a finite representation of a formal language that may be an infinite set . Automata are often classified by the class of formal languages they can recognize, typically illustrated by the Chomsky hierarchy , which describes the relations between various languages and kinds of formalized logics. TIPS: It is obvious that automata theory is an important tool for research formal language. Automata play a major role in theory of computation , compiler construction \uff08\u7f16\u8bd1\u5668\u7684\u6784\u5efa\uff09, artificial intelligence , parsing and formal verification . Classes of automata: Turing machine Pushdown automaton Finite-state machine Combinational logic The study of the mathematical properties of such automata is automata theory. The picture is a visualization of an automaton that recognizes strings containing an even number of 0*s. The automaton starts in state *S1 , and transitions to the non-accepting state S2 upon reading the symbol 0 . Reading another 0 causes the automaton to transition back to the accepting state S1 . In both states the symbol 1 is ignored by making a transition to the current state. TIPS: The diagram above is called a state diagram . Automata Following is an introductory definition of one type of automaton, which attempts to help one grasp the essential concepts involved in automata theory/theories. Informal description An automaton runs when it is given some sequence of inputs in discrete (individual) time steps or steps. An automaton processes one input picked from a set of symbols or letters , which is called an alphabet . The symbols received by the automaton as input at any step are a finite sequence of symbols called words . An automaton has a finite set of states . At each moment during a run of the automaton, the automaton is in one of its states. When the automaton receives new input it moves to another state (or transitions) based on a function that takes the current state and symbol as parameters. This function is called the transition function . The automaton reads the symbols of the input word one after another and transitions from state to state according to the transition function until the word is read completely. Once the input word has been read, the automaton is said to have stopped. The state at which the automaton stops is called the final state . Depending on the final state , it's said that the automaton either accepts or rejects an input word. There is a subset of states of the automaton, which is defined as the set of accepting states . If the final state is an accepting state , then the automaton accepts the word. Otherwise, the word is rejected . The set of all the words accepted by an automaton is called the language recognized by the automaton . In short, an automaton is a mathematical object that takes a word as input and decides whether to accept it or reject it. Since all computational problems are reducible into the accept/reject question on inputs, (all problem instances can be represented in a finite length of symbols), automata theory plays a crucial role in computational theory . TIPS: In formal language, automata act as recognizer or matcher consuming characters or tokens . The graph above is a good example of automaton. Automata can be used not only in formal language, it has many application in computer science, such as: Event-driven finite-state machine State machine replication In these applications, what automaton consume can be event. See Finite-state machine for more detail. Formal definition Automaton Classes of automata The following is an incomplete list of types of automata. Automaton Recognizable language Nondeterministic/Deterministic Finite state machine (FSM) regular languages Deterministic pushdown automaton (DPDA) deterministic context-free languages Pushdown automaton (PDA) context-free languages Linear bounded automaton (LBA) context-sensitive languages Turing machine recursively enumerable languages Deterministic B\u00fcchi automaton \u03c9-limit languages Nondeterministic B\u00fcchi automaton \u03c9-regular languages Rabin automaton , Streett automaton , Parity automaton , Muller automaton Discrete, continuous, and hybrid automata Normally automata theory describes the states of abstract machines but there are analog automata or continuous automata or hybrid discrete-continuous automata , which use analog data, continuous time, or both. Hierarchy in terms of powers The following is an incomplete hierarchy in terms of powers of different types of virtual machines. The hierarchy reflects the nested categories of languages the machines are able to accept.[ 1] Automaton Deterministic Finite Automaton (DFA) -- Lowest Power (same power) $ Applications Each model in automata theory plays important roles in several applied areas. Finite automata are used in text processing, compilers, and hardware design. Context-free grammar (CFGs) are used in programming languages and artificial intelligence. Originally, CFGs were used in the study of the human languages. Cellular automata are used in the field of biology, the most common example being John Conway 's Game of Life . Some other examples which could be explained using automata theory in biology include mollusk and pine cones growth and pigmentation patterns. Going further, a theory suggesting that the whole universe is computed by some sort of a discrete automaton, is advocated by some scientists. The idea originated in the work of Konrad Zuse , and was popularized in America by Edward Fredkin . Automata also appear in the theory of finite fields: the set of irreducible polynomials which can be written as composition of degree two polynomials is in fact a regular language.[ 2] Automata simulators Automata simulators are pedagogical tools used to teach, learn and research automata theory. An automata simulator takes as input the description of an automaton and then simulates its working for an arbitrary input string. The description of the automaton can be entered in several ways. An automaton can be defined in a symbolic language or its specification may be entered in a predesigned form or its transition diagram may be drawn by clicking and dragging the mouse. Well known automata simulators include Turing's World, JFLAP, VAS, TAGS and SimStudio.[ 3]","title":"Automata-theory"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#wikipedia#automata#theory","text":"TIPS: In Chinese, Automata theory means \u81ea\u52a8\u673a\u7406\u8bba. Automata theory is the study of abstract machines and automata , as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science ). The word automata (the plural of automaton ) comes from the Greek word \u03b1\u1f50\u03c4\u03cc\u03bc\u03b1\u03c4\u03b1, which means \"self-making\". The figure at right illustrates a finite-state machine , which belongs to a well-known type of automaton. This automaton\uff08\u81ea\u52a8\u673a\uff09 consists of states (represented in the figure by circles) and transitions (represented by arrows). As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function , which takes the current state and the recent symbol as its inputs. Automata theory is closely related to formal language theory. An automaton is a finite representation of a formal language that may be an infinite set . Automata are often classified by the class of formal languages they can recognize, typically illustrated by the Chomsky hierarchy , which describes the relations between various languages and kinds of formalized logics. TIPS: It is obvious that automata theory is an important tool for research formal language. Automata play a major role in theory of computation , compiler construction \uff08\u7f16\u8bd1\u5668\u7684\u6784\u5efa\uff09, artificial intelligence , parsing and formal verification . Classes of automata: Turing machine Pushdown automaton Finite-state machine Combinational logic The study of the mathematical properties of such automata is automata theory. The picture is a visualization of an automaton that recognizes strings containing an even number of 0*s. The automaton starts in state *S1 , and transitions to the non-accepting state S2 upon reading the symbol 0 . Reading another 0 causes the automaton to transition back to the accepting state S1 . In both states the symbol 1 is ignored by making a transition to the current state. TIPS: The diagram above is called a state diagram .","title":"wikipedia Automata theory"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#automata","text":"Following is an introductory definition of one type of automaton, which attempts to help one grasp the essential concepts involved in automata theory/theories.","title":"Automata"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#informal#description","text":"An automaton runs when it is given some sequence of inputs in discrete (individual) time steps or steps. An automaton processes one input picked from a set of symbols or letters , which is called an alphabet . The symbols received by the automaton as input at any step are a finite sequence of symbols called words . An automaton has a finite set of states . At each moment during a run of the automaton, the automaton is in one of its states. When the automaton receives new input it moves to another state (or transitions) based on a function that takes the current state and symbol as parameters. This function is called the transition function . The automaton reads the symbols of the input word one after another and transitions from state to state according to the transition function until the word is read completely. Once the input word has been read, the automaton is said to have stopped. The state at which the automaton stops is called the final state . Depending on the final state , it's said that the automaton either accepts or rejects an input word. There is a subset of states of the automaton, which is defined as the set of accepting states . If the final state is an accepting state , then the automaton accepts the word. Otherwise, the word is rejected . The set of all the words accepted by an automaton is called the language recognized by the automaton . In short, an automaton is a mathematical object that takes a word as input and decides whether to accept it or reject it. Since all computational problems are reducible into the accept/reject question on inputs, (all problem instances can be represented in a finite length of symbols), automata theory plays a crucial role in computational theory . TIPS: In formal language, automata act as recognizer or matcher consuming characters or tokens . The graph above is a good example of automaton. Automata can be used not only in formal language, it has many application in computer science, such as: Event-driven finite-state machine State machine replication In these applications, what automaton consume can be event. See Finite-state machine for more detail.","title":"Informal description"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#formal#definition","text":"Automaton","title":"Formal definition"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#classes#of#automata","text":"The following is an incomplete list of types of automata. Automaton Recognizable language Nondeterministic/Deterministic Finite state machine (FSM) regular languages Deterministic pushdown automaton (DPDA) deterministic context-free languages Pushdown automaton (PDA) context-free languages Linear bounded automaton (LBA) context-sensitive languages Turing machine recursively enumerable languages Deterministic B\u00fcchi automaton \u03c9-limit languages Nondeterministic B\u00fcchi automaton \u03c9-regular languages Rabin automaton , Streett automaton , Parity automaton , Muller automaton","title":"Classes of automata"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#discrete#continuous#and#hybrid#automata","text":"Normally automata theory describes the states of abstract machines but there are analog automata or continuous automata or hybrid discrete-continuous automata , which use analog data, continuous time, or both.","title":"Discrete, continuous, and hybrid automata"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#hierarchy#in#terms#of#powers","text":"The following is an incomplete hierarchy in terms of powers of different types of virtual machines. The hierarchy reflects the nested categories of languages the machines are able to accept.[ 1] Automaton Deterministic Finite Automaton (DFA) -- Lowest Power (same power) $","title":"Hierarchy in terms of powers"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#applications","text":"Each model in automata theory plays important roles in several applied areas. Finite automata are used in text processing, compilers, and hardware design. Context-free grammar (CFGs) are used in programming languages and artificial intelligence. Originally, CFGs were used in the study of the human languages. Cellular automata are used in the field of biology, the most common example being John Conway 's Game of Life . Some other examples which could be explained using automata theory in biology include mollusk and pine cones growth and pigmentation patterns. Going further, a theory suggesting that the whole universe is computed by some sort of a discrete automaton, is advocated by some scientists. The idea originated in the work of Konrad Zuse , and was popularized in America by Edward Fredkin . Automata also appear in the theory of finite fields: the set of irreducible polynomials which can be written as composition of degree two polynomials is in fact a regular language.[ 2]","title":"Applications"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Automata-theory/#automata#simulators","text":"Automata simulators are pedagogical tools used to teach, learn and research automata theory. An automata simulator takes as input the description of an automaton and then simulates its working for an arbitrary input string. The description of the automaton can be entered in several ways. An automaton can be defined in a symbolic language or its specification may be entered in a predesigned form or its transition diagram may be drawn by clicking and dragging the mouse. Well known automata simulators include Turing's World, JFLAP, VAS, TAGS and SimStudio.[ 3]","title":"Automata simulators"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/","text":"Finite-state machine TIPS: In Chinese, Finite-state machine means \u6709\u7a77\u81ea\u52a8\u673a A finite-state machine ( FSM ) or finite-state automaton ( FSA , plural: automata ), finite automaton , or simply a state machine , is a mathematical model of computation . It is an abstract machine that can be in exactly one of a finite number of states at any given time. The FSM can change from one state to another in response to some external inputs and/or a condition is satisfied; the change from one state to another is called a transition .[ 1] An FSM is defined by a list of its states, its initial state, and the conditions for each transition. Finite state machines are of two types: deterministic finite state machines non-deterministic finite state machines .[ 2] A deterministic finite-state machine can be constructed equivalent to any non-deterministic one. The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines , which dispense products when the proper combination of coins is deposited, elevators , whose sequence of stops is determined by the floors requested by riders, traffic lights , which change sequence when cars are waiting, and combination locks , which require the input of a sequence of numbers in the proper order. TIPS: The state machine can do much more, see Usage for more detail. The finite state machine has less computational power than some other models of computation such as the Turing machine .[ 3] The computational power distinction means there are computational tasks that a Turing machine can do but a FSM cannot. This is because a FSM's memory is limited by the number of states it has. FSMs are studied in the more general field of automata theory . TIPS: Turing machine operates on an infinite memory. TIPS: There is an incomplete hierarchy in terms of powers of different types of abstract machines in page automata theory Concepts and terminology A state is a description of the status of a system that is waiting to execute a transition . A transition is a set of actions to be executed when a condition is fulfilled or when an event is received. For example, when using an audio system to listen to the radio (the system is in the \"radio\" state), receiving a \"next\" stimulus results in moving to the next station. When the system is in the \"CD\" state, the \"next\" stimulus results in moving to the next track. Identical stimuli trigger different actions depending on the current state. In some finite-state machine representations, it is also possible to associate actions with a state: an entry action: performed when entering the state, and an exit action: performed when exiting the state. Representations state transition table UML state machines SDL state machines State diagram Classification Finite state machines can be subdivided into transducers\uff08\u8f6c\u6362\u5668\uff09 acceptors classifiers sequencers.[ 6] Acceptors (recognizers) Acceptors (also called recognizers and sequence detectors ), produce binary output , indicating whether or not the received input is accepted. Each state of an FSM is either \"accepting\" or \"not accepting\". Once all input has been received, if the current state is an accepting state , the input is accepted; otherwise it is rejected. As a rule, input is a sequence of symbols (characters); actions are not used. The example in figure 4 shows a finite state machine that accepts the string \"nice\". In this FSM, the only accepting state is state 7. TIPS: In machine learning terms, it's a dichotomy. Fig. 4 Acceptor FSM: parsing the string \"nice\" A (possibly infinite) set of symbol sequences, aka. formal language, is called a regular language if there is some Finite State Machine that accepts exactly that set. For example, the set of binary strings with an even number of zeroes is a regular language (cf. Fig. 5), while the set of all strings whose length is a prime number is not. Fig. 5: Representation of a finite-state machine; this example shows one that determines whether a binary number has an even number of 0s, where S*1 is an **accepting state* . A machine could also be described as defining a language, that would contain every string accepted by the machine but none of the rejected ones; that language is \"accepted\" by the machine. By definition, the languages accepted by FSMs are the regular languages \u2014; a language is regular if there is some FSM that accepts it. The problem of determining the language accepted by a given finite state acceptor is an instance of the algebraic path problem \u2014itself a generalization of the shortest path problem to graphs with edges weighted by the elements of an (arbitrary) semiring . TIPS: This passage reminds me of regular expression . The start state can also be an accepting state, in which case the automaton accepts the empty string. An example of an accepting state appears in Fig.5: a deterministic finite automaton (DFA) that detects whether the binary input string contains an even number of 0s. S*1 (which is also the start state) indicates the state at which an even number of 0s has been input. S1 is therefore an **accepting state* . This machine will finish in an accept state , if the binary string contains an even number of 0s (including any binary string containing no 0s). Examples of strings accepted by this DFA are \u03b5 (the empty string ), 1, 11, 11\u2026, 00, 010, 1010, 10110, etc. Classifiers A classifier is a generalization of a finite state machine that, similar to an acceptor, produces a single output on termination but has more than two terminal states. TIPS: In machine learning terms, it's a multi-class classifier. Determinism A further distinction is between deterministic ( DFA ) and non-deterministic ( NFA , GNFA ) automata. In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality. A finite state machine with only one state is called a \"combinatorial FSM\". It only allows actions upon transition into a state. This concept is useful in cases where a number of finite state machines are required to work together, and when it is convenient to consider a purely combinatorial part as a form of FSM to suit the design tools.[ 10] Mathematical model In accordance with the general classification, the following formal definitions are found: A deterministic finite state machine or acceptor deterministic finite state machine is a quintuple $ (\\Sigma ,S,s_{0},\\delta ,F) $, where: $ \\Sigma $ is the input alphabet (a finite, non-empty set of symbols). $ S $ is a finite, non-empty set of states. $ s_{0} $ is an initial state, an element of $ S $. $ \\delta $ is the state-transition function: $ \\delta :S\\times \\Sigma \\rightarrow S $ (in a nondeterministic finite automaton it would be $ \\delta :S\\times \\Sigma \\rightarrow {\\mathcal {P}}(S) $, i.e., $ \\delta $ would return a set of states). $ F $ is the set of final states, a (possibly empty) subset of $ S $. For both deterministic and non-deterministic FSMs, it is conventional to allow $ \\delta $ to be a partial function , i.e. $ \\delta (q,x) $ does not have to be defined for every combination of $ q\\in S $ and $ x\\in \\Sigma $. If an FSM $ M $ is in a state $ q $, the next symbol is $ x $ and $ \\delta (q,x) $ is not defined, then $ M $ can announce an error (i.e. reject the input). This is useful in definitions of general state machines, but less useful when transforming the machine. Some algorithms in their default form may require total functions . A finite state machine has the same computational power as a Turing machine that is restricted such that its head may only perform \"read\" operations, and always has to move from left to right. That is, each formal language accepted by a finite state machine is accepted by such a kind of restricted Turing machine, and vice versa.[ 15] TIPS: We can conclude that Turing machine is an generalization of finite state machine or finite state machine is a restrictive version of Turing machine . Optimization Main article: DFA minimization Optimizing an FSM means finding a machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm .[ 17] [ 18] Other techniques include using an implication table , or the Moore reduction procedure . Additionally, acyclic FSAs can be minimized in linear time.[ 19] Usage In addition to their use in modeling reactive systems presented here, finite state machines are significant in many different areas, including electrical engineering , linguistics , computer science , philosophy , biology , mathematics , and logic . Finite state machines are a class of automata studied in automata theory and the theory of computation . In computer science, finite state machines are widely used in modeling of application behavior, design of hardware digital systems , software engineering , compilers , network protocols , and the study of computation and languages. Software applications The following concepts are commonly used to build software applications with finite state machines: Automata-based programming Event-driven finite-state machine Virtual finite-state machine State design pattern State machine replication Regular expression Finite state machines and compilers Finite automata are often used in the frontend of programming language compilers. Such a frontend may comprise several finite state machines that implement a lexical analyzer and a parser. Starting from a sequence of characters, the lexical analyzer builds a sequence of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The lexical analyzer and the parser handle the regular and context-free parts of the programming language's grammar.[ 22]","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#finite-state#machine","text":"TIPS: In Chinese, Finite-state machine means \u6709\u7a77\u81ea\u52a8\u673a A finite-state machine ( FSM ) or finite-state automaton ( FSA , plural: automata ), finite automaton , or simply a state machine , is a mathematical model of computation . It is an abstract machine that can be in exactly one of a finite number of states at any given time. The FSM can change from one state to another in response to some external inputs and/or a condition is satisfied; the change from one state to another is called a transition .[ 1] An FSM is defined by a list of its states, its initial state, and the conditions for each transition. Finite state machines are of two types: deterministic finite state machines non-deterministic finite state machines .[ 2] A deterministic finite-state machine can be constructed equivalent to any non-deterministic one. The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines , which dispense products when the proper combination of coins is deposited, elevators , whose sequence of stops is determined by the floors requested by riders, traffic lights , which change sequence when cars are waiting, and combination locks , which require the input of a sequence of numbers in the proper order. TIPS: The state machine can do much more, see Usage for more detail. The finite state machine has less computational power than some other models of computation such as the Turing machine .[ 3] The computational power distinction means there are computational tasks that a Turing machine can do but a FSM cannot. This is because a FSM's memory is limited by the number of states it has. FSMs are studied in the more general field of automata theory . TIPS: Turing machine operates on an infinite memory. TIPS: There is an incomplete hierarchy in terms of powers of different types of abstract machines in page automata theory","title":"Finite-state machine"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#concepts#and#terminology","text":"A state is a description of the status of a system that is waiting to execute a transition . A transition is a set of actions to be executed when a condition is fulfilled or when an event is received. For example, when using an audio system to listen to the radio (the system is in the \"radio\" state), receiving a \"next\" stimulus results in moving to the next station. When the system is in the \"CD\" state, the \"next\" stimulus results in moving to the next track. Identical stimuli trigger different actions depending on the current state. In some finite-state machine representations, it is also possible to associate actions with a state: an entry action: performed when entering the state, and an exit action: performed when exiting the state.","title":"Concepts and terminology"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#representations","text":"state transition table UML state machines SDL state machines State diagram","title":"Representations"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#classification","text":"Finite state machines can be subdivided into transducers\uff08\u8f6c\u6362\u5668\uff09 acceptors classifiers sequencers.[ 6]","title":"Classification"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#acceptors#recognizers","text":"Acceptors (also called recognizers and sequence detectors ), produce binary output , indicating whether or not the received input is accepted. Each state of an FSM is either \"accepting\" or \"not accepting\". Once all input has been received, if the current state is an accepting state , the input is accepted; otherwise it is rejected. As a rule, input is a sequence of symbols (characters); actions are not used. The example in figure 4 shows a finite state machine that accepts the string \"nice\". In this FSM, the only accepting state is state 7. TIPS: In machine learning terms, it's a dichotomy. Fig. 4 Acceptor FSM: parsing the string \"nice\" A (possibly infinite) set of symbol sequences, aka. formal language, is called a regular language if there is some Finite State Machine that accepts exactly that set. For example, the set of binary strings with an even number of zeroes is a regular language (cf. Fig. 5), while the set of all strings whose length is a prime number is not. Fig. 5: Representation of a finite-state machine; this example shows one that determines whether a binary number has an even number of 0s, where S*1 is an **accepting state* . A machine could also be described as defining a language, that would contain every string accepted by the machine but none of the rejected ones; that language is \"accepted\" by the machine. By definition, the languages accepted by FSMs are the regular languages \u2014; a language is regular if there is some FSM that accepts it. The problem of determining the language accepted by a given finite state acceptor is an instance of the algebraic path problem \u2014itself a generalization of the shortest path problem to graphs with edges weighted by the elements of an (arbitrary) semiring . TIPS: This passage reminds me of regular expression . The start state can also be an accepting state, in which case the automaton accepts the empty string. An example of an accepting state appears in Fig.5: a deterministic finite automaton (DFA) that detects whether the binary input string contains an even number of 0s. S*1 (which is also the start state) indicates the state at which an even number of 0s has been input. S1 is therefore an **accepting state* . This machine will finish in an accept state , if the binary string contains an even number of 0s (including any binary string containing no 0s). Examples of strings accepted by this DFA are \u03b5 (the empty string ), 1, 11, 11\u2026, 00, 010, 1010, 10110, etc.","title":"Acceptors (recognizers)"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#classifiers","text":"A classifier is a generalization of a finite state machine that, similar to an acceptor, produces a single output on termination but has more than two terminal states. TIPS: In machine learning terms, it's a multi-class classifier.","title":"Classifiers"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#determinism","text":"A further distinction is between deterministic ( DFA ) and non-deterministic ( NFA , GNFA ) automata. In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality. A finite state machine with only one state is called a \"combinatorial FSM\". It only allows actions upon transition into a state. This concept is useful in cases where a number of finite state machines are required to work together, and when it is convenient to consider a purely combinatorial part as a form of FSM to suit the design tools.[ 10]","title":"Determinism"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#mathematical#model","text":"In accordance with the general classification, the following formal definitions are found: A deterministic finite state machine or acceptor deterministic finite state machine is a quintuple $ (\\Sigma ,S,s_{0},\\delta ,F) $, where: $ \\Sigma $ is the input alphabet (a finite, non-empty set of symbols). $ S $ is a finite, non-empty set of states. $ s_{0} $ is an initial state, an element of $ S $. $ \\delta $ is the state-transition function: $ \\delta :S\\times \\Sigma \\rightarrow S $ (in a nondeterministic finite automaton it would be $ \\delta :S\\times \\Sigma \\rightarrow {\\mathcal {P}}(S) $, i.e., $ \\delta $ would return a set of states). $ F $ is the set of final states, a (possibly empty) subset of $ S $. For both deterministic and non-deterministic FSMs, it is conventional to allow $ \\delta $ to be a partial function , i.e. $ \\delta (q,x) $ does not have to be defined for every combination of $ q\\in S $ and $ x\\in \\Sigma $. If an FSM $ M $ is in a state $ q $, the next symbol is $ x $ and $ \\delta (q,x) $ is not defined, then $ M $ can announce an error (i.e. reject the input). This is useful in definitions of general state machines, but less useful when transforming the machine. Some algorithms in their default form may require total functions . A finite state machine has the same computational power as a Turing machine that is restricted such that its head may only perform \"read\" operations, and always has to move from left to right. That is, each formal language accepted by a finite state machine is accepted by such a kind of restricted Turing machine, and vice versa.[ 15] TIPS: We can conclude that Turing machine is an generalization of finite state machine or finite state machine is a restrictive version of Turing machine .","title":"Mathematical model"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#optimization","text":"Main article: DFA minimization Optimizing an FSM means finding a machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm .[ 17] [ 18] Other techniques include using an implication table , or the Moore reduction procedure . Additionally, acyclic FSAs can be minimized in linear time.[ 19]","title":"Optimization"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#usage","text":"In addition to their use in modeling reactive systems presented here, finite state machines are significant in many different areas, including electrical engineering , linguistics , computer science , philosophy , biology , mathematics , and logic . Finite state machines are a class of automata studied in automata theory and the theory of computation . In computer science, finite state machines are widely used in modeling of application behavior, design of hardware digital systems , software engineering , compilers , network protocols , and the study of computation and languages.","title":"Usage"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#software#applications","text":"The following concepts are commonly used to build software applications with finite state machines: Automata-based programming Event-driven finite-state machine Virtual finite-state machine State design pattern State machine replication Regular expression","title":"Software applications"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/#finite#state#machines#and#compilers","text":"Finite automata are often used in the frontend of programming language compilers. Such a frontend may comprise several finite state machines that implement a lexical analyzer and a parser. Starting from a sequence of characters, the lexical analyzer builds a sequence of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The lexical analyzer and the parser handle the regular and context-free parts of the programming language's grammar.[ 22]","title":"Finite state machines and compilers"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/Event-driven-finite-state-machine/","text":"Event-driven programming and state machine https://en.wikipedia.org/wiki/Event-driven_finite-state_machine https://softwareengineering.stackexchange.com/questions/379056/what-is-the-relationship-between-event-driven-design-and-state-machines-charts https://barrgroup.com/embedded-systems/how-to/state-machines-event-driven-systems","title":"Event-driven-finite-state-machine"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Automata-theory/Finite-state-machine/Event-driven-finite-state-machine/#event-driven#programming#and#state#machine","text":"https://en.wikipedia.org/wiki/Event-driven_finite-state_machine https://softwareengineering.stackexchange.com/questions/379056/what-is-the-relationship-between-event-driven-design-and-state-machines-charts https://barrgroup.com/embedded-systems/how-to/state-machines-event-driven-systems","title":"Event-driven programming and state machine"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computability-theory/Computability-theory/","text":"Computability theory","title":"Computability-theory"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computability-theory/Computability-theory/#computability#theory","text":"","title":"Computability theory"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-Computational-complexity-theory/","text":"Computational complexity theory wikipedia Computational complexity theory stackoverflow What does O(log n) mean exactly? I am learning about Big O Notation running times and amortized\uff08\u5206\u644a\uff09 times. I understand the notion of O(n) linear time, meaning that the size of the input affects the growth of the algorithm proportionally...and the same goes for, for example, quadratic time O(n^2) O(n^2) etc..even algorithms, such as permutation generators, with O(n!) times, that grow by factorials. For example, the following function is O(n) because the algorithm grows in proportion to its input n : f ( int n ) { int i ; for ( i = 0 ; i < n ; ++ i ) printf ( \"%d\" , i ); } Similarly, if there was a nested loop, the time would be O(n^2) O(n^2) . But what exactly is O(log n) ? For example, what does it mean to say that the height of a complete binary tree is O(log n) ? I do know (maybe not in great detail) what Logarithm is, in the sense that: log_{10}{ 100} = 2 log_{10}{ 100} = 2 , but I cannot understand how to identify a function with a logarithmic time. A I cannot understand how to identify a function with a log time. The most common attributes of logarithmic running-time function are that: the choice of the next element on which to perform some action is one of several possibilities, and only one will need to be chosen. or the elements on which the action is performed are digits of n This is why, for example, looking up people in a phone book is O(log n). You don't need to check every person in the phone book to find the right one; instead, you can simply divide-and-conquer by looking based on where their name is alphabetically, and in every section you only need to explore a subset of each section before you eventually find someone's phone number. Of course, a bigger phone book will still take you a longer time, but it won't grow as quickly as the proportional increase in the additional size. We can expand the phone book example to compare other kinds of operations and their running time. We will assume our phone book has businesses \uff08\u4f01\u4e1a\uff09 (the \"Yellow Pages\") which have unique names and people (the \"White Pages\") which may not have unique names. A phone number is assigned to at most one person or business. We will also assume that it takes constant time to flip to a specific page. Here are the running times of some operations we might perform on the phone book, from best to worst: O(1) (best case): Given the page that a business's name is on and the business name, find the phone number. O(1) (average case): Given the page that a person's name is on and their name, find the phone number. O(log n): Given a person's name, find the phone number by picking a random point about halfway through the part of the book you haven't searched yet, then checking to see whether the person's name is at that point. Then repeat the process about halfway through the part of the book where the person's name lies. (This is a binary search for a person's name.) O(n): Find all people whose phone numbers contain the digit \"5\". O(n): Given a phone number, find the person or business with that number. O(n log n): There was a mix-up at the printer's office, and our phone book had all its pages inserted in a random order. Fix the ordering so that it's correct by looking at the first name on each page and then putting that page in the appropriate spot in a new, empty phone book. For the below examples, we're now at the printer's office. Phone books are waiting to be mailed to each resident or business, and there's a sticker on each phone book identifying where it should be mailed to. Every person or business gets one phone book. O(n log n): We want to personalize the phone book, so we're going to find each person or business's name in their designated copy, then circle their name in the book and write a short thank-you note for their patronage. O(n2): A mistake occurred at the office, and every entry in each of the phone books has an extra \"0\" at the end of the phone number. Take some white-out and remove each zero. O(n \u00b7 n!): We're ready to load the phonebooks onto the shipping dock. Unfortunately, the robot that was supposed to load the books has gone haywire: it's putting the books onto the truck in a random order! Even worse, it loads all the books onto the truck, then checks to see if they're in the right order, and if not, it unloads them and starts over. (This is the dreaded bogo sort .) O(nn): You fix the robot so that it's loading things correctly. The next day, one of your co-workers plays a prank on you and wires the loading dock robot to the automated printing systems. Every time the robot goes to load an original book, the factory printer makes a duplicate run of all the phonebooks! Fortunately, the robot's bug-detection systems are sophisticated enough that the robot doesn't try printing even more copies when it encounters a duplicate book for loading, but it still has to load every original and duplicate book that's been printed. For more mathematical explanation you can checkout how the time complexity arrives to log n here. https://hackernoon.com/what-does-the-time-complexity-o-log-n-actually-mean-45f94bb5bfbf","title":"TODO-Computational-complexity-theory"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-Computational-complexity-theory/#computational#complexity#theory","text":"","title":"Computational complexity theory"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-Computational-complexity-theory/#wikipedia#computational#complexity#theory","text":"","title":"wikipedia Computational complexity theory"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-Computational-complexity-theory/#stackoverflow#what#does#olog#n#mean#exactly","text":"I am learning about Big O Notation running times and amortized\uff08\u5206\u644a\uff09 times. I understand the notion of O(n) linear time, meaning that the size of the input affects the growth of the algorithm proportionally...and the same goes for, for example, quadratic time O(n^2) O(n^2) etc..even algorithms, such as permutation generators, with O(n!) times, that grow by factorials. For example, the following function is O(n) because the algorithm grows in proportion to its input n : f ( int n ) { int i ; for ( i = 0 ; i < n ; ++ i ) printf ( \"%d\" , i ); } Similarly, if there was a nested loop, the time would be O(n^2) O(n^2) . But what exactly is O(log n) ? For example, what does it mean to say that the height of a complete binary tree is O(log n) ? I do know (maybe not in great detail) what Logarithm is, in the sense that: log_{10}{ 100} = 2 log_{10}{ 100} = 2 , but I cannot understand how to identify a function with a logarithmic time.","title":"stackoverflow What does O(log n) mean exactly?"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-Computational-complexity-theory/#a","text":"I cannot understand how to identify a function with a log time. The most common attributes of logarithmic running-time function are that: the choice of the next element on which to perform some action is one of several possibilities, and only one will need to be chosen. or the elements on which the action is performed are digits of n This is why, for example, looking up people in a phone book is O(log n). You don't need to check every person in the phone book to find the right one; instead, you can simply divide-and-conquer by looking based on where their name is alphabetically, and in every section you only need to explore a subset of each section before you eventually find someone's phone number. Of course, a bigger phone book will still take you a longer time, but it won't grow as quickly as the proportional increase in the additional size. We can expand the phone book example to compare other kinds of operations and their running time. We will assume our phone book has businesses \uff08\u4f01\u4e1a\uff09 (the \"Yellow Pages\") which have unique names and people (the \"White Pages\") which may not have unique names. A phone number is assigned to at most one person or business. We will also assume that it takes constant time to flip to a specific page. Here are the running times of some operations we might perform on the phone book, from best to worst: O(1) (best case): Given the page that a business's name is on and the business name, find the phone number. O(1) (average case): Given the page that a person's name is on and their name, find the phone number. O(log n): Given a person's name, find the phone number by picking a random point about halfway through the part of the book you haven't searched yet, then checking to see whether the person's name is at that point. Then repeat the process about halfway through the part of the book where the person's name lies. (This is a binary search for a person's name.) O(n): Find all people whose phone numbers contain the digit \"5\". O(n): Given a phone number, find the person or business with that number. O(n log n): There was a mix-up at the printer's office, and our phone book had all its pages inserted in a random order. Fix the ordering so that it's correct by looking at the first name on each page and then putting that page in the appropriate spot in a new, empty phone book. For the below examples, we're now at the printer's office. Phone books are waiting to be mailed to each resident or business, and there's a sticker on each phone book identifying where it should be mailed to. Every person or business gets one phone book. O(n log n): We want to personalize the phone book, so we're going to find each person or business's name in their designated copy, then circle their name in the book and write a short thank-you note for their patronage. O(n2): A mistake occurred at the office, and every entry in each of the phone books has an extra \"0\" at the end of the phone number. Take some white-out and remove each zero. O(n \u00b7 n!): We're ready to load the phonebooks onto the shipping dock. Unfortunately, the robot that was supposed to load the books has gone haywire: it's putting the books onto the truck in a random order! Even worse, it loads all the books onto the truck, then checks to see if they're in the right order, and if not, it unloads them and starts over. (This is the dreaded bogo sort .) O(nn): You fix the robot so that it's loading things correctly. The next day, one of your co-workers plays a prank on you and wires the loading dock robot to the automated printing systems. Every time the robot goes to load an original book, the factory printer makes a duplicate run of all the phonebooks! Fortunately, the robot's bug-detection systems are sophisticated enough that the robot doesn't try printing even more copies when it encounters a duplicate book for loading, but it still has to load every original and duplicate book that's been printed. For more mathematical explanation you can checkout how the time complexity arrives to log n here. https://hackernoon.com/what-does-the-time-complexity-o-log-n-actually-mean-45f94bb5bfbf","title":"A"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-NP-completeness/","text":"NP-completeness wikipedia NP-completeness TODO: stackoverflow What is an NP-complete in computer science?","title":"TODO-NP-completeness"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-NP-completeness/#np-completeness","text":"","title":"NP-completeness"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-NP-completeness/#wikipedia#np-completeness","text":"","title":"wikipedia NP-completeness"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-NP-completeness/#todo#stackoverflow#what#is#an#np-complete#in#computer#science","text":"","title":"TODO: stackoverflow What is an NP-complete in computer science?"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-NP-hardness/","text":"NP-hardness","title":"TODO-NP-hardness"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Computational-complexity-theory/TODO-NP-hardness/#np-hardness","text":"","title":"NP-hardness"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Great-Theoretical-Ideas-in-Computer-Science/","text":"Great Theoretical Ideas in Computer Science \u8ba1\u7b97\u673a\u4e13\u4e1a\u7684\u4e00\u95e8\u8bfe\u7a0b\u3002\u662f\u5728\u6d4f\u89c8 linkedin Cosku Acay \u7684 \"\u6240\u5b66\u8bfe\u7a0b\" \u4e2d\u53d1\u73b0\u7684\u8fd9\u95e8\u8bfe\u7a0b\u3002 cmu Great Theoretical Ideas in Computer Science mit Great Ideas in Theoretical Computer Science","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Great-Theoretical-Ideas-in-Computer-Science/#great#theoretical#ideas#in#computer#science","text":"\u8ba1\u7b97\u673a\u4e13\u4e1a\u7684\u4e00\u95e8\u8bfe\u7a0b\u3002\u662f\u5728\u6d4f\u89c8 linkedin Cosku Acay \u7684 \"\u6240\u5b66\u8bfe\u7a0b\" \u4e2d\u53d1\u73b0\u7684\u8fd9\u95e8\u8bfe\u7a0b\u3002","title":"Great Theoretical Ideas in Computer Science"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Great-Theoretical-Ideas-in-Computer-Science/#cmu#great#theoretical#ideas#in#computer#science","text":"","title":"cmu Great Theoretical Ideas in Computer Science"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Great-Theoretical-Ideas-in-Computer-Science/#mit#great#ideas#in#theoretical#computer#science","text":"","title":"mit Great Ideas in Theoretical Computer Science"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/","text":"Model of computation In computer science , and more specifically in computability theory and computational complexity theory , a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized. The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology. Models Models of computation can be classified in three categories: sequential models, functional models, and concurrent models. Sequential models include: Finite state machines Pushdown automata Turing machine Functional models include: Lambda calculus Recursive functions Combinatory logic Abstract rewriting systems Concurrent models include: Cellular automaton Kahn process networks Petri nets Synchronous Data Flow Interaction nets Actor model Models differ in their expressive power ; for example, each function that can be computed by a Finite state machine can also be computed by a Turing machine , but not vice versa. Categories There are many models of computation, differing in the set of admissible operations and their computations cost. They fall into the following broad categories: Abstract machine and models equivalent to it (e.g. lambda calculus is equivalent to the Turing machine ) - used in proofs of computability and upper bounds on computational complexity of algorithms . Decision tree models - used in proofs of lower bounds on computational complexity of algorithmic problems.","title":"Model-of-computation"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/#model#of#computation","text":"In computer science , and more specifically in computability theory and computational complexity theory , a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized. The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.","title":"Model of computation"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/#models","text":"Models of computation can be classified in three categories: sequential models, functional models, and concurrent models. Sequential models include: Finite state machines Pushdown automata Turing machine Functional models include: Lambda calculus Recursive functions Combinatory logic Abstract rewriting systems Concurrent models include: Cellular automaton Kahn process networks Petri nets Synchronous Data Flow Interaction nets Actor model Models differ in their expressive power ; for example, each function that can be computed by a Finite state machine can also be computed by a Turing machine , but not vice versa.","title":"Models"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/#categories","text":"There are many models of computation, differing in the set of admissible operations and their computations cost. They fall into the following broad categories: Abstract machine and models equivalent to it (e.g. lambda calculus is equivalent to the Turing machine ) - used in proofs of computability and upper bounds on computational complexity of algorithms . Decision tree models - used in proofs of lower bounds on computational complexity of algorithmic problems.","title":"Categories"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/","text":"Lambda calculus \"lambda calculus\"\u7684\u4e2d\u6587\u610f\u601d\u662f\"lambda\u7b97\u5b50\"\u3002 wikipedia Lambda calculus cnblogs lambda calculus\u5165\u95e8 **lambda\u7b97\u5b50**\u662f\u4e00\u5207**\u51fd\u6570\u5f0f\u8bed\u8a00**\u7684\u57fa\u7840\uff0c\u660e\u767d**lambda\u7b97\u5b50**\u5bf9\u4e8e\u638c\u63e1**\u51fd\u6570\u5f0f\u8bed\u8a00**\u6709\u7740\u8bb8\u591a\u597d\u5904\u3002 1 \u57fa\u7840 \u03bb \u7b97\u5b50\u662f\u51fd\u6570\u5f0f\u7f16\u7a0b\u7684\u7406\u8bba\u57fa\u7840\uff0c\u662f\u56fe\u7075\u673a\u5916\u7684\u53e6\u4e00\u79cd\u8ba1\u7b97\u6a21\u578b\u3002 \u5b83\u5341\u5206\u7b80\u6d01\uff0c\u53ea\u6709\u4e09\u6761\u4ea7\u751f\u89c4\u5219\uff0c\u5374\u53ef\u4ee5\u8868\u8fbe\u4e00\u5207\u53ef\u8ba1\u7b97\u7684\u51fd\u6570\u3002 \u03bb \u7b97\u5b50\u7684\u6838\u5fc3\u6982\u5ff5\u662f\u8868\u8fbe\u5f0fexpression\u3002\u03bb \u7b97\u5b50\u7684\u4ea7\u751f\u89c4\u5219\u5982\u4e0b\uff1a <expression> ::= <name>|<function>|<application> <function> ::= \u03bb <name>.<expression> <application> ::= <expression><expression> \u9700\u6ce8\u610f\uff1a \u5176\u4e2d\u7684 <name> \u4ee3\u8868\u6807\u8bc6\u7b26\u3002 \u8868\u8fbe\u5f0f\u9ed8\u8ba4\u5de6\u7ed3\u5408\uff08\u5373\u4ece\u6700\u5de6\u5f00\u59cb\u8d77\u4f5c\u7528\uff09\uff0c\u5982\u679c\u9700\u8981\u6539\u53d8\u987a\u5e8f\u53ef\u4ee5\u52a0\u62ec\u53f7\u3002 zhihu lambda calculus \u6700\u8ba9\u4f60\u9707\u64bc\u4eba\u5fc3\u7684\u662f\u4ec0\u4e48\uff1f \u4f5c\u8005\uff1a\u533f\u540d\u7528\u6237 \u94fe\u63a5\uff1a https://www.zhihu.com/question/27522346/answer/386070874 \u6765\u6e90\uff1a\u77e5\u4e4e \u8457\u4f5c\u6743\u5f52\u4f5c\u8005\u6240\u6709\u3002\u5546\u4e1a\u8f6c\u8f7d\u8bf7\u8054\u7cfb\u4f5c\u8005\u83b7\u5f97\u6388\u6743\uff0c\u975e\u5546\u4e1a\u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904\u3002 \u8bf4\u8bf4\u81ea\u5df1\u6bd4\u4e00\u5207\u7686\u51fd\u6570\u66f4\u8fdb\u4e00\u6b65\u7684\u7406\u89e3\u5427\uff1a\u51fd\u6570\u4e0e\u503c\u7684\u7edf\u4e00\u3002 1.\u4e00\u5207\u7686\u51fd\u6570\u3002\u6240\u4ee5lambda calculus\u662f\u53ea\u6709\u4e00\u6761\u5f52\u7ea6\u89c4\u5219\u7684\u6700\u7b80\u8fd0\u7b97\u6a21\u578b\u3002 2.\u4e00\u5207\u7686\u503c\u3002\u6240\u4ee5lambda calculus\u5bf9\u51fd\u6570\u4e0e\u503c\u7684\u64cd\u4f5c\u662f\u5b8c\u5168\u4e00\u81f4\u7684\u3002\u8fd9\u79cd\u8ba1\u7b97\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u9636\u51fd\u6570\uff0c\u6210\u4e3a\u4e86\u51fd\u6570\u5f0f\u7f16\u7a0b\u7684\u57fa\u7840\u3002 \u76f8\u6bd4\u4e4b\u4e0b\uff0c\u56fe\u7075\u673a\u7684tape\u548ctransition function\u4e24\u4e2a\u6982\u5ff5\u8fd8\u662f\u5206\u79bb\u7684\u3002 \u53e6\u5916\u4e2a\u4eba\u7528\u8ba1\u7b97\u6a21\u5f0f\u7684\u62bd\u8c61\u8fc7\u7a0b\u53ef\u4ee5\u7c7b\u6bd4\u8ba1\u7b97\u673a\u7684\u53d1\u5c55\u3002 \u8d77\u521d\u6211\u4eec\u6709\u7b97\u76d8\uff0c\u5b83\u53ef\u4ee5\u7528\u6765\u8bb0\u5f55\u7279\u5b9a\u6570\u636e\uff0c\u4f46\u6211\u4eec\u4f9d\u65e7\u9700\u8981\u8111\u5185\u7684\u65b9\u7a0b\u6765\u8fdb\u884c\u8fd0\u7b97\u3002 \u540e\u6765\u6211\u4eec\u6709\u4e86\u5dee\u5206\u673a\u7b49\u7b49special purpose computer\uff0c\u5b83\u4eec\u672c\u8eab\u76f8\u5f53\u4e8e\u4e00\u4e2a\u4e0e\u5177\u4f53\u6570\u503c\u65e0\u5173\u7684\u65b9\u7a0b\uff0c\u5373\u4e00\u9636\u65b9\u7a0b\u3002\u5b83\u53ef\u4ee5\u64cd\u4f5c\u6570\u503c\uff0c\u4f46\u4e0d\u53ef\u4ee5\u64cd\u4f5c\u5176\u4ed6\u51fd\u6570\u3002 \u6700\u540e\u6211\u4eec\u6709\u4e86\u51af\u00b7\u8bfa\u4f9d\u66fc\uff0c\u5728\u4ed6\u7684general purpose computer\u4e2d\uff0c\u503c\u4e0e\u51fd\u6570\u5728\u5e95\u5c42\u7684\u5b9e\u73b0\u662f\u7edf\u4e00\u7684\u3002\u786c\u4ef6\u5e76\u4e0d\u53bb\u5206\u8fa8\u4e00\u6bb500101001\u662f\u5177\u4f53\u7684\u503c\u8fd8\u662f\u51fd\u6570\u3002\u51af\u00b7\u8bfa\u4f9d\u66fc\u7684\u673a\u5668\u76f8\u5f53\u4e8e\u4e00\u4e2a\u9ad8\u9636\u51fd\u6570\u89e3\u91ca\u5668\uff0c\u9ad8\u9636\u51fd\u6570\uff08\u53ef\u4ee5\u64cd\u4f5c\u51fd\u6570\u7684\u51fd\u6570\uff09\u62bd\u8c61\u7684\u5177\u4f53\u5b9e\u73b0\uff0c\u5176\u5bf9\u5f85\u503c\u4e0e\u51fd\u6570\u7684\u65b9\u5f0f\u4e0elambda calculus\u89e3\u91ca\u5668\u662f\u4e00\u81f4\u7684\u3002\u6240\u4ee5\u6211\u4eec\u5c31\u77e5\u9053\u4e86\uff0c\u51af\u8bfa\u4f9d\u66fc\u7684general purpose computer\u7ed3\u6784\u4e3a\u4ec0\u4e48\u4e00\u5b9a\u4f1a\u88ab\u8bbe\u8ba1\u6210\u8fd9\u6837\u3002 csdn \u795e\u5947\u7684\u03bb-calculus NOTE: \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d \u5148\u4eba\u9057\u98ce \u03bb-calculus(\u82f1\u6587\u505alambda calculus)\u4e8e1930s\u7531\u963f\u9686\u4f50\u00b7\u90b1\u5947\u6240\u5f15\u5165\uff0c\u5f7c\u65f6\u5728\u6570\u4f4d\u5929\u624d\u601d\u60f3\u5bb6\u7684\u63a8\u52a8\u4e0b\u6570\u7406\u903b\u8f91\u5b66\u79d1\u5df2\u521d\u89c1\u6210\u884c\uff0c\u5176\u4e2d\u5173\u4e8e\u8ba1\u7b97\u673a\u6570\u7406\u903b\u8f91\u7684\u5c24\u4ee5\u6208\u7279\u5f17\u91cc\u5fb7\u00b7\u83b1\u5e03\u5c3c\u5179\u3001\u4e54\u6cbb\u00b7\u5e03\u5c14\u3001\u683c\u5965\u5c14\u683c\u00b7\u5eb7\u6258\u5c14\u3001\u5927\u536b\u00b7\u5e0c\u5c14\u4f2f\u7279\u3001\u5e93\u5c14\u7279\u00b7\u54e5\u5fb7\u5c14\u3001\u827e\u4f26\u00b7\u56fe\u7075\u3001\u963f\u9686\u4f50\u00b7\u90b1\u5947\u7b49\u4eba\u5907\u53d7\u63a8\u5d07\uff0c\u7d27\u968f\u5176\u540e\u7684\u51af\u00b7\u8bfa\u4f9d\u66fc\u3001\u7ea6\u7ff0\u00b7\u9ea6\u5361\u9521\u3001\u4e39\u5c3c\u5c14\u00b7\u798f\u745e\u5fb7\u66fc\u7b49\u4eba\u4ea6\u4ee4\u543e\u8f88\u9876\u793c\u819c\u62dc\uff0c\u4ed6\u4eec\u7684\u6210\u679c\u6bb7\u6cfd\u540e\u4e16\uff0c\u6b4c\u529f\u9882\u5fb7\u3001\u4ed8\u6893\u6587\u4e66\u90fd\u663e\u5f97\u5fae\u4e0d\u8db3\u9053\u3002 \u6f5c\u9f99\u5728\u6e0a \u03bb-calculus\u4e4b\u6240\u4ee5\u6709\u7740\u8bf1\u4eba\u7684\u9b45\u529b\uff0c\u5728\u4e8e\u5b83\u7684\u7b80\u6d01\u548c\u5f3a\u5927\u3002\u5b83\u53ef\u4ee5\u88ab\u79f0\u4e3a\u662f\u6700\u5c0f\u7684\u901a\u7528\u7a0b\u5e8f\u8bbe\u8ba1\u8bed\u8a00\u3002\u5b83\u7b80\u6d01\u5230\u53ea\u5305\u542b\u4e24\u6761\u53d8\u6362\u89c4\u5219:\u53d8\u91cf\u66ff\u6362(\u7b14\u8005\u4e1a\u4f59\u65f6\u95f4\u53c2\u8bd1\u7684\u9b54\u6cd5\u4e66\u7e41\u4f53\u7248\u4e2d1.1.5\u8282\u6709\u5bf9\u6b64\u7684\u8be6\u7ec6\u89e3\u91ca\uff0c\u8d34\u51fa\u94fe\u63a5\u8bf7\u770b\u5b98\u4eec\u4e0d\u541d\u65a7\u6b63: \u7a0b\u5e8f\u61c9\u7528\u7684\u7f6e\u63db\u6a21\u578b \u3001\u53d8\u91cf\u7ed1\u5b9a\u4ee5\u53ca\u4e00\u6761\u51fd\u6570\u5b9a\u4e49\u65b9\u5f0f\u3002\u662f\u7684\uff0c\u6ca1\u4e86\u3002\u5b83\u7684\u5f3a\u5927\u5728\u4e8e\u4efb\u4f55\u4e00\u4e2a\u53ef\u8ba1\u7b97\u51fd\u6570\u90fd\u53ef\u4ee5\u7528\u5b83\u7684\u5f62\u5f0f\u6765\u8fdb\u884c\u8868\u8fbe\u548c\u6c42\u503c\uff0c\u6240\u4ee5\u5b83\u662f\u56fe\u7075\u5b8c\u5907\u7684\u3002\u76f8\u6bd4\u4eca\u5929\u5404\u79cd\u6d41\u884c\u7a0b\u5e8f\u8bed\u8a00\u4e2d\u5177\u6709\u7684\u4ee5\u53ca\u65f6\u4e0d\u65f6\u8981\u65b0\u589e\u7684\u65f6\u9ae6\u8bed\u6cd5\u548c\u6982\u5ff5\uff0c\u57fa\u4e8e\u03bb-calculus\u53d1\u660e\u7684Lisp\u4e4b\u6d41\u51e0\u4e4e\u8ba9\u4eba\u89c9\u5f97\u7279\u522b\u843d\u540e\u4e86(\u5b9e\u9645\u5e76\u4e0d\u662f\u8fd9\u6837\uff0c\u300a\u9ed1\u5ba2\u4e0e\u753b\u5bb6\u300b\u6709\u5bf9Lisp\u7684\u9ad8\u5ea6\u8912\u5956\uff0c\u4ee5\u53caSICP\u4f7f\u7528Scheme\uff0c\u4e39\u5c3c\u5c14\u6240\u8457\u7684\u5404\u79cdScheme\u4e66\u7c4d\u90fd\u662f\u5bf9Lisp\u7684\u9ad8\u5ea6\u80af\u5b9a)(Scheme\u662fLisp\u7684\u4e00\u79cd\u65b9\u8a00)\u3002 \u725b\u5200\u5c0f\u8bd5 \u03bb-calculus\u8868\u8fbe\u5f0f\u7b80\u77ed\u7684\u8bed\u6cd5\u89c4\u5219\u4f7f\u7528BNF\u6807\u8bb0\u5982\u4e0b: <expr> ::= <constant> | <variable> | (<expr> <expr>) | (\u03bb <variable>.<expr>) \u5176\u4e2d <constant> \u53ef\u4ee5\u662f\u8bf8\u59820\u30011\u8fd9\u6837\u7684\u6570\u5b57\uff0c\u6216\u8005\u9884\u5b9a\u4e49\u7684\u51fd\u6570: +\u3001-\u3001*\u7b49\u3002 <variable> \u662fx\u3001y\u7b49\u8fd9\u6837\u7684\u540d\u5b57\u3002 (<expr> <expr>) \u8868\u793a\u51fd\u6570\u8c03\u7528\u3002\u5de6\u8fb9\u7684\u4e3a\u8981\u8c03\u7528\u7684\u51fd\u6570\uff0c\u53f3\u8fb9\u7684\u4e3a\u53c2\u6570\u3002 (\u03bb <variable>.<expr>) \u88ab\u79f0\u4e3alambda\u62bd\u8c61(lambda abstraction)\uff0c\u7528\u4ee5\u5b9a\u4e49\u65b0\u7684\u51fd\u6570\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#lambda#calculus","text":"\"lambda calculus\"\u7684\u4e2d\u6587\u610f\u601d\u662f\"lambda\u7b97\u5b50\"\u3002","title":"Lambda calculus"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#wikipedia#lambda#calculus","text":"","title":"wikipedia Lambda calculus"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#cnblogs#lambda#calculus","text":"**lambda\u7b97\u5b50**\u662f\u4e00\u5207**\u51fd\u6570\u5f0f\u8bed\u8a00**\u7684\u57fa\u7840\uff0c\u660e\u767d**lambda\u7b97\u5b50**\u5bf9\u4e8e\u638c\u63e1**\u51fd\u6570\u5f0f\u8bed\u8a00**\u6709\u7740\u8bb8\u591a\u597d\u5904\u3002","title":"cnblogs lambda calculus\u5165\u95e8"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#1","text":"\u03bb \u7b97\u5b50\u662f\u51fd\u6570\u5f0f\u7f16\u7a0b\u7684\u7406\u8bba\u57fa\u7840\uff0c\u662f\u56fe\u7075\u673a\u5916\u7684\u53e6\u4e00\u79cd\u8ba1\u7b97\u6a21\u578b\u3002 \u5b83\u5341\u5206\u7b80\u6d01\uff0c\u53ea\u6709\u4e09\u6761\u4ea7\u751f\u89c4\u5219\uff0c\u5374\u53ef\u4ee5\u8868\u8fbe\u4e00\u5207\u53ef\u8ba1\u7b97\u7684\u51fd\u6570\u3002 \u03bb \u7b97\u5b50\u7684\u6838\u5fc3\u6982\u5ff5\u662f\u8868\u8fbe\u5f0fexpression\u3002\u03bb \u7b97\u5b50\u7684\u4ea7\u751f\u89c4\u5219\u5982\u4e0b\uff1a <expression> ::= <name>|<function>|<application> <function> ::= \u03bb <name>.<expression> <application> ::= <expression><expression> \u9700\u6ce8\u610f\uff1a \u5176\u4e2d\u7684 <name> \u4ee3\u8868\u6807\u8bc6\u7b26\u3002 \u8868\u8fbe\u5f0f\u9ed8\u8ba4\u5de6\u7ed3\u5408\uff08\u5373\u4ece\u6700\u5de6\u5f00\u59cb\u8d77\u4f5c\u7528\uff09\uff0c\u5982\u679c\u9700\u8981\u6539\u53d8\u987a\u5e8f\u53ef\u4ee5\u52a0\u62ec\u53f7\u3002","title":"1 \u57fa\u7840"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#zhihu#lambda#calculus","text":"\u4f5c\u8005\uff1a\u533f\u540d\u7528\u6237 \u94fe\u63a5\uff1a https://www.zhihu.com/question/27522346/answer/386070874 \u6765\u6e90\uff1a\u77e5\u4e4e \u8457\u4f5c\u6743\u5f52\u4f5c\u8005\u6240\u6709\u3002\u5546\u4e1a\u8f6c\u8f7d\u8bf7\u8054\u7cfb\u4f5c\u8005\u83b7\u5f97\u6388\u6743\uff0c\u975e\u5546\u4e1a\u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904\u3002 \u8bf4\u8bf4\u81ea\u5df1\u6bd4\u4e00\u5207\u7686\u51fd\u6570\u66f4\u8fdb\u4e00\u6b65\u7684\u7406\u89e3\u5427\uff1a\u51fd\u6570\u4e0e\u503c\u7684\u7edf\u4e00\u3002 1.\u4e00\u5207\u7686\u51fd\u6570\u3002\u6240\u4ee5lambda calculus\u662f\u53ea\u6709\u4e00\u6761\u5f52\u7ea6\u89c4\u5219\u7684\u6700\u7b80\u8fd0\u7b97\u6a21\u578b\u3002 2.\u4e00\u5207\u7686\u503c\u3002\u6240\u4ee5lambda calculus\u5bf9\u51fd\u6570\u4e0e\u503c\u7684\u64cd\u4f5c\u662f\u5b8c\u5168\u4e00\u81f4\u7684\u3002\u8fd9\u79cd\u8ba1\u7b97\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u9636\u51fd\u6570\uff0c\u6210\u4e3a\u4e86\u51fd\u6570\u5f0f\u7f16\u7a0b\u7684\u57fa\u7840\u3002 \u76f8\u6bd4\u4e4b\u4e0b\uff0c\u56fe\u7075\u673a\u7684tape\u548ctransition function\u4e24\u4e2a\u6982\u5ff5\u8fd8\u662f\u5206\u79bb\u7684\u3002 \u53e6\u5916\u4e2a\u4eba\u7528\u8ba1\u7b97\u6a21\u5f0f\u7684\u62bd\u8c61\u8fc7\u7a0b\u53ef\u4ee5\u7c7b\u6bd4\u8ba1\u7b97\u673a\u7684\u53d1\u5c55\u3002 \u8d77\u521d\u6211\u4eec\u6709\u7b97\u76d8\uff0c\u5b83\u53ef\u4ee5\u7528\u6765\u8bb0\u5f55\u7279\u5b9a\u6570\u636e\uff0c\u4f46\u6211\u4eec\u4f9d\u65e7\u9700\u8981\u8111\u5185\u7684\u65b9\u7a0b\u6765\u8fdb\u884c\u8fd0\u7b97\u3002 \u540e\u6765\u6211\u4eec\u6709\u4e86\u5dee\u5206\u673a\u7b49\u7b49special purpose computer\uff0c\u5b83\u4eec\u672c\u8eab\u76f8\u5f53\u4e8e\u4e00\u4e2a\u4e0e\u5177\u4f53\u6570\u503c\u65e0\u5173\u7684\u65b9\u7a0b\uff0c\u5373\u4e00\u9636\u65b9\u7a0b\u3002\u5b83\u53ef\u4ee5\u64cd\u4f5c\u6570\u503c\uff0c\u4f46\u4e0d\u53ef\u4ee5\u64cd\u4f5c\u5176\u4ed6\u51fd\u6570\u3002 \u6700\u540e\u6211\u4eec\u6709\u4e86\u51af\u00b7\u8bfa\u4f9d\u66fc\uff0c\u5728\u4ed6\u7684general purpose computer\u4e2d\uff0c\u503c\u4e0e\u51fd\u6570\u5728\u5e95\u5c42\u7684\u5b9e\u73b0\u662f\u7edf\u4e00\u7684\u3002\u786c\u4ef6\u5e76\u4e0d\u53bb\u5206\u8fa8\u4e00\u6bb500101001\u662f\u5177\u4f53\u7684\u503c\u8fd8\u662f\u51fd\u6570\u3002\u51af\u00b7\u8bfa\u4f9d\u66fc\u7684\u673a\u5668\u76f8\u5f53\u4e8e\u4e00\u4e2a\u9ad8\u9636\u51fd\u6570\u89e3\u91ca\u5668\uff0c\u9ad8\u9636\u51fd\u6570\uff08\u53ef\u4ee5\u64cd\u4f5c\u51fd\u6570\u7684\u51fd\u6570\uff09\u62bd\u8c61\u7684\u5177\u4f53\u5b9e\u73b0\uff0c\u5176\u5bf9\u5f85\u503c\u4e0e\u51fd\u6570\u7684\u65b9\u5f0f\u4e0elambda calculus\u89e3\u91ca\u5668\u662f\u4e00\u81f4\u7684\u3002\u6240\u4ee5\u6211\u4eec\u5c31\u77e5\u9053\u4e86\uff0c\u51af\u8bfa\u4f9d\u66fc\u7684general purpose computer\u7ed3\u6784\u4e3a\u4ec0\u4e48\u4e00\u5b9a\u4f1a\u88ab\u8bbe\u8ba1\u6210\u8fd9\u6837\u3002","title":"zhihu lambda calculus \u6700\u8ba9\u4f60\u9707\u64bc\u4eba\u5fc3\u7684\u662f\u4ec0\u4e48\uff1f"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#csdn#-calculus","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d","title":"csdn \u795e\u5947\u7684\u03bb-calculus"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#_1","text":"\u03bb-calculus(\u82f1\u6587\u505alambda calculus)\u4e8e1930s\u7531\u963f\u9686\u4f50\u00b7\u90b1\u5947\u6240\u5f15\u5165\uff0c\u5f7c\u65f6\u5728\u6570\u4f4d\u5929\u624d\u601d\u60f3\u5bb6\u7684\u63a8\u52a8\u4e0b\u6570\u7406\u903b\u8f91\u5b66\u79d1\u5df2\u521d\u89c1\u6210\u884c\uff0c\u5176\u4e2d\u5173\u4e8e\u8ba1\u7b97\u673a\u6570\u7406\u903b\u8f91\u7684\u5c24\u4ee5\u6208\u7279\u5f17\u91cc\u5fb7\u00b7\u83b1\u5e03\u5c3c\u5179\u3001\u4e54\u6cbb\u00b7\u5e03\u5c14\u3001\u683c\u5965\u5c14\u683c\u00b7\u5eb7\u6258\u5c14\u3001\u5927\u536b\u00b7\u5e0c\u5c14\u4f2f\u7279\u3001\u5e93\u5c14\u7279\u00b7\u54e5\u5fb7\u5c14\u3001\u827e\u4f26\u00b7\u56fe\u7075\u3001\u963f\u9686\u4f50\u00b7\u90b1\u5947\u7b49\u4eba\u5907\u53d7\u63a8\u5d07\uff0c\u7d27\u968f\u5176\u540e\u7684\u51af\u00b7\u8bfa\u4f9d\u66fc\u3001\u7ea6\u7ff0\u00b7\u9ea6\u5361\u9521\u3001\u4e39\u5c3c\u5c14\u00b7\u798f\u745e\u5fb7\u66fc\u7b49\u4eba\u4ea6\u4ee4\u543e\u8f88\u9876\u793c\u819c\u62dc\uff0c\u4ed6\u4eec\u7684\u6210\u679c\u6bb7\u6cfd\u540e\u4e16\uff0c\u6b4c\u529f\u9882\u5fb7\u3001\u4ed8\u6893\u6587\u4e66\u90fd\u663e\u5f97\u5fae\u4e0d\u8db3\u9053\u3002","title":"\u5148\u4eba\u9057\u98ce"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#_2","text":"\u03bb-calculus\u4e4b\u6240\u4ee5\u6709\u7740\u8bf1\u4eba\u7684\u9b45\u529b\uff0c\u5728\u4e8e\u5b83\u7684\u7b80\u6d01\u548c\u5f3a\u5927\u3002\u5b83\u53ef\u4ee5\u88ab\u79f0\u4e3a\u662f\u6700\u5c0f\u7684\u901a\u7528\u7a0b\u5e8f\u8bbe\u8ba1\u8bed\u8a00\u3002\u5b83\u7b80\u6d01\u5230\u53ea\u5305\u542b\u4e24\u6761\u53d8\u6362\u89c4\u5219:\u53d8\u91cf\u66ff\u6362(\u7b14\u8005\u4e1a\u4f59\u65f6\u95f4\u53c2\u8bd1\u7684\u9b54\u6cd5\u4e66\u7e41\u4f53\u7248\u4e2d1.1.5\u8282\u6709\u5bf9\u6b64\u7684\u8be6\u7ec6\u89e3\u91ca\uff0c\u8d34\u51fa\u94fe\u63a5\u8bf7\u770b\u5b98\u4eec\u4e0d\u541d\u65a7\u6b63: \u7a0b\u5e8f\u61c9\u7528\u7684\u7f6e\u63db\u6a21\u578b \u3001\u53d8\u91cf\u7ed1\u5b9a\u4ee5\u53ca\u4e00\u6761\u51fd\u6570\u5b9a\u4e49\u65b9\u5f0f\u3002\u662f\u7684\uff0c\u6ca1\u4e86\u3002\u5b83\u7684\u5f3a\u5927\u5728\u4e8e\u4efb\u4f55\u4e00\u4e2a\u53ef\u8ba1\u7b97\u51fd\u6570\u90fd\u53ef\u4ee5\u7528\u5b83\u7684\u5f62\u5f0f\u6765\u8fdb\u884c\u8868\u8fbe\u548c\u6c42\u503c\uff0c\u6240\u4ee5\u5b83\u662f\u56fe\u7075\u5b8c\u5907\u7684\u3002\u76f8\u6bd4\u4eca\u5929\u5404\u79cd\u6d41\u884c\u7a0b\u5e8f\u8bed\u8a00\u4e2d\u5177\u6709\u7684\u4ee5\u53ca\u65f6\u4e0d\u65f6\u8981\u65b0\u589e\u7684\u65f6\u9ae6\u8bed\u6cd5\u548c\u6982\u5ff5\uff0c\u57fa\u4e8e\u03bb-calculus\u53d1\u660e\u7684Lisp\u4e4b\u6d41\u51e0\u4e4e\u8ba9\u4eba\u89c9\u5f97\u7279\u522b\u843d\u540e\u4e86(\u5b9e\u9645\u5e76\u4e0d\u662f\u8fd9\u6837\uff0c\u300a\u9ed1\u5ba2\u4e0e\u753b\u5bb6\u300b\u6709\u5bf9Lisp\u7684\u9ad8\u5ea6\u8912\u5956\uff0c\u4ee5\u53caSICP\u4f7f\u7528Scheme\uff0c\u4e39\u5c3c\u5c14\u6240\u8457\u7684\u5404\u79cdScheme\u4e66\u7c4d\u90fd\u662f\u5bf9Lisp\u7684\u9ad8\u5ea6\u80af\u5b9a)(Scheme\u662fLisp\u7684\u4e00\u79cd\u65b9\u8a00)\u3002","title":"\u6f5c\u9f99\u5728\u6e0a"},{"location":"Relation-structure-computation/Computation/Theory-of-computation/Model-of-computation/Lambda-calculus/#_3","text":"\u03bb-calculus\u8868\u8fbe\u5f0f\u7b80\u77ed\u7684\u8bed\u6cd5\u89c4\u5219\u4f7f\u7528BNF\u6807\u8bb0\u5982\u4e0b: <expr> ::= <constant> | <variable> | (<expr> <expr>) | (\u03bb <variable>.<expr>) \u5176\u4e2d <constant> \u53ef\u4ee5\u662f\u8bf8\u59820\u30011\u8fd9\u6837\u7684\u6570\u5b57\uff0c\u6216\u8005\u9884\u5b9a\u4e49\u7684\u51fd\u6570: +\u3001-\u3001*\u7b49\u3002 <variable> \u662fx\u3001y\u7b49\u8fd9\u6837\u7684\u540d\u5b57\u3002 (<expr> <expr>) \u8868\u793a\u51fd\u6570\u8c03\u7528\u3002\u5de6\u8fb9\u7684\u4e3a\u8981\u8c03\u7528\u7684\u51fd\u6570\uff0c\u53f3\u8fb9\u7684\u4e3a\u53c2\u6570\u3002 (\u03bb <variable>.<expr>) \u88ab\u79f0\u4e3alambda\u62bd\u8c61(lambda abstraction)\uff0c\u7528\u4ee5\u5b9a\u4e49\u65b0\u7684\u51fd\u6570\u3002","title":"\u725b\u5200\u5c0f\u8bd5"},{"location":"Relation-structure-computation/Model/","text":"\u5173\u4e8e\u672c\u7ae0 \u6309\u7167 Relation-structure-computation\\index.md \u4e2d\u603b\u7ed3\u7684\"\" \u57fa\u4e8erelation\u6765\u5efa\u7acbmodel \"\u601d\u60f3\uff0c\u4f7f\u7528 Relation-structure-computation\\Relation \u7ae0\u8282\u4e2d\u63cf\u8ff0\u7684relation\u7406\u8bba\u6765\u8fdb\u884c\u5206\u6790\uff0c\u5bf9\u5982\u4e0b\u95ee\u9898\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u5206\u6790: 1) \u5728 Relation-structure-computation/index \u7ae0\u8282\u63d0\u51fa\u7684: relation\u51b3\u5b9astructure 2) \u5728 Structure/Structure/index \u7ae0\u8282\u63d0\u51fa\u7684\uff1a Relation\u7684\u54ea\u4e9b\u7279\u6027\u51b3\u5b9a\u4e86structure\uff0c\u6216\u8005\u8bf4\uff1a\u4ec0\u4e48\u6837\u7684relation\u53ef\u4ee5\u5f62\u6210\u4ec0\u4e48\u6837\u7684\u7ed3\u6784 \u672c\u7ae0\u6240\u63cf\u8ff0\u7684\u4e3b\u8981\u662f**abstract structure**\u3002 \u5e38\u89c1relation\u3001structure\u3001algorithm \u672c\u8282\u4ee5computer science\u4e2d\uff0c\u975e\u5e38\u5178\u578b\u7684relation\u4ee5\u53ca\u5b83\u7684structure\u3001algorithm\u4f5c\u4e3a\u4e3b\u8981\u5185\u5bb9\u3002 \u4e0b\u9762\u662f\u6211\u4eec\u5206\u6790\u7684\u89d2\u5ea6: 1) \u5173\u7cfb\u7684\u6027\u8d28 2) entity\u53c2\u4e0e\u8fd9\u79cd\u5173\u7cfb\u7684cardinality \uff08\u57fa\u6570\uff09 3) \u5173\u7cfb\u5f62\u6210\u7684structure\u7684\u5f62\u72b6 Relation Structure Cardinality \u8bf4\u660e Hierarchy relation graph N:N \u6839\u636e\u5f62\u72b6\u6765\u5bf9\u8fd9\u79cdrelation\u547d\u540d\u7684 Nesting relation tree 1:N Dependency relation graph \u5173\u4e8egraph\uff0c\u53c2\u89c1 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Graph \u7ae0\u8282\uff1b \u5173\u4e8etree\uff0c\u53c2\u89c1 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Tree \u7ae0\u8282\uff1b Chain\u3001tree\u3001hierarchy\u3001graph \u5982\u679c\u4ece\u201c\u4f7f\u7528graph\u6765\u8868\u793arelation\u201d\u7684\u89d2\u5ea6\u6765\u601d\u8003\uff0cchain\u3001hierarchy\u3001tree\u90fd\u662f\u4e00\u79cdgraph\uff0c\u6216\u8005\u8bf4\u5b83\u4eec\u90fd\u662fgraph\u7684\u9000\u5316\u3002 \u5b83\u4eec\u7684relation\u6709\u7740\u4e0d\u540c\u7684\u7279\u6027\uff0c\u8fd9\u662f\u9700\u8981\u8fdb\u884c\u6df1\u5165\u6316\u6398\u7684\u3002 Stream Stream\u662f\u4e00\u79cd\u975e\u5e38\u5f3a\u5927\u7684\u62bd\u8c61\u7ed3\u6784\uff0c\u5728computer science\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u672c\u7ae0\u5c06\u5bf9\u5b83\u8fdb\u884c\u4e13\u95e8\u7684\u603b\u7ed3\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Model/#_1","text":"\u6309\u7167 Relation-structure-computation\\index.md \u4e2d\u603b\u7ed3\u7684\"\" \u57fa\u4e8erelation\u6765\u5efa\u7acbmodel \"\u601d\u60f3\uff0c\u4f7f\u7528 Relation-structure-computation\\Relation \u7ae0\u8282\u4e2d\u63cf\u8ff0\u7684relation\u7406\u8bba\u6765\u8fdb\u884c\u5206\u6790\uff0c\u5bf9\u5982\u4e0b\u95ee\u9898\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u5206\u6790: 1) \u5728 Relation-structure-computation/index \u7ae0\u8282\u63d0\u51fa\u7684: relation\u51b3\u5b9astructure 2) \u5728 Structure/Structure/index \u7ae0\u8282\u63d0\u51fa\u7684\uff1a Relation\u7684\u54ea\u4e9b\u7279\u6027\u51b3\u5b9a\u4e86structure\uff0c\u6216\u8005\u8bf4\uff1a\u4ec0\u4e48\u6837\u7684relation\u53ef\u4ee5\u5f62\u6210\u4ec0\u4e48\u6837\u7684\u7ed3\u6784 \u672c\u7ae0\u6240\u63cf\u8ff0\u7684\u4e3b\u8981\u662f**abstract structure**\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Model/#relationstructurealgorithm","text":"\u672c\u8282\u4ee5computer science\u4e2d\uff0c\u975e\u5e38\u5178\u578b\u7684relation\u4ee5\u53ca\u5b83\u7684structure\u3001algorithm\u4f5c\u4e3a\u4e3b\u8981\u5185\u5bb9\u3002 \u4e0b\u9762\u662f\u6211\u4eec\u5206\u6790\u7684\u89d2\u5ea6: 1) \u5173\u7cfb\u7684\u6027\u8d28 2) entity\u53c2\u4e0e\u8fd9\u79cd\u5173\u7cfb\u7684cardinality \uff08\u57fa\u6570\uff09 3) \u5173\u7cfb\u5f62\u6210\u7684structure\u7684\u5f62\u72b6 Relation Structure Cardinality \u8bf4\u660e Hierarchy relation graph N:N \u6839\u636e\u5f62\u72b6\u6765\u5bf9\u8fd9\u79cdrelation\u547d\u540d\u7684 Nesting relation tree 1:N Dependency relation graph \u5173\u4e8egraph\uff0c\u53c2\u89c1 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Graph \u7ae0\u8282\uff1b \u5173\u4e8etree\uff0c\u53c2\u89c1 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Tree \u7ae0\u8282\uff1b","title":"\u5e38\u89c1relation\u3001structure\u3001algorithm"},{"location":"Relation-structure-computation/Model/#chaintreehierarchygraph","text":"\u5982\u679c\u4ece\u201c\u4f7f\u7528graph\u6765\u8868\u793arelation\u201d\u7684\u89d2\u5ea6\u6765\u601d\u8003\uff0cchain\u3001hierarchy\u3001tree\u90fd\u662f\u4e00\u79cdgraph\uff0c\u6216\u8005\u8bf4\u5b83\u4eec\u90fd\u662fgraph\u7684\u9000\u5316\u3002 \u5b83\u4eec\u7684relation\u6709\u7740\u4e0d\u540c\u7684\u7279\u6027\uff0c\u8fd9\u662f\u9700\u8981\u8fdb\u884c\u6df1\u5165\u6316\u6398\u7684\u3002","title":"Chain\u3001tree\u3001hierarchy\u3001graph"},{"location":"Relation-structure-computation/Model/#stream","text":"Stream\u662f\u4e00\u79cd\u975e\u5e38\u5f3a\u5927\u7684\u62bd\u8c61\u7ed3\u6784\uff0c\u5728computer science\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u672c\u7ae0\u5c06\u5bf9\u5b83\u8fdb\u884c\u4e13\u95e8\u7684\u603b\u7ed3\u3002","title":"Stream"},{"location":"Relation-structure-computation/Model/TODO/","text":"\u9700\u8981\u5c06\u5173\u7cfb\u548c\u5176\u5bf9\u5e94\u7684\u7ed3\u6784\u653e\u5230\u4e00\u8d77\u6765\u8fdb\u884c\u63cf\u8ff0","title":"TODO"},{"location":"Relation-structure-computation/Model/Chain/Chain/","text":"Chain \u5728\u672c\u6587\uff0c\u6211\u4eec\u5c06chain\u5f53\u505a\u4e00\u79cd\u5f62\u72b6\u3002\u672c\u6587\u63a2\u7d22\u6309\u7167\u54ea\u79cd\u5173\u7cfb\u6765\u7ec4\u7ec7\u5143\u7d20\u4f1a\u5f62\u6210chain\u3002\u8fd9\u4e2a\u95ee\u9898\u5728\u7ef4\u57fa\u767e\u79d1 Total order \u4e2d\u5df2\u7ecf\u7ed9\u51fa\u4e86\u7b54\u6848\uff1a A set paired with a total order is called a chain \u5373\u6309\u7167 Total order \u6765\u8fdb\u884c\u7ec4\u7ec7\u7684\u8bdd\uff0c\u5219\u4f1a\u5f62\u6210chain\u3002 \u5728\u7ef4\u57fa\u767e\u79d1 Total order \u7684 Chains \u7ae0\u8282\u6709\u5bf9\u6b64\u7684\u66f4\u591a\u7684\u8ba8\u8bba\u3002 \u4ee5chain\u547d\u540d\u7684\u672f\u8bed \u79d1\u5b66\u9886\u57df\u7684\u547d\u540d\u5176\u5b9e\u4e5f\u9075\u5faa\u8fd9\u4e00\u5b9a\u7684\u89c4\u8303\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u5728\u79d1\u5b66\u9886\u57df\u4f7f\u7528chain\u6765\u547d\u540d\u7684\u4e00\u4e9b\u6982\u5ff5\uff1a Chain (disambiguation) Markov chain Chain rule Hash chain Blockchain \u8fd9\u4e9b\u6240\u8868\u793a\u7684\u6982\u5ff5\uff0c\u5176\u5b9e\u90fd\u5177\u5907\u94fe\u5f0f\u7ed3\u6784\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u4e0b\u9762\u8fd8\u6709\u4ee5\u4e0b\u5177\u5907\u94fe\u5f0f\u7ed3\u6784\u7684\uff1a Linked timestamping \u80fd\u591f\u5f62\u6210chain\u7ed3\u6784\u7684relation\u9700\u8981\u6709\u54ea\u4e9b\u7279\u6027\uff1f TODO: Chain and list List\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u5b9e\u73b0\uff0c\u800cchain\u5219\u504f\u5411\u4e8e\u89c6\u89c9\u7684\u5f62\u6001\u3002 \u7ebf\u6027\u3001\u975e\u7ebf\u6027 Linearity \u5176\u5b9e\u662f\u6839\u636e\u4eba\u7684\u89c6\u89c9\u6765\u5b9a\u4e49\u7684\uff0c\u5305\u62ec\u6211\u4eec\u7684\u5404\u79cd\u6570\u636e\u7ed3\u6784\uff0c\u5b83\u4eec\u80cc\u540e\u7684\u6570\u5b66\u542b\u4e49\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002\u5176\u5b9e\uff0c\u4f7f\u7528\u89c6\u89c9\u4e5f\u80fd\u591f\u5e2e\u52a9\u4eba\u6765\u8fdb\u884c\u7406\u89e3\u7684 \uff0c\u5728\u5b66\u4e60\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e00\u76f4\u90fd\u662f\u5728\u4e0d\u65ad\u5730\u8fdb\u884c\u53ef\u89c6\u5316\u5730\u5b66\u4e60\u3002 \u53c2\u89c1 Chain","title":"Chain"},{"location":"Relation-structure-computation/Model/Chain/Chain/#chain","text":"\u5728\u672c\u6587\uff0c\u6211\u4eec\u5c06chain\u5f53\u505a\u4e00\u79cd\u5f62\u72b6\u3002\u672c\u6587\u63a2\u7d22\u6309\u7167\u54ea\u79cd\u5173\u7cfb\u6765\u7ec4\u7ec7\u5143\u7d20\u4f1a\u5f62\u6210chain\u3002\u8fd9\u4e2a\u95ee\u9898\u5728\u7ef4\u57fa\u767e\u79d1 Total order \u4e2d\u5df2\u7ecf\u7ed9\u51fa\u4e86\u7b54\u6848\uff1a A set paired with a total order is called a chain \u5373\u6309\u7167 Total order \u6765\u8fdb\u884c\u7ec4\u7ec7\u7684\u8bdd\uff0c\u5219\u4f1a\u5f62\u6210chain\u3002 \u5728\u7ef4\u57fa\u767e\u79d1 Total order \u7684 Chains \u7ae0\u8282\u6709\u5bf9\u6b64\u7684\u66f4\u591a\u7684\u8ba8\u8bba\u3002","title":"Chain"},{"location":"Relation-structure-computation/Model/Chain/Chain/#chain_1","text":"\u79d1\u5b66\u9886\u57df\u7684\u547d\u540d\u5176\u5b9e\u4e5f\u9075\u5faa\u8fd9\u4e00\u5b9a\u7684\u89c4\u8303\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u5728\u79d1\u5b66\u9886\u57df\u4f7f\u7528chain\u6765\u547d\u540d\u7684\u4e00\u4e9b\u6982\u5ff5\uff1a Chain (disambiguation) Markov chain Chain rule Hash chain Blockchain \u8fd9\u4e9b\u6240\u8868\u793a\u7684\u6982\u5ff5\uff0c\u5176\u5b9e\u90fd\u5177\u5907\u94fe\u5f0f\u7ed3\u6784\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u4e0b\u9762\u8fd8\u6709\u4ee5\u4e0b\u5177\u5907\u94fe\u5f0f\u7ed3\u6784\u7684\uff1a Linked timestamping","title":"\u4ee5chain\u547d\u540d\u7684\u672f\u8bed"},{"location":"Relation-structure-computation/Model/Chain/Chain/#chainrelation","text":"TODO:","title":"\u80fd\u591f\u5f62\u6210chain\u7ed3\u6784\u7684relation\u9700\u8981\u6709\u54ea\u4e9b\u7279\u6027\uff1f"},{"location":"Relation-structure-computation/Model/Chain/Chain/#chain#and#list","text":"List\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u5b9e\u73b0\uff0c\u800cchain\u5219\u504f\u5411\u4e8e\u89c6\u89c9\u7684\u5f62\u6001\u3002","title":"Chain and list"},{"location":"Relation-structure-computation/Model/Chain/Chain/#_1","text":"Linearity \u5176\u5b9e\u662f\u6839\u636e\u4eba\u7684\u89c6\u89c9\u6765\u5b9a\u4e49\u7684\uff0c\u5305\u62ec\u6211\u4eec\u7684\u5404\u79cd\u6570\u636e\u7ed3\u6784\uff0c\u5b83\u4eec\u80cc\u540e\u7684\u6570\u5b66\u542b\u4e49\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002\u5176\u5b9e\uff0c\u4f7f\u7528\u89c6\u89c9\u4e5f\u80fd\u591f\u5e2e\u52a9\u4eba\u6765\u8fdb\u884c\u7406\u89e3\u7684 \uff0c\u5728\u5b66\u4e60\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e00\u76f4\u90fd\u662f\u5728\u4e0d\u65ad\u5730\u8fdb\u884c\u53ef\u89c6\u5316\u5730\u5b66\u4e60\u3002","title":"\u7ebf\u6027\u3001\u975e\u7ebf\u6027"},{"location":"Relation-structure-computation/Model/Chain/Chain/#_2","text":"Chain","title":"\u53c2\u89c1"},{"location":"Relation-structure-computation/Model/Chain/VS-chain-VS-hierarchy/","text":"Chain VS hierarchy \u7ecf\u8fc7\u524d\u9762\u7ae0\u8282\u7684\u5206\u6790\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86chain\u5bf9\u5e94\u7684\u662ftotal order\uff0chierarchy\u5bf9\u5e94\u7684\u662fpartial order\uff0c\u8fd9\u5c31\u662f\u4e24\u8005\u672c\u8d28\u7684\u5dee\u5f02\u6240\u5728\u3002 \u5176\u5b9e\uff0c\u5982\u679c\u5c06hierarchy\u7684\u540c\u4e00\u5c42\u7684\u6240\u6709\u8282\u70b9\u5408\u5e76\u4e3a\u4e00\u4e2a\u8282\u70b9\uff0c\u5219hierarchy\u5c31\u8f6c\u6362\u6210\u4e86chain\u3002\u8fd9\u8bf4\u660ehierarchy\u662f\u5177\u5907\u4e00\u5b9a\u7684\u7ebf\u6027\u7279\u5f81\u7684\u3002","title":"VS-chain-VS-hierarchy"},{"location":"Relation-structure-computation/Model/Chain/VS-chain-VS-hierarchy/#chain#vs#hierarchy","text":"\u7ecf\u8fc7\u524d\u9762\u7ae0\u8282\u7684\u5206\u6790\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86chain\u5bf9\u5e94\u7684\u662ftotal order\uff0chierarchy\u5bf9\u5e94\u7684\u662fpartial order\uff0c\u8fd9\u5c31\u662f\u4e24\u8005\u672c\u8d28\u7684\u5dee\u5f02\u6240\u5728\u3002 \u5176\u5b9e\uff0c\u5982\u679c\u5c06hierarchy\u7684\u540c\u4e00\u5c42\u7684\u6240\u6709\u8282\u70b9\u5408\u5e76\u4e3a\u4e00\u4e2a\u8282\u70b9\uff0c\u5219hierarchy\u5c31\u8f6c\u6362\u6210\u4e86chain\u3002\u8fd9\u8bf4\u660ehierarchy\u662f\u5177\u5907\u4e00\u5b9a\u7684\u7ebf\u6027\u7279\u5f81\u7684\u3002","title":"Chain VS hierarchy"},{"location":"Relation-structure-computation/Model/Circular/","text":"Circular \u5173\u4e8erelation\uff0c\u4e00\u4e2a\u6bd4\u8f83\u6709\u8da3\u7684\u95ee\u9898\u5c31\u662f\uff1acircular\u3002\u6211\u4eec\u8ba8\u8bba\u4e24\u79cdcircular\uff1a \u540c\u4e00\u4e2arelation\u5185\u5f62\u6210circular \u591a\u4e2arelation\u95f4\u5f62\u6210circular \u540c\u4e00\u4e2arelation\u5185\u5f62\u6210circular \u6cbf\u7740relation\u4e0d\u505c\u5730\u8fdb\u884crewrite\uff08\u6216\u8005\u8bf4 \u63a8\u5bfc\uff0c\u53c2\u89c1\u524d\u9762\u76f8\u5e94\u7684\u7ae0\u8282\uff09\uff0c\u901a\u8fc7\u6700\u7ec8\u5230\u8fbe\u4e86\u6e90\u70b9\uff0c\u5219\u5f62\u6210\u4e86circular\u3002\u5f53\u4f7f\u7528graph\u6765\u8868\u793arelation\u65f6\uff0c\u8fd9\u79cd\u73b0\u8c61\u662f\u975e\u5e38\u4efb\u610f\u7406\u89e3\u7684\u3002 \u591a\u4e2arelation\u95f4\u5f62\u6210circulars \u5728 Book-Discrete-Mathematics-and-Its-Applications\\Chpater-9-Relations\\Supplementary Exercises \u4e2d\u7ed9\u51fa\u4e86circular\u7684\u63cf\u8ff0\uff1a Arelation R is called circular if aRb and bRc imply that cRa . Show that R is reflexive and circular if and only if it is an equivalence relation.","title":"Introduction"},{"location":"Relation-structure-computation/Model/Circular/#circular","text":"\u5173\u4e8erelation\uff0c\u4e00\u4e2a\u6bd4\u8f83\u6709\u8da3\u7684\u95ee\u9898\u5c31\u662f\uff1acircular\u3002\u6211\u4eec\u8ba8\u8bba\u4e24\u79cdcircular\uff1a \u540c\u4e00\u4e2arelation\u5185\u5f62\u6210circular \u591a\u4e2arelation\u95f4\u5f62\u6210circular","title":"Circular"},{"location":"Relation-structure-computation/Model/Circular/#relationcircular","text":"\u6cbf\u7740relation\u4e0d\u505c\u5730\u8fdb\u884crewrite\uff08\u6216\u8005\u8bf4 \u63a8\u5bfc\uff0c\u53c2\u89c1\u524d\u9762\u76f8\u5e94\u7684\u7ae0\u8282\uff09\uff0c\u901a\u8fc7\u6700\u7ec8\u5230\u8fbe\u4e86\u6e90\u70b9\uff0c\u5219\u5f62\u6210\u4e86circular\u3002\u5f53\u4f7f\u7528graph\u6765\u8868\u793arelation\u65f6\uff0c\u8fd9\u79cd\u73b0\u8c61\u662f\u975e\u5e38\u4efb\u610f\u7406\u89e3\u7684\u3002","title":"\u540c\u4e00\u4e2arelation\u5185\u5f62\u6210circular"},{"location":"Relation-structure-computation/Model/Circular/#relationcirculars","text":"\u5728 Book-Discrete-Mathematics-and-Its-Applications\\Chpater-9-Relations\\Supplementary Exercises \u4e2d\u7ed9\u51fa\u4e86circular\u7684\u63cf\u8ff0\uff1a Arelation R is called circular if aRb and bRc imply that cRa . Show that R is reflexive and circular if and only if it is an equivalence relation.","title":"\u591a\u4e2arelation\u95f4\u5f62\u6210circulars"},{"location":"Relation-structure-computation/Model/Dependency-relation-model/Dependency-relation-model/","text":"Dependency relation model Example makefile makefile\u5c31\u662f\u5178\u578b\u7684\u63cf\u8ff0dependency relation\uff0c\u5b83\u6240\u63cf\u8ff0\u7684\u6700\u7ec8\u6784\u6210\u4e86\u4e00\u4e2adependency graph\u3002 Circular \u5bf9\u4e8edependency relation\uff0c\u6700\u6700\u9700\u8981\u907f\u514d\u7684\u662f\u76f8\u4e92\u4f9d\u8d56\uff0c\u4ece Relation-structure-computation\\Relation\\Relation\\Relation.md \u4e2d\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u8fd9\u662f\u5178\u578b\u7684\u201c\u540c\u4e00\u4e2arelation\u5185\u5f62\u6210circular\u201d\u3002 Example dead lock\uff0cAPUE\u4e2d\u4ecb\u7ecd\u4e86\u3002 c\u5934\u6587\u4ef6\u4e92\u5305\u542b\u3001Python\u4e92\u5bfc\u5165 circular reference\uff08 Reference counting \uff09 ...... \u4e0e\u6b64\u76f8\u5173\u7684\u6587\u7ae0\uff1a Acyclic dependencies principle Detection algorithm \u5bf9\u4e8edependency relation\uff0c\u6700\u6700\u9700\u8981\u907f\u514d\u7684\u662f\u76f8\u4e92\u4f9d\u8d56\uff0c\u90a3\u5982\u4f55\u6765\u5224\u5b9a\u662f\u5426\u5b58\u5728\u76f8\u4e92\u4f9d\u8d56\u7684\uff1f\u663e\u7136\uff0c\u76f8\u4e92\u4f9d\u8d56\u5c31\u662f\u65e0\u6cd5\u62cd\u5e8f\u7684\u5178\u578b\u60c5\u51b5\uff0c\u8fd9\u5c31\u9700\u8981\u4f7f\u7528graph theory\u4e2d\u7684topological sorting\u3002 wikipedia Topological sorting \u5728\u9f99\u4e66\u7684chapter 5.2 Evaluation Orders for SDD's\u4e2d\u6709\u5173\u4e8e\u6b64\u7684\u8ba8\u8bba\u3002","title":"Dependency-relation-model"},{"location":"Relation-structure-computation/Model/Dependency-relation-model/Dependency-relation-model/#dependency#relation#model","text":"","title":"Dependency relation model"},{"location":"Relation-structure-computation/Model/Dependency-relation-model/Dependency-relation-model/#example","text":"","title":"Example"},{"location":"Relation-structure-computation/Model/Dependency-relation-model/Dependency-relation-model/#makefile","text":"makefile\u5c31\u662f\u5178\u578b\u7684\u63cf\u8ff0dependency relation\uff0c\u5b83\u6240\u63cf\u8ff0\u7684\u6700\u7ec8\u6784\u6210\u4e86\u4e00\u4e2adependency graph\u3002","title":"makefile"},{"location":"Relation-structure-computation/Model/Dependency-relation-model/Dependency-relation-model/#circular","text":"\u5bf9\u4e8edependency relation\uff0c\u6700\u6700\u9700\u8981\u907f\u514d\u7684\u662f\u76f8\u4e92\u4f9d\u8d56\uff0c\u4ece Relation-structure-computation\\Relation\\Relation\\Relation.md \u4e2d\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u8fd9\u662f\u5178\u578b\u7684\u201c\u540c\u4e00\u4e2arelation\u5185\u5f62\u6210circular\u201d\u3002","title":"Circular"},{"location":"Relation-structure-computation/Model/Dependency-relation-model/Dependency-relation-model/#example_1","text":"dead lock\uff0cAPUE\u4e2d\u4ecb\u7ecd\u4e86\u3002 c\u5934\u6587\u4ef6\u4e92\u5305\u542b\u3001Python\u4e92\u5bfc\u5165 circular reference\uff08 Reference counting \uff09 ...... \u4e0e\u6b64\u76f8\u5173\u7684\u6587\u7ae0\uff1a Acyclic dependencies principle","title":"Example"},{"location":"Relation-structure-computation/Model/Dependency-relation-model/Dependency-relation-model/#detection#algorithm","text":"\u5bf9\u4e8edependency relation\uff0c\u6700\u6700\u9700\u8981\u907f\u514d\u7684\u662f\u76f8\u4e92\u4f9d\u8d56\uff0c\u90a3\u5982\u4f55\u6765\u5224\u5b9a\u662f\u5426\u5b58\u5728\u76f8\u4e92\u4f9d\u8d56\u7684\uff1f\u663e\u7136\uff0c\u76f8\u4e92\u4f9d\u8d56\u5c31\u662f\u65e0\u6cd5\u62cd\u5e8f\u7684\u5178\u578b\u60c5\u51b5\uff0c\u8fd9\u5c31\u9700\u8981\u4f7f\u7528graph theory\u4e2d\u7684topological sorting\u3002","title":"Detection algorithm"},{"location":"Relation-structure-computation/Model/Dependency-relation-model/Dependency-relation-model/#wikipedia#topological#sorting","text":"\u5728\u9f99\u4e66\u7684chapter 5.2 Evaluation Orders for SDD's\u4e2d\u6709\u5173\u4e8e\u6b64\u7684\u8ba8\u8bba\u3002","title":"wikipedia Topological sorting"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/","text":"Hierarchy \u672c\u6587\u52a8\u673a \u5728\u5199\u4f5c\u672c\u6587\u7684\u65f6\u5019\uff0c\u4e3b\u8981\u6709\u5982\u4e0b\u4e24\u4e2a\u52a8\u673a\uff1a \u5199\u4f5c\u52a8\u673a\u4e00 \u5bf9\u6587\u7ae0 Relation-structure-computation\\Structure\\Structure\\Structure.md \u7684\u201c \u7ed3\u6784\u7684\u5f62\u72b6 \u201d\u7ae0\u8282\u4e2d\u7684\u8bdd\u9898\u201c\u6df1\u5165\u5206\u6790\u6211\u4eec\u8089\u773c\u770b\u5230\u7684\u5404\u79cd\u5f62\u72b6\u80cc\u540e\u7684relation\u7684\u6027\u8d28\u201d\u8fdb\u884c\u89e3\u7b54\u3002 \u5199\u4f5c\u52a8\u673a\u4e8c \u672c\u6587\u60f3\u641e\u6e05\u695ahierarchy structure\u548ctree structure\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u56e0\u4e3a\u5728\u6b64\u4e4b\u524d\u6211\u4e00\u76f4\u5c06hierarchy structure\u770b\u505a\u662ftree structure\u7684\u540c\u4e49\u8bcd\uff0c\u5373hierarchy structure\u5c31\u4e00\u5b9a\u662ftree structure\uff0c\u8fd9\u4e2a\u89c2\u5ff5\u662f\u6e90\u81ea\u4e8e Tree structure \u4e2d\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. \u4fc3\u4f7f\u6211\u5bf9\u8fd9\u4e2a\u89c2\u5ff5\u4ea7\u751f\u6000\u7591\u7684\u662f\u5728\u6587\u7ae0 Relation-structure-computation\\Structure\\Structure\\Structure.md \u4e2d\u63d0\u51fa\u7684\u4e00\u4e2a\u95ee\u9898\uff1a \u6309\u7167inheritance\u5173\u7cfb\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u4e0d\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811\uff1b\u5982\u679c\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u56fe \u6309\u7167inheritance\u5173\u7cfb\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u652f\u6301\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u5b83\u4f9d\u7136\u662fhierarchy\u7ed3\u6784\uff0c\u4f46\u662f\u5b83\u4e0d\u80fd\u662ftree\u4e86\uff0c\u56e0\u4e3a\u5b83\u6210\u73af\u4e86\uff08\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1 Discrete Mathematics and Its Applications \u4e2dTree\u7ae0\u8282\uff09\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a |--- Class 2 -------------| | | class 1---| |- class 4 | | |--- Class 3 -------------| \u4e0a\u56fe\u4ece\u5de6\u81f3\u53f3\u8868\u793a\u7ee7\u627f\u5173\u7cfb\uff0c\u4e0a\u56fe\u4e0d\u662f\u4e00\u4e2atree\uff0c\u800c\u662fgraph\u3002\u4f46\u662f\u4e0a\u56fe\u662f\u6ee1\u8db3hierarchy\u7ed3\u6784\u7684\u3002 \u5199\u4f5c\u52a8\u673a\u4e09 \u201chierarchy\u201d\u7ed3\u6784\u662fcomputer science\u4e2d\u975e\u5e38\u5e38\u89c1\u7684\u4e00\u79cd\u7ed3\u6784\u3002 \u7ef4\u57fa\u767e\u79d1\u7684 Hierarchy \u57fa\u672c\u4e0a\u80fd\u591f\u56de\u7b54\u4e0a\u8ff0\u95ee\u9898\uff0c\u6240\u4ee5\u4e00\u4e0b\u6b63\u6587\u90e8\u5206\u662f\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1\u7684 Hierarchy \u3002 \u6b63\u6587 \u5982\u4e0b\u662f\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1\u7684 Hierarchy \u7684\u7b14\u8bb0\u3002 A hierarchy (from the Greek hierarkhia , \"rule of a high priest\", from hierarkhes , \"president of sacred rites\") is an arrangement of items (objects, names, values, categories, etc.) in which the items are represented as being \"above\", \"below\", or \"at the same level as\" one another. A hierarchy can link entities either directly or indirectly, and either vertically or diagonally. The only direct links in a hierarchy, insofar as they are hierarchical, are to one's immediate superior\uff08\u4e0a\u7ea7\uff09 or to one of one's subordinates\uff08\u4e0b\u5c5e\uff09, although a system that is largely hierarchical can also incorporate alternative hierarchies. Hierarchical links can extend \"vertically\" upwards or downwards via multiple links in the same direction, following a path . All parts of the hierarchy that are not linked vertically to one another nevertheless can be \"horizontally\" linked through a path by traveling up the hierarchy to find a common direct or indirect superior, and then down again. This is akin to two co-workers or colleagues \uff08\u540c\u4e00\u5c42\uff09; each reports to a common superior, but they have the same relative amount of authority. Organizational forms exist that are both alternative and complementary to hierarchy. Heterarchy is one such form. \u201chierarchy\u201d\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u5c42\u7ea7\u201d\uff0c\u6211\u4eec\u5e73\u65f6\u5e38\u5e38\u6240\u8bf4\u7684\u201c\u7b49\u7ea7\u201d\u4e0e\u5b83\u7684\u542b\u4e49\u7c7b\u4f3c\uff0c\u5176\u5b9e\u3002Hierarchy\u6982\u5ff5\u6240\u5f3a\u8c03\u7684\u662flevel\u4ee5\u53calevel\u4e4b\u95f4\u7684\u5173\u7cfb\uff08above-below\u5173\u7cfb\u6216superior-subordinates\u5173\u7cfb\uff09\u3002\u53ef\u4ee5\u5c06Hierarchy\u770b\u505a\u662f\u4e00\u79cd\u7ed3\u6784\uff08\u5143\u7d20\u548c\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff09\uff0cHierarchy\u7684\u5173\u7cfb\u4e3aabove-below\u5173\u7cfb\u3001superior-subordinates\u5173\u7cfb\u3002 Hierarchy\u7ed3\u6784\uff0c\u5982\u679c\u4f7f\u7528computer science \u8bed\u8a00\u6765\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u7c7b\u4f3c\u4e8egraph structure\u3002 Hierarchy\u7ed3\u6784\uff0c\u5982\u679c\u4f7f\u7528math \u8bed\u8a00\u6765\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u7c7b\u4f3c\u4e8e partially ordered set \uff0c\u53c2\u89c1\u539f\u6587 Mathematical representation \u7ae0\u8282\u3002 Hierarchy\u7ed3\u6784\u5e76\u6ca1\u6709\u9650\u5236\u4e24\u5c42\u5143\u7d20\u4e4b\u95f4\u7684link\uff0c\u6309\u7167\u4e0a\u8ff0\u7684\u63cf\u8ff0\uff0c\u5b83\u662f\u5141\u8bb8\u67d0\u4e00\u5c42\u4e2d\u7684\u67d0\u4e2a\u5143\u7d20\u540c\u65f6\u6709\u4e24\u4e2asuperior\uff0c\u8fd9\u79cd\u60c5\u51b5\u5c31\u662f\u672c\u6587\u5f00\u5934\u6240\u5217\u4e3e\u7684\u591a\u7ee7\u627f\u3002 Nomenclature \u547d\u540d\u6cd5 \u7a0d\u5fae\u6d4f\u89c8\u4e86\u4e00\u4e0b\uff0c\u539f\u6587\u8fd9\u4e00\u6bb5\u4e2d\u7ed9\u51fa\u7684\u4e00\u4e9b\u672f\u8bed\u662f\u6bd4\u8f83\u597d\u7406\u89e3\u7684\u3002 Informal representation In plain English, a hierarchy can be thought of as a set in which: No element is superior to itself, and One element, the hierarch \uff08\u6559\u4e3b\uff09, is superior to all of the other elements in the set. The first requirement is also interpreted to mean that a hierarchy can have no circular relationships ; the association between two objects is always transitive . The second requirement asserts that a hierarchy must have a leader or root that is common to all of the objects. \u5173\u4e8e\u7b2c\u4e00\u70b9\uff0c\u539f\u6587\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u201ca hierarchy can have no circular relationships \u201d\uff0c\u201c circular relationships \"\u6307\u5411\u7684\u662f\u56fe\u8bba\u4e2d\u7684 circle \uff0c\u4f5c\u8005\u6240\u60f3\u8981\u8868\u8fbe\u7684\u662f\uff0c\u4e00\u4e2ahierarchy\u662f\u4e0d\u80fd\u591f\u5b58\u5728\u73af\u7684\uff0c\u5426\u5219\u65e0\u6cd5\u4e25\u683c\u5730\u5f62\u6210\u4e00\u5c42\u4e00\u5c42\u7684hierarchy\u7ed3\u6784\u3002\u5c31\u7b2c\u4e00\u70b9\u7684\u539f\u8bdd\uff0c\u6211\u89c9\u5f97\u4f7f\u7528relation\u7684\u7406\u8bba\u6765\u7406\u89e3\u7684\u8bdd\uff0c\u5b83\u8868\u793a\u7684\u662f\u8fd9\u4e2a\u5173\u7cfb\u4e0d\u80fd\u591f\u662f\u4e00\u4e2a reflexive relation \uff0c\u5426\u5219\u5c31\u4f1a\u51fa\u73b0\u73af\u800c\u65e0\u6cd5\u5f62\u6210\u4e00\u5c42\u4e00\u5c42\u7684hierarchy\u7ed3\u6784\u3002 Mathematical representation Mathematically, in its most general form, a hierarchy is a partially ordered set or poset . Subtypes \u201csubtype\u201d\u5373\u5b50\u7c7b\uff0c\u6240\u4ee5\u539f\u6587\u7684\u8fd9\u4e00\u8282\u6240\u63cf\u8ff0\u7684\u662f\u7279\u6b8a\u7c7b\u578b\u7684hierarchy\u3002 Nested hierarchy A nested hierarchy or inclusion hierarchy is a hierarchical ordering of nested sets . A nested hierarchy or inclusion hierarchy is a hierarchical ordering of nested sets . The concept of nesting is exemplified in Russian matryoshka dolls \uff08\u4fc4\u7f57\u65af\u5957\u5a03\uff09. Each doll is encompassed by another doll, all the way to the outer doll. The outer doll holds all of the inner dolls, the next outer doll holds all the remaining inner dolls, and so on. Matryoshkas represent a nested hierarchy where each level contains only one object, i.e., there is only one of each size of doll; a generalized nested hierarchy allows for multiple objects within levels but with each object having only one parent at each level . Nested hierarchies are the organizational schemes behind taxonomies \uff08\u5206\u7c7b\u5b66\uff09and systematic classifications. In many programming taxonomies and syntax models (as well as fractals in mathematics), nested hierarchies, including Russian dolls, are also used to illustrate the properties of self-similarity and recursion . Recursion itself is included as a subset of hierarchical programming, and recursive thinking can be synonymous with a form of hierarchical thinking and logic. Nested hierarchy structure\u5728hierarchy structure\u4e0a\u6dfb\u52a0\u7684\u9650\u5236\u662fnested \uff0c\u5b83\u4fdd\u8bc1\u4e86\u201c each object having only one parent at each level \"\uff0c\u663e\u7136\uff0c\u8fd9\u4e2a\u9650\u5236\u5c31\u662fgraph\u4e2d\u4e0d\u518d\u53ef\u80fd\u4ea7\u751f\u73af\u4e86\uff0c\u4e00\u4e2a\u4e0d\u5e26\u73af\u7684\u56fe\u5c31\u662ftree\uff0c\u6240\u4ee5nested hierarchy structure\u662ftree structure\uff0c\u76f8\u6bd4\u4e8ehierarchy\u800c\u8a00\uff0cnesting\u66f4\u52a0\u80fd\u591f\u4f53\u73b0tree\u7684\u7ed3\u6784\u7279\u5f81\u3002 nested hierarchy structure\u662f\u4e00\u79cd\u7279\u6b8a\u7684hierarchy structure\uff0ctree\u662f\u4e00\u79cd\u7279\u6b8a\u7684graph\u3002 \u601d\u8003\uff1a\u600e\u6837\u7684\u5173\u7cfb\u624d\u80fd\u591f\u4ea7\u751fnested hierarchy structure\uff1f\u8fd9\u4e2a\u95ee\u9898\u5728 Tree-structure \u4e2d\u4f1a\u8fdb\u884c\u8be6\u7ec6\u8ba8\u8bba\u3002 \u539f\u6587\u8fd9\u4e00\u8282\u540e\u9762\u7684\u5185\u5bb9pass\u6389\u4e86\u3002 \u8ba8\u8bba \u6309\u7167\u54ea\u79cdrelation\u6765\u7ec4\u7ec7\u5143\u7d20\u4f1a\u5f62\u6210hierarchy\uff1f \u5728\u539f\u6587\u7684 Informal representation \u4e2d\u6709\u975e\u4e25\u683c\u7684\u63cf\u8ff0\u3002\u5176\u5b9e\u7b80\u800c\u8a00\u4e4b\u5c31\u662f\u5143\u7d20\u4e4b\u95f4\u9700\u8981\u5b58\u5728\u7740 partial order \u3002 transitive N:N Hierarchy and data structure Hierarchy structure\u53ef\u4ee5\u4f7f\u7528graph\u6765\u8fdb\u884c\u8868\u793a\u3002 Nested hierarchy structure\u53ef\u4ee5\u662f\u8981\u5f04tree\u6765\u8fdb\u884c\u8868\u793a\u3002 Hierarchy structure and recursion \u7531\u4e8ehierarchy relation\u57fa\u672c\u201ctransitive\u201d\u6027\u8d28\uff0c\u6240\u6709 hierarchical \u7ed3\u6784\u7684\u5f80\u5f80\u5177\u5907recursive\u7684\u7279\u5f81\u3002 \u4ece\u6570\u5b66\u7684\u89d2\u5ea6\u6765\u770b\uff0c Hierarchy structure\u662f\u4e00\u4e2a partially ordered set \uff0c\u6240\u4ee5\u8ba8\u8bbahierarchy structure\u7684\u9012\u5f52\u6027\u6700\u7ec8\u5e94\u8be5\u8fd8\u662f\u5f52\u4e8e\u8ba8\u8bbaposet\u7684\u9012\u5f52\u6027\u3002\u6211\u89c9\u5f97\u662f\u53ef\u4ee5\u4f7f\u7528tree\u7684\u9012\u5f52\u6027\u6765\u601d\u8003poset\u7684\u9012\u5f52\u6027\u7684\uff0c\u4e00\u4e2aposet\u7684subset\u4e5f\u5e94\u8be5\u662f\u4e00\u4e2aposet\u3002\u53e6\u5916\u4e00\u4e2a\u5c31\u662fposet\u5404\u5c42\u4f7f\u7528\u7684\u662f\u540c\u4e00\u4e2arelation\u3002","title":"Hierarchy-relation-model"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#hierarchy","text":"","title":"Hierarchy"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#_1","text":"\u5728\u5199\u4f5c\u672c\u6587\u7684\u65f6\u5019\uff0c\u4e3b\u8981\u6709\u5982\u4e0b\u4e24\u4e2a\u52a8\u673a\uff1a","title":"\u672c\u6587\u52a8\u673a"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#_2","text":"\u5bf9\u6587\u7ae0 Relation-structure-computation\\Structure\\Structure\\Structure.md \u7684\u201c \u7ed3\u6784\u7684\u5f62\u72b6 \u201d\u7ae0\u8282\u4e2d\u7684\u8bdd\u9898\u201c\u6df1\u5165\u5206\u6790\u6211\u4eec\u8089\u773c\u770b\u5230\u7684\u5404\u79cd\u5f62\u72b6\u80cc\u540e\u7684relation\u7684\u6027\u8d28\u201d\u8fdb\u884c\u89e3\u7b54\u3002","title":"\u5199\u4f5c\u52a8\u673a\u4e00"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#_3","text":"\u672c\u6587\u60f3\u641e\u6e05\u695ahierarchy structure\u548ctree structure\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u56e0\u4e3a\u5728\u6b64\u4e4b\u524d\u6211\u4e00\u76f4\u5c06hierarchy structure\u770b\u505a\u662ftree structure\u7684\u540c\u4e49\u8bcd\uff0c\u5373hierarchy structure\u5c31\u4e00\u5b9a\u662ftree structure\uff0c\u8fd9\u4e2a\u89c2\u5ff5\u662f\u6e90\u81ea\u4e8e Tree structure \u4e2d\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. \u4fc3\u4f7f\u6211\u5bf9\u8fd9\u4e2a\u89c2\u5ff5\u4ea7\u751f\u6000\u7591\u7684\u662f\u5728\u6587\u7ae0 Relation-structure-computation\\Structure\\Structure\\Structure.md \u4e2d\u63d0\u51fa\u7684\u4e00\u4e2a\u95ee\u9898\uff1a \u6309\u7167inheritance\u5173\u7cfb\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u4e0d\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811\uff1b\u5982\u679c\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f\u56fe \u6309\u7167inheritance\u5173\u7cfb\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u652f\u6301\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u5b83\u4f9d\u7136\u662fhierarchy\u7ed3\u6784\uff0c\u4f46\u662f\u5b83\u4e0d\u80fd\u662ftree\u4e86\uff0c\u56e0\u4e3a\u5b83\u6210\u73af\u4e86\uff08\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1 Discrete Mathematics and Its Applications \u4e2dTree\u7ae0\u8282\uff09\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a |--- Class 2 -------------| | | class 1---| |- class 4 | | |--- Class 3 -------------| \u4e0a\u56fe\u4ece\u5de6\u81f3\u53f3\u8868\u793a\u7ee7\u627f\u5173\u7cfb\uff0c\u4e0a\u56fe\u4e0d\u662f\u4e00\u4e2atree\uff0c\u800c\u662fgraph\u3002\u4f46\u662f\u4e0a\u56fe\u662f\u6ee1\u8db3hierarchy\u7ed3\u6784\u7684\u3002","title":"\u5199\u4f5c\u52a8\u673a\u4e8c"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#_4","text":"\u201chierarchy\u201d\u7ed3\u6784\u662fcomputer science\u4e2d\u975e\u5e38\u5e38\u89c1\u7684\u4e00\u79cd\u7ed3\u6784\u3002 \u7ef4\u57fa\u767e\u79d1\u7684 Hierarchy \u57fa\u672c\u4e0a\u80fd\u591f\u56de\u7b54\u4e0a\u8ff0\u95ee\u9898\uff0c\u6240\u4ee5\u4e00\u4e0b\u6b63\u6587\u90e8\u5206\u662f\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1\u7684 Hierarchy \u3002","title":"\u5199\u4f5c\u52a8\u673a\u4e09"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#_5","text":"\u5982\u4e0b\u662f\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1\u7684 Hierarchy \u7684\u7b14\u8bb0\u3002 A hierarchy (from the Greek hierarkhia , \"rule of a high priest\", from hierarkhes , \"president of sacred rites\") is an arrangement of items (objects, names, values, categories, etc.) in which the items are represented as being \"above\", \"below\", or \"at the same level as\" one another. A hierarchy can link entities either directly or indirectly, and either vertically or diagonally. The only direct links in a hierarchy, insofar as they are hierarchical, are to one's immediate superior\uff08\u4e0a\u7ea7\uff09 or to one of one's subordinates\uff08\u4e0b\u5c5e\uff09, although a system that is largely hierarchical can also incorporate alternative hierarchies. Hierarchical links can extend \"vertically\" upwards or downwards via multiple links in the same direction, following a path . All parts of the hierarchy that are not linked vertically to one another nevertheless can be \"horizontally\" linked through a path by traveling up the hierarchy to find a common direct or indirect superior, and then down again. This is akin to two co-workers or colleagues \uff08\u540c\u4e00\u5c42\uff09; each reports to a common superior, but they have the same relative amount of authority. Organizational forms exist that are both alternative and complementary to hierarchy. Heterarchy is one such form. \u201chierarchy\u201d\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u5c42\u7ea7\u201d\uff0c\u6211\u4eec\u5e73\u65f6\u5e38\u5e38\u6240\u8bf4\u7684\u201c\u7b49\u7ea7\u201d\u4e0e\u5b83\u7684\u542b\u4e49\u7c7b\u4f3c\uff0c\u5176\u5b9e\u3002Hierarchy\u6982\u5ff5\u6240\u5f3a\u8c03\u7684\u662flevel\u4ee5\u53calevel\u4e4b\u95f4\u7684\u5173\u7cfb\uff08above-below\u5173\u7cfb\u6216superior-subordinates\u5173\u7cfb\uff09\u3002\u53ef\u4ee5\u5c06Hierarchy\u770b\u505a\u662f\u4e00\u79cd\u7ed3\u6784\uff08\u5143\u7d20\u548c\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff09\uff0cHierarchy\u7684\u5173\u7cfb\u4e3aabove-below\u5173\u7cfb\u3001superior-subordinates\u5173\u7cfb\u3002 Hierarchy\u7ed3\u6784\uff0c\u5982\u679c\u4f7f\u7528computer science \u8bed\u8a00\u6765\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u7c7b\u4f3c\u4e8egraph structure\u3002 Hierarchy\u7ed3\u6784\uff0c\u5982\u679c\u4f7f\u7528math \u8bed\u8a00\u6765\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u7c7b\u4f3c\u4e8e partially ordered set \uff0c\u53c2\u89c1\u539f\u6587 Mathematical representation \u7ae0\u8282\u3002 Hierarchy\u7ed3\u6784\u5e76\u6ca1\u6709\u9650\u5236\u4e24\u5c42\u5143\u7d20\u4e4b\u95f4\u7684link\uff0c\u6309\u7167\u4e0a\u8ff0\u7684\u63cf\u8ff0\uff0c\u5b83\u662f\u5141\u8bb8\u67d0\u4e00\u5c42\u4e2d\u7684\u67d0\u4e2a\u5143\u7d20\u540c\u65f6\u6709\u4e24\u4e2asuperior\uff0c\u8fd9\u79cd\u60c5\u51b5\u5c31\u662f\u672c\u6587\u5f00\u5934\u6240\u5217\u4e3e\u7684\u591a\u7ee7\u627f\u3002","title":"\u6b63\u6587"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#nomenclature","text":"\u7a0d\u5fae\u6d4f\u89c8\u4e86\u4e00\u4e0b\uff0c\u539f\u6587\u8fd9\u4e00\u6bb5\u4e2d\u7ed9\u51fa\u7684\u4e00\u4e9b\u672f\u8bed\u662f\u6bd4\u8f83\u597d\u7406\u89e3\u7684\u3002","title":"Nomenclature \u547d\u540d\u6cd5"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#informal#representation","text":"In plain English, a hierarchy can be thought of as a set in which: No element is superior to itself, and One element, the hierarch \uff08\u6559\u4e3b\uff09, is superior to all of the other elements in the set. The first requirement is also interpreted to mean that a hierarchy can have no circular relationships ; the association between two objects is always transitive . The second requirement asserts that a hierarchy must have a leader or root that is common to all of the objects. \u5173\u4e8e\u7b2c\u4e00\u70b9\uff0c\u539f\u6587\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u201ca hierarchy can have no circular relationships \u201d\uff0c\u201c circular relationships \"\u6307\u5411\u7684\u662f\u56fe\u8bba\u4e2d\u7684 circle \uff0c\u4f5c\u8005\u6240\u60f3\u8981\u8868\u8fbe\u7684\u662f\uff0c\u4e00\u4e2ahierarchy\u662f\u4e0d\u80fd\u591f\u5b58\u5728\u73af\u7684\uff0c\u5426\u5219\u65e0\u6cd5\u4e25\u683c\u5730\u5f62\u6210\u4e00\u5c42\u4e00\u5c42\u7684hierarchy\u7ed3\u6784\u3002\u5c31\u7b2c\u4e00\u70b9\u7684\u539f\u8bdd\uff0c\u6211\u89c9\u5f97\u4f7f\u7528relation\u7684\u7406\u8bba\u6765\u7406\u89e3\u7684\u8bdd\uff0c\u5b83\u8868\u793a\u7684\u662f\u8fd9\u4e2a\u5173\u7cfb\u4e0d\u80fd\u591f\u662f\u4e00\u4e2a reflexive relation \uff0c\u5426\u5219\u5c31\u4f1a\u51fa\u73b0\u73af\u800c\u65e0\u6cd5\u5f62\u6210\u4e00\u5c42\u4e00\u5c42\u7684hierarchy\u7ed3\u6784\u3002","title":"Informal representation"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#mathematical#representation","text":"Mathematically, in its most general form, a hierarchy is a partially ordered set or poset .","title":"Mathematical representation"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#subtypes","text":"\u201csubtype\u201d\u5373\u5b50\u7c7b\uff0c\u6240\u4ee5\u539f\u6587\u7684\u8fd9\u4e00\u8282\u6240\u63cf\u8ff0\u7684\u662f\u7279\u6b8a\u7c7b\u578b\u7684hierarchy\u3002","title":"Subtypes"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#nested#hierarchy","text":"A nested hierarchy or inclusion hierarchy is a hierarchical ordering of nested sets . A nested hierarchy or inclusion hierarchy is a hierarchical ordering of nested sets . The concept of nesting is exemplified in Russian matryoshka dolls \uff08\u4fc4\u7f57\u65af\u5957\u5a03\uff09. Each doll is encompassed by another doll, all the way to the outer doll. The outer doll holds all of the inner dolls, the next outer doll holds all the remaining inner dolls, and so on. Matryoshkas represent a nested hierarchy where each level contains only one object, i.e., there is only one of each size of doll; a generalized nested hierarchy allows for multiple objects within levels but with each object having only one parent at each level . Nested hierarchies are the organizational schemes behind taxonomies \uff08\u5206\u7c7b\u5b66\uff09and systematic classifications. In many programming taxonomies and syntax models (as well as fractals in mathematics), nested hierarchies, including Russian dolls, are also used to illustrate the properties of self-similarity and recursion . Recursion itself is included as a subset of hierarchical programming, and recursive thinking can be synonymous with a form of hierarchical thinking and logic. Nested hierarchy structure\u5728hierarchy structure\u4e0a\u6dfb\u52a0\u7684\u9650\u5236\u662fnested \uff0c\u5b83\u4fdd\u8bc1\u4e86\u201c each object having only one parent at each level \"\uff0c\u663e\u7136\uff0c\u8fd9\u4e2a\u9650\u5236\u5c31\u662fgraph\u4e2d\u4e0d\u518d\u53ef\u80fd\u4ea7\u751f\u73af\u4e86\uff0c\u4e00\u4e2a\u4e0d\u5e26\u73af\u7684\u56fe\u5c31\u662ftree\uff0c\u6240\u4ee5nested hierarchy structure\u662ftree structure\uff0c\u76f8\u6bd4\u4e8ehierarchy\u800c\u8a00\uff0cnesting\u66f4\u52a0\u80fd\u591f\u4f53\u73b0tree\u7684\u7ed3\u6784\u7279\u5f81\u3002 nested hierarchy structure\u662f\u4e00\u79cd\u7279\u6b8a\u7684hierarchy structure\uff0ctree\u662f\u4e00\u79cd\u7279\u6b8a\u7684graph\u3002 \u601d\u8003\uff1a\u600e\u6837\u7684\u5173\u7cfb\u624d\u80fd\u591f\u4ea7\u751fnested hierarchy structure\uff1f\u8fd9\u4e2a\u95ee\u9898\u5728 Tree-structure \u4e2d\u4f1a\u8fdb\u884c\u8be6\u7ec6\u8ba8\u8bba\u3002 \u539f\u6587\u8fd9\u4e00\u8282\u540e\u9762\u7684\u5185\u5bb9pass\u6389\u4e86\u3002","title":"Nested hierarchy"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#_6","text":"","title":"\u8ba8\u8bba"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#relationhierarchy","text":"\u5728\u539f\u6587\u7684 Informal representation \u4e2d\u6709\u975e\u4e25\u683c\u7684\u63cf\u8ff0\u3002\u5176\u5b9e\u7b80\u800c\u8a00\u4e4b\u5c31\u662f\u5143\u7d20\u4e4b\u95f4\u9700\u8981\u5b58\u5728\u7740 partial order \u3002 transitive N:N","title":"\u6309\u7167\u54ea\u79cdrelation\u6765\u7ec4\u7ec7\u5143\u7d20\u4f1a\u5f62\u6210hierarchy\uff1f"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#hierarchy#and#data#structure","text":"Hierarchy structure\u53ef\u4ee5\u4f7f\u7528graph\u6765\u8fdb\u884c\u8868\u793a\u3002 Nested hierarchy structure\u53ef\u4ee5\u662f\u8981\u5f04tree\u6765\u8fdb\u884c\u8868\u793a\u3002","title":"Hierarchy and data structure"},{"location":"Relation-structure-computation/Model/Hierarchy-relation-model/Hierarchy-relation-model/#hierarchy#structure#and#recursion","text":"\u7531\u4e8ehierarchy relation\u57fa\u672c\u201ctransitive\u201d\u6027\u8d28\uff0c\u6240\u6709 hierarchical \u7ed3\u6784\u7684\u5f80\u5f80\u5177\u5907recursive\u7684\u7279\u5f81\u3002 \u4ece\u6570\u5b66\u7684\u89d2\u5ea6\u6765\u770b\uff0c Hierarchy structure\u662f\u4e00\u4e2a partially ordered set \uff0c\u6240\u4ee5\u8ba8\u8bbahierarchy structure\u7684\u9012\u5f52\u6027\u6700\u7ec8\u5e94\u8be5\u8fd8\u662f\u5f52\u4e8e\u8ba8\u8bbaposet\u7684\u9012\u5f52\u6027\u3002\u6211\u89c9\u5f97\u662f\u53ef\u4ee5\u4f7f\u7528tree\u7684\u9012\u5f52\u6027\u6765\u601d\u8003poset\u7684\u9012\u5f52\u6027\u7684\uff0c\u4e00\u4e2aposet\u7684subset\u4e5f\u5e94\u8be5\u662f\u4e00\u4e2aposet\u3002\u53e6\u5916\u4e00\u4e2a\u5c31\u662fposet\u5404\u5c42\u4f7f\u7528\u7684\u662f\u540c\u4e00\u4e2arelation\u3002","title":"Hierarchy structure and recursion"},{"location":"Relation-structure-computation/Model/Nesting-relation-model/Nesting-relation-model/","text":"Nesting relation model \u5728 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Tree\\Tree-structure.md \u5bf9\u5b83\u8fdb\u884c\u4e86\u8be6\u7ec6\u4ecb\u7ecd\u3002","title":"Nesting-relation-model"},{"location":"Relation-structure-computation/Model/Nesting-relation-model/Nesting-relation-model/#nesting#relation#model","text":"\u5728 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Tree\\Tree-structure.md \u5bf9\u5b83\u8fdb\u884c\u4e86\u8be6\u7ec6\u4ecb\u7ecd\u3002","title":"Nesting relation model"},{"location":"Relation-structure-computation/Model/Stream/","text":"Stream \u201cstream\u201d\u5373\u201c\u6d41\u201d\uff0cstream\u662f\u4e00\u79cd\u975e\u5e38\u5f3a\u5927\u7684**\u62bd\u8c61\u7ed3\u6784**\uff0c\u5728computer science\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff1a\u8ba1\u7b97\u673a\u79d1\u5b66\u662f\u5173\u4e8e**\u6570\u636e**\u7684\u79d1\u5b66\uff0c\u6211\u4eec\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u5904\u7406\u7740\u5404\u79cd\u5404\u6837\u7684**\u6570\u636e**\uff0c\u6211\u4eec\u53ef\u4ee5\u5f62\u8c61\u5730\u5c06**\u6570\u636e**\u7684\u4f20\u9012\u770b\u505a\u662f\" \u6c34\u6d41 \"\u4e00\u822c: **\u6570\u636e**\u5c31\u50cf**\u6c34\u6d41**\u7684\u6d41\u8f6c\u4e00\u822c\u5728**\u8ba1\u7b97\u673a\u7cfb\u7edf**\u4e2d\u8fdb\u884c\u6d41\u8f6c\u3002 NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684**\u6570\u636e**\u3001**\u8ba1\u7b97\u673a\u7cfb\u7edf**\u90fd\u662f\u62bd\u8c61\u6982\u5ff5\uff0c\u5b83\u4eec\u53ef\u4ee5\u6307\u4ee3\u5f88\u591a\u5185\u5bb9 Stream\u53ef\u4ee5\u7528\u505a\u63cf\u8ff0**\u6570\u636e\u6d41**\u7684**\u62bd\u8c61\u7ed3\u6784**\u3002 \u7ef4\u57fa\u767e\u79d1 Stream (computing) In computer science , a stream is a sequence of data elements made available over time. A stream can be thought of as items on a conveyor belt (\u4f20\u9001\u5e26) being processed one at a time rather than in large batches. TRANSLATION: \u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0cstream\u662f\u968f\u65f6\u95f4\u63a8\u79fb\u53ef\u7528\u7684\u4e00\u7cfb\u5217\u6570\u636e\u5143\u7d20\u3002 \u53ef\u4ee5\u5c06stream\u89c6\u4e3a\u4f20\u9001\u5e26\u4e0a\u7684\u7269\u54c1\uff0c\u4e00\u6b21\u4e00\u4e2a\u5730\u5904\u7406\uff0c\u800c\u4e0d\u662f\u5927\u6279\u91cf\u5904\u7406\u3002 Streams are processed differently from batch data \u2013 normal functions cannot operate on streams as a whole(\u4e0d\u80fd\u591f\u6574\u4e2a\u7684\u64cd\u4f5cstream), as they have potentially unlimited data , and formally, streams are codata (potentially unlimited), not data (which is finite). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8bba\u8ff0\u7684\u662fstream \u548c batch\u3002 Functions that operate on a stream, producing another stream, are known as filters , and can be connected in pipelines , analogously to function composition . Filters may operate on one item of a stream at a time, or may base an item of output on multiple items of input, such as a moving average . NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5176\u5b9e\u7ed9\u51fa\u4e86stream\u7684programming model\uff0c\u663e\u7136\uff0c\u5b83\u975e\u5e38\u7c7b\u4f3c\u4e8efunctional programming\u7684\u3002 Examples The term \"stream\" is used in a number of similar ways: 1) \"Stream editing\", as with sed , awk , and perl . Stream editing processes a file or files, in-place, without having to load the file(s) into a user interface. One example of such use is to do a search and replace on all the files in a directory, from the command line. 2) On Unix and related systems based on the C language , a stream is a source or sink of data, usually individual bytes or characters . Streams are an abstraction used when reading or writing files, or communicating over network sockets . The standard streams are three streams made available to all programs. 3) I/O devices can be interpreted as streams , as they produce or consume potentially unlimited data over time. NOTE: 2\u30013\u6240\u6307\u4e3astream-based IO\u3002 4) In object-oriented programming , input streams are generally implemented as iterators . NOTE: \u5982\u4f55\u5bf9iterator\u8fdb\u884c\u64cd\u4f5c\uff1f 5) In the Scheme language and some others, a stream is a lazily evaluated or delayed sequence of data elements. A stream can be used similarly to a list, but later elements are only calculated when needed. Streams can therefore represent infinite sequences and series . 6) In the Smalltalk standard library and in other programming languages as well, a stream is an external iterator . As in Scheme, streams can represent finite or infinite sequences. 7) Stream processing \u2014 in parallel processing , especially in graphic processing, the term stream is applied to hardware as well as software . There it defines the quasi-continuous flow of data that is processed in a dataflow programming language as soon as the program state meets the starting condition of the stream. NOTE: stream processing\u662fparallel computing\u4e2d\u7684\u91cd\u8981model\uff0c\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Model\\Stream-model \u7ae0\u8282\u3002 Applications Streams can be used as the underlying data type for channels in interprocess communication . Other uses NOTE: \u975e\u5e38\u91cd\u8981\u7684\u63cf\u8ff0 Stream\u7684\u5355\u4f4d \u5bf9stream\u7684\u5355\u4f4d\u3001\u7ec4\u6210\u7684\u5206\u6790\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u4e00\u822c\u5c06\u5355\u4f4d\u653e\u5230\u5177\u4f53stream\u7684\u547d\u540d\u4e2d\uff0c\u4e0b\u9762\u662f\u6211\u76ee\u524d\u6240\u9047\u5230\u7684\u5177\u4f53stream: \u5177\u4f53stream \u8bf4\u660e Bitstream \u5355\u4f4d\u662fbit byte stream \u5355\u4f4d\u662fbyte character stream \u5355\u4f4d\u662fcharacter data stream \u5355\u4f4d\u662fdata\uff0c\u663e\u7136data\u662f\u4e00\u4e2a\u62bd\u8c61\u7684\u6982\u5ff5\uff0c\u5b83\u662fbig data\u3001parallel computing\u7b49\u65f6\u4ee3\u80cc\u666f\u7684\u4ea7\u7269\u3002 \u53e6\u5916\u4e0e\u5b83\u76f8\u5173\u7684\u662f: dataflow programming paradigm\uff0c\u5176\u5b9edataflow\u5c31\u662fdata stream message/event stream \u5355\u4f4d\u662fmessage/event\uff0c\u53c2\u89c1\u4e0b\u9762\u7684 Stream-based message/event processing system \u7ae0\u8282 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0a\u9762\u8fd9\u4e9bstream\u5e76\u4e0d\u662f\u5177\u4f53\u7684\u5206\u7c7b\uff0c\u66f4\u591a\u7684\u662f\u5728\u5177\u4f53\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u62bd\u8c61\u3002 Byte stream VS character stream geeksforgeeks Character Stream Vs Byte Stream in Java stackoverflow Byte Stream and Character stream iitk Character Streams versus Byte Streams \u901a\u8fc7\u4e0a\u8ff0\u6587\u7ae0\u53ef\u4ee5\u603b\u7ed3\uff1abyte stream\u548ccharacter stream\u7684\u4e3b\u8981\u5dee\u522b\u5728\u4e8e\u6bcf\u6b21\u5904\u7406\u7684unit\u4e0d\u540c\uff1a byte stream\u7684unit\u662fbyte\uff0ccharacter stream\u7684unit\u662fcharacter\uff1b\u4e00\u4e2acharacter\u7531\u4e00\u4e2a\u6216\u8005\u591a\u4e2abyte\u7ec4\u6210\u3002 Byte stream VS bit stream stackoverflow Difference between byte stream and bit stream \u7ef4\u57fa\u767e\u79d1 Bitstream A bitstream (or bit stream ), also known as binary sequence , is a sequence of bits . A bytestream is a sequence of bytes . Typically, each byte is an 8-bit quantity ( octets ), and so the term octet stream is sometimes used interchangeably. An octet may be encoded as a sequence of 8 bits in multiple different ways (see endianness ) so there is no unique and direct translation between bytestreams and bitstreams . Bitstreams and bytestreams are used extensively in telecommunications and computing . For example, synchronous bitstreams are carried by SONET , and Transmission Control Protocol transports an asynchronous bytestream. Stream\u7684\u65b9\u5411 \u5728\u4f7f\u7528stream\u6765\u8fdb\u884c\u62bd\u8c61\u7684\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u8003\u8651stream\u7684\u65b9\u5411\uff1a \u6e90: \u6570\u636e\u4ece\u4f55\u5904\u6d41\u5165 \u76ee\u7684: \u6570\u636e\u6d41\u5411\u4f55\u5904 \u7279\u6027 stream\u6709\u7740\u975e\u5e38\u597d\u7684\u7279\u6027\uff0c\u8fd9\u4e9b\u4f18\u826f\u7684\u7279\u6027\u51b3\u5b9a\u4e86\u5b83\u5728computer science\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002 \u7ed3\u6784\u7684\u7b80\u5355 stream\u7684\u7ed3\u6784\u662f\u975e\u5e38\u7b80\u5355\u7684: \u76f8\u90bb\u5173\u7cfb\uff0c\u8fd9\u5c31\u51b3\u5b9a\u4e86\u5b83\u662f\u7ebf\u6027\u7684\uff0c\u8fd9\u79cd\u7ed3\u6784\u662f\u4e00\u4e2a\u975e\u5e38\u666e\u904d\u7684\u7ed3\u6784\uff0c\u5728computer science\u4e2d\u5e7f\u6cdb\u5b58\u5728\u3002 \u6709\u5e8f\u6027 stream\u4e2d\u7684\u5143\u7d20\u9ed8\u8ba4\u662f\u6709\u5e8f\u7684\u3002 Computation\u7684\u7b80\u5355 stream\u7ed3\u6784\u7684\u7b80\u5355\u51b3\u5b9a\u4e86\u5bf9\u5b83\u8fdb\u884ccomputation\u662f\u975e\u5e38\u7b80\u5355\u7684\u3002stream\u7ed3\u6784\u662fdiscrete\u7684\uff0c\u6211\u4eec\u53ef\u4ee5one-by-one\u5730\u6765\u5bf9\u5b83\u8ba1\u7b97\uff0c\u4e00\u79cd\u6700\u6700\u5e38\u89c1\u7684\u65b9\u5f0f\u5c31\u662fiteration\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Computation\\Iteration \u7ae0\u8282\uff0c\u5173\u4e8e\u6b64\uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Stream (computing) \u4e2d\u4e5f\u8fdb\u884c\u4e86\u8be6\u7ec6\u5730\u4ecb\u7ecd\u3002 Application Stream-base IO \u53c2\u89c1: 1) \u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\IO-\u6d41\u6d3e\\Stream \u7ae0\u8282\u3002 2) \u5de5\u7a0bprogramming-language\u7684 C-family-language\\C++\\Library\\Standard-library\\IO-library \u7ae0\u8282\u3002 Stream-oriented protocol TCP\u5c31\u662f\u5178\u578b\u7684\u4f8b\u5b50\uff0c\u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Network\\Theory\\TCP \u7ae0\u8282\u3002 Parallel computing \u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Model\\Stream-model \u7ae0\u8282\u3002 Stream-based message/event processing system \u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Application\\Message-processing-system\\Stream-based-message-processing-system \u7ae0\u8282\u3002 Dataflow programming paradigm \u5173\u4e8eDataflow programming paradigm\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Dataflow-programming \u7ae0\u8282\u3002 TensorFlow TensorFlow\u5c31\u662f\u5178\u578b\u7684\u91c7\u7528dataflow programming\u7684\uff0c\u5728TensorFlow\u7684\u4e2d\uff0c\u663e\u7136\u5b83\u7684stream\u7684\u5355\u4f4d\u662ftensor\u3002 Programming model \u5728\u7ef4\u57fa\u767e\u79d1 Stream (computing) \u4e2d\uff0c\u5176\u5b9e\u5df2\u7ecf\u6d89\u53ca\u4e86\u5bf9stream\u7684programming model\u7684\u63cf\u8ff0\uff0c\u4ece\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cstream\u7684programming model\u53ef\u4ee5\u91c7\u7528functional programming paradigm\u7684\u601d\u60f3\u3002 \u5728\u4e0b\u9762\u7ae0\u8282\u4e2d\uff0c\u63cf\u8ff0\u4e86\u76f8\u5173\u5185\u5bb9: 1) \u5de5\u7a0bparallel-computing\u7684 Programming-model \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Model/Stream/#stream","text":"\u201cstream\u201d\u5373\u201c\u6d41\u201d\uff0cstream\u662f\u4e00\u79cd\u975e\u5e38\u5f3a\u5927\u7684**\u62bd\u8c61\u7ed3\u6784**\uff0c\u5728computer science\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff1a\u8ba1\u7b97\u673a\u79d1\u5b66\u662f\u5173\u4e8e**\u6570\u636e**\u7684\u79d1\u5b66\uff0c\u6211\u4eec\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u5904\u7406\u7740\u5404\u79cd\u5404\u6837\u7684**\u6570\u636e**\uff0c\u6211\u4eec\u53ef\u4ee5\u5f62\u8c61\u5730\u5c06**\u6570\u636e**\u7684\u4f20\u9012\u770b\u505a\u662f\" \u6c34\u6d41 \"\u4e00\u822c: **\u6570\u636e**\u5c31\u50cf**\u6c34\u6d41**\u7684\u6d41\u8f6c\u4e00\u822c\u5728**\u8ba1\u7b97\u673a\u7cfb\u7edf**\u4e2d\u8fdb\u884c\u6d41\u8f6c\u3002 NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684**\u6570\u636e**\u3001**\u8ba1\u7b97\u673a\u7cfb\u7edf**\u90fd\u662f\u62bd\u8c61\u6982\u5ff5\uff0c\u5b83\u4eec\u53ef\u4ee5\u6307\u4ee3\u5f88\u591a\u5185\u5bb9 Stream\u53ef\u4ee5\u7528\u505a\u63cf\u8ff0**\u6570\u636e\u6d41**\u7684**\u62bd\u8c61\u7ed3\u6784**\u3002","title":"Stream"},{"location":"Relation-structure-computation/Model/Stream/#stream#computing","text":"In computer science , a stream is a sequence of data elements made available over time. A stream can be thought of as items on a conveyor belt (\u4f20\u9001\u5e26) being processed one at a time rather than in large batches. TRANSLATION: \u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0cstream\u662f\u968f\u65f6\u95f4\u63a8\u79fb\u53ef\u7528\u7684\u4e00\u7cfb\u5217\u6570\u636e\u5143\u7d20\u3002 \u53ef\u4ee5\u5c06stream\u89c6\u4e3a\u4f20\u9001\u5e26\u4e0a\u7684\u7269\u54c1\uff0c\u4e00\u6b21\u4e00\u4e2a\u5730\u5904\u7406\uff0c\u800c\u4e0d\u662f\u5927\u6279\u91cf\u5904\u7406\u3002 Streams are processed differently from batch data \u2013 normal functions cannot operate on streams as a whole(\u4e0d\u80fd\u591f\u6574\u4e2a\u7684\u64cd\u4f5cstream), as they have potentially unlimited data , and formally, streams are codata (potentially unlimited), not data (which is finite). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8bba\u8ff0\u7684\u662fstream \u548c batch\u3002 Functions that operate on a stream, producing another stream, are known as filters , and can be connected in pipelines , analogously to function composition . Filters may operate on one item of a stream at a time, or may base an item of output on multiple items of input, such as a moving average . NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5176\u5b9e\u7ed9\u51fa\u4e86stream\u7684programming model\uff0c\u663e\u7136\uff0c\u5b83\u975e\u5e38\u7c7b\u4f3c\u4e8efunctional programming\u7684\u3002","title":"\u7ef4\u57fa\u767e\u79d1 Stream (computing)"},{"location":"Relation-structure-computation/Model/Stream/#examples","text":"The term \"stream\" is used in a number of similar ways: 1) \"Stream editing\", as with sed , awk , and perl . Stream editing processes a file or files, in-place, without having to load the file(s) into a user interface. One example of such use is to do a search and replace on all the files in a directory, from the command line. 2) On Unix and related systems based on the C language , a stream is a source or sink of data, usually individual bytes or characters . Streams are an abstraction used when reading or writing files, or communicating over network sockets . The standard streams are three streams made available to all programs. 3) I/O devices can be interpreted as streams , as they produce or consume potentially unlimited data over time. NOTE: 2\u30013\u6240\u6307\u4e3astream-based IO\u3002 4) In object-oriented programming , input streams are generally implemented as iterators . NOTE: \u5982\u4f55\u5bf9iterator\u8fdb\u884c\u64cd\u4f5c\uff1f 5) In the Scheme language and some others, a stream is a lazily evaluated or delayed sequence of data elements. A stream can be used similarly to a list, but later elements are only calculated when needed. Streams can therefore represent infinite sequences and series . 6) In the Smalltalk standard library and in other programming languages as well, a stream is an external iterator . As in Scheme, streams can represent finite or infinite sequences. 7) Stream processing \u2014 in parallel processing , especially in graphic processing, the term stream is applied to hardware as well as software . There it defines the quasi-continuous flow of data that is processed in a dataflow programming language as soon as the program state meets the starting condition of the stream. NOTE: stream processing\u662fparallel computing\u4e2d\u7684\u91cd\u8981model\uff0c\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Model\\Stream-model \u7ae0\u8282\u3002","title":"Examples"},{"location":"Relation-structure-computation/Model/Stream/#applications","text":"Streams can be used as the underlying data type for channels in interprocess communication .","title":"Applications"},{"location":"Relation-structure-computation/Model/Stream/#other#uses","text":"NOTE: \u975e\u5e38\u91cd\u8981\u7684\u63cf\u8ff0","title":"Other uses"},{"location":"Relation-structure-computation/Model/Stream/#stream_1","text":"\u5bf9stream\u7684\u5355\u4f4d\u3001\u7ec4\u6210\u7684\u5206\u6790\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u4e00\u822c\u5c06\u5355\u4f4d\u653e\u5230\u5177\u4f53stream\u7684\u547d\u540d\u4e2d\uff0c\u4e0b\u9762\u662f\u6211\u76ee\u524d\u6240\u9047\u5230\u7684\u5177\u4f53stream: \u5177\u4f53stream \u8bf4\u660e Bitstream \u5355\u4f4d\u662fbit byte stream \u5355\u4f4d\u662fbyte character stream \u5355\u4f4d\u662fcharacter data stream \u5355\u4f4d\u662fdata\uff0c\u663e\u7136data\u662f\u4e00\u4e2a\u62bd\u8c61\u7684\u6982\u5ff5\uff0c\u5b83\u662fbig data\u3001parallel computing\u7b49\u65f6\u4ee3\u80cc\u666f\u7684\u4ea7\u7269\u3002 \u53e6\u5916\u4e0e\u5b83\u76f8\u5173\u7684\u662f: dataflow programming paradigm\uff0c\u5176\u5b9edataflow\u5c31\u662fdata stream message/event stream \u5355\u4f4d\u662fmessage/event\uff0c\u53c2\u89c1\u4e0b\u9762\u7684 Stream-based message/event processing system \u7ae0\u8282 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0a\u9762\u8fd9\u4e9bstream\u5e76\u4e0d\u662f\u5177\u4f53\u7684\u5206\u7c7b\uff0c\u66f4\u591a\u7684\u662f\u5728\u5177\u4f53\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u62bd\u8c61\u3002","title":"Stream\u7684\u5355\u4f4d"},{"location":"Relation-structure-computation/Model/Stream/#byte#stream#vs#character#stream","text":"geeksforgeeks Character Stream Vs Byte Stream in Java stackoverflow Byte Stream and Character stream iitk Character Streams versus Byte Streams \u901a\u8fc7\u4e0a\u8ff0\u6587\u7ae0\u53ef\u4ee5\u603b\u7ed3\uff1abyte stream\u548ccharacter stream\u7684\u4e3b\u8981\u5dee\u522b\u5728\u4e8e\u6bcf\u6b21\u5904\u7406\u7684unit\u4e0d\u540c\uff1a byte stream\u7684unit\u662fbyte\uff0ccharacter stream\u7684unit\u662fcharacter\uff1b\u4e00\u4e2acharacter\u7531\u4e00\u4e2a\u6216\u8005\u591a\u4e2abyte\u7ec4\u6210\u3002","title":"Byte stream VS character stream"},{"location":"Relation-structure-computation/Model/Stream/#byte#stream#vs#bit#stream","text":"stackoverflow Difference between byte stream and bit stream","title":"Byte stream VS bit stream"},{"location":"Relation-structure-computation/Model/Stream/#bitstream","text":"A bitstream (or bit stream ), also known as binary sequence , is a sequence of bits . A bytestream is a sequence of bytes . Typically, each byte is an 8-bit quantity ( octets ), and so the term octet stream is sometimes used interchangeably. An octet may be encoded as a sequence of 8 bits in multiple different ways (see endianness ) so there is no unique and direct translation between bytestreams and bitstreams . Bitstreams and bytestreams are used extensively in telecommunications and computing . For example, synchronous bitstreams are carried by SONET , and Transmission Control Protocol transports an asynchronous bytestream.","title":"\u7ef4\u57fa\u767e\u79d1Bitstream"},{"location":"Relation-structure-computation/Model/Stream/#stream_2","text":"\u5728\u4f7f\u7528stream\u6765\u8fdb\u884c\u62bd\u8c61\u7684\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u8003\u8651stream\u7684\u65b9\u5411\uff1a \u6e90: \u6570\u636e\u4ece\u4f55\u5904\u6d41\u5165 \u76ee\u7684: \u6570\u636e\u6d41\u5411\u4f55\u5904","title":"Stream\u7684\u65b9\u5411"},{"location":"Relation-structure-computation/Model/Stream/#_1","text":"stream\u6709\u7740\u975e\u5e38\u597d\u7684\u7279\u6027\uff0c\u8fd9\u4e9b\u4f18\u826f\u7684\u7279\u6027\u51b3\u5b9a\u4e86\u5b83\u5728computer science\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002","title":"\u7279\u6027"},{"location":"Relation-structure-computation/Model/Stream/#_2","text":"stream\u7684\u7ed3\u6784\u662f\u975e\u5e38\u7b80\u5355\u7684: \u76f8\u90bb\u5173\u7cfb\uff0c\u8fd9\u5c31\u51b3\u5b9a\u4e86\u5b83\u662f\u7ebf\u6027\u7684\uff0c\u8fd9\u79cd\u7ed3\u6784\u662f\u4e00\u4e2a\u975e\u5e38\u666e\u904d\u7684\u7ed3\u6784\uff0c\u5728computer science\u4e2d\u5e7f\u6cdb\u5b58\u5728\u3002","title":"\u7ed3\u6784\u7684\u7b80\u5355"},{"location":"Relation-structure-computation/Model/Stream/#_3","text":"stream\u4e2d\u7684\u5143\u7d20\u9ed8\u8ba4\u662f\u6709\u5e8f\u7684\u3002","title":"\u6709\u5e8f\u6027"},{"location":"Relation-structure-computation/Model/Stream/#computation","text":"stream\u7ed3\u6784\u7684\u7b80\u5355\u51b3\u5b9a\u4e86\u5bf9\u5b83\u8fdb\u884ccomputation\u662f\u975e\u5e38\u7b80\u5355\u7684\u3002stream\u7ed3\u6784\u662fdiscrete\u7684\uff0c\u6211\u4eec\u53ef\u4ee5one-by-one\u5730\u6765\u5bf9\u5b83\u8ba1\u7b97\uff0c\u4e00\u79cd\u6700\u6700\u5e38\u89c1\u7684\u65b9\u5f0f\u5c31\u662fiteration\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Computation\\Iteration \u7ae0\u8282\uff0c\u5173\u4e8e\u6b64\uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Stream (computing) \u4e2d\u4e5f\u8fdb\u884c\u4e86\u8be6\u7ec6\u5730\u4ecb\u7ecd\u3002","title":"Computation\u7684\u7b80\u5355"},{"location":"Relation-structure-computation/Model/Stream/#application","text":"","title":"Application"},{"location":"Relation-structure-computation/Model/Stream/#stream-base#io","text":"\u53c2\u89c1: 1) \u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\IO-\u6d41\u6d3e\\Stream \u7ae0\u8282\u3002 2) \u5de5\u7a0bprogramming-language\u7684 C-family-language\\C++\\Library\\Standard-library\\IO-library \u7ae0\u8282\u3002","title":"Stream-base IO"},{"location":"Relation-structure-computation/Model/Stream/#stream-oriented#protocol","text":"TCP\u5c31\u662f\u5178\u578b\u7684\u4f8b\u5b50\uff0c\u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Network\\Theory\\TCP \u7ae0\u8282\u3002","title":"Stream-oriented protocol"},{"location":"Relation-structure-computation/Model/Stream/#parallel#computing","text":"\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Model\\Stream-model \u7ae0\u8282\u3002","title":"Parallel computing"},{"location":"Relation-structure-computation/Model/Stream/#stream-based#messageevent#processing#system","text":"\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Application\\Message-processing-system\\Stream-based-message-processing-system \u7ae0\u8282\u3002","title":"Stream-based message/event processing system"},{"location":"Relation-structure-computation/Model/Stream/#dataflow#programming#paradigm","text":"\u5173\u4e8eDataflow programming paradigm\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Dataflow-programming \u7ae0\u8282\u3002","title":"Dataflow programming paradigm"},{"location":"Relation-structure-computation/Model/Stream/#tensorflow","text":"TensorFlow\u5c31\u662f\u5178\u578b\u7684\u91c7\u7528dataflow programming\u7684\uff0c\u5728TensorFlow\u7684\u4e2d\uff0c\u663e\u7136\u5b83\u7684stream\u7684\u5355\u4f4d\u662ftensor\u3002","title":"TensorFlow"},{"location":"Relation-structure-computation/Model/Stream/#programming#model","text":"\u5728\u7ef4\u57fa\u767e\u79d1 Stream (computing) \u4e2d\uff0c\u5176\u5b9e\u5df2\u7ecf\u6d89\u53ca\u4e86\u5bf9stream\u7684programming model\u7684\u63cf\u8ff0\uff0c\u4ece\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cstream\u7684programming model\u53ef\u4ee5\u91c7\u7528functional programming paradigm\u7684\u601d\u60f3\u3002 \u5728\u4e0b\u9762\u7ae0\u8282\u4e2d\uff0c\u63cf\u8ff0\u4e86\u76f8\u5173\u5185\u5bb9: 1) \u5de5\u7a0bparallel-computing\u7684 Programming-model \u7ae0\u8282\u3002","title":"Programming model"},{"location":"Relation-structure-computation/Relation/","text":"\u5173\u4e8e\u672c\u7ae0 \u201crelation\u201d\u5373\u5173\u7cfb\uff0c\u8fd9\u4e2a\u8bcd\u662f\u6211\u4eec\u7ecf\u5e38\u8bf4\u8d77\u7684\uff0c\u5728\u6570\u5b66\u4e2d\uff0cRelation\u662f\u4e13\u95e8\u7814\u7a76\u5b83\u7684\u4e00\u4e2a\u6570\u5b66\u5206\u652f\u3002 \u672c\u7ae0\u9996\u5148\u5feb\u901f\u68b3\u7406Relation\u76f8\u5173\u77e5\u8bc6\uff0c\u4e3b\u8981\u662f\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1 Finitary relation \u3002\u53e6\u5916\u5728\u7ecf\u5178\u6559\u6750 Discrete Mathematics and Its Applications \u4e2d\u4e5f\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u8ba8\u8bbaRelation\u3002 Entity\u2013relationship-model\uff1a\u5176\u4e2d\u7684\u7406\u8bba\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u5c06relation\u5e94\u7528\u4e8e\u5b9e\u9645\u95ee\u9898\u4e2d\u3002 Order-theory\uff1arelation\u4e5f\u662f\u5177\u6709\u65b9\u5411\u7684\uff0c\u6240\u4ee5\u6709\u5fc5\u8981\u4e86\u89e3order-theory\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Relation/#_1","text":"\u201crelation\u201d\u5373\u5173\u7cfb\uff0c\u8fd9\u4e2a\u8bcd\u662f\u6211\u4eec\u7ecf\u5e38\u8bf4\u8d77\u7684\uff0c\u5728\u6570\u5b66\u4e2d\uff0cRelation\u662f\u4e13\u95e8\u7814\u7a76\u5b83\u7684\u4e00\u4e2a\u6570\u5b66\u5206\u652f\u3002 \u672c\u7ae0\u9996\u5148\u5feb\u901f\u68b3\u7406Relation\u76f8\u5173\u77e5\u8bc6\uff0c\u4e3b\u8981\u662f\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1 Finitary relation \u3002\u53e6\u5916\u5728\u7ecf\u5178\u6559\u6750 Discrete Mathematics and Its Applications \u4e2d\u4e5f\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u8ba8\u8bbaRelation\u3002 Entity\u2013relationship-model\uff1a\u5176\u4e2d\u7684\u7406\u8bba\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u5c06relation\u5e94\u7528\u4e8e\u5b9e\u9645\u95ee\u9898\u4e2d\u3002 Order-theory\uff1arelation\u4e5f\u662f\u5177\u6709\u65b9\u5411\u7684\uff0c\u6240\u4ee5\u6709\u5fc5\u8981\u4e86\u89e3order-theory\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/","text":"Entity\u2013relationship model \u4f7f\u7528Entity\u2013relationship model\u53ef\u4ee5\u63cf\u8ff0\u975e\u5e38\u591a\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u5b9e\u9645\u751f\u6d3b\u4e2d\u7684\u95ee\u9898\u3002 \u7ef4\u57fa\u767e\u79d1 Entity\u2013relationship model \u7ef4\u57fa\u767e\u79d1 Cardinality (data modeling) NOTE: Cardinality \u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u57fa\u6570\u201d One-to-one relation model \u7ef4\u57fa\u767e\u79d1 One-to-one (data model ) One-to-many relation model \u7ef4\u57fa\u767e\u79d1 One-to-many (data model) \u4e00\u5bf9\u591a\u5173\u7cfb\u4f7f\u7528\u6709\u5411\u56fe\u6765\u8868\u793a\u5c31\u662f\u4e00\u4e2a\u8282\u70b9\u4e0e\u591a\u4e2a\u8282\u70b9\u76f8\u90bb\uff0c\u76f8\u90bb\u8282\u70b9\u90fd\u6709\u8fb9\u6307\u5411\u5b83\u3002 Example \u591a\u6001 race condition \u5bf9\u4e8e\u5355\u6838CPU\u3001\u652f\u6301multitask\u7684OS\uff0c\u5219\u591a\u4e2aprocess\u5171\u4eab\u4e00\u4e2aCPU\uff0c\u8fd9\u4e5f\u662f\u4e00\u79cdone-to-many relation thread\u5171\u4eabprocess\u7684resource \u591a\u4e2aprocess\u5171\u4eabcomputer resource\u3002 many-to-many relation model \u7ef4\u57fa\u767e\u79d1 Many-to-many (data model) Example \u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\u591a\u6838CPU READING LIST https://fmhelp.filemaker.com/help/18/fmp/en/index.html#page/FMP_Help/many-to-many-relationships.html https://support.airtable.com/hc/en-us/articles/218734758-A-beginner-s-guide-to-many-to-many-relationships https://dzone.com/articles/how-to-handle-a-many-to-many-relationship-in-datab","title":"Entity\u2013relationship-model"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#entityrelationship#model","text":"\u4f7f\u7528Entity\u2013relationship model\u53ef\u4ee5\u63cf\u8ff0\u975e\u5e38\u591a\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u5b9e\u9645\u751f\u6d3b\u4e2d\u7684\u95ee\u9898\u3002","title":"Entity\u2013relationship model"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#entityrelationship#model_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Entity\u2013relationship model"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#cardinality#data#modeling","text":"NOTE: Cardinality \u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u57fa\u6570\u201d","title":"\u7ef4\u57fa\u767e\u79d1Cardinality (data modeling)"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#one-to-one#relation#model","text":"","title":"One-to-one relation model"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#one-to-one#data#model","text":"","title":"\u7ef4\u57fa\u767e\u79d1One-to-one (data model)"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#one-to-many#relation#model","text":"","title":"One-to-many relation model"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#one-to-many#data#model","text":"\u4e00\u5bf9\u591a\u5173\u7cfb\u4f7f\u7528\u6709\u5411\u56fe\u6765\u8868\u793a\u5c31\u662f\u4e00\u4e2a\u8282\u70b9\u4e0e\u591a\u4e2a\u8282\u70b9\u76f8\u90bb\uff0c\u76f8\u90bb\u8282\u70b9\u90fd\u6709\u8fb9\u6307\u5411\u5b83\u3002","title":"\u7ef4\u57fa\u767e\u79d1One-to-many (data model)"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#example","text":"\u591a\u6001 race condition \u5bf9\u4e8e\u5355\u6838CPU\u3001\u652f\u6301multitask\u7684OS\uff0c\u5219\u591a\u4e2aprocess\u5171\u4eab\u4e00\u4e2aCPU\uff0c\u8fd9\u4e5f\u662f\u4e00\u79cdone-to-many relation thread\u5171\u4eabprocess\u7684resource \u591a\u4e2aprocess\u5171\u4eabcomputer resource\u3002","title":"Example"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#many-to-many#relation#model","text":"","title":"many-to-many relation model"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#many-to-many#data#model","text":"","title":"\u7ef4\u57fa\u767e\u79d1Many-to-many (data model)"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#example_1","text":"\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\u591a\u6838CPU","title":"Example"},{"location":"Relation-structure-computation/Relation/Entity%E2%80%93relationship-model/Entity%E2%80%93relationship-model/#reading#list","text":"https://fmhelp.filemaker.com/help/18/fmp/en/index.html#page/FMP_Help/many-to-many-relationships.html https://support.airtable.com/hc/en-us/articles/218734758-A-beginner-s-guide-to-many-to-many-relationships https://dzone.com/articles/how-to-handle-a-many-to-many-relationship-in-datab","title":"READING LIST"},{"location":"Relation-structure-computation/Relation/Order-theory/","text":"","title":"Index"},{"location":"Relation-structure-computation/Relation/Order-theory/Greatest-and-least-elements/","text":"Greatest and least elements \u6700\u5927\u5143\u7d20\u4e0e\u6700\u5c0f\u5143\u7d20\uff0c\u6211\u4eec\u5bfb\u5e38\u7684\u7406\u89e3\u662f\u6309\u7167\u5927\u5c0f\u6765\u8fdb\u884c\u6392\u5e8f\uff0c\u4e0b\u9762\u770b\u770border theory\u4e2d\u5982\u4f55\u6765\u5b9a\u4e49\u6700\u5927\u5143\u7d20\u4e0e\u6700\u5c0f\u5143\u7d20\u3002 \u7ef4\u57fa\u767e\u79d1 Greatest and least elements","title":"Greatest-and-least-elements"},{"location":"Relation-structure-computation/Relation/Order-theory/Greatest-and-least-elements/#greatest#and#least#elements","text":"\u6700\u5927\u5143\u7d20\u4e0e\u6700\u5c0f\u5143\u7d20\uff0c\u6211\u4eec\u5bfb\u5e38\u7684\u7406\u89e3\u662f\u6309\u7167\u5927\u5c0f\u6765\u8fdb\u884c\u6392\u5e8f\uff0c\u4e0b\u9762\u770b\u770border theory\u4e2d\u5982\u4f55\u6765\u5b9a\u4e49\u6700\u5927\u5143\u7d20\u4e0e\u6700\u5c0f\u5143\u7d20\u3002","title":"Greatest and least elements"},{"location":"Relation-structure-computation/Relation/Order-theory/Greatest-and-least-elements/#greatest#and#least#elements_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Greatest and least elements"},{"location":"Relation-structure-computation/Relation/Order-theory/Order-theory/","text":"Order theory \u6392\u5e8f\u95ee\u9898 \u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u4e00\u7c7b\u5178\u578b\u95ee\u9898\uff0c \u6709\u5f88\u591a\u7684\u95ee\u9898\u6700\u7ec8\u90fd\u53ef\u4ee5\u8f6c\u6362\u4e3a\u6392\u5e8f\u95ee\u9898\uff0c\u6bd4\u5982\uff08TODO \u589e\u52a0\u4e00\u4e9b\u4f8b\u5b50\uff0c\u5982\u5faa\u73af\u4f9d\u8d56\u56fe\uff09\uff0c\u672c\u7ae0\u5c06\u7814\u7a76\u6392\u5e8f\u7684\u7406\u8bba\uff1aorder theory\u8fdb\u884c\u603b\u7ed3\uff0c\u7ecf\u8fc7\u672c\u7ae0\uff0c\u6211\u4eec\u5c06\u5bf9order\u6709\u4e00\u4e2a\u66f4\u52a0\u79d1\u5b66\u7684\u8ba4\u8bc6\u3002 Order theory\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5bf9\u4e8e\u5b9e\u73b0computation\u975e\u5e38\u91cd\u8981\uff0c\u6b63\u5982\u5728 Relation-structure-computation\\Make-it-computational \u7ae0\u8282\u6240\u603b\u7ed3\u7684: \u53ea\u6709**\u6709\u5e8f**\u624d\u80fd\u591f\u5b9e\u73b0computation\uff0c\u624d\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u6027 wikipedia Order theory \u7b14\u8bb0 \u901a\u8fc7\u7ef4\u57fa\u767e\u79d1 Order theory \uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cOrder theory\u662f\u5efa\u7acb\u5728 binary relation \u4e4b\u4e0a\u7684\uff0c\u5b83\u6240\u7814\u7a76\u7684\u662f**\u540c\u4e00**\u96c6\u5408\u4e2d\u7684\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 Order theory \u5c06\u201c\u6392\u5e8f\u201d\u7684\u6982\u5ff5\u8fdb\u884c\u4e86\u62d3\u5e7f\uff0c\u5b83\u544a\u8bc9\u6211\u4eec\u96c6\u5408\u4e2d\u7684\u5143\u7d20\u6309\u7167\u600e\u6837\u7684\u5173\u7cfb\u6765\u8fdb\u884c\u7ec4\u7ec7\uff0c\u5219\u5b83\u4eec\u662f\u53ef\u4ee5\u8fdb\u884c\u201c\u6392\u5e8f\u201d\u7684\uff0c\u4e0b\u9762\u7684 Binary relations \u7ae0\u8282\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u540e\u9762\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u4f9d\u636e\u8fd9\u4e2a\u7406\u8bba\uff0c\u5efa\u7acb\u8d77\u975e\u5e38\u591a\u7684\u6982\u5ff5\uff0c\u8fd9\u4e9b\u6982\u5ff5\u6709\u4e00\u4e9b\u662f\u6211\u4eec\u5e73\u65f6\u6240\u719f\u77e5\u7684\uff0c\u4f46\u662f\u4eceorder theory\u7684\uff0c\u6211\u4eec\u5c06\u4f1a\u83b7\u5f97\u65b0\u7684\u8ba4\u77e5\u3002 Binary relations \u4e0b\u9762\u662f\u7ef4\u57fa\u767e\u79d1 Binary relations \u4e2d\u6240\u603b\u7ed3\u7684\u4e00\u4e9bbinary relation\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u7684\u662forder\u3002 Symmetric Antisymmetric Connex Well-founded Has joins Has meets Equivalence relation \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 Preorder (Quasiorder) \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 Partial order \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 Total preorder \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 Total order \u2717 \u2713 \u2713 \u2717 \u2717 \u2717 Prewellordering \u2717 \u2717 \u2713 \u2713 \u2717 \u2717 Well-quasi-ordering \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 Well-ordering \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 Lattice \u2717 \u2713 \u2717 \u2717 \u2713 \u2713 Join-semilattice \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 Meet-semilattice \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 A \"\u2713\" indicates that the column property is required in the row definition. For example, the definition of an equivalence relation requires it to be symmetric. All definitions tacitly require transitivity and reflexivity . NOTE: transitivity \u662f\u53ef\u4ee5\u8fdb\u884c\u201c\u6392\u5e8f\u201d\u7684\u524d\u63d0\u6761\u4ef6\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Relation/Order-theory/Order-theory/#order#theory","text":"\u6392\u5e8f\u95ee\u9898 \u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u4e00\u7c7b\u5178\u578b\u95ee\u9898\uff0c \u6709\u5f88\u591a\u7684\u95ee\u9898\u6700\u7ec8\u90fd\u53ef\u4ee5\u8f6c\u6362\u4e3a\u6392\u5e8f\u95ee\u9898\uff0c\u6bd4\u5982\uff08TODO \u589e\u52a0\u4e00\u4e9b\u4f8b\u5b50\uff0c\u5982\u5faa\u73af\u4f9d\u8d56\u56fe\uff09\uff0c\u672c\u7ae0\u5c06\u7814\u7a76\u6392\u5e8f\u7684\u7406\u8bba\uff1aorder theory\u8fdb\u884c\u603b\u7ed3\uff0c\u7ecf\u8fc7\u672c\u7ae0\uff0c\u6211\u4eec\u5c06\u5bf9order\u6709\u4e00\u4e2a\u66f4\u52a0\u79d1\u5b66\u7684\u8ba4\u8bc6\u3002 Order theory\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5bf9\u4e8e\u5b9e\u73b0computation\u975e\u5e38\u91cd\u8981\uff0c\u6b63\u5982\u5728 Relation-structure-computation\\Make-it-computational \u7ae0\u8282\u6240\u603b\u7ed3\u7684: \u53ea\u6709**\u6709\u5e8f**\u624d\u80fd\u591f\u5b9e\u73b0computation\uff0c\u624d\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u6027","title":"Order theory"},{"location":"Relation-structure-computation/Relation/Order-theory/Order-theory/#wikipedia#order#theory","text":"","title":"wikipedia Order theory"},{"location":"Relation-structure-computation/Relation/Order-theory/Order-theory/#_1","text":"\u901a\u8fc7\u7ef4\u57fa\u767e\u79d1 Order theory \uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cOrder theory\u662f\u5efa\u7acb\u5728 binary relation \u4e4b\u4e0a\u7684\uff0c\u5b83\u6240\u7814\u7a76\u7684\u662f**\u540c\u4e00**\u96c6\u5408\u4e2d\u7684\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 Order theory \u5c06\u201c\u6392\u5e8f\u201d\u7684\u6982\u5ff5\u8fdb\u884c\u4e86\u62d3\u5e7f\uff0c\u5b83\u544a\u8bc9\u6211\u4eec\u96c6\u5408\u4e2d\u7684\u5143\u7d20\u6309\u7167\u600e\u6837\u7684\u5173\u7cfb\u6765\u8fdb\u884c\u7ec4\u7ec7\uff0c\u5219\u5b83\u4eec\u662f\u53ef\u4ee5\u8fdb\u884c\u201c\u6392\u5e8f\u201d\u7684\uff0c\u4e0b\u9762\u7684 Binary relations \u7ae0\u8282\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u540e\u9762\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u4f9d\u636e\u8fd9\u4e2a\u7406\u8bba\uff0c\u5efa\u7acb\u8d77\u975e\u5e38\u591a\u7684\u6982\u5ff5\uff0c\u8fd9\u4e9b\u6982\u5ff5\u6709\u4e00\u4e9b\u662f\u6211\u4eec\u5e73\u65f6\u6240\u719f\u77e5\u7684\uff0c\u4f46\u662f\u4eceorder theory\u7684\uff0c\u6211\u4eec\u5c06\u4f1a\u83b7\u5f97\u65b0\u7684\u8ba4\u77e5\u3002","title":"\u7b14\u8bb0"},{"location":"Relation-structure-computation/Relation/Order-theory/Order-theory/#binary#relations","text":"\u4e0b\u9762\u662f\u7ef4\u57fa\u767e\u79d1 Binary relations \u4e2d\u6240\u603b\u7ed3\u7684\u4e00\u4e9bbinary relation\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u7684\u662forder\u3002 Symmetric Antisymmetric Connex Well-founded Has joins Has meets Equivalence relation \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 Preorder (Quasiorder) \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 Partial order \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 Total preorder \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 Total order \u2717 \u2713 \u2713 \u2717 \u2717 \u2717 Prewellordering \u2717 \u2717 \u2713 \u2713 \u2717 \u2717 Well-quasi-ordering \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 Well-ordering \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 Lattice \u2717 \u2713 \u2717 \u2717 \u2713 \u2713 Join-semilattice \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 Meet-semilattice \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 A \"\u2713\" indicates that the column property is required in the row definition. For example, the definition of an equivalence relation requires it to be symmetric. All definitions tacitly require transitivity and reflexivity . NOTE: transitivity \u662f\u53ef\u4ee5\u8fdb\u884c\u201c\u6392\u5e8f\u201d\u7684\u524d\u63d0\u6761\u4ef6\u3002","title":"Binary relations"},{"location":"Relation-structure-computation/Relation/Order-theory/Partially-order/","text":"Partially ordered set \u201cpartially ordered set\u201d\u5373\u201c\u504f\u5e8f\u96c6\u201d\u3002 \u7ef4\u57fa\u767e\u79d1 Partially ordered set \u7b14\u8bb0 Partial order VS total order \u5728\u539f\u6587\u4e2d\u7684\u5bf9\u4e00\u6bb5\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002 Partial order\u7684 reflexive \u7684\u601d\u8003\uff1f \u5728\u539f\u6587\u7b2c\u4e8c\u6bb5\u4e2d\uff1a Formally, a partial order is any binary relation that is reflexive (each element is comparable to itself), antisymmetric (no two different elements precede each other), and transitive (the start of a chain of precedence relations must precede the end of the chain). \u5176\u4e2d\u5f3a\u8c03\u4e86partial order\u9700\u8981\u662f reflexive \uff0c\u4f46\u662f\u7b2c\u4e09\u6bb5\u4e2d\u6240\u679a\u4e3e\u7684 genealogical descendancy \u663e\u7136\u662f\u4e0d\u5177\u5907 reflexive \u7684\u3002","title":"Partially-order"},{"location":"Relation-structure-computation/Relation/Order-theory/Partially-order/#partially#ordered#set","text":"\u201cpartially ordered set\u201d\u5373\u201c\u504f\u5e8f\u96c6\u201d\u3002","title":"Partially ordered set"},{"location":"Relation-structure-computation/Relation/Order-theory/Partially-order/#partially#ordered#set_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Partially ordered set"},{"location":"Relation-structure-computation/Relation/Order-theory/Partially-order/#_1","text":"","title":"\u7b14\u8bb0"},{"location":"Relation-structure-computation/Relation/Order-theory/Partially-order/#partial#order#vs#total#order","text":"\u5728\u539f\u6587\u4e2d\u7684\u5bf9\u4e00\u6bb5\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"Partial order VS total order"},{"location":"Relation-structure-computation/Relation/Order-theory/Partially-order/#partial#order#reflexive","text":"\u5728\u539f\u6587\u7b2c\u4e8c\u6bb5\u4e2d\uff1a Formally, a partial order is any binary relation that is reflexive (each element is comparable to itself), antisymmetric (no two different elements precede each other), and transitive (the start of a chain of precedence relations must precede the end of the chain). \u5176\u4e2d\u5f3a\u8c03\u4e86partial order\u9700\u8981\u662f reflexive \uff0c\u4f46\u662f\u7b2c\u4e09\u6bb5\u4e2d\u6240\u679a\u4e3e\u7684 genealogical descendancy \u663e\u7136\u662f\u4e0d\u5177\u5907 reflexive \u7684\u3002","title":"Partial order\u7684 reflexive \u7684\u601d\u8003\uff1f"},{"location":"Relation-structure-computation/Relation/Order-theory/Preorder/","text":"Preorder \u201cpreorder\u201d\u5373\u201c\u524d\u5e8f\u201d \u7ef4\u57fa\u767e\u79d1 Preorder","title":"Preorder"},{"location":"Relation-structure-computation/Relation/Order-theory/Preorder/#preorder","text":"\u201cpreorder\u201d\u5373\u201c\u524d\u5e8f\u201d","title":"Preorder"},{"location":"Relation-structure-computation/Relation/Order-theory/Preorder/#preorder_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Preorder"},{"location":"Relation-structure-computation/Relation/Order-theory/Total-order/","text":"Total order \u201ctotal order\u201d\u5373\u5168\u5e8f\u3002 \u7ef4\u57fa\u767e\u79d1 Total order \u7b14\u8bb0 Formal definition \u539f\u6587\u4e2d\u7684\u5bf9\u201ctotal order\u201d\u7684\u201cformal definition\u201d\u5982\u4e0b\uff1a Formally, a binary relation $\\leq $ is a total order on a set X X if the following statements hold for all a,b a,b and c c in X X : Antisymmetry If a\\leq b a\\leq b and $ b\\leq a$ then a=b a=b ; Transitivity If a\\leq b a\\leq b and $ b\\leq c$then a\\leq c a\\leq c ; Connexity a\\leq b a\\leq b or b\\leq a b\\leq a }. $\\leq $\u8868\u793a\u7684\u662f\u4e00\u4e2abinary relation\uff0c\u800c\u975e\u6211\u4eec\u76f4\u89c2\u7406\u89e3\u7684\u201c\u5c0f\u4e8e\u7b49\u4e8e\u201d\u3002\u53e6\u5916\u4e00\u70b9\u662f\uff0c\u4e0a\u8ff0\u5b9a\u4e49\u4e2d\u201call\u201d\u5f3a\u8c03\u4e86\u96c6\u5408\u4e2d\u7684\u6240\u6709\u5143\u7d20\u90fd\u9700\u8981\u6ee1\u8db3\u8fd9\u79cd\u5173\u7cfb\u3002 Chain \u5bf9\u201ctotal order\u201d\u7684\u76f4\u89c2\u7406\u89e3\u662f\u5b83\u662f\u4e00\u4e2a\u80fd\u591f\u5c06\u8fd9\u4e2a\u96c6\u5408\u201c\u4e32\u201d\u8d77\u6765\u7684\u5173\u7cfb\uff0c\u8fd9\u6837\u8fd9\u4e2a\u96c6\u5408\u5c31\u5f62\u6210\u4e86\u201cchain\u201d\uff0c\u663e\u7136\uff0c\u201cchain\u201d\u662f\u4ece\u89c6\u89c9\u76f4\u89c2\u89d2\u5ea6\u6765\u5bf9\u5176\u8fdb\u884c\u547d\u540d\u7684\uff0c\u5bf9\u6b64\u5728\u539f\u6587\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A set paired with a total order is called a chain , a totally ordered set , a simply ordered set , or a linearly ordered set . \u539f\u6587\u7684 Chains \u7ae0\u8282\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002 Connex relation \u201cconnex relation\u201d\u5373\u201c\u8fde\u901a\u5173\u7cfb\u201d \u7ef4\u57fa\u767e\u79d1 Connex relation","title":"Total-order"},{"location":"Relation-structure-computation/Relation/Order-theory/Total-order/#total#order","text":"\u201ctotal order\u201d\u5373\u5168\u5e8f\u3002","title":"Total order"},{"location":"Relation-structure-computation/Relation/Order-theory/Total-order/#total#order_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Total order"},{"location":"Relation-structure-computation/Relation/Order-theory/Total-order/#_1","text":"","title":"\u7b14\u8bb0"},{"location":"Relation-structure-computation/Relation/Order-theory/Total-order/#formal#definition","text":"\u539f\u6587\u4e2d\u7684\u5bf9\u201ctotal order\u201d\u7684\u201cformal definition\u201d\u5982\u4e0b\uff1a Formally, a binary relation $\\leq $ is a total order on a set X X if the following statements hold for all a,b a,b and c c in X X : Antisymmetry If a\\leq b a\\leq b and $ b\\leq a$ then a=b a=b ; Transitivity If a\\leq b a\\leq b and $ b\\leq c$then a\\leq c a\\leq c ; Connexity a\\leq b a\\leq b or b\\leq a b\\leq a }. $\\leq $\u8868\u793a\u7684\u662f\u4e00\u4e2abinary relation\uff0c\u800c\u975e\u6211\u4eec\u76f4\u89c2\u7406\u89e3\u7684\u201c\u5c0f\u4e8e\u7b49\u4e8e\u201d\u3002\u53e6\u5916\u4e00\u70b9\u662f\uff0c\u4e0a\u8ff0\u5b9a\u4e49\u4e2d\u201call\u201d\u5f3a\u8c03\u4e86\u96c6\u5408\u4e2d\u7684\u6240\u6709\u5143\u7d20\u90fd\u9700\u8981\u6ee1\u8db3\u8fd9\u79cd\u5173\u7cfb\u3002","title":"Formal definition"},{"location":"Relation-structure-computation/Relation/Order-theory/Total-order/#chain","text":"\u5bf9\u201ctotal order\u201d\u7684\u76f4\u89c2\u7406\u89e3\u662f\u5b83\u662f\u4e00\u4e2a\u80fd\u591f\u5c06\u8fd9\u4e2a\u96c6\u5408\u201c\u4e32\u201d\u8d77\u6765\u7684\u5173\u7cfb\uff0c\u8fd9\u6837\u8fd9\u4e2a\u96c6\u5408\u5c31\u5f62\u6210\u4e86\u201cchain\u201d\uff0c\u663e\u7136\uff0c\u201cchain\u201d\u662f\u4ece\u89c6\u89c9\u76f4\u89c2\u89d2\u5ea6\u6765\u5bf9\u5176\u8fdb\u884c\u547d\u540d\u7684\uff0c\u5bf9\u6b64\u5728\u539f\u6587\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A set paired with a total order is called a chain , a totally ordered set , a simply ordered set , or a linearly ordered set . \u539f\u6587\u7684 Chains \u7ae0\u8282\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"Chain"},{"location":"Relation-structure-computation/Relation/Order-theory/Total-order/#connex#relation","text":"\u201cconnex relation\u201d\u5373\u201c\u8fde\u901a\u5173\u7cfb\u201d","title":"Connex relation"},{"location":"Relation-structure-computation/Relation/Order-theory/Total-order/#connex#relation_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Connex relation"},{"location":"Relation-structure-computation/Relation/Order-theory/Well-order/","text":"Well-order \u5728\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1 Recursive definition \u7684 Form of recursive definitions \u7ae0\u8282\u65f6\uff0c\u53d1\u73b0\u4e86\u8fd9\u4e2a\u6982\u5ff5\uff0c\u9042\u5bf9\u5b83\u8fdb\u884c\u4e86\u6574\u7406\u3002 \u7ef4\u57fa\u767e\u79d1 Well-order \u7b14\u8bb0 Well-order VS total order \u539f\u6587\u5bf9 Well-order \u7684\u5b9a\u4e49\u5982\u4e0b\uff1a In mathematics , a well-order (or well-ordering or well-order relation ) on a set S is a total order on S with the property that every non-empty subset of S has a least element in this ordering. \u663e\u7136\uff0c\u5b83\u662f\u5728 total order \u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u4e00\u4e2a\u9650\u5236\uff0c\u90a3\u8fd9\u9650\u5236\u6709\u4f55\u610f\u4e49\u5462\uff1f\u662f\u4fdd\u8bc1 S \u7684\u6bcf\u4e2a\u5b50\u96c6\u4f9d\u7136\u6ee1\u8db3 total order \uff1f \u5176\u5b9e\u7406\u89e3well order\u7684\u4e00\u4e2a\u5f88\u597d\u7684\u65b9\u6cd5\u662f\u7ed3\u5408\u5b9e\u4f8b\u6765\u8fdb\u884c\u7406\u89e3\uff0cset theory\u6240\u63cf\u8ff0\u7684tree\u5c31\u662f\u4e00\u4e2a\u5178\u578b\u7684well order\uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Tree (set theory) \uff1a In set theory , a tree is a partially ordered set ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. Frequently trees are assumed to have only one root (i.e. minimal element ), as the typical questions investigated in this field are easily reduced to questions about single-rooted trees. \u7ef4\u57fa\u767e\u79d1Well-founded relation \u5e76\u6ca1\u6709\u7406\u89e3\u7ef4\u57fa\u767e\u79d1 Well-founded relation \u3002","title":"Well-order"},{"location":"Relation-structure-computation/Relation/Order-theory/Well-order/#well-order","text":"\u5728\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1 Recursive definition \u7684 Form of recursive definitions \u7ae0\u8282\u65f6\uff0c\u53d1\u73b0\u4e86\u8fd9\u4e2a\u6982\u5ff5\uff0c\u9042\u5bf9\u5b83\u8fdb\u884c\u4e86\u6574\u7406\u3002","title":"Well-order"},{"location":"Relation-structure-computation/Relation/Order-theory/Well-order/#well-order_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Well-order"},{"location":"Relation-structure-computation/Relation/Order-theory/Well-order/#_1","text":"","title":"\u7b14\u8bb0"},{"location":"Relation-structure-computation/Relation/Order-theory/Well-order/#well-order#vs#total#order","text":"\u539f\u6587\u5bf9 Well-order \u7684\u5b9a\u4e49\u5982\u4e0b\uff1a In mathematics , a well-order (or well-ordering or well-order relation ) on a set S is a total order on S with the property that every non-empty subset of S has a least element in this ordering. \u663e\u7136\uff0c\u5b83\u662f\u5728 total order \u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u4e00\u4e2a\u9650\u5236\uff0c\u90a3\u8fd9\u9650\u5236\u6709\u4f55\u610f\u4e49\u5462\uff1f\u662f\u4fdd\u8bc1 S \u7684\u6bcf\u4e2a\u5b50\u96c6\u4f9d\u7136\u6ee1\u8db3 total order \uff1f \u5176\u5b9e\u7406\u89e3well order\u7684\u4e00\u4e2a\u5f88\u597d\u7684\u65b9\u6cd5\u662f\u7ed3\u5408\u5b9e\u4f8b\u6765\u8fdb\u884c\u7406\u89e3\uff0cset theory\u6240\u63cf\u8ff0\u7684tree\u5c31\u662f\u4e00\u4e2a\u5178\u578b\u7684well order\uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Tree (set theory) \uff1a In set theory , a tree is a partially ordered set ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. Frequently trees are assumed to have only one root (i.e. minimal element ), as the typical questions investigated in this field are easily reduced to questions about single-rooted trees.","title":"Well-order VS  total order"},{"location":"Relation-structure-computation/Relation/Order-theory/Well-order/#well-founded#relation","text":"\u5e76\u6ca1\u6709\u7406\u89e3\u7ef4\u57fa\u767e\u79d1 Well-founded relation \u3002","title":"\u7ef4\u57fa\u767e\u79d1Well-founded relation"},{"location":"Relation-structure-computation/Relation/Relation/","text":"Relation \u73b0\u4ee3\u6570\u5b66\u7684\u5f88\u591a\u6982\u5ff5\u90fd\u662f\u5efa\u7acb\u5728 Set \u7684\u57fa\u7840\u4e4b\u4e0a\uff0c\u6bd4\u5982\u6211\u4eec\u719f\u77e5\u7684 Function \u6982\u5ff5\uff0cRelation\u4e5f\u662f\u5982\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u4e2d\u4ecb\u7ecdRelation\u7684\u662f Finitary relation \u3002 Definition of relation \u5173\u4e8erelation\u7684\u5b9a\u4e49\uff0c\u5728 Discrete-math\\Book-Discrete-Mathematics-and-Its-Applications\\Chpater-9-Relations \u4e2d\u7ed9\u51fa\u4e86\uff1a Relationships between elements of sets occur in many contexts. Every day we deal with relationships such as those between a business and its telephone number, an employee and his or her salary, a person and a relative, and so on. In mathematics we study relationships such as those between a positive integer and one that it divides, an integer and one that it is congruent to modulo 5, a real number and one that is larger than it, a real number x and the value f(x) where f is a function, and so on. Relationships such as that between a program and a variable it uses, and that between a computer language and a valid statement in this language often arise in computer science. Relationships between elements of sets are represented using the structure called a relation , which is just as sub set of the Cartesian product of the sets. Relations can be used to solve problems such as determining which pairs of cities are linked by airline flights in a network, finding a viable order for the different phases of a complicated project, or producing a useful way to store information in computer databases. Relationship\u548crelation \u9700\u8981\u533a\u5206\u6e05\u695arelationship\u548crelation\uff1a relationship\uff1a\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u201c\u5173\u7cfb\u201d\uff0c\u6bd4\u5982\u201c\u5927\u4e8e\u201d\u3001\u201c\u5c0f\u4e8e\u201d relation\uff1a\u5b83\u662fdiscrete math\u4e2d\u5bf9relationship\u7684\u8868\u793a\uff0c\u5b83\u662f\u4e00\u4e2aset \u4e25\u683c\u6765\u8bf4\uff0c\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u201c\u5173\u7cfb\u201d\u5e94\u8be5\u4f7f\u7528\u201crelationship\u201d\u8fd9\u4e2a\u8bcd\u8bed\u3002 wikipedia Finitary relation Finitary relation \u7684\u542b\u4e49\u662f\u201c\u6709\u9650\u5143\u5173\u7cfb\u201d\uff0c\u6211\u4eec\u4e3b\u8981\u8ba8\u8bba\u7684\u662f Binary relation \uff08\u4e8c\u5143\u5173\u7cfb\uff09\u3002 wikipedia Binary relation In mathematics , a binary relation over two sets A and B is a set of ordered pairs ( a , b ), consisting of elements a of A and elements b of B . That is, it is a subset of the Cartesian product A \u00d7 B . It encodes the information of relation: an element a is related to an element b , if and only if the pair ( a , b ) belongs to the set. NOTE: relation\u7684\u672c\u8d28\u4e0a\u662fset\u3002 \u67e5\u770b Binary relation \u548c Function \u53ef\u77e5\uff0c Binary relation \u662f\u4e00\u4e2a\u6bd4 Function \u66f4\u52a0\u5bbd\u6cdb\u7684\u6982\u5ff5\uff1a Function \u662f\u4e00\u79cd Binary relation \uff0c\u4f46\u662f\u53cd\u4e4b\u5219\u4e0d\u4e00\u5b9a\u6210\u7acb\u3002 \u5173\u7cfb\u7684\u6027\u8d28 Transitive relation \"transitive\"\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u4f20\u9012\u6027\u201d\uff0ctransitive\u7279\u6027\u662f\u4e00\u4e2a\u975e\u5e38\u4f18\u826f\u7684\u7279\u6027\uff0c\u5b83\u662f\u5f88\u591a**computation**\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u4e0b\u9762\u4f1a\u5bf9\u6b64\u8fdb\u884c\u8bf4\u660e\u3002 Transitivity is a key property of both partial orders and equivalence relations \u3002 Application in computation 1) Order \u8981\u60f3\u8fdb\u884c\u6392\u5e8f\uff0c\u5219\u5173\u7cfb\u5fc5\u987b\u5177\u5907transitive\u7279\u6027\u3002 2) Rewrite\uff08proof\uff09 \u6309\u7167\u5de5\u7a0b automata-and-formal-language \u4e2d\u7684\u601d\u60f3\uff0crewrite\u7684\u8fc7\u7a0b\u5176\u5b9e\u5c31\u662f\u201c\u63a8\u5bfc\u201d\u7684\u8fc7\u7a0b\u3002 \u4e00\u4e2a\u4f8b\u5b50\u5c31\u662f\uff1a stock\uff1a\u8bc1\u5238\u3001\u80a1\u7968\uff1b \u80a1\u7968\uff1aA\u80a1\u3001B\u80a1; \u5219stock\uff1aA\u80a1\u3001B\u80a1\u3002 3) Closure \u53c2\u89c1 Closures-of-relation.md 4) Recursive \u5177\u5907\u4f20\u9012\u6027\u7684relation\uff0c\u5b83\u7684computation\u53ef\u4ee5\u91c7\u7528recursion\uff1a\u91cd\u590d\u5730\u5e94\u7528\u8fd9\u4e2arelation\u3002 \u5177\u5907\u4f20\u9012\u6027\u7684relation\uff0c\u5b83\u6240\u5f62\u6210\u7684\u7ed3\u6784\u5f80\u5f80\u5177\u5907\u9012\u5f52\u6027\u3002 \u5728 Relation-structure-computation\\Computation\\index.md \u7684\u201crelation and recursion\u201d\u7ae0\u8282\uff0c\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 Reflexive relation \u201creflexive\u201d\u5373\u201c\u53cd\u5c04\u6027\u201d\u3002 Symmetric relation \u201csymmetric\u201d\u5373\u201c\u5bf9\u79f0\u201d Relational algebra \u5173\u7cfb\u4ee3\u6570\uff0c\u8fd9\u662fDBMS\u7684\u57fa\u7840\u3002 TO READ wikipedia Hasse diagram","title":"Introduction"},{"location":"Relation-structure-computation/Relation/Relation/#relation","text":"\u73b0\u4ee3\u6570\u5b66\u7684\u5f88\u591a\u6982\u5ff5\u90fd\u662f\u5efa\u7acb\u5728 Set \u7684\u57fa\u7840\u4e4b\u4e0a\uff0c\u6bd4\u5982\u6211\u4eec\u719f\u77e5\u7684 Function \u6982\u5ff5\uff0cRelation\u4e5f\u662f\u5982\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u4e2d\u4ecb\u7ecdRelation\u7684\u662f Finitary relation \u3002","title":"Relation"},{"location":"Relation-structure-computation/Relation/Relation/#definition#of#relation","text":"\u5173\u4e8erelation\u7684\u5b9a\u4e49\uff0c\u5728 Discrete-math\\Book-Discrete-Mathematics-and-Its-Applications\\Chpater-9-Relations \u4e2d\u7ed9\u51fa\u4e86\uff1a Relationships between elements of sets occur in many contexts. Every day we deal with relationships such as those between a business and its telephone number, an employee and his or her salary, a person and a relative, and so on. In mathematics we study relationships such as those between a positive integer and one that it divides, an integer and one that it is congruent to modulo 5, a real number and one that is larger than it, a real number x and the value f(x) where f is a function, and so on. Relationships such as that between a program and a variable it uses, and that between a computer language and a valid statement in this language often arise in computer science. Relationships between elements of sets are represented using the structure called a relation , which is just as sub set of the Cartesian product of the sets. Relations can be used to solve problems such as determining which pairs of cities are linked by airline flights in a network, finding a viable order for the different phases of a complicated project, or producing a useful way to store information in computer databases.","title":"Definition of relation"},{"location":"Relation-structure-computation/Relation/Relation/#relationshiprelation","text":"\u9700\u8981\u533a\u5206\u6e05\u695arelationship\u548crelation\uff1a relationship\uff1a\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u201c\u5173\u7cfb\u201d\uff0c\u6bd4\u5982\u201c\u5927\u4e8e\u201d\u3001\u201c\u5c0f\u4e8e\u201d relation\uff1a\u5b83\u662fdiscrete math\u4e2d\u5bf9relationship\u7684\u8868\u793a\uff0c\u5b83\u662f\u4e00\u4e2aset \u4e25\u683c\u6765\u8bf4\uff0c\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u201c\u5173\u7cfb\u201d\u5e94\u8be5\u4f7f\u7528\u201crelationship\u201d\u8fd9\u4e2a\u8bcd\u8bed\u3002","title":"Relationship\u548crelation"},{"location":"Relation-structure-computation/Relation/Relation/#wikipedia#finitary#relation","text":"Finitary relation \u7684\u542b\u4e49\u662f\u201c\u6709\u9650\u5143\u5173\u7cfb\u201d\uff0c\u6211\u4eec\u4e3b\u8981\u8ba8\u8bba\u7684\u662f Binary relation \uff08\u4e8c\u5143\u5173\u7cfb\uff09\u3002","title":"wikipedia Finitary relation"},{"location":"Relation-structure-computation/Relation/Relation/#wikipedia#binary#relation","text":"In mathematics , a binary relation over two sets A and B is a set of ordered pairs ( a , b ), consisting of elements a of A and elements b of B . That is, it is a subset of the Cartesian product A \u00d7 B . It encodes the information of relation: an element a is related to an element b , if and only if the pair ( a , b ) belongs to the set. NOTE: relation\u7684\u672c\u8d28\u4e0a\u662fset\u3002 \u67e5\u770b Binary relation \u548c Function \u53ef\u77e5\uff0c Binary relation \u662f\u4e00\u4e2a\u6bd4 Function \u66f4\u52a0\u5bbd\u6cdb\u7684\u6982\u5ff5\uff1a Function \u662f\u4e00\u79cd Binary relation \uff0c\u4f46\u662f\u53cd\u4e4b\u5219\u4e0d\u4e00\u5b9a\u6210\u7acb\u3002","title":"wikipedia Binary relation"},{"location":"Relation-structure-computation/Relation/Relation/#_1","text":"","title":"\u5173\u7cfb\u7684\u6027\u8d28"},{"location":"Relation-structure-computation/Relation/Relation/#transitive#relation","text":"\"transitive\"\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u4f20\u9012\u6027\u201d\uff0ctransitive\u7279\u6027\u662f\u4e00\u4e2a\u975e\u5e38\u4f18\u826f\u7684\u7279\u6027\uff0c\u5b83\u662f\u5f88\u591a**computation**\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u4e0b\u9762\u4f1a\u5bf9\u6b64\u8fdb\u884c\u8bf4\u660e\u3002 Transitivity is a key property of both partial orders and equivalence relations \u3002","title":"Transitive relation"},{"location":"Relation-structure-computation/Relation/Relation/#application#in#computation","text":"1) Order \u8981\u60f3\u8fdb\u884c\u6392\u5e8f\uff0c\u5219\u5173\u7cfb\u5fc5\u987b\u5177\u5907transitive\u7279\u6027\u3002 2) Rewrite\uff08proof\uff09 \u6309\u7167\u5de5\u7a0b automata-and-formal-language \u4e2d\u7684\u601d\u60f3\uff0crewrite\u7684\u8fc7\u7a0b\u5176\u5b9e\u5c31\u662f\u201c\u63a8\u5bfc\u201d\u7684\u8fc7\u7a0b\u3002 \u4e00\u4e2a\u4f8b\u5b50\u5c31\u662f\uff1a stock\uff1a\u8bc1\u5238\u3001\u80a1\u7968\uff1b \u80a1\u7968\uff1aA\u80a1\u3001B\u80a1; \u5219stock\uff1aA\u80a1\u3001B\u80a1\u3002 3) Closure \u53c2\u89c1 Closures-of-relation.md 4) Recursive \u5177\u5907\u4f20\u9012\u6027\u7684relation\uff0c\u5b83\u7684computation\u53ef\u4ee5\u91c7\u7528recursion\uff1a\u91cd\u590d\u5730\u5e94\u7528\u8fd9\u4e2arelation\u3002 \u5177\u5907\u4f20\u9012\u6027\u7684relation\uff0c\u5b83\u6240\u5f62\u6210\u7684\u7ed3\u6784\u5f80\u5f80\u5177\u5907\u9012\u5f52\u6027\u3002 \u5728 Relation-structure-computation\\Computation\\index.md \u7684\u201crelation and recursion\u201d\u7ae0\u8282\uff0c\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Application in computation"},{"location":"Relation-structure-computation/Relation/Relation/#reflexive#relation","text":"\u201creflexive\u201d\u5373\u201c\u53cd\u5c04\u6027\u201d\u3002","title":"Reflexive relation"},{"location":"Relation-structure-computation/Relation/Relation/#symmetric#relation","text":"\u201csymmetric\u201d\u5373\u201c\u5bf9\u79f0\u201d","title":"Symmetric relation"},{"location":"Relation-structure-computation/Relation/Relation/#relational#algebra","text":"\u5173\u7cfb\u4ee3\u6570\uff0c\u8fd9\u662fDBMS\u7684\u57fa\u7840\u3002","title":"Relational algebra"},{"location":"Relation-structure-computation/Relation/Relation/#to#read","text":"wikipedia Hasse diagram","title":"TO READ"},{"location":"Relation-structure-computation/Relation/Relation/Closures-of-relation/","text":"Closures of Relations Closure (mathematics) Thoughts transitive closure\uff1a\u9a8c\u8bc1relation\u4e0d\u65ad\u5730\u8fdb\u884cexpand\u3002","title":"Closures-of-relation"},{"location":"Relation-structure-computation/Relation/Relation/Closures-of-relation/#closures#of#relations","text":"","title":"Closures of Relations"},{"location":"Relation-structure-computation/Relation/Relation/Closures-of-relation/#closure#mathematics","text":"","title":"Closure (mathematics)"},{"location":"Relation-structure-computation/Relation/Relation/Closures-of-relation/#thoughts","text":"transitive closure\uff1a\u9a8c\u8bc1relation\u4e0d\u65ad\u5730\u8fdb\u884cexpand\u3002","title":"Thoughts"},{"location":"Relation-structure-computation/Relation/Relation/Representation-of-relation/","text":"Representation/implementation/of relation \u5728computer science\u4e2d\uff0c\u5982\u4f55\u5b9e\u73b0relation\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u95ee\u9898\uff0c\u5728computer science\u7684\u4e0d\u540c\u9886\u57df\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u8bed\u8a00\u6765\uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u6211\u4eec\u5f80\u5f80\u63cf\u8ff0\u65b9\u5f0f\u5f80\u5f80\u662fhuman-readable \u4e14 machine-readable\u7684\u3002 DBMS \u5728DBMS\u4e2d\uff0c\u5e7f\u6cdb\u91c7\u7528entity-relationship model\uff0c\u4f7f\u7528table\u6765\u4fdd\u5b58relation OOP \u5728OOP\u4e2d\uff0cinheritance\u3001combination\u53ef\u4ee5\u63cf\u8ff0\u5927\u90e8\u5206relationship Book-Discrete-Mathematics-and-Its-Applications \u5728 Book-Discrete-Mathematics-and-Its-Applications\\Chpater-9-Relations\\9.3-Representing-Relations \u4e2d\u7ed9\u51fa\u4e86\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\uff1a Representing Relations Using Matrices Representing Relations Using Digraphs( directed graphs ) directed graph\u662f\u4e00\u79cd\u975e\u5e38\u5f3a\u5927\u7684\u8868\u8fbe\u65b9\u5f0f\uff0c\u5728\u672c\u4e66\u4e2d\uff0c\u57fa\u672c\u4e0a\u90fd\u662f\u57fa\u4e8edirected graph\u6765\u63cf\u8ff0relation\u7684\u3002\u5728 Relation-structure-computation\\Structure\\Data-structure\\Graph \u4e2d\u5bf9graph\u8fdb\u884c\u63cf\u8ff0\u3002 ASDL cpython \u4f7f\u7528ASDL\u6765\u63cf\u8ff0\u81ea\u5df1\u7684AST\uff0c\u5173\u4e8eASDL\uff0c\u53c2\u89c1\uff1a https://www.usenix.org/legacy/publications/library/proceedings/dsl97/full_papers/wang/wang.pdf http://www.oilshell.org/blog/2016/12/11.html https://stackoverflow.com/questions/8873126/zephyr-asdl-abstract-syntax-description-language https://devguide.python.org/compiler/ https://github.com/python/cpython/blob/master/Parser/Python.asdl http://asdl.sourceforge.net/ https://www.cs.princeton.edu/research/techreps/TR-554-97 \u663e\u7136\uff0c\u5b83\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6811\u63cf\u8ff0\u8bed\u8a00\u3002 https://en.wikipedia.org/wiki/Algebraic_data_type https://en.wikipedia.org/wiki/Abstract_syntax Grammar Grammar\u662f\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u63cf\u8ff0\uff0c\u4f7f\u7528 production \u6765\u8868\u793agrammar\u3002 Function \u6309\u7167\u6570\u5b66\u5b9a\u4e49\uff0crelation\u662f\u96c6\u5408\u7684\u7b1b\u5361\u5c14\u79ef\u7684\u5b50\u96c6\u3002\u8868\u793a\u4e00\u4e2arelation\uff0c\u4e00\u79cd\u53ef\u9009\u7684\u65b9\u5f0f\u662f\u679a\u4e3e\u51fa\u6240\u6709\u7684ordered pair\uff0c\u53ef\u4ee5\u5c06\u5b83\u4eec\u7ec4\u7ec7\u6210graph\u3002\u53e6\u5916\u4e00\u79cd\u65b9\u5f0f\u662f\u4f7f\u7528\u7c7b\u4f3c\u4e8egrammar\u3001function\u7684\u5de5\u5177\u6765\u63cf\u8ff0relation\u7684\u201c\u751f\u6210\u65b9\u6cd5\u201d\uff0c\u6240\u8c13\u201c\u751f\u6210\u65b9\u6cd5\u201d\u5176\u5b9e\u5c31\u662f\u786e\u5b9a\u54ea\u4e9bordered pair\u662f\u7b26\u5408\u8be5\u5173\u7cfb\u7684\u3002","title":"Representation-of-relation"},{"location":"Relation-structure-computation/Relation/Relation/Representation-of-relation/#representationimplementationof#relation","text":"\u5728computer science\u4e2d\uff0c\u5982\u4f55\u5b9e\u73b0relation\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u95ee\u9898\uff0c\u5728computer science\u7684\u4e0d\u540c\u9886\u57df\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u8bed\u8a00\u6765\uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u6211\u4eec\u5f80\u5f80\u63cf\u8ff0\u65b9\u5f0f\u5f80\u5f80\u662fhuman-readable \u4e14 machine-readable\u7684\u3002","title":"Representation/implementation/of relation"},{"location":"Relation-structure-computation/Relation/Relation/Representation-of-relation/#dbms","text":"\u5728DBMS\u4e2d\uff0c\u5e7f\u6cdb\u91c7\u7528entity-relationship model\uff0c\u4f7f\u7528table\u6765\u4fdd\u5b58relation","title":"DBMS"},{"location":"Relation-structure-computation/Relation/Relation/Representation-of-relation/#oop","text":"\u5728OOP\u4e2d\uff0cinheritance\u3001combination\u53ef\u4ee5\u63cf\u8ff0\u5927\u90e8\u5206relationship","title":"OOP"},{"location":"Relation-structure-computation/Relation/Relation/Representation-of-relation/#book-discrete-mathematics-and-its-applications","text":"\u5728 Book-Discrete-Mathematics-and-Its-Applications\\Chpater-9-Relations\\9.3-Representing-Relations \u4e2d\u7ed9\u51fa\u4e86\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\uff1a Representing Relations Using Matrices Representing Relations Using Digraphs( directed graphs ) directed graph\u662f\u4e00\u79cd\u975e\u5e38\u5f3a\u5927\u7684\u8868\u8fbe\u65b9\u5f0f\uff0c\u5728\u672c\u4e66\u4e2d\uff0c\u57fa\u672c\u4e0a\u90fd\u662f\u57fa\u4e8edirected graph\u6765\u63cf\u8ff0relation\u7684\u3002\u5728 Relation-structure-computation\\Structure\\Data-structure\\Graph \u4e2d\u5bf9graph\u8fdb\u884c\u63cf\u8ff0\u3002","title":"Book-Discrete-Mathematics-and-Its-Applications"},{"location":"Relation-structure-computation/Relation/Relation/Representation-of-relation/#asdl","text":"cpython \u4f7f\u7528ASDL\u6765\u63cf\u8ff0\u81ea\u5df1\u7684AST\uff0c\u5173\u4e8eASDL\uff0c\u53c2\u89c1\uff1a https://www.usenix.org/legacy/publications/library/proceedings/dsl97/full_papers/wang/wang.pdf http://www.oilshell.org/blog/2016/12/11.html https://stackoverflow.com/questions/8873126/zephyr-asdl-abstract-syntax-description-language https://devguide.python.org/compiler/ https://github.com/python/cpython/blob/master/Parser/Python.asdl http://asdl.sourceforge.net/ https://www.cs.princeton.edu/research/techreps/TR-554-97 \u663e\u7136\uff0c\u5b83\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6811\u63cf\u8ff0\u8bed\u8a00\u3002 https://en.wikipedia.org/wiki/Algebraic_data_type https://en.wikipedia.org/wiki/Abstract_syntax","title":"ASDL"},{"location":"Relation-structure-computation/Relation/Relation/Representation-of-relation/#grammar","text":"Grammar\u662f\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u63cf\u8ff0\uff0c\u4f7f\u7528 production \u6765\u8868\u793agrammar\u3002","title":"Grammar"},{"location":"Relation-structure-computation/Relation/Relation/Representation-of-relation/#function","text":"\u6309\u7167\u6570\u5b66\u5b9a\u4e49\uff0crelation\u662f\u96c6\u5408\u7684\u7b1b\u5361\u5c14\u79ef\u7684\u5b50\u96c6\u3002\u8868\u793a\u4e00\u4e2arelation\uff0c\u4e00\u79cd\u53ef\u9009\u7684\u65b9\u5f0f\u662f\u679a\u4e3e\u51fa\u6240\u6709\u7684ordered pair\uff0c\u53ef\u4ee5\u5c06\u5b83\u4eec\u7ec4\u7ec7\u6210graph\u3002\u53e6\u5916\u4e00\u79cd\u65b9\u5f0f\u662f\u4f7f\u7528\u7c7b\u4f3c\u4e8egrammar\u3001function\u7684\u5de5\u5177\u6765\u63cf\u8ff0relation\u7684\u201c\u751f\u6210\u65b9\u6cd5\u201d\uff0c\u6240\u8c13\u201c\u751f\u6210\u65b9\u6cd5\u201d\u5176\u5b9e\u5c31\u662f\u786e\u5b9a\u54ea\u4e9bordered pair\u662f\u7b26\u5408\u8be5\u5173\u7cfb\u7684\u3002","title":"Function"},{"location":"Relation-structure-computation/Structure/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u9996\u5148\u63cf\u8ff0\u5e7f\u4e49\u7684structure\u7684\u542b\u4e49\uff0c\u7136\u540e\u63cf\u8ff0computer science\u4e2d\u7684data structure\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/#_1","text":"\u672c\u7ae0\u9996\u5148\u63cf\u8ff0\u5e7f\u4e49\u7684structure\u7684\u542b\u4e49\uff0c\u7136\u540e\u63cf\u8ff0computer science\u4e2d\u7684data structure\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0 data structure\uff0c\u4e3b\u8981\u662f\u57fa\u4e8e Wikipedia Data structures \u7684\u7ec4\u7ec7\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u6211\u7684\u5b9e\u8df5\uff0c\u8fdb\u884c\u4e86\u4e00\u5b9a\u7684\u6269\u5145\u3002 \u5bf9\u4e8e\u6bcf\u79cddata structure\uff0c\u4f1a\u5bf9\u5b83\u7684\u5404\u65b9\u9762\uff08\u63cf\u8ff0\u3001\u8868\u793a\u3001\u5b9e\u73b0\u3001\u64cd\u4f5c\uff09\u8fdb\u884c\u603b\u7ed3\u3002 \u9700\u8981\u5bf9\u5404\u79cddata structure\u8fdb\u884c\u5bf9\u6bd4\u4ee5\u7a81\u51fa**\u7279\u6027**\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u9009\u62e9data structure\u63d0\u4f9b\u53c2\u8003\u610f\u89c1\u3002 \u63d0\u4f9b\u5404\u79cddata structure\u7684\u5b9e\u73b0\uff0c\u5bf9\u4e8e\u6bcf\u79cd\u7ed3\u6784\uff0c\u5c3d\u53ef\u80fd\u5730\u63d0\u4f9bpython\u3001 c++ \u3001c\u5b9e\u73b0\u3002 \u603b\u7684\u6765\u8bf4\uff0c\u672c\u7ae0\u53ef\u4ee5\u4f5c\u4e3a data structure \u7684\u77e5\u8bc6\u5e93\uff0c\u63d0\u4f9b\u6587\u6863(\u7406\u8bba\u77e5\u8bc6)\u4e0e\u5b9e\u73b0(\u7a0b\u5e8f\u4ee3\u7801)\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/#_1","text":"\u672c\u7ae0\u63cf\u8ff0 data structure\uff0c\u4e3b\u8981\u662f\u57fa\u4e8e Wikipedia Data structures \u7684\u7ec4\u7ec7\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u6211\u7684\u5b9e\u8df5\uff0c\u8fdb\u884c\u4e86\u4e00\u5b9a\u7684\u6269\u5145\u3002 \u5bf9\u4e8e\u6bcf\u79cddata structure\uff0c\u4f1a\u5bf9\u5b83\u7684\u5404\u65b9\u9762\uff08\u63cf\u8ff0\u3001\u8868\u793a\u3001\u5b9e\u73b0\u3001\u64cd\u4f5c\uff09\u8fdb\u884c\u603b\u7ed3\u3002 \u9700\u8981\u5bf9\u5404\u79cddata structure\u8fdb\u884c\u5bf9\u6bd4\u4ee5\u7a81\u51fa**\u7279\u6027**\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u9009\u62e9data structure\u63d0\u4f9b\u53c2\u8003\u610f\u89c1\u3002 \u63d0\u4f9b\u5404\u79cddata structure\u7684\u5b9e\u73b0\uff0c\u5bf9\u4e8e\u6bcf\u79cd\u7ed3\u6784\uff0c\u5c3d\u53ef\u80fd\u5730\u63d0\u4f9bpython\u3001 c++ \u3001c\u5b9e\u73b0\u3002 \u603b\u7684\u6765\u8bf4\uff0c\u672c\u7ae0\u53ef\u4ee5\u4f5c\u4e3a data structure \u7684\u77e5\u8bc6\u5e93\uff0c\u63d0\u4f9b\u6587\u6863(\u7406\u8bba\u77e5\u8bc6)\u4e0e\u5b9e\u73b0(\u7a0b\u5e8f\u4ee3\u7801)\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/","text":"TODO algorithms use memory to gain performance improvement \u4f7f\u7528\u7a7a\u95f4\u6362\u53d6\u65f6\u95f4 https://www.sciencedirect.com/topics/computer-science/performance-gain https://stackoverflow.com/questions/1898161/memory-vs-performance lintcode https://www.zhihu.com/question/31218682 hash Hash table \u76f8\u5173\uff1a https://www.infoq.com/articles/redis-time-series Hash functions Hash list What exactly (and precisely) is \u201chash?\u201d AVL tree \u76f8\u5173\uff1a https://opensourceforu.com/2016/03/the-life-of-a-process/ https://en.wikipedia.org/wiki/AVL_tree Bit field https://en.wikipedia.org/wiki/Bit_field \u5173\u4e8ebit field\uff0c \u5728Unix OS\u4e2d\u4f7f\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u6211\u4e4b\u524d\u5df2\u7ecf\u9047\u5230\u4e86\u975e\u5e38\u591a\uff0c\u53ea\u662f\u6ca1\u6709\u610f\u8bc6\u5230\u5b83\u4eec\u5c31\u662fbit field\uff1b\u4eca\u5929\uff0820190518\uff09\u5728\u9605\u8bfb Unix file types \u7684\u65f6\u5019\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 Internally, ls obtains the stat structure[ 2] associated with the file and transforms the mode_t field into a human-readable format. Note that mode_t is actually a bit field with two parts; the file type is stored within the S_IFMT mask . It can be tested with some macros like S_ISDIR (for the S_IFDIR value with mask S_IFMT ) to get the file type flags. \u9664\u6b64\u4e4b\u5916\uff0cUnix OS\u5f88\u591asystem call\u90fd\u63a5\u6536\u8fd9\u79cd\u901a\u8fc7bit or\u6765\u7ec4\u5408\u5404\u79cd\u6807\u5fd7\u7684\u65b9\u6cd5\uff1b https://github.com/lattera/glibc/blob/master/bits/fcntl.h https://en.wikipedia.org/wiki/Bitwise_operation \u6709\u5e8fdata structure multiset std::multiset https://en.wikipedia.org/wiki/Multiset priority queue std::priority_queue Skip list https://en.wikipedia.org/wiki/Skip_list \u76f8\u5173\uff1a https://www.infoq.com/articles/redis-time-series redis\u4f7f\u7528skip list\u6765\u5b9e\u73b0\u6709\u5e8f\u96c6\u5408 https://jameshfisher.com/2018/04/22/redis-sorted-set/ http://ticki.github.io/blog/skip-lists-done-right/ std::multiset vs. std::priority_queue speed comparision","title":"TODO"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#todo","text":"","title":"TODO"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#algorithms#use#memory#to#gain#performance#improvement","text":"\u4f7f\u7528\u7a7a\u95f4\u6362\u53d6\u65f6\u95f4 https://www.sciencedirect.com/topics/computer-science/performance-gain https://stackoverflow.com/questions/1898161/memory-vs-performance","title":"algorithms use memory to gain performance improvement"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#lintcode","text":"https://www.zhihu.com/question/31218682","title":"lintcode"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#hash","text":"Hash table \u76f8\u5173\uff1a https://www.infoq.com/articles/redis-time-series Hash functions Hash list What exactly (and precisely) is \u201chash?\u201d","title":"hash"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#avl#tree","text":"\u76f8\u5173\uff1a https://opensourceforu.com/2016/03/the-life-of-a-process/ https://en.wikipedia.org/wiki/AVL_tree","title":"AVL tree"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#bit#field","text":"https://en.wikipedia.org/wiki/Bit_field \u5173\u4e8ebit field\uff0c \u5728Unix OS\u4e2d\u4f7f\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u6211\u4e4b\u524d\u5df2\u7ecf\u9047\u5230\u4e86\u975e\u5e38\u591a\uff0c\u53ea\u662f\u6ca1\u6709\u610f\u8bc6\u5230\u5b83\u4eec\u5c31\u662fbit field\uff1b\u4eca\u5929\uff0820190518\uff09\u5728\u9605\u8bfb Unix file types \u7684\u65f6\u5019\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 Internally, ls obtains the stat structure[ 2] associated with the file and transforms the mode_t field into a human-readable format. Note that mode_t is actually a bit field with two parts; the file type is stored within the S_IFMT mask . It can be tested with some macros like S_ISDIR (for the S_IFDIR value with mask S_IFMT ) to get the file type flags. \u9664\u6b64\u4e4b\u5916\uff0cUnix OS\u5f88\u591asystem call\u90fd\u63a5\u6536\u8fd9\u79cd\u901a\u8fc7bit or\u6765\u7ec4\u5408\u5404\u79cd\u6807\u5fd7\u7684\u65b9\u6cd5\uff1b https://github.com/lattera/glibc/blob/master/bits/fcntl.h https://en.wikipedia.org/wiki/Bitwise_operation","title":"Bit field"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#data#structure","text":"","title":"\u6709\u5e8fdata structure"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#multiset","text":"std::multiset https://en.wikipedia.org/wiki/Multiset","title":"multiset"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#priority#queue","text":"std::priority_queue","title":"priority queue"},{"location":"Relation-structure-computation/Structure/Data-structure/TODO/#skip#list","text":"https://en.wikipedia.org/wiki/Skip_list \u76f8\u5173\uff1a https://www.infoq.com/articles/redis-time-series redis\u4f7f\u7528skip list\u6765\u5b9e\u73b0\u6709\u5e8f\u96c6\u5408 https://jameshfisher.com/2018/04/22/redis-sorted-set/ http://ticki.github.io/blog/skip-lists-done-right/ std::multiset vs. std::priority_queue speed comparision","title":"Skip list"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/","text":"\u5173\u4e8e\u672c\u7ae0 Array \uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u6700\u5148\u8ba4\u8bc6\u7684data structure\uff0c\u5b83\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u5b9e\u73b0\u5f88\u591a\u7b80\u5355\u4f46\u662f\u9ad8\u6548\u7684\u529f\u80fd\u3002\u672c\u7ae0\u8fd8\u4f1a\u4ecb\u7ecd\u4e00\u4e9barray\u7684\u9ad8\u7ea7\u4e00\u4e9b\u7684\u73a9\u6cd5\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/#_1","text":"Array \uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u6700\u5148\u8ba4\u8bc6\u7684data structure\uff0c\u5b83\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u5b9e\u73b0\u5f88\u591a\u7b80\u5355\u4f46\u662f\u9ad8\u6548\u7684\u529f\u80fd\u3002\u672c\u7ae0\u8fd8\u4f1a\u4ecb\u7ecd\u4e00\u4e9barray\u7684\u9ad8\u7ea7\u4e00\u4e9b\u7684\u73a9\u6cd5\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Array-data-structure/","text":"Array data structure","title":"Array-data-structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Array-data-structure/#array#data#structure","text":"","title":"Array data structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/","text":"Bit array A bit array (also known as bit map , bit set , bit string , or bit vector ) is an array data structure that compactly stores bits . It can be used to implement a simple set data structure . A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores kw bits, where w is the number of bits in the unit of storage, such as a byte or word , and k is some nonnegative integer. If w does not divide the number of bits to be stored, some space is wasted due to internal fragmentation . NOTE: \u6709 \u5982\u4e0b\u95ee\u9898\uff1a - \u5982\u4f55\u4f7f\u7528binary literal\u6765\u521d\u59cb\u5316bit array\uff1f - \u5982\u4f55index bit array\uff1f - \u4f7f\u7528\u4ec0\u4e48\u7c7b\u578b\u6765\u8868\u793abit array\uff1f Definition A bit array is a mapping from some domain (almost always a range of integers) to values in the set {0, 1}. The values can be interpreted as dark/light, absent/present, locked/unlocked, valid/invalid, et cetera. The point is that there are only two possible values, so they can be stored in one bit. As with other arrays, the access to a single bit can be managed by applying an index to the array . Assuming its size (or length) to be n bits, the array can be used to specify a subset of the domain (e.g. {0, 1, 2, ..., n \u22121}), where a 1-bit indicates the presence and a 0-bit the absence of a number in the set. This set data structure uses about n / w words of space, where w is the number of bits in each machine word . Whether the least significant bit (of the word) or the most significant bit indicates the smallest-index number is largely irrelevant, but the former tends to be preferred (on little-endian machines). Basic operations Although most machines are not able to address individual bits in memory, nor have instructions to manipulate single bits, each bit in a word can be singled out and manipulated using bitwise operations . In particular: OR can be used to set a bit to one: 11101010 OR 00000100 = 11101110 AND can be used to set a bit to zero: 11101010 AND 11111101 = 11101000 AND together with zero-testing can be used to determine if a bit is set: 11101010 AND 00000001 = 00000000 = 0 11101010 AND 00000010 = 00000010 \u2260 0 XOR can be used to invert or toggle a bit: 11101010 XOR 00000100 = 11101110 11101110 XOR 00000100 = 11101010 NOT can be used to invert all bits. NOT 10110010 = 01001101 To obtain the bit mask needed for these operations, we can use a bit shift operator to shift the number 1 to the left by the appropriate number of places, as well as bitwise negation if necessary. Given two bit arrays of the same size representing sets, we can compute their union , intersection , and set-theoretic difference using n / w simple bit operations each (2*n*/ w for difference), as well as the complement of either: for i from 0 to n/w-1 complement_a[i] := not a[i] union[i] := a[i] or b[i] intersection[i] := a[i] and b[i] difference[i] := a[i] and (not b[i]) NOTE: \u4e0a\u8ff0\u7b97\u6cd5\u662f\u4e00\u6b21\u64cd\u4f5c\u4e00\u4e2abyte\uff0c\u800c\u4e0d\u662fbit If we wish to iterate through the bits of a bit array, we can do this efficiently using a doubly nested loop that loops through each word, one at a time. Only n / w memory accesses are required: for i from 0 to n/w-1 index := 0 // if needed word := a[i] for b from 0 to w-1 value := word and 1 \u2260 0 word := word shift right 1 // do something with value index := index + 1 // if needed Both of these code samples exhibit ideal locality of reference , which will subsequently receive large performance boost from a data cache. If a cache line is k words, only about n / wk cache misses will occur. More complex operations As with character strings it is straightforward to define length , substring , lexicographical compare , concatenation , reverse operations. The implementation of some of these operations is sensitive to endianness . Population / Hamming weight If we wish to find the number of 1 bits in a bit array, sometimes called the population count or Hamming weight, there are efficient branch-free algorithms that can compute the number of bits in a word using a series of simple bit operations. We simply run such an algorithm on each word and keep a running total. Counting zeros is similar. See the Hamming weight article for examples of an efficient implementation. Inversion Vertical flipping of a one-bit-per-pixel image, or some FFT algorithms, requires flipping the bits of individual words (so b31 b30 ... b0 becomes b0 ... b30 b31 ). When this operation is not available on the processor, it's still possible to proceed by successive passes, in this example on 32 bits: exchange two 16bit halfwords exchange bytes by pairs (0xddccbbaa -> 0xccddaabb) ... swap bits by pairs swap bits (b31 b30 ... b1 b0 -> b30 b31 ... b0 b1) The last operation can be written ((x&0x55555555)<<1) | (x&0xaaaaaaaa)>>1)). Find first one The find first set or find first one operation identifies the index or position of the 1-bit with the smallest index in an array, and has widespread hardware support (for arrays not larger than a word) and efficient algorithms for its computation. When a priority queue is stored in a bit array, find first one can be used to identify the highest priority element in the queue. To expand a word-size find first one to longer arrays, one can find the first nonzero word and then run find first one on that word. The related operations find first zero , count leading zeros , count leading ones , count trailing zeros , count trailing ones , and log base 2 (see find first set ) can also be extended to a bit array in a straightforward manner. \u5f00\u6e90\u5e93 BitArray","title":"[Bit array](https://en.wikipedia.org/wiki/Bit_array)"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/#bit#array","text":"A bit array (also known as bit map , bit set , bit string , or bit vector ) is an array data structure that compactly stores bits . It can be used to implement a simple set data structure . A bit array is effective at exploiting bit-level parallelism in hardware to perform operations quickly. A typical bit array stores kw bits, where w is the number of bits in the unit of storage, such as a byte or word , and k is some nonnegative integer. If w does not divide the number of bits to be stored, some space is wasted due to internal fragmentation . NOTE: \u6709 \u5982\u4e0b\u95ee\u9898\uff1a - \u5982\u4f55\u4f7f\u7528binary literal\u6765\u521d\u59cb\u5316bit array\uff1f - \u5982\u4f55index bit array\uff1f - \u4f7f\u7528\u4ec0\u4e48\u7c7b\u578b\u6765\u8868\u793abit array\uff1f","title":"Bit array"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/#definition","text":"A bit array is a mapping from some domain (almost always a range of integers) to values in the set {0, 1}. The values can be interpreted as dark/light, absent/present, locked/unlocked, valid/invalid, et cetera. The point is that there are only two possible values, so they can be stored in one bit. As with other arrays, the access to a single bit can be managed by applying an index to the array . Assuming its size (or length) to be n bits, the array can be used to specify a subset of the domain (e.g. {0, 1, 2, ..., n \u22121}), where a 1-bit indicates the presence and a 0-bit the absence of a number in the set. This set data structure uses about n / w words of space, where w is the number of bits in each machine word . Whether the least significant bit (of the word) or the most significant bit indicates the smallest-index number is largely irrelevant, but the former tends to be preferred (on little-endian machines).","title":"Definition"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/#basic#operations","text":"Although most machines are not able to address individual bits in memory, nor have instructions to manipulate single bits, each bit in a word can be singled out and manipulated using bitwise operations . In particular: OR can be used to set a bit to one: 11101010 OR 00000100 = 11101110 AND can be used to set a bit to zero: 11101010 AND 11111101 = 11101000 AND together with zero-testing can be used to determine if a bit is set: 11101010 AND 00000001 = 00000000 = 0 11101010 AND 00000010 = 00000010 \u2260 0 XOR can be used to invert or toggle a bit: 11101010 XOR 00000100 = 11101110 11101110 XOR 00000100 = 11101010 NOT can be used to invert all bits. NOT 10110010 = 01001101 To obtain the bit mask needed for these operations, we can use a bit shift operator to shift the number 1 to the left by the appropriate number of places, as well as bitwise negation if necessary. Given two bit arrays of the same size representing sets, we can compute their union , intersection , and set-theoretic difference using n / w simple bit operations each (2*n*/ w for difference), as well as the complement of either: for i from 0 to n/w-1 complement_a[i] := not a[i] union[i] := a[i] or b[i] intersection[i] := a[i] and b[i] difference[i] := a[i] and (not b[i]) NOTE: \u4e0a\u8ff0\u7b97\u6cd5\u662f\u4e00\u6b21\u64cd\u4f5c\u4e00\u4e2abyte\uff0c\u800c\u4e0d\u662fbit If we wish to iterate through the bits of a bit array, we can do this efficiently using a doubly nested loop that loops through each word, one at a time. Only n / w memory accesses are required: for i from 0 to n/w-1 index := 0 // if needed word := a[i] for b from 0 to w-1 value := word and 1 \u2260 0 word := word shift right 1 // do something with value index := index + 1 // if needed Both of these code samples exhibit ideal locality of reference , which will subsequently receive large performance boost from a data cache. If a cache line is k words, only about n / wk cache misses will occur.","title":"Basic operations"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/#more#complex#operations","text":"As with character strings it is straightforward to define length , substring , lexicographical compare , concatenation , reverse operations. The implementation of some of these operations is sensitive to endianness .","title":"More complex operations"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/#population#hamming#weight","text":"If we wish to find the number of 1 bits in a bit array, sometimes called the population count or Hamming weight, there are efficient branch-free algorithms that can compute the number of bits in a word using a series of simple bit operations. We simply run such an algorithm on each word and keep a running total. Counting zeros is similar. See the Hamming weight article for examples of an efficient implementation.","title":"Population / Hamming weight"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/#inversion","text":"Vertical flipping of a one-bit-per-pixel image, or some FFT algorithms, requires flipping the bits of individual words (so b31 b30 ... b0 becomes b0 ... b30 b31 ). When this operation is not available on the processor, it's still possible to proceed by successive passes, in this example on 32 bits: exchange two 16bit halfwords exchange bytes by pairs (0xddccbbaa -> 0xccddaabb) ... swap bits by pairs swap bits (b31 b30 ... b1 b0 -> b30 b31 ... b0 b1) The last operation can be written ((x&0x55555555)<<1) | (x&0xaaaaaaaa)>>1)).","title":"Inversion"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/#find#first#one","text":"The find first set or find first one operation identifies the index or position of the 1-bit with the smallest index in an array, and has widespread hardware support (for arrays not larger than a word) and efficient algorithms for its computation. When a priority queue is stored in a bit array, find first one can be used to identify the highest priority element in the queue. To expand a word-size find first one to longer arrays, one can find the first nonzero word and then run find first one on that word. The related operations find first zero , count leading zeros , count leading ones , count trailing zeros , count trailing ones , and log base 2 (see find first set ) can also be extended to a bit array in a straightforward manner.","title":"Find first one"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Bit-array/Bit-array/#_1","text":"BitArray","title":"\u5f00\u6e90\u5e93"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/","text":"Circular buffer \u7ef4\u57fa\u767e\u79d1 Circular buffer A circular buffer , circular queue , cyclic buffer or ring buffer is a data structure that uses a single, fixed-size buffer as if it were connected end-to-end. This structure lends itself easily to buffering data streams . Uses The useful property of a circular buffer is that it does not need to have its elements shuffled around(\u79fb\u52a8) when one is consumed. (If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.) In other words, the circular buffer is well-suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer. Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a linked list approach may be preferred instead. In some situations, overwriting(\u76d6\u5199) circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the producer-consumer problem then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the sound card ) is unable to momentarily keep up. Also, the LZ77 family of lossless data compression algorithms operates on the assumption that strings seen more recently in a data stream are more likely to occur soon in the stream. Implementations store the most recent data in a circular buffer. Circular buffer mechanics A circular buffer can be implemented using four pointers , or two pointers and two integers: buffer start in memory buffer end in memory, or buffer capacity start of valid data (index or pointer) end of valid data (index or pointer), or amount of data currently in the buffer (integer) In utilizing full buffer capacity with pointer-based implementation strategy, the buffer's full or empty state could not be resolved directly from checking the positions of the start and end indexes.[ 1] Therefore, an additional mechanism must be implemented for checking this. One common way to deal with this, when using 2 pointers, is to only allow the buffer to hold (size \u2212 1) items. When both pointers are equal, the buffer is empty, and when the end pointer is one less than the start pointer, the buffer is full. NOTE: \u5982\u4f55\u6765\u5224\u5b9a\u4e00\u4e2acircular buffer\u662ffull\u6216empty\uff1f\u5f53start\u548cend\u6307\u5411\u540c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u65f6\u5019\uff0c\u65e2\u6709\u53ef\u80fd\u8868\u793a\u7684\u662fempty\uff0c\u4e5f\u6709\u53ef\u80fd\u8868\u793a\u7684\u662ffull\u3002\u6240\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u65b9\u5f0f\u662f\uff0c\u5fc5\u987b\u8981\u660e\u786e\u5730\u533a\u5206\u4e24\u8005\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7ed9\u51fa\u4e86\u5b9e\u73b0\u601d\u8def\uff1a\u5f53empty\u65f6\uff0cstart\u5bf9\u4e8eend\uff0c\u663e\u7136\u8fd9\u662f\u5c06start\u548cend\u6307\u5411\u540c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u72b6\u6001\u5224\u5b9a\u4e3aempty\u72b6\u6001\uff1b\u90a3full\u72b6\u6001\u5462\uff1f\u5f53start\u5728end\u540e\uff0c\u4e14\u4e24\u8005\u4e4b\u95f4\u95f4\u9694\u4e00\u4e2a\u5143\u7d20\u7684\u65f6\u5019\uff0c\u8868\u793a\u7684\u662ffull\u72b6\u6001\uff0c\u663e\u7136\u8fd9\u79cd\u65b9\u5f0f\uff0c\u4f7f\u7528\u4e00\u4e2a\u5143\u7d20\u6765\u4f5c\u4e3a\u8fd9\u79cd\u72b6\u6001\u7684\u6807\u5fd7\u3002 Use case \u4fdd\u5b58\u6700\u8fd1\u4e00\u6bb5\u65f6\u95f4\u5185\u7684\u884c\u60c5 Implementation spdlog circular_q.h \u8fd9\u662f\u5b9e\u73b0\u662f\u6309\u7167\u4e0a\u8ff0\u7ef4\u57fa\u767e\u79d1 Circular buffer \u4e2d\u63cf\u8ff0\u7684\u5b9e\u73b0\u7684\uff0c\u5728\u672c\u76ee\u5f55\u4e0b\uff0c\u6536\u5f55\u4e86\u5b83\u7684\u6e90\u4ee3\u7801\u3002 CIRCULAR QUEUE IN C \u8fd9\u4e2a\u5b9e\u73b0\u4e5f\u975e\u5e38\u597d\uff0c\u5728\u672c\u76ee\u5f55\u4e0b\uff0c\u6536\u5f55\u4e86\u5b83\u7684\u6e90\u4ee3\u7801\u3002 Others Implementing a Queue using a circular array","title":"Circular buffer"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#circular#buffer","text":"","title":"Circular buffer"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#circular#buffer_1","text":"A circular buffer , circular queue , cyclic buffer or ring buffer is a data structure that uses a single, fixed-size buffer as if it were connected end-to-end. This structure lends itself easily to buffering data streams .","title":"\u7ef4\u57fa\u767e\u79d1Circular buffer"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#uses","text":"The useful property of a circular buffer is that it does not need to have its elements shuffled around(\u79fb\u52a8) when one is consumed. (If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.) In other words, the circular buffer is well-suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer. Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a linked list approach may be preferred instead. In some situations, overwriting(\u76d6\u5199) circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the producer-consumer problem then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the sound card ) is unable to momentarily keep up. Also, the LZ77 family of lossless data compression algorithms operates on the assumption that strings seen more recently in a data stream are more likely to occur soon in the stream. Implementations store the most recent data in a circular buffer.","title":"Uses"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#circular#buffer#mechanics","text":"A circular buffer can be implemented using four pointers , or two pointers and two integers: buffer start in memory buffer end in memory, or buffer capacity start of valid data (index or pointer) end of valid data (index or pointer), or amount of data currently in the buffer (integer) In utilizing full buffer capacity with pointer-based implementation strategy, the buffer's full or empty state could not be resolved directly from checking the positions of the start and end indexes.[ 1] Therefore, an additional mechanism must be implemented for checking this. One common way to deal with this, when using 2 pointers, is to only allow the buffer to hold (size \u2212 1) items. When both pointers are equal, the buffer is empty, and when the end pointer is one less than the start pointer, the buffer is full. NOTE: \u5982\u4f55\u6765\u5224\u5b9a\u4e00\u4e2acircular buffer\u662ffull\u6216empty\uff1f\u5f53start\u548cend\u6307\u5411\u540c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u65f6\u5019\uff0c\u65e2\u6709\u53ef\u80fd\u8868\u793a\u7684\u662fempty\uff0c\u4e5f\u6709\u53ef\u80fd\u8868\u793a\u7684\u662ffull\u3002\u6240\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u65b9\u5f0f\u662f\uff0c\u5fc5\u987b\u8981\u660e\u786e\u5730\u533a\u5206\u4e24\u8005\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7ed9\u51fa\u4e86\u5b9e\u73b0\u601d\u8def\uff1a\u5f53empty\u65f6\uff0cstart\u5bf9\u4e8eend\uff0c\u663e\u7136\u8fd9\u662f\u5c06start\u548cend\u6307\u5411\u540c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u72b6\u6001\u5224\u5b9a\u4e3aempty\u72b6\u6001\uff1b\u90a3full\u72b6\u6001\u5462\uff1f\u5f53start\u5728end\u540e\uff0c\u4e14\u4e24\u8005\u4e4b\u95f4\u95f4\u9694\u4e00\u4e2a\u5143\u7d20\u7684\u65f6\u5019\uff0c\u8868\u793a\u7684\u662ffull\u72b6\u6001\uff0c\u663e\u7136\u8fd9\u79cd\u65b9\u5f0f\uff0c\u4f7f\u7528\u4e00\u4e2a\u5143\u7d20\u6765\u4f5c\u4e3a\u8fd9\u79cd\u72b6\u6001\u7684\u6807\u5fd7\u3002","title":"Circular buffer mechanics"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#use#case","text":"\u4fdd\u5b58\u6700\u8fd1\u4e00\u6bb5\u65f6\u95f4\u5185\u7684\u884c\u60c5","title":"Use case"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#implementation","text":"","title":"Implementation"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#spdlog#circular_qh","text":"\u8fd9\u662f\u5b9e\u73b0\u662f\u6309\u7167\u4e0a\u8ff0\u7ef4\u57fa\u767e\u79d1 Circular buffer \u4e2d\u63cf\u8ff0\u7684\u5b9e\u73b0\u7684\uff0c\u5728\u672c\u76ee\u5f55\u4e0b\uff0c\u6536\u5f55\u4e86\u5b83\u7684\u6e90\u4ee3\u7801\u3002","title":"spdlog circular_q.h"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#circular#queue#in#c","text":"\u8fd9\u4e2a\u5b9e\u73b0\u4e5f\u975e\u5e38\u597d\uff0c\u5728\u672c\u76ee\u5f55\u4e0b\uff0c\u6536\u5f55\u4e86\u5b83\u7684\u6e90\u4ee3\u7801\u3002","title":"CIRCULAR QUEUE IN C"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/Circular-buffer/#others","text":"Implementing a Queue using a circular array","title":"Others"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/martinbroadhurst-cirque/","text":"martinbroadhurst-cirque Header file: cirque.h #ifndef CIRQUE_H #define CIRQUE_H /** * \u6e90\uff1a http://www.martinbroadhurst.com/cirque-in-c.html */ struct cirque { unsigned int head ; /* First element */ unsigned int tail ; /* 1 past the last element */ unsigned int is_full ; void ** entries ; unsigned int size ; }; typedef struct cirque cirque ; // \u5bf9\u6bcf\u4e2a\u5143\u7d20\u6267\u884c\u7684\u51fd\u6570 typedef void ( * cirque_forfn )( void * ); // \u6784\u9020\u4e00\u4e2acirque cirque * cirque_create ( void ); // \u6790\u6784\u4e00\u4e2acircue void cirque_delete ( cirque * queue ); // \u63d2\u5165\u65b0\u5143\u7d20 unsigned int cirque_insert ( cirque * queue , void * data ); void * cirque_remove ( cirque * queue ); void * cirque_peek ( const cirque * queue ); unsigned int cirque_get_count ( const cirque * queue ); void cirque_for_each ( const cirque * queue , cirque_forfn fun ); #endif /* CIRQUE_H */ Source file: cirque.c #include <stdlib.h> #include <cirque.h> cirque * cirque_create ( void ) { const unsigned int size = 4 ; cirque * queue = malloc ( sizeof ( cirque )); if ( queue ) { queue -> entries = malloc ( size * sizeof ( void * )); if ( queue -> entries ) { queue -> size = size ; queue -> head = 0 ; queue -> tail = 0 ; queue -> is_full = 0 ; } else { free ( queue ); queue = NULL ; } } return queue ; } void cirque_delete ( cirque * queue ) { if ( queue ) { free ( queue -> entries ); free ( queue ); } } static void cirque_resize ( cirque * queue ) { void ** temp = malloc ( queue -> size * 2 * sizeof ( void * )); if ( temp ) { unsigned int i = 0 ; unsigned int h = queue -> head ; do { temp [ i ] = queue -> entries [ h ++ ]; if ( h == queue -> size ) { h = 0 ; } i ++ ; } while ( h != queue -> tail ); free ( queue -> entries ); queue -> entries = temp ; queue -> head = 0 ; queue -> tail = queue -> size ; queue -> size *= 2 ; queue -> is_full = 0 ; } } static unsigned int cirque_is_empty ( const cirque * queue ) { return ( queue -> head == queue -> tail ) && ! queue -> is_full ; } unsigned int cirque_insert ( cirque * queue , void * data ) { unsigned int result ; if ( queue -> is_full ) { cirque_resize ( queue ); if ( queue -> is_full ) { result = 0 ; } } if ( ! queue -> is_full ) { queue -> entries [ queue -> tail ++ ] = data ; if ( queue -> tail == queue -> size ) { queue -> tail = 0 ; } if ( queue -> tail == queue -> head ) { queue -> is_full = 1 ; } result = 1 ; } return result ; } void * cirque_remove ( cirque * queue ) { void * data = NULL ; if ( ! cirque_is_empty ( queue )) { if ( queue -> is_full ) { queue -> is_full = 0 ; } data = queue -> entries [ queue -> head ++ ]; if ( queue -> head == queue -> size ) { queue -> head = 0 ; } } return data ; } void * cirque_peek ( const cirque * queue ) { void * data = NULL ; if ( ! cirque_is_empty ( queue )) { data = queue -> entries [ queue -> head ]; } return data ; } unsigned int cirque_get_count ( const cirque * queue ) { unsigned int count ; if ( cirque_is_empty ( queue )) { count = 0 ; } else if ( queue -> is_full ) { count = queue -> size ; } else if ( queue -> tail > queue -> head ) { count = queue -> tail - queue -> head ; } else { count = queue -> size - queue -> head ; if ( queue -> tail > 0 ) { count += queue -> tail - 1 ; } } return count ; } void cirque_for_each ( const cirque * queue , cirque_forfn fun ) { if ( ! cirque_is_empty ( queue )) { unsigned int h = queue -> head ; do { fun ( queue -> entries [ h ++ ]); if ( h == queue -> size ) { h = 0 ; } } while ( h != queue -> tail ); } } main.c #include <stdio.h> #include <string.h> #include <stdlib.h> #include <cirque.h> int main ( void ) { cirque * queue ; char buf [ 16 ]; unsigned int f ; const unsigned int max = 32 ; const unsigned int limit = 16 ; queue = cirque_create (); for ( f = 0 ; f < max ; f ++ ) { sprintf ( buf , \"Item %d\" , f ); if ( f >= limit ) { /* Start removing at limit to show the queue doesn't keep growing */ char * data = cirque_remove ( queue ); printf ( \"Removed %s \\n \" , data ); free ( data ); } printf ( \"Inserting %s \\n \" , buf ); cirque_insert ( queue , strdup ( buf )); } cirque_for_each ( queue , ( cirque_forfn ) puts ); printf ( \"Size is %d (should be %d) \\n \" , queue -> size , limit ); cirque_for_each ( queue , free ); cirque_delete ( queue ); return 0 ; }","title":"martinbroadhurst-cirque"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/martinbroadhurst-cirque/#martinbroadhurst-cirque","text":"","title":"martinbroadhurst-cirque"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/martinbroadhurst-cirque/#header#file#cirqueh","text":"#ifndef CIRQUE_H #define CIRQUE_H /** * \u6e90\uff1a http://www.martinbroadhurst.com/cirque-in-c.html */ struct cirque { unsigned int head ; /* First element */ unsigned int tail ; /* 1 past the last element */ unsigned int is_full ; void ** entries ; unsigned int size ; }; typedef struct cirque cirque ; // \u5bf9\u6bcf\u4e2a\u5143\u7d20\u6267\u884c\u7684\u51fd\u6570 typedef void ( * cirque_forfn )( void * ); // \u6784\u9020\u4e00\u4e2acirque cirque * cirque_create ( void ); // \u6790\u6784\u4e00\u4e2acircue void cirque_delete ( cirque * queue ); // \u63d2\u5165\u65b0\u5143\u7d20 unsigned int cirque_insert ( cirque * queue , void * data ); void * cirque_remove ( cirque * queue ); void * cirque_peek ( const cirque * queue ); unsigned int cirque_get_count ( const cirque * queue ); void cirque_for_each ( const cirque * queue , cirque_forfn fun ); #endif /* CIRQUE_H */","title":"Header file: cirque.h"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/martinbroadhurst-cirque/#source#file#cirquec","text":"#include <stdlib.h> #include <cirque.h> cirque * cirque_create ( void ) { const unsigned int size = 4 ; cirque * queue = malloc ( sizeof ( cirque )); if ( queue ) { queue -> entries = malloc ( size * sizeof ( void * )); if ( queue -> entries ) { queue -> size = size ; queue -> head = 0 ; queue -> tail = 0 ; queue -> is_full = 0 ; } else { free ( queue ); queue = NULL ; } } return queue ; } void cirque_delete ( cirque * queue ) { if ( queue ) { free ( queue -> entries ); free ( queue ); } } static void cirque_resize ( cirque * queue ) { void ** temp = malloc ( queue -> size * 2 * sizeof ( void * )); if ( temp ) { unsigned int i = 0 ; unsigned int h = queue -> head ; do { temp [ i ] = queue -> entries [ h ++ ]; if ( h == queue -> size ) { h = 0 ; } i ++ ; } while ( h != queue -> tail ); free ( queue -> entries ); queue -> entries = temp ; queue -> head = 0 ; queue -> tail = queue -> size ; queue -> size *= 2 ; queue -> is_full = 0 ; } } static unsigned int cirque_is_empty ( const cirque * queue ) { return ( queue -> head == queue -> tail ) && ! queue -> is_full ; } unsigned int cirque_insert ( cirque * queue , void * data ) { unsigned int result ; if ( queue -> is_full ) { cirque_resize ( queue ); if ( queue -> is_full ) { result = 0 ; } } if ( ! queue -> is_full ) { queue -> entries [ queue -> tail ++ ] = data ; if ( queue -> tail == queue -> size ) { queue -> tail = 0 ; } if ( queue -> tail == queue -> head ) { queue -> is_full = 1 ; } result = 1 ; } return result ; } void * cirque_remove ( cirque * queue ) { void * data = NULL ; if ( ! cirque_is_empty ( queue )) { if ( queue -> is_full ) { queue -> is_full = 0 ; } data = queue -> entries [ queue -> head ++ ]; if ( queue -> head == queue -> size ) { queue -> head = 0 ; } } return data ; } void * cirque_peek ( const cirque * queue ) { void * data = NULL ; if ( ! cirque_is_empty ( queue )) { data = queue -> entries [ queue -> head ]; } return data ; } unsigned int cirque_get_count ( const cirque * queue ) { unsigned int count ; if ( cirque_is_empty ( queue )) { count = 0 ; } else if ( queue -> is_full ) { count = queue -> size ; } else if ( queue -> tail > queue -> head ) { count = queue -> tail - queue -> head ; } else { count = queue -> size - queue -> head ; if ( queue -> tail > 0 ) { count += queue -> tail - 1 ; } } return count ; } void cirque_for_each ( const cirque * queue , cirque_forfn fun ) { if ( ! cirque_is_empty ( queue )) { unsigned int h = queue -> head ; do { fun ( queue -> entries [ h ++ ]); if ( h == queue -> size ) { h = 0 ; } } while ( h != queue -> tail ); } }","title":"Source file: cirque.c"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/martinbroadhurst-cirque/#mainc","text":"#include <stdio.h> #include <string.h> #include <stdlib.h> #include <cirque.h> int main ( void ) { cirque * queue ; char buf [ 16 ]; unsigned int f ; const unsigned int max = 32 ; const unsigned int limit = 16 ; queue = cirque_create (); for ( f = 0 ; f < max ; f ++ ) { sprintf ( buf , \"Item %d\" , f ); if ( f >= limit ) { /* Start removing at limit to show the queue doesn't keep growing */ char * data = cirque_remove ( queue ); printf ( \"Removed %s \\n \" , data ); free ( data ); } printf ( \"Inserting %s \\n \" , buf ); cirque_insert ( queue , strdup ( buf )); } cirque_for_each ( queue , ( cirque_forfn ) puts ); printf ( \"Size is %d (should be %d) \\n \" , queue -> size , limit ); cirque_for_each ( queue , free ); cirque_delete ( queue ); return 0 ; }","title":"main.c"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/spdlog-circular_q/","text":"spdlog circular_q \u6e90\uff1a https://github.com/gabime/spdlog/blob/v1.x/include/spdlog/details/circular_q.h \u672c\u7a0b\u5e8f\u4e2doverrun\u4e0eoverwrite\u540c\u4e49\uff0c\u5173\u4e8eoverwrite\uff0c\u53c2\u89c1 https://en.wikipedia.org/wiki/Circular_buffer tail_ \u6240\u6307\u5411\u7684\u4f4d\u7f6e\u662f\u7559\u7a7a\u7684 \u672c\u7a0b\u5e8f\u91c7\u7528\u7684\u662fhttps://en.wikipedia.org/wiki/Circular_buffer#Circular_buffer_mechanics\u4e2d\u63cf\u8ff0\u7684\u8868\u793abuffer\u7684full\u72b6\u6001\u3001empty\u72b6\u6001\u7684\u65b9\u6cd5 \u5f53 tail_ \u4e0e head_ \u76f8\u7b49\u65f6\uff0c\u8868\u793abuffer\u6ee1\u4e86\uff0c\u5219\u5c31\u9700\u8981overwrite\uff0c\u6b64\u65f6\u9700\u8981\u5c06 head_ \u540e\u79fb\uff0c\u79fb\u52a8\u540e\uff0cbuffer\u4f9d\u7136\u662ffull\u7684\u72b6\u6001\u3002 // Copyright(c) 2015-present, Gabi Melman & spdlog contributors. // Distributed under the MIT License (http://opensource.org/licenses/MIT) // circular q view of std::vector. #pragma once #include \"stddef.h\" #include <vector> /** * \u6e90\uff1ahttps://github.com/gabime/spdlog/blob/v1.x/include/spdlog/details/circular_q.h * \u672c\u7a0b\u5e8f\u4e2doverrun\u4e0eoverwrite\u540c\u4e49\uff0c\u5173\u4e8eoverwrite\uff0c\u53c2\u89c1 https://en.wikipedia.org/wiki/Circular_buffer * * tail_\u6240\u6307\u5411\u7684\u4f4d\u7f6e\u662f\u7559\u7a7a\u7684 * \u672c\u7a0b\u5e8f\u91c7\u7528\u7684\u662fhttps://en.wikipedia.org/wiki/Circular_buffer#Circular_buffer_mechanics\u4e2d\u63cf\u8ff0\u7684\u8868\u793abuffer\u7684full\u72b6\u6001\u3001empty\u72b6\u6001\u7684\u65b9\u6cd5 * \u5f53tail_\u4e0ehead_\u76f8\u7b49\u65f6\uff0c\u8868\u793a\u4f60buffer\u6ee1\u4e86\uff0c\u5219\u5c31\u9700\u8981overwrite\uff0c\u6b64\u65f6\u9700\u8981\u5c06head_\u540e\u79fb\uff0c\u79fb\u52a8\u540e\uff0cbuffer\u4f9d\u7136\u662ffull\u7684\u72b6\u6001 */ namespace spdlog { namespace details { template < typename T > class circular_q { size_t max_items_ = 0 ; // index typename std :: vector < T >:: size_type head_ = 0 ; typename std :: vector < T >:: size_type tail_ = 0 ; size_t overrun_counter_ = 0 ; std :: vector < T > v_ ; public : using value_type = T ; // empty ctor - create a disabled queue with no elements allocated at all circular_q () = default ; explicit circular_q ( size_t max_items ) : max_items_ ( max_items + 1 ) // one item is reserved as marker for full q , v_ ( max_items_ ) {} circular_q ( const circular_q & ) = default ; circular_q & operator = ( const circular_q & ) = default ; // move cannot be default, // since we need to reset head_, tail_, etc to zero in the moved object circular_q ( circular_q && other ) noexcept { copy_moveable ( std :: move ( other )); } circular_q & operator = ( circular_q && other ) noexcept { copy_moveable ( std :: move ( other )); return * this ; } // push back, overrun (oldest) item if no room left void push_back ( T && item ) { if ( max_items_ > 0 ) { v_ [ tail_ ] = std :: move ( item ); tail_ = ( tail_ + 1 ) % max_items_ ; if ( tail_ == head_ ) // overrun last item if full { head_ = ( head_ + 1 ) % max_items_ ; ++ overrun_counter_ ; } } } // Return reference to the front item. // If there are no elements in the container, the behavior is undefined. const T & front () const { return v_ [ head_ ]; } T & front () { return v_ [ head_ ]; } // Return number of elements actually stored size_t size () const { if ( tail_ >= head_ ) { return tail_ - head_ ; } else { return max_items_ - ( head_ - tail_ ); } } // Return const reference to item by index. // If index is out of range 0\u9225?size()-1, the behavior is undefined. const T & at ( size_t i ) const { assert ( i < size ()); return v_ [( head_ + i ) % max_items_ ]; } // Pop item from front. // If there are no elements in the container, the behavior is undefined. void pop_front () { head_ = ( head_ + 1 ) % max_items_ ; } bool empty () const { return tail_ == head_ ; } bool full () const { // head is ahead of the tail by 1 if ( max_items_ > 0 ) { return (( tail_ + 1 ) % max_items_ ) == head_ ; } return false ; } size_t overrun_counter () const { return overrun_counter_ ; } private : // copy from other&& and reset it to disabled state void copy_moveable ( circular_q && other ) noexcept { max_items_ = other . max_items_ ; head_ = other . head_ ; tail_ = other . tail_ ; overrun_counter_ = other . overrun_counter_ ; v_ = std :: move ( other . v_ ); // put &&other in disabled, but valid state other . max_items_ = 0 ; other . head_ = other . tail_ = 0 ; other . overrun_counter_ = 0 ; } }; } // namespace details } // namespace spdlog #include \"circular_q.h\" using namespace spdlog :: details ; int main () { circular_q < int > my_q ( 10 ); for ( int i = 0 ; i < 100 ; i ++ ) { my_q . push_back ( i ); } } // g++ --std=c++11 main.cpp","title":"spdlog `circular_q`"},{"location":"Relation-structure-computation/Structure/Data-structure/Array/Circular-array/spdlog-circular_q/#spdlog#circular_q","text":"\u6e90\uff1a https://github.com/gabime/spdlog/blob/v1.x/include/spdlog/details/circular_q.h \u672c\u7a0b\u5e8f\u4e2doverrun\u4e0eoverwrite\u540c\u4e49\uff0c\u5173\u4e8eoverwrite\uff0c\u53c2\u89c1 https://en.wikipedia.org/wiki/Circular_buffer tail_ \u6240\u6307\u5411\u7684\u4f4d\u7f6e\u662f\u7559\u7a7a\u7684 \u672c\u7a0b\u5e8f\u91c7\u7528\u7684\u662fhttps://en.wikipedia.org/wiki/Circular_buffer#Circular_buffer_mechanics\u4e2d\u63cf\u8ff0\u7684\u8868\u793abuffer\u7684full\u72b6\u6001\u3001empty\u72b6\u6001\u7684\u65b9\u6cd5 \u5f53 tail_ \u4e0e head_ \u76f8\u7b49\u65f6\uff0c\u8868\u793abuffer\u6ee1\u4e86\uff0c\u5219\u5c31\u9700\u8981overwrite\uff0c\u6b64\u65f6\u9700\u8981\u5c06 head_ \u540e\u79fb\uff0c\u79fb\u52a8\u540e\uff0cbuffer\u4f9d\u7136\u662ffull\u7684\u72b6\u6001\u3002 // Copyright(c) 2015-present, Gabi Melman & spdlog contributors. // Distributed under the MIT License (http://opensource.org/licenses/MIT) // circular q view of std::vector. #pragma once #include \"stddef.h\" #include <vector> /** * \u6e90\uff1ahttps://github.com/gabime/spdlog/blob/v1.x/include/spdlog/details/circular_q.h * \u672c\u7a0b\u5e8f\u4e2doverrun\u4e0eoverwrite\u540c\u4e49\uff0c\u5173\u4e8eoverwrite\uff0c\u53c2\u89c1 https://en.wikipedia.org/wiki/Circular_buffer * * tail_\u6240\u6307\u5411\u7684\u4f4d\u7f6e\u662f\u7559\u7a7a\u7684 * \u672c\u7a0b\u5e8f\u91c7\u7528\u7684\u662fhttps://en.wikipedia.org/wiki/Circular_buffer#Circular_buffer_mechanics\u4e2d\u63cf\u8ff0\u7684\u8868\u793abuffer\u7684full\u72b6\u6001\u3001empty\u72b6\u6001\u7684\u65b9\u6cd5 * \u5f53tail_\u4e0ehead_\u76f8\u7b49\u65f6\uff0c\u8868\u793a\u4f60buffer\u6ee1\u4e86\uff0c\u5219\u5c31\u9700\u8981overwrite\uff0c\u6b64\u65f6\u9700\u8981\u5c06head_\u540e\u79fb\uff0c\u79fb\u52a8\u540e\uff0cbuffer\u4f9d\u7136\u662ffull\u7684\u72b6\u6001 */ namespace spdlog { namespace details { template < typename T > class circular_q { size_t max_items_ = 0 ; // index typename std :: vector < T >:: size_type head_ = 0 ; typename std :: vector < T >:: size_type tail_ = 0 ; size_t overrun_counter_ = 0 ; std :: vector < T > v_ ; public : using value_type = T ; // empty ctor - create a disabled queue with no elements allocated at all circular_q () = default ; explicit circular_q ( size_t max_items ) : max_items_ ( max_items + 1 ) // one item is reserved as marker for full q , v_ ( max_items_ ) {} circular_q ( const circular_q & ) = default ; circular_q & operator = ( const circular_q & ) = default ; // move cannot be default, // since we need to reset head_, tail_, etc to zero in the moved object circular_q ( circular_q && other ) noexcept { copy_moveable ( std :: move ( other )); } circular_q & operator = ( circular_q && other ) noexcept { copy_moveable ( std :: move ( other )); return * this ; } // push back, overrun (oldest) item if no room left void push_back ( T && item ) { if ( max_items_ > 0 ) { v_ [ tail_ ] = std :: move ( item ); tail_ = ( tail_ + 1 ) % max_items_ ; if ( tail_ == head_ ) // overrun last item if full { head_ = ( head_ + 1 ) % max_items_ ; ++ overrun_counter_ ; } } } // Return reference to the front item. // If there are no elements in the container, the behavior is undefined. const T & front () const { return v_ [ head_ ]; } T & front () { return v_ [ head_ ]; } // Return number of elements actually stored size_t size () const { if ( tail_ >= head_ ) { return tail_ - head_ ; } else { return max_items_ - ( head_ - tail_ ); } } // Return const reference to item by index. // If index is out of range 0\u9225?size()-1, the behavior is undefined. const T & at ( size_t i ) const { assert ( i < size ()); return v_ [( head_ + i ) % max_items_ ]; } // Pop item from front. // If there are no elements in the container, the behavior is undefined. void pop_front () { head_ = ( head_ + 1 ) % max_items_ ; } bool empty () const { return tail_ == head_ ; } bool full () const { // head is ahead of the tail by 1 if ( max_items_ > 0 ) { return (( tail_ + 1 ) % max_items_ ) == head_ ; } return false ; } size_t overrun_counter () const { return overrun_counter_ ; } private : // copy from other&& and reset it to disabled state void copy_moveable ( circular_q && other ) noexcept { max_items_ = other . max_items_ ; head_ = other . head_ ; tail_ = other . tail_ ; overrun_counter_ = other . overrun_counter_ ; v_ = std :: move ( other . v_ ); // put &&other in disabled, but valid state other . max_items_ = 0 ; other . head_ = other . tail_ = 0 ; other . overrun_counter_ = 0 ; } }; } // namespace details } // namespace spdlog #include \"circular_q.h\" using namespace spdlog :: details ; int main () { circular_q < int > my_q ( 10 ); for ( int i = 0 ; i < 100 ; i ++ ) { my_q . push_back ( i ); } } // g++ --std=c++11 main.cpp","title":"spdlog circular_q"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbagraph\uff0c\u4ee5\u53ca\u4e00\u79cd\u7279\u6b8a\u7684graph\uff1atree\uff0c\u4e24\u8005\u5728computer science\u4e2d\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684application\u3002 See also: 1) tree 2) graph","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/#_1","text":"\u672c\u7ae0\u8ba8\u8bbagraph\uff0c\u4ee5\u53ca\u4e00\u79cd\u7279\u6b8a\u7684graph\uff1atree\uff0c\u4e24\u8005\u5728computer science\u4e2d\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684application\u3002 See also: 1) tree 2) graph","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/","text":"PR\u00dcFER CODES AND LABELED TREES IN C search BREADTH-FIRST SEARCH OF A GRAPH IN C \u6df1\u5ea6\u4f18\u5148\u641c\u7d22 TRAVELING SALESMAN PROBLEM USING BACKTRACKING IN C REALISING A GRAPH DEGREE SEQUENCE IN C TOPOLOGICAL SORT IN C GREEDY VERTEX COVER IN C COMPLEMENT OF A GRAPH IN C MAXIMUM CLIQUE USING BACKTRACKING IN C BELLMAN-FORD ALGORITHM IN C FLOYD-WARSHALL ALGORITHM IN C REPETITIVE NEAREST NEIGHBOUR ALGORITHM FOR TSP IN C NEAREST NEIGHBOUR ALGORITHM FOR TSP IN C CHEAPEST-LINK ALGORITHM FOR TSP IN C GREEDY MAXIMUM INDEPENDENT SET IN C SPANNING TREES SPANNING TREES OF A GRAPH IN C SPANNING FOREST OF A GRAPH IN C \u751f\u6210\u68ee\u6797 SPANNING TREE OF A GRAPH IN C \u751f\u6210\u6811 BOR\u016eVKA\u2019S MINIMAL SPANNING TREE (MST) ALGORITHM IN C KRUSKAL\u2019S MINIMUM SPANNING TREE (MST) ALGORITHM IN C PRIM\u2019S MINIMUM SPANNING TREE (MST) ALGORITHM IN C component CONNECTED COMPONENTS OF A GRAPH IN C path EULER CIRCUITS USING BACKTRACKING IN C DIJKSTRA\u2019S SHORTEST PATHS ALGORITHM IN C HAMILTONIAN CIRCUITS USING BACKTRACKING IN C circle GRAPH CYCLE DETECTION IN C coloring VERTEX COLOURING WITH BACKTRACKING IN C representation GRAPH DATA STRUCTURES GRAPH ALGORITHMS","title":"Martinbroadhurst"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/#search","text":"BREADTH-FIRST SEARCH OF A GRAPH IN C \u6df1\u5ea6\u4f18\u5148\u641c\u7d22 TRAVELING SALESMAN PROBLEM USING BACKTRACKING IN C REALISING A GRAPH DEGREE SEQUENCE IN C TOPOLOGICAL SORT IN C GREEDY VERTEX COVER IN C COMPLEMENT OF A GRAPH IN C MAXIMUM CLIQUE USING BACKTRACKING IN C BELLMAN-FORD ALGORITHM IN C FLOYD-WARSHALL ALGORITHM IN C REPETITIVE NEAREST NEIGHBOUR ALGORITHM FOR TSP IN C NEAREST NEIGHBOUR ALGORITHM FOR TSP IN C CHEAPEST-LINK ALGORITHM FOR TSP IN C GREEDY MAXIMUM INDEPENDENT SET IN C","title":"search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/#spanning#trees","text":"SPANNING TREES OF A GRAPH IN C SPANNING FOREST OF A GRAPH IN C \u751f\u6210\u68ee\u6797 SPANNING TREE OF A GRAPH IN C \u751f\u6210\u6811 BOR\u016eVKA\u2019S MINIMAL SPANNING TREE (MST) ALGORITHM IN C KRUSKAL\u2019S MINIMUM SPANNING TREE (MST) ALGORITHM IN C PRIM\u2019S MINIMUM SPANNING TREE (MST) ALGORITHM IN C","title":"SPANNING TREES"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/#component","text":"CONNECTED COMPONENTS OF A GRAPH IN C","title":"component"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/#path","text":"EULER CIRCUITS USING BACKTRACKING IN C DIJKSTRA\u2019S SHORTEST PATHS ALGORITHM IN C HAMILTONIAN CIRCUITS USING BACKTRACKING IN C","title":"path"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/#circle","text":"GRAPH CYCLE DETECTION IN C","title":"circle"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/#coloring","text":"VERTEX COLOURING WITH BACKTRACKING IN C","title":"coloring"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/#representation","text":"GRAPH DATA STRUCTURES","title":"representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/martinbroadhurst/#graph#algorithms","text":"","title":"GRAPH ALGORITHMS"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bba\u4e86\u56fe\u7684\u5b9a\u4e49\uff0c\u6784\u9020\uff0c\u57fa\u672c\u64cd\u4f5c\uff0c\u4ee5\u53ca\u56fe\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u5404\u4e2a\u9886\u57df\u5404\u79cd\u5404\u6837\u7684\u5e94\u7528\u3002 \u672c\u7ae0\u5185\u5bb9\u4e3b\u8981\u6765\u81ea\u300a Discrete Mathematics and Its Applications \u300b\u548c\u7ef4\u57fa\u767e\u79d1\u3001boost graph library\u3002 \u4e3a\u4ec0\u4e48\u8981\u7814\u7a76\u56fe\uff1f \u5728 The Boost Graph Library (BGL) \u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u4ecb\u7ecd\uff1a Graphs are mathematical abstractions that are useful for solving many types of problems in computer science. \u9700\u8981\u4ecegraph\u548crelation\u6765\u8c08\u8d77\uff0crelation\u662f\u5e7f\u6cdb\u5b58\u5728\u7684\uff0cgraph\u662f\u63cf\u8ff0relation\u7684\u5f3a\u5927\u5de5\u5177\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/#_1","text":"\u672c\u7ae0\u8ba8\u8bba\u4e86\u56fe\u7684\u5b9a\u4e49\uff0c\u6784\u9020\uff0c\u57fa\u672c\u64cd\u4f5c\uff0c\u4ee5\u53ca\u56fe\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u5404\u4e2a\u9886\u57df\u5404\u79cd\u5404\u6837\u7684\u5e94\u7528\u3002 \u672c\u7ae0\u5185\u5bb9\u4e3b\u8981\u6765\u81ea\u300a Discrete Mathematics and Its Applications \u300b\u548c\u7ef4\u57fa\u767e\u79d1\u3001boost graph library\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/#_2","text":"\u5728 The Boost Graph Library (BGL) \u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u4ecb\u7ecd\uff1a Graphs are mathematical abstractions that are useful for solving many types of problems in computer science. \u9700\u8981\u4ecegraph\u548crelation\u6765\u8c08\u8d77\uff0crelation\u662f\u5e7f\u6cdb\u5b58\u5728\u7684\uff0cgraph\u662f\u63cf\u8ff0relation\u7684\u5f3a\u5927\u5de5\u5177\u3002","title":"\u4e3a\u4ec0\u4e48\u8981\u7814\u7a76\u56fe\uff1f"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/","text":"Graph (discrete mathematics) and Graph theory graph terminology undirected graph First, we give some terminology that describes the vertices and edges of undirected graphs . DEFINITION Two vertices u and v in an undirected graph G are called adjacent (or neighbors ) in G if u and v are endpoints of an edge e of G . Such an edge e is called incident with the vertices u and v and e is said to connect u and v . DEFINITION The set of all neighbors of a vertex v of G = (V,E) , denoted by N(v) , is called the neighborhood of v . If A is a subset of V , we denote by N(A) the set of all vertices in G that are adjacent to at least one vertex in A . So, N(A) =U v\u2208A N(v) . NOTE: \u4e0a\u8bc9\u5b9a\u4e49\u5176\u5b9e\u5b9a\u4e49\u7684\u662f\u4e00\u4e2a\u70b9\u7684*neighborhood*\u548c\u4e00\u4e2a\u70b9\u96c6\u7684*neighborhood*\u3002 DEFINITION The degree of a vertex in an undirected graph is the number of edges incident with it, except that a loop at a vertex contributes twice to the degree of that vertex. The degree of the vertex v is denoted by deg(v) . THE HANDSHAKING THEOREM Let G = (V,E) be an undirected graph with m edges. Then $$ 2m = \\sum_{v \\in V} deg(v) $$ (Note that this applies even if multiple edges and loops are present.) THEOREM An undirected graph has an even number of vertices of odd degree. NOTE: \u7ffb\u8bd1\u6210\u4e2d\u6587\u662f\uff1a\u4e00\u4e2a\u65e0\u5411\u56fe\u6709\u5076\u6570\u4e2a\u5947\u6570\u5ea6\u7684\u70b9\u3002 directed graph Terminology for graphs with directed edges reflects the fact that edges in directed graphs have directions. DEFINITION When (u,v) is an edge of the graph G with directed edges, u is said to be adjacent to v and v is said to be adjacent from u . The vertex u is called the initial vertex of (u,v) , and v is called the terminal or end vertex of (u,v) . The initial vertex and terminal vertex of a loop are the same. DEFINITION In a graph with directed edges the in-degree of a vertex v , denoted by deg \u2212 (v) , is the number of edges with v as their terminal vertex. The out-degree of v , denoted by deg + (v) , is the number of edges with v as their initial vertex. (Note that a loop at a vertex contributes 1 to both the in-degree and the out-degree of this vertex.) THEOREM Let G = (V,E) be a graph with directed edges. Then $$ \\sum_{v \\in V} deg^-(v) = \\sum_{v \\in V} deg^+(v)=|E| $$ There are many properties of a graph with directed edges that do not depend on the direction of its edges.Consequently, it is often useful to ignore these directions. The undirected graph that results from ignoring directions of edges is called the underlying undirected graph . A graph with directed edges and its underlying undirected graph have the same number of edges. NOTE: \u8fd9\u4e9b\u5c5e\u4e8e\u5728\u540e\u6587\u4e2d\u4f1a\u5927\u91cf\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u5148\u719f\u6089\u719f\u6089\u3002 Some Special Simple Graphs Complete Graphs A complete graph on n vertices, denoted by K_n K_n , is a simple graph that contains exactly one edge between each pair of distinct vertices. The graphs K_n K_n , for n = 1,2,3,4,5,6, are displayed in Figure 3. A simple graph for which there is at least one pair of distinct vertex not connected by an edge is called noncomplete . Cycles A cycle C_n C_n , n \u2265 3, consists of n vertices v_1 ,v_2 ,...,v_n v_1 ,v_2 ,...,v_n and edges \\{v_1 ,v_2 \\} \\{v_1 ,v_2 \\} , \\{v_2 ,v_3 \\} \\{v_2 ,v_3 \\} ,..., \\{v_{n\u22121} ,v_n \\} \\{v_{n\u22121} ,v_n \\} , and {v_n ,v_1 } {v_n ,v_1 } . The cycles C_3 C_3 , C_4 C_4 , C_5 C_5 , and C_6 C_6 are displayed in Figure 4. Wheels We obtain a wheel W_n W_n when we add an additional vertex to a cycle C_n C_n , for n \u2265 3, and connect this new vertex to each of the n vertices in C_n C_n , by new edges. The wheels W_3 W_3 , W_4 W_4 , W_5 W_5 , and W_6 W_6 are displayed in Figure 5. n-Cubes An n-dimensional hypercube,or n-cube,denoted by Q_n Q_n ,is a graph that has vertices representing the 2^n 2^n bit strings of length n . Two vertices are adjacent if and only if the bit strings that they represent differ in exactly one bit position. We display Q_1 Q_1 ,$ Q_2$ , and Q_3 Q_3 in Figure 6. Note that you can construct the (n + 1)-cube Q_{n+1} Q_{n+1} from the n-cube Q_n Q_n by making two copies of Q_n Q_n , prefacing the labels on the vertices with a 0 in one copy of Q_n Q_n and with a 1 in the other copy of Q_n Q_n , and adding edges connecting two vertices that have labels differing only in the first bit. In Figure 6, Q_ 3 Q_ 3 is constructed from Q_2 Q_2 by drawing two copies of Q_2 Q_2 as the top and bottom faces of Q_3 Q_3 , adding 0 at the beginning of the label of each vertex in the bottom face and 1 at the beginning of the label of each vertex in the top face. (Here, by face we mean a face of a cube in three-dimensional space. Think of drawing the graph Q_3 Q_3 in three-dimensional space with copies of Q_2 Q_2 as the top and bottom faces of a cube and then drawing the projection of the resulting depiction in the plane.) NOTE: How about Q_4 Q_4 ? see the wikipedia entry Hypercube . Bipartite Graphs Sometimes a graph has the property that its vertex set can be divided into two disjoint subsets such that each edge connects a vertex in one of these subsets to a vertex in the other subset. For example, consider the graph representing marriages between men and women in a village, where each person is represented by a vertex and a marriage is represented by an edge. In this graph, each edge connects a vertex in the subset of vertices representing males and a vertex in the subset of vertices representing females. This leads us to Definition 5. DEFINITION A simple graph G is called bipartite if its vertex set V can be partitioned into two disjoint sets V_1 V_1 and V_2 V_2 such that every edge in the graph connects a vertex in V_1 V_1 and a vertex in V_2 V_2 (so that no edge in G connects either two vertices in V_1 V_1 or two vertices in V_2 V_2 ). When this condition holds, we call the pair (V_1 ,V_2 ) (V_1 ,V_2 ) a bipartition of the vertex set V of G. In Example 9 we will show that C_6 C_6 is bipartite, and in Example 10 we will show that K_3 K_3 is not bipartite. Theorem below provides a useful criterion for determining whether a graph is bipartite. THEOREM 4 A simple graph is bipartite if and only if it is possible to assign one of two different colors to each vertex of the graph so that no two adjacent vertices are assigned the same color. Theorem 4 is an example of a result in the part of graph theory known as graph colorings . Graph colorings is an important part of graph theory with important applications. We will study graph colorings further in Section 10.8. Another useful criterion for determining whether a graph is bipartite is based on the notion of a path , a topic we study in Section 10.4. A graph is bipartite if and only if it is not possible to start at a vertex and return to this vertex by traversing an odd number of distinct edges. We will make this notion more precise when we discuss paths and circuits in graphs in Section 10.4 (see Exercise 63 in that section). Complete Bipartite Graphs Bipartite Graphs and Matchings boost graph library#Review of Elementary Graph Theory","title":"Graph-and-Graph-theory.md"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#graph#discrete#mathematics#and#graph#theory","text":"","title":"Graph (discrete mathematics) and Graph theory"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#graph#terminology","text":"","title":"graph terminology"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#undirected#graph","text":"First, we give some terminology that describes the vertices and edges of undirected graphs .","title":"undirected graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#definition","text":"Two vertices u and v in an undirected graph G are called adjacent (or neighbors ) in G if u and v are endpoints of an edge e of G . Such an edge e is called incident with the vertices u and v and e is said to connect u and v .","title":"DEFINITION"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#definition_1","text":"The set of all neighbors of a vertex v of G = (V,E) , denoted by N(v) , is called the neighborhood of v . If A is a subset of V , we denote by N(A) the set of all vertices in G that are adjacent to at least one vertex in A . So, N(A) =U v\u2208A N(v) . NOTE: \u4e0a\u8bc9\u5b9a\u4e49\u5176\u5b9e\u5b9a\u4e49\u7684\u662f\u4e00\u4e2a\u70b9\u7684*neighborhood*\u548c\u4e00\u4e2a\u70b9\u96c6\u7684*neighborhood*\u3002","title":"DEFINITION"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#definition_2","text":"The degree of a vertex in an undirected graph is the number of edges incident with it, except that a loop at a vertex contributes twice to the degree of that vertex. The degree of the vertex v is denoted by deg(v) .","title":"DEFINITION"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#the#handshaking#theorem","text":"Let G = (V,E) be an undirected graph with m edges. Then $$ 2m = \\sum_{v \\in V} deg(v) $$ (Note that this applies even if multiple edges and loops are present.)","title":"THE HANDSHAKING THEOREM"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#theorem","text":"An undirected graph has an even number of vertices of odd degree. NOTE: \u7ffb\u8bd1\u6210\u4e2d\u6587\u662f\uff1a\u4e00\u4e2a\u65e0\u5411\u56fe\u6709\u5076\u6570\u4e2a\u5947\u6570\u5ea6\u7684\u70b9\u3002","title":"THEOREM"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#directed#graph","text":"Terminology for graphs with directed edges reflects the fact that edges in directed graphs have directions.","title":"directed graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#definition_3","text":"When (u,v) is an edge of the graph G with directed edges, u is said to be adjacent to v and v is said to be adjacent from u . The vertex u is called the initial vertex of (u,v) , and v is called the terminal or end vertex of (u,v) . The initial vertex and terminal vertex of a loop are the same.","title":"DEFINITION"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#definition_4","text":"In a graph with directed edges the in-degree of a vertex v , denoted by deg \u2212 (v) , is the number of edges with v as their terminal vertex. The out-degree of v , denoted by deg + (v) , is the number of edges with v as their initial vertex. (Note that a loop at a vertex contributes 1 to both the in-degree and the out-degree of this vertex.)","title":"DEFINITION"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#theorem_1","text":"Let G = (V,E) be a graph with directed edges. Then $$ \\sum_{v \\in V} deg^-(v) = \\sum_{v \\in V} deg^+(v)=|E| $$ There are many properties of a graph with directed edges that do not depend on the direction of its edges.Consequently, it is often useful to ignore these directions. The undirected graph that results from ignoring directions of edges is called the underlying undirected graph . A graph with directed edges and its underlying undirected graph have the same number of edges. NOTE: \u8fd9\u4e9b\u5c5e\u4e8e\u5728\u540e\u6587\u4e2d\u4f1a\u5927\u91cf\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u5148\u719f\u6089\u719f\u6089\u3002","title":"THEOREM"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#some#special#simple#graphs","text":"","title":"Some Special Simple Graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#complete#graphs","text":"A complete graph on n vertices, denoted by K_n K_n , is a simple graph that contains exactly one edge between each pair of distinct vertices. The graphs K_n K_n , for n = 1,2,3,4,5,6, are displayed in Figure 3. A simple graph for which there is at least one pair of distinct vertex not connected by an edge is called noncomplete .","title":"Complete Graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#cycles","text":"A cycle C_n C_n , n \u2265 3, consists of n vertices v_1 ,v_2 ,...,v_n v_1 ,v_2 ,...,v_n and edges \\{v_1 ,v_2 \\} \\{v_1 ,v_2 \\} , \\{v_2 ,v_3 \\} \\{v_2 ,v_3 \\} ,..., \\{v_{n\u22121} ,v_n \\} \\{v_{n\u22121} ,v_n \\} , and {v_n ,v_1 } {v_n ,v_1 } . The cycles C_3 C_3 , C_4 C_4 , C_5 C_5 , and C_6 C_6 are displayed in Figure 4.","title":"Cycles"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#wheels","text":"We obtain a wheel W_n W_n when we add an additional vertex to a cycle C_n C_n , for n \u2265 3, and connect this new vertex to each of the n vertices in C_n C_n , by new edges. The wheels W_3 W_3 , W_4 W_4 , W_5 W_5 , and W_6 W_6 are displayed in Figure 5.","title":"Wheels"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#n-cubes","text":"An n-dimensional hypercube,or n-cube,denoted by Q_n Q_n ,is a graph that has vertices representing the 2^n 2^n bit strings of length n . Two vertices are adjacent if and only if the bit strings that they represent differ in exactly one bit position. We display Q_1 Q_1 ,$ Q_2$ , and Q_3 Q_3 in Figure 6. Note that you can construct the (n + 1)-cube Q_{n+1} Q_{n+1} from the n-cube Q_n Q_n by making two copies of Q_n Q_n , prefacing the labels on the vertices with a 0 in one copy of Q_n Q_n and with a 1 in the other copy of Q_n Q_n , and adding edges connecting two vertices that have labels differing only in the first bit. In Figure 6, Q_ 3 Q_ 3 is constructed from Q_2 Q_2 by drawing two copies of Q_2 Q_2 as the top and bottom faces of Q_3 Q_3 , adding 0 at the beginning of the label of each vertex in the bottom face and 1 at the beginning of the label of each vertex in the top face. (Here, by face we mean a face of a cube in three-dimensional space. Think of drawing the graph Q_3 Q_3 in three-dimensional space with copies of Q_2 Q_2 as the top and bottom faces of a cube and then drawing the projection of the resulting depiction in the plane.) NOTE: How about Q_4 Q_4 ? see the wikipedia entry Hypercube .","title":"n-Cubes"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#bipartite#graphs","text":"Sometimes a graph has the property that its vertex set can be divided into two disjoint subsets such that each edge connects a vertex in one of these subsets to a vertex in the other subset. For example, consider the graph representing marriages between men and women in a village, where each person is represented by a vertex and a marriage is represented by an edge. In this graph, each edge connects a vertex in the subset of vertices representing males and a vertex in the subset of vertices representing females. This leads us to Definition 5.","title":"Bipartite Graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#definition_5","text":"A simple graph G is called bipartite if its vertex set V can be partitioned into two disjoint sets V_1 V_1 and V_2 V_2 such that every edge in the graph connects a vertex in V_1 V_1 and a vertex in V_2 V_2 (so that no edge in G connects either two vertices in V_1 V_1 or two vertices in V_2 V_2 ). When this condition holds, we call the pair (V_1 ,V_2 ) (V_1 ,V_2 ) a bipartition of the vertex set V of G. In Example 9 we will show that C_6 C_6 is bipartite, and in Example 10 we will show that K_3 K_3 is not bipartite. Theorem below provides a useful criterion for determining whether a graph is bipartite.","title":"DEFINITION"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#theorem#4","text":"A simple graph is bipartite if and only if it is possible to assign one of two different colors to each vertex of the graph so that no two adjacent vertices are assigned the same color. Theorem 4 is an example of a result in the part of graph theory known as graph colorings . Graph colorings is an important part of graph theory with important applications. We will study graph colorings further in Section 10.8. Another useful criterion for determining whether a graph is bipartite is based on the notion of a path , a topic we study in Section 10.4. A graph is bipartite if and only if it is not possible to start at a vertex and return to this vertex by traversing an odd number of distinct edges. We will make this notion more precise when we discuss paths and circuits in graphs in Section 10.4 (see Exercise 63 in that section).","title":"THEOREM 4"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#complete#bipartite#graphs","text":"","title":"Complete Bipartite Graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#bipartite#graphs#and#matchings","text":"","title":"Bipartite Graphs and Matchings"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-and-Graph-theory/#boost#graph#libraryreview#of#elementary#graph#theory","text":"","title":"boost graph library#Review of Elementary Graph Theory"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Directed-graph/Directed-graph/","text":"Directed graph","title":"Directed-graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Directed-graph/Directed-graph/#directed#graph","text":"","title":"Directed graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Directed-graph/Directed-acyclic-graph/Directed-acyclic-graph/","text":"Directed acyclic graph Directed acyclic graph","title":"Directed-acyclic-graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Directed-graph/Directed-acyclic-graph/Directed-acyclic-graph/#directed#acyclic#graph","text":"","title":"Directed acyclic graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/","text":"Graph representation \u7ef4\u57fa\u767e\u79d1 Graph (abstract data type)#Representations Representing weighted graphs NOTE: \u5982\u4f55\u6765\u8868\u793a\u4e00\u4e2aweighted graph\uff1f\u6b63\u5982\u5728Summary\u7ae0\u8282\u6240\u8ff0\uff0c\u5373\u9700\u8981\u8003\u8651structure\u4e5f\u9700\u8981\u8003\u8651property\uff08\u4e5f\u5c31\u662f\u8fd9\u91cc\u8bf4\u7684weight\uff09 Weighted graphs Weighted graph = a graph whose edges have weights The weight of an edge can represent : Cost or distance Capacity = the maximim amount of flow that can be transported from one place to another Representing weighted graphs using an adjacency list Each node in the adjacency graph will contain: A neighbor node ID (this field was already discussed previously) A cost field (this field is new) Example Graph: Representation: Class used to represent (define) edges : /* ========================================== Edges is stored as Node of a linked list ========================================== */ public class Edge { int NodeID ; // The neighbor node double Weight ; // Weight of edge Node next ; // Link variable } Class used to represent (define) a graph: /* ============================================================= The graph is an array of Edge (Edge[i] = all edges of node i) ============================================================== */ public class Graph { public Edge [] graph ; // Array of Edges } Representing weighted graphs using an adjacency array Representing a weighted graph using an adjacency array: If there is no edge between node i and node j, the value of the array element a[i][j] = some very large value Otherwise, a[i][j] is a floating value that is equal to the weight of the edge (i, j) Example: Representation: 0 1 2 3 4 5 6 7 8 +- -+ | * 3 * 2 * * * * 4 | // 0 | 3 * * * * * * 4 * | // 1 | * * * 6 * 1 * 2 * | // 2 | 2 * 6 * 1 * * * * | // 3 M = | * * * 1 * * * * 8 | // 4 | * * 1 * * * 8 * * | // 5 | * * * * * 8 * * * | // 6 | * 4 2 * * * * * * | // 7 | 4 * * * 8 * * * * | // 8 +- -+ * = a very large value (infinite) Class used to represent a graph using an adjacency matrix: public class Graph { /* ======================================= The edges of the graph ======================================= */ double [][] M ; // M[i][j] = weight of edge (i,j) ... } Representing graphs Summary \u6b63\u5982\u4e00\u4e2aweb page\u6d89\u53castructure\u548c\u662ftype\uff0c\u4e00\u4e2agraph\u6d89\u53ca\u5230structure\u548cproperty\uff08edge\u7684property\u548cvertex\u7684property\uff09\uff0c\u6240\u4ee5graph representation\u5c31\u6d89\u53ca\u4e24\u8005\u3002 \u5173\u4e8egraph representation\uff0c\u53ef\u4ee5\u9605\u8bfbboost graph library\u3002 \u4ecerelation\u7684\u89d2\u5ea6\u6765\u5206\u6790graph representation \u4f7f\u7528graph\u6765\u8868\u793arelation\uff0crelation\u662f\u6709\u5c5e\u6027\u7684\uff0c\u6240\u4ee5graph\u9700\u8981\u80fd\u591f\u8868\u793a\u51fa\u8fd9\u4e9brelation\u7684\u5c5e\u6027\uff0c\u5404\u79cdalgorithm\u5176\u5b9e\u5c31\u662f\u57fa\u4e8erelation\u548crelation\u7684\u5c5e\u6027\u7684\u3002 \u8fd9\u5c31\u662fgraph\u3001representation\u3001property\u3001algorithm\u3002","title":"Graph-representations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#graph#representation","text":"","title":"Graph representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#graph#abstract#data#typerepresentations","text":"","title":"\u7ef4\u57fa\u767e\u79d1Graph (abstract data type)#Representations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#representing#weighted#graphs","text":"NOTE: \u5982\u4f55\u6765\u8868\u793a\u4e00\u4e2aweighted graph\uff1f\u6b63\u5982\u5728Summary\u7ae0\u8282\u6240\u8ff0\uff0c\u5373\u9700\u8981\u8003\u8651structure\u4e5f\u9700\u8981\u8003\u8651property\uff08\u4e5f\u5c31\u662f\u8fd9\u91cc\u8bf4\u7684weight\uff09","title":"Representing weighted graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#weighted#graphs","text":"Weighted graph = a graph whose edges have weights The weight of an edge can represent : Cost or distance Capacity = the maximim amount of flow that can be transported from one place to another","title":"Weighted graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#representing#weighted#graphs#using#an#adjacency#list","text":"Each node in the adjacency graph will contain: A neighbor node ID (this field was already discussed previously) A cost field (this field is new)","title":"Representing weighted graphs using an adjacency list"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#example","text":"Graph: Representation: Class used to represent (define) edges : /* ========================================== Edges is stored as Node of a linked list ========================================== */ public class Edge { int NodeID ; // The neighbor node double Weight ; // Weight of edge Node next ; // Link variable } Class used to represent (define) a graph: /* ============================================================= The graph is an array of Edge (Edge[i] = all edges of node i) ============================================================== */ public class Graph { public Edge [] graph ; // Array of Edges }","title":"Example"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#representing#weighted#graphs#using#an#adjacency#array","text":"Representing a weighted graph using an adjacency array: If there is no edge between node i and node j, the value of the array element a[i][j] = some very large value Otherwise, a[i][j] is a floating value that is equal to the weight of the edge (i, j) Example: Representation: 0 1 2 3 4 5 6 7 8 +- -+ | * 3 * 2 * * * * 4 | // 0 | 3 * * * * * * 4 * | // 1 | * * * 6 * 1 * 2 * | // 2 | 2 * 6 * 1 * * * * | // 3 M = | * * * 1 * * * * 8 | // 4 | * * 1 * * * 8 * * | // 5 | * * * * * 8 * * * | // 6 | * 4 2 * * * * * * | // 7 | 4 * * * 8 * * * * | // 8 +- -+ * = a very large value (infinite) Class used to represent a graph using an adjacency matrix: public class Graph { /* ======================================= The edges of the graph ======================================= */ double [][] M ; // M[i][j] = weight of edge (i,j) ... }","title":"Representing weighted graphs using an adjacency array"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#representing#graphs","text":"","title":"Representing graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#summary","text":"\u6b63\u5982\u4e00\u4e2aweb page\u6d89\u53castructure\u548c\u662ftype\uff0c\u4e00\u4e2agraph\u6d89\u53ca\u5230structure\u548cproperty\uff08edge\u7684property\u548cvertex\u7684property\uff09\uff0c\u6240\u4ee5graph representation\u5c31\u6d89\u53ca\u4e24\u8005\u3002 \u5173\u4e8egraph representation\uff0c\u53ef\u4ee5\u9605\u8bfbboost graph library\u3002","title":"Summary"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Graph-representations/#relationgraph#representation","text":"\u4f7f\u7528graph\u6765\u8868\u793arelation\uff0crelation\u662f\u6709\u5c5e\u6027\u7684\uff0c\u6240\u4ee5graph\u9700\u8981\u80fd\u591f\u8868\u793a\u51fa\u8fd9\u4e9brelation\u7684\u5c5e\u6027\uff0c\u5404\u79cdalgorithm\u5176\u5b9e\u5c31\u662f\u57fa\u4e8erelation\u548crelation\u7684\u5c5e\u6027\u7684\u3002 \u8fd9\u5c31\u662fgraph\u3001representation\u3001property\u3001algorithm\u3002","title":"\u4ecerelation\u7684\u89d2\u5ea6\u6765\u5206\u6790graph representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/hackerearth-graph%20representation/","text":"Graph Representation type of node root node leaf node","title":"[Graph Representation](https://www.hackerearth.com/zh/practice/algorithms/graphs/graph-representation/tutorial/)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/hackerearth-graph%20representation/#graph#representation","text":"","title":"Graph Representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/hackerearth-graph%20representation/#type#of#node","text":"","title":"type of node"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/hackerearth-graph%20representation/#root#node","text":"","title":"root node"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/hackerearth-graph%20representation/#leaf#node","text":"","title":"leaf node"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Adjacency-list/Adjacency-list/","text":"Adjacency list \u7ef4\u57fa\u767e\u79d1 Adjacency list Implementation details An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighboring vertices or edges. There are many variations of this basic idea, differing in the details of how they implement the association between vertices and collections, in how they implement the collections, in whether they include both vertices and edges or only vertices as first class objects, and in what kinds of objects are used to represent the vertices and edges. An implementation suggested by Guido van Rossum uses a hash table to associate each vertex in a graph with an array of adjacent vertices. In this representation, a vertex may be represented by any hashable object. There is no explicit representation of edges as objects.[ 1] Cormen et al. suggest an implementation in which the vertices are represented by index numbers.[ 2] Their representation uses an array indexed by vertex number, in which the array cell for each vertex points to a singly linked list of the neighboring vertices of that vertex. In this representation, the nodes of the singly linked list may be interpreted as edge objects; however, they do not store the full information about each edge (they only store one of the two endpoints of the edge) and in undirected graphs there will be two different linked list nodes for each edge (one within the lists for each of the two endpoints of the edge). The object oriented incidence list structure suggested by Goodrich and Tamassia has special classes of vertex objects and edge objects. Each vertex object has an instance variable pointing to a collection object that lists the neighboring edge objects. In turn, each edge object points to the two vertex objects at its endpoints.[ 3] This version of the adjacency list uses more memory than the version in which adjacent vertices are listed directly, but the existence of explicit edge objects allows it extra flexibility in storing additional information about edges. 10.3 Representing Graphs and Graph Isomorphism","title":"Adjacency-list"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Adjacency-list/Adjacency-list/#adjacency#list","text":"","title":"Adjacency list"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Adjacency-list/Adjacency-list/#adjacency#list_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Adjacency list"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Adjacency-list/Adjacency-list/#implementation#details","text":"An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighboring vertices or edges. There are many variations of this basic idea, differing in the details of how they implement the association between vertices and collections, in how they implement the collections, in whether they include both vertices and edges or only vertices as first class objects, and in what kinds of objects are used to represent the vertices and edges. An implementation suggested by Guido van Rossum uses a hash table to associate each vertex in a graph with an array of adjacent vertices. In this representation, a vertex may be represented by any hashable object. There is no explicit representation of edges as objects.[ 1] Cormen et al. suggest an implementation in which the vertices are represented by index numbers.[ 2] Their representation uses an array indexed by vertex number, in which the array cell for each vertex points to a singly linked list of the neighboring vertices of that vertex. In this representation, the nodes of the singly linked list may be interpreted as edge objects; however, they do not store the full information about each edge (they only store one of the two endpoints of the edge) and in undirected graphs there will be two different linked list nodes for each edge (one within the lists for each of the two endpoints of the edge). The object oriented incidence list structure suggested by Goodrich and Tamassia has special classes of vertex objects and edge objects. Each vertex object has an instance variable pointing to a collection object that lists the neighboring edge objects. In turn, each edge object points to the two vertex objects at its endpoints.[ 3] This version of the adjacency list uses more memory than the version in which adjacent vertices are listed directly, but the existence of explicit edge objects allows it extra flexibility in storing additional information about edges.","title":"Implementation details"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Adjacency-list/Adjacency-list/#103#representing#graphs#and#graph#isomorphism","text":"","title":"10.3 Representing Graphs and Graph Isomorphism"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Adjacency-list/Python-Patterns-Implementing-Graphs/","text":"Python Patterns - Implementing Graphs Graphs are networks consisting of nodes connected by edges or arcs . In directed graphs , the connections between nodes have a direction, and are called arcs ; in undirected graphs , the connections have no direction and are called edges . We mainly discuss directed graphs . Algorithms in graphs include finding a path between two nodes, finding the shortest path between two nodes, determining cycles in the graph (a cycle is a non-empty path from a node to itself), finding a path that reaches all nodes (the famous \"traveling salesman problem\"), and so on. Sometimes the nodes or arcs of a graph have weights or costs associated with them, and we are interested in finding the cheapest path . There's considerable literature on graph algorithms, which are an important part of discrete mathematics. Graphs also have much practical use in computer algorithms. Obvious examples can be found in the management of networks, but examples abound in many other areas. For instance, caller-callee relationships in a computer program can be seen as a graph (where cycles indicate recursion, and unreachable nodes represent dead code). Few programming languages provide direct support for graphs as a data type, and Python is no exception. However, graphs are easily built out of lists and dictionaries . For instance, here's a simple graph (I can't use drawings in these columns, so I write down the graph's arcs): A -> B A -> C B -> C B -> D C -> D D -> C E -> F F -> C This graph has six nodes (A-F) and eight arcs. It can be represented by the following Python data structure: graph = { 'A' : [ 'B' , 'C' ], 'B' : [ 'C' , 'D' ], 'C' : [ 'D' ], 'D' : [ 'C' ], 'E' : [ 'F' ], 'F' : [ 'C' ]} This is a dictionary whose keys are the nodes of the graph. For each key, the corresponding value is a list containing the nodes that are connected by a direct arc from this node. This is about as simple as it gets (even simpler, the nodes could be represented by numbers instead of names, but names are more convenient and can easily be made to carry more information, such as city names). Let's write a simple function to determine a path between two nodes. It takes a graph and the start and end nodes as arguments. It will return a list of nodes (including the start and end nodes) comprising the path. When no path can be found, it returns None. The same node will not occur more than once on the path returned (i.e. it won't contain cycles). The algorithm uses an important technique called backtracking : it tries each possibility in turn until it finds a solution. def find_path ( graph , start , end , path = []): path = path + [ start ] if start == end : return path if start not in graph : # \u539f\u6587\u7684\u5199\u6cd5\u662f\uff1aif not graph.has_key(start)\uff0cpython 3\u4e2ddict\u6ca1\u6709has_key\u6210\u5458\u51fd\u6570 return None for node in graph [ start ]: # adjacent nodes if node not in path : newpath = find_path ( graph , node , end , path ) if newpath : return newpath return None A sample run (using the graph above): >>> find_path ( graph , 'A' , 'D' ) [ 'A' , 'B' , 'C' , 'D' ] >>> The second ' if ' statement is necessary only in case there are nodes that are listed as end points for arcs but that don't have outgoing arcs themselves, and aren't listed in the graph at all. Such nodes could also be contained in the graph, with an empty list of outgoing arcs, but sometimes it is more convenient not to require this. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6ca1\u6709\u8bfb\u61c2 Note that while the user calls find_path() with three arguments, it calls itself with a fourth argument: the path that has already been traversed. The default value for this argument is the empty list, ' [] ', meaning no nodes have been traversed yet. This argument is used to avoid cycles (the first ' if ' inside the ' for ' loop). The ' path ' argument is not modified: the assignment \" path = path + [start] \" creates a new list. If we had written \" path.append(start) \" instead, we would have modified the variable ' path ' in the caller, with disastrous results. (Using tuples, we could have been sure this would not happen, at the cost of having to write \" path = path + (start,) \" since \" (start) \" isn't a singleton tuple -- it is just a parenthesized expression.) It is simple to change this function to return a list of all paths (without cycles) instead of the first path it finds: def find_all_paths ( graph , start , end , path = []): path = path + [ start ] if start == end : return [ path ] if start not in graph : # \u539f\u6587\u7684\u5199\u6cd5\u662f\uff1aif not graph.has_key(start)\uff0cpython 3\u4e2ddict\u6ca1\u6709has_key\u6210\u5458\u51fd\u6570 return [] paths = [] for node in graph [ start ]: if node not in path : newpaths = find_all_paths ( graph , node , end , path ) for newpath in newpaths : paths . append ( newpath ) return paths A sample run: >>> find_all_paths ( graph , 'A' , 'D' ) [[ 'A' , 'B' , 'C' , 'D' ], [ 'A' , 'B' , 'D' ], [ 'A' , 'C' , 'D' ]] >>> Another variant finds the shortest path: def find_shortest_path ( graph , start , end , path = []): path = path + [ start ] if start == end : return path if start not in graph : # \u539f\u6587\u7684\u5199\u6cd5\u662f\uff1aif not graph.has_key(start)\uff0cpython 3\u4e2ddict\u6ca1\u6709has_key\u6210\u5458\u51fd\u6570 return None shortest = None for node in graph [ start ]: if node not in path : newpath = find_shortest_path ( graph , node , end , path ) if newpath : if not shortest or len ( newpath ) < len ( shortest ): shortest = newpath return shortest Sample run: >>> find_shortest_path ( graph , 'A' , 'D' ) [ 'A' , 'C' , 'D' ] >>> These functions are about as simple as they get. Yet, they are nearly optimal (for code written in Python). In another Python Patterns column, I will try to analyze their running speed and improve their performance, at the cost of more code. UPDATE: Eryk Kopczy\u0144ski pointed out that these functions are not optimal. To the contrary, \"this program runs in exponential time, while find_shortest_path can be done in linear time using BFS [Breadth First Search]. Furthermore a linear BFS is simpler:\" # Code by Eryk Kopczy\u0144ski def find_shortest_path ( graph , start , end ): dist = { start : [ start ]} # \u8bb0\u5f55\u4e0b\u4ecestart\u5230\u8be5\u8282\u70b9\u7684\u8def\u5f84 q = deque ( start ) while len ( q ): at = q . popleft () for next in graph [ at ]: if next not in dist : dist [ next ] = [ dist [ at ], next ] q . append ( next ) return dist [ end ] Note that this returns the path in a weird format, e.g., [[['A'], 'B'], 'D'] . In particular, len(find_shortest_path(graph, 'A', 'D')) will give the incorrect answer (2, because the outer list is of length 2). This is because append is done as [dist[at], next] instead of dist[at]+[next] . The second method would use quadratic time and memory, but still should be fine for relatively small graphs; otherwise, it is easy to turn the list into the correct format. Another variation would be to add more data abstraction: create a class to represent graphs, whose methods implement the various algorithms. While this appeals to the desire for structured programming, it doesn't make the code any more efficient (to the contrary). It does make it easier to add various labels to the nodes or arcs and to add algorithms that take those labels into account (e.g. to find the shortest route between two cities on a map). This, too, will be the subject of another column.","title":"Python-Patterns-Implementing-Graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Adjacency-list/Python-Patterns-Implementing-Graphs/#python#patterns#-#implementing#graphs","text":"Graphs are networks consisting of nodes connected by edges or arcs . In directed graphs , the connections between nodes have a direction, and are called arcs ; in undirected graphs , the connections have no direction and are called edges . We mainly discuss directed graphs . Algorithms in graphs include finding a path between two nodes, finding the shortest path between two nodes, determining cycles in the graph (a cycle is a non-empty path from a node to itself), finding a path that reaches all nodes (the famous \"traveling salesman problem\"), and so on. Sometimes the nodes or arcs of a graph have weights or costs associated with them, and we are interested in finding the cheapest path . There's considerable literature on graph algorithms, which are an important part of discrete mathematics. Graphs also have much practical use in computer algorithms. Obvious examples can be found in the management of networks, but examples abound in many other areas. For instance, caller-callee relationships in a computer program can be seen as a graph (where cycles indicate recursion, and unreachable nodes represent dead code). Few programming languages provide direct support for graphs as a data type, and Python is no exception. However, graphs are easily built out of lists and dictionaries . For instance, here's a simple graph (I can't use drawings in these columns, so I write down the graph's arcs): A -> B A -> C B -> C B -> D C -> D D -> C E -> F F -> C This graph has six nodes (A-F) and eight arcs. It can be represented by the following Python data structure: graph = { 'A' : [ 'B' , 'C' ], 'B' : [ 'C' , 'D' ], 'C' : [ 'D' ], 'D' : [ 'C' ], 'E' : [ 'F' ], 'F' : [ 'C' ]} This is a dictionary whose keys are the nodes of the graph. For each key, the corresponding value is a list containing the nodes that are connected by a direct arc from this node. This is about as simple as it gets (even simpler, the nodes could be represented by numbers instead of names, but names are more convenient and can easily be made to carry more information, such as city names). Let's write a simple function to determine a path between two nodes. It takes a graph and the start and end nodes as arguments. It will return a list of nodes (including the start and end nodes) comprising the path. When no path can be found, it returns None. The same node will not occur more than once on the path returned (i.e. it won't contain cycles). The algorithm uses an important technique called backtracking : it tries each possibility in turn until it finds a solution. def find_path ( graph , start , end , path = []): path = path + [ start ] if start == end : return path if start not in graph : # \u539f\u6587\u7684\u5199\u6cd5\u662f\uff1aif not graph.has_key(start)\uff0cpython 3\u4e2ddict\u6ca1\u6709has_key\u6210\u5458\u51fd\u6570 return None for node in graph [ start ]: # adjacent nodes if node not in path : newpath = find_path ( graph , node , end , path ) if newpath : return newpath return None A sample run (using the graph above): >>> find_path ( graph , 'A' , 'D' ) [ 'A' , 'B' , 'C' , 'D' ] >>> The second ' if ' statement is necessary only in case there are nodes that are listed as end points for arcs but that don't have outgoing arcs themselves, and aren't listed in the graph at all. Such nodes could also be contained in the graph, with an empty list of outgoing arcs, but sometimes it is more convenient not to require this. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6ca1\u6709\u8bfb\u61c2 Note that while the user calls find_path() with three arguments, it calls itself with a fourth argument: the path that has already been traversed. The default value for this argument is the empty list, ' [] ', meaning no nodes have been traversed yet. This argument is used to avoid cycles (the first ' if ' inside the ' for ' loop). The ' path ' argument is not modified: the assignment \" path = path + [start] \" creates a new list. If we had written \" path.append(start) \" instead, we would have modified the variable ' path ' in the caller, with disastrous results. (Using tuples, we could have been sure this would not happen, at the cost of having to write \" path = path + (start,) \" since \" (start) \" isn't a singleton tuple -- it is just a parenthesized expression.) It is simple to change this function to return a list of all paths (without cycles) instead of the first path it finds: def find_all_paths ( graph , start , end , path = []): path = path + [ start ] if start == end : return [ path ] if start not in graph : # \u539f\u6587\u7684\u5199\u6cd5\u662f\uff1aif not graph.has_key(start)\uff0cpython 3\u4e2ddict\u6ca1\u6709has_key\u6210\u5458\u51fd\u6570 return [] paths = [] for node in graph [ start ]: if node not in path : newpaths = find_all_paths ( graph , node , end , path ) for newpath in newpaths : paths . append ( newpath ) return paths A sample run: >>> find_all_paths ( graph , 'A' , 'D' ) [[ 'A' , 'B' , 'C' , 'D' ], [ 'A' , 'B' , 'D' ], [ 'A' , 'C' , 'D' ]] >>> Another variant finds the shortest path: def find_shortest_path ( graph , start , end , path = []): path = path + [ start ] if start == end : return path if start not in graph : # \u539f\u6587\u7684\u5199\u6cd5\u662f\uff1aif not graph.has_key(start)\uff0cpython 3\u4e2ddict\u6ca1\u6709has_key\u6210\u5458\u51fd\u6570 return None shortest = None for node in graph [ start ]: if node not in path : newpath = find_shortest_path ( graph , node , end , path ) if newpath : if not shortest or len ( newpath ) < len ( shortest ): shortest = newpath return shortest Sample run: >>> find_shortest_path ( graph , 'A' , 'D' ) [ 'A' , 'C' , 'D' ] >>> These functions are about as simple as they get. Yet, they are nearly optimal (for code written in Python). In another Python Patterns column, I will try to analyze their running speed and improve their performance, at the cost of more code. UPDATE: Eryk Kopczy\u0144ski pointed out that these functions are not optimal. To the contrary, \"this program runs in exponential time, while find_shortest_path can be done in linear time using BFS [Breadth First Search]. Furthermore a linear BFS is simpler:\" # Code by Eryk Kopczy\u0144ski def find_shortest_path ( graph , start , end ): dist = { start : [ start ]} # \u8bb0\u5f55\u4e0b\u4ecestart\u5230\u8be5\u8282\u70b9\u7684\u8def\u5f84 q = deque ( start ) while len ( q ): at = q . popleft () for next in graph [ at ]: if next not in dist : dist [ next ] = [ dist [ at ], next ] q . append ( next ) return dist [ end ] Note that this returns the path in a weird format, e.g., [[['A'], 'B'], 'D'] . In particular, len(find_shortest_path(graph, 'A', 'D')) will give the incorrect answer (2, because the outer list is of length 2). This is because append is done as [dist[at], next] instead of dist[at]+[next] . The second method would use quadratic time and memory, but still should be fine for relatively small graphs; otherwise, it is easy to turn the list into the correct format. Another variation would be to add more data abstraction: create a class to represent graphs, whose methods implement the various algorithms. While this appeals to the desire for structured programming, it doesn't make the code any more efficient (to the contrary). It does make it easier to add various labels to the nodes or arcs and to add algorithms that take those labels into account (e.g. to find the shortest route between two cities on a map). This, too, will be the subject of another column.","title":"Python Patterns - Implementing Graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Incidence-matrix/Incidence-matrix/","text":"Incidence matrix \u662f\u5728\u5b66\u4e60boost IncidenceGraph \u65f6\uff0c\u60f3\u5230\u7684incidence matrix\u3002 \u7ef4\u57fa\u767e\u79d1 Incidence matrix","title":"Incidence-matrix"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Incidence-matrix/Incidence-matrix/#incidence#matrix","text":"\u662f\u5728\u5b66\u4e60boost IncidenceGraph \u65f6\uff0c\u60f3\u5230\u7684incidence matrix\u3002","title":"Incidence matrix"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/01-Graph-and-Graph-theory/Graph-representations/Incidence-matrix/Incidence-matrix/#incidence#matrix_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Incidence matrix"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/","text":"Graph Models This mainly includes application-specific graphs. COMMUNICATION NETWORKS We can model different communications networks using vertices to represent devices and edges to represent the particular type of communications links of interest. SOFTWARE DESIGN APPLICATIONS NOTE: graph\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u5176\u4ed6\u7684\u5b66\u79d1\u4e2d\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u6211\u5c06\u6b64\u7bc7\u4f5c\u4e3a\u6211\u603b\u7ed3\u5404\u79cd\u5404\u6837\u7684graph model\u3002\u4e3a\u4ec0\u4e48\u4f7f\u7528graph\u6765\u63cf\u8ff0\u95ee\u9898\u5462\uff1f\u6211\u89c9\u5f97\u56fe\u6709\u5982\u4e0b\u4f18\u52bf\uff1a\u56fe\u80fd\u591f\u975e\u5e38\u597d\u5730\u63cf\u8ff0\u5b9e\u4f53(vertex)\u548c\u5b9e\u4f53\u4e4b\u95f4\u5173\u7cfb(edge)\u3002 \u8fd9\u79cd\u5173\u7cfb\u53ef\u80fd\u662f\u62bd\u8c61\u7684dependency\uff0cprecedence\uff0c\u4e5f\u53ef\u80fd\u8868\u793a\u4fe1\u606f/\u6570\u636e/\u63a7\u5236\u7684\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\uff08\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\u4e5f\u662f\u4e00\u79cd\u5173\u7cfb\uff09\u3002 \u56fe\u80fd\u591f\u63cf\u8ff0\u4fe1\u606f/\u6570\u636e/\u63a7\u5236\u7684\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\uff0c\u5176\u4e2d\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\u4e5f\u662f\u4e00\u79cd\u5173\u7cfb \u4f7f\u7528\u56fe\u6765\u5efa\u6a21\u540e\uff0c\u5f88\u591a\u95ee\u9898\u7684\u89e3\u51b3\u5c31\u53d8\u5f97\u7b80\u5355\u4e86\u3002 Graph models are useful tools in the design of software. We will briefly describe two of these models here. Dependency Graph Module Dependency Graphs One of the most important tasks in designing software is how to structure a program into different parts,or modules. Understanding how the different modules of a program interact is essential not only for program design, but also for testing and maintenance of the resulting software. A module dependency graph provides a useful tool for understanding how different modules of a program interact. In a program dependency graph, each module is represented by a vertex. There is a directed edge from a module to a second module if the second module depends on the first. An example of a program dependency graph for a web browser is shown in Figure 9. NOTE: \u4f7f\u7528edge\u6765\u8868\u793adependency\u5173\u7cfb\uff0c\u5982\u4e0b\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u5173\u4e8e\u8868\u8fbe\u4f9d\u8d56\u5173\u7cfb\u7684\u56fe\u7684\u6587\u7ae0\uff1a Dependency graph Wait-for graph Precedence graph Precedence Graphs and Concurrent Processing Computer programs can be executed more rapidly by executing certain statements concurrently. It is important not to execute a statement that requires results of statements not yet executed. The dependence of statements on previous statements can be represented by a directed graph. Each statement is represented by a vertex, and there is an edge from one statement to a second statement if the second statement cannot be executed before the first statement. This resulting graph is called a precedence graph . A computer program and its graph are displayed in Figure 10. For instance, the graph shows that statement S5 cannot be executed before statements S1 , S2 , and S4 are executed. NOTE: \u4f7f\u7528edge\u6765\u8868\u793aprecedence\u5173\u7cfb\u3002precedence\u5173\u7cfb VS dependency\u5173\u7cfb\uff1f Precedence graph Concurrency control Serializability Serialization Conflict Serializability in DBMS NOTE: Wait-for graph vs Precedence graph ?\u4e24\u8005\u90fd\u5728concurrent control\u76f8\u5173\u95ee\u9898\u4e2d\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u6bd4\u8f83\u4e00\u4e0b\u5b83\u4eec\u3002 Control-flow graph NOTE: \u4f7f\u7528edge\u6765\u8868\u793acontrol\u7684flow \u5728\u7f16\u8bd1\u539f\u7406\u4e2d\u4f7f\u7528\u7684\u63a7\u5236\u6d41\u56fe Dataflow NOTE:\u4f7f\u7528edge\u6765\u8868\u793adata\u7684flow Dataflow programming Data-flow analysis TensorFlow : A machine-learning library based on dataflow programming. \u5728tensorflow\u4e2d\uff0cnode\u8868\u793aoperation\uff0cedge\u8868\u793atensor\uff0c\u4e0e\u6b64\u7c7b\u4f3c\u7684\u6709\uff0c computational graph https://www.codingame.com/playgrounds/9487/deep-learning-from-scratch---theory-and-implementation/computational-graphs http://colah.github.io/posts/2015-08-Backprop/ http://www.cs.columbia.edu/~mcollins/ff2.pdf Knowledge Graph word graph In jieba , based on a prefix dictionary structure to achieve efficient word graph scanning. Build a directed acyclic graph (DAG) for all possible word combinations. directed acyclic word graph probabilistic graphical model Tree diagram (probability theory) Automata theory Finite-state machine TOURNAMENTS We now give some examples that show how graphs can also be used to model different kinds of tournaments. Round-Robin Tournaments A tournament where each team plays every other team exactly once and no ties are allowed is called a round-robin tournament . Such tournaments can be modeled using directed graphs where each team is represented by a vertex. Note that (a,b) is an edge if team a beats team b . This graph is a simple directed graph, containing no loops or multiple directed edges (because no two teams play each other more than once). Such a directed graph model is presented in Figure 13. We see that Team 1 is undefeated in this tournament, and Team 3 is winless. Single-Elimination Tournaments A tournament where each contestant is eliminated after one loss is called a single-elimination tournament. Single-elimination tournaments are often used in sports, including tennis championships and the yearly NCAA basketball championship. We can model such a tournament using a vertex to represent each game and a directed edge to connect a game to the next game the winner of this game played in. The graph in Figure 14 represents the games played by the final 16 teams in the 2010 NCAA women\u2019s basketball tournament. NOTE: Bracket (tournament) More \u66f4\u591a\u56fe\u6a21\u578b\u53c2\u89c1 Application-specific graphs","title":"Graph-Model"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#graph#models","text":"This mainly includes application-specific graphs.","title":"Graph Models"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#communication#networks","text":"We can model different communications networks using vertices to represent devices and edges to represent the particular type of communications links of interest.","title":"COMMUNICATION NETWORKS"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#software#design#applications","text":"NOTE: graph\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u5176\u4ed6\u7684\u5b66\u79d1\u4e2d\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u6211\u5c06\u6b64\u7bc7\u4f5c\u4e3a\u6211\u603b\u7ed3\u5404\u79cd\u5404\u6837\u7684graph model\u3002\u4e3a\u4ec0\u4e48\u4f7f\u7528graph\u6765\u63cf\u8ff0\u95ee\u9898\u5462\uff1f\u6211\u89c9\u5f97\u56fe\u6709\u5982\u4e0b\u4f18\u52bf\uff1a\u56fe\u80fd\u591f\u975e\u5e38\u597d\u5730\u63cf\u8ff0\u5b9e\u4f53(vertex)\u548c\u5b9e\u4f53\u4e4b\u95f4\u5173\u7cfb(edge)\u3002 \u8fd9\u79cd\u5173\u7cfb\u53ef\u80fd\u662f\u62bd\u8c61\u7684dependency\uff0cprecedence\uff0c\u4e5f\u53ef\u80fd\u8868\u793a\u4fe1\u606f/\u6570\u636e/\u63a7\u5236\u7684\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\uff08\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\u4e5f\u662f\u4e00\u79cd\u5173\u7cfb\uff09\u3002 \u56fe\u80fd\u591f\u63cf\u8ff0\u4fe1\u606f/\u6570\u636e/\u63a7\u5236\u7684\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\uff0c\u5176\u4e2d\u6d41\u52a8/\u4f20\u9012/\u8f6c\u79fb\u4e5f\u662f\u4e00\u79cd\u5173\u7cfb \u4f7f\u7528\u56fe\u6765\u5efa\u6a21\u540e\uff0c\u5f88\u591a\u95ee\u9898\u7684\u89e3\u51b3\u5c31\u53d8\u5f97\u7b80\u5355\u4e86\u3002 Graph models are useful tools in the design of software. We will briefly describe two of these models here.","title":"SOFTWARE DESIGN APPLICATIONS"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#dependency#graph","text":"Module Dependency Graphs One of the most important tasks in designing software is how to structure a program into different parts,or modules. Understanding how the different modules of a program interact is essential not only for program design, but also for testing and maintenance of the resulting software. A module dependency graph provides a useful tool for understanding how different modules of a program interact. In a program dependency graph, each module is represented by a vertex. There is a directed edge from a module to a second module if the second module depends on the first. An example of a program dependency graph for a web browser is shown in Figure 9. NOTE: \u4f7f\u7528edge\u6765\u8868\u793adependency\u5173\u7cfb\uff0c\u5982\u4e0b\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u5173\u4e8e\u8868\u8fbe\u4f9d\u8d56\u5173\u7cfb\u7684\u56fe\u7684\u6587\u7ae0\uff1a Dependency graph Wait-for graph","title":"Dependency Graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#precedence#graph","text":"Precedence Graphs and Concurrent Processing Computer programs can be executed more rapidly by executing certain statements concurrently. It is important not to execute a statement that requires results of statements not yet executed. The dependence of statements on previous statements can be represented by a directed graph. Each statement is represented by a vertex, and there is an edge from one statement to a second statement if the second statement cannot be executed before the first statement. This resulting graph is called a precedence graph . A computer program and its graph are displayed in Figure 10. For instance, the graph shows that statement S5 cannot be executed before statements S1 , S2 , and S4 are executed. NOTE: \u4f7f\u7528edge\u6765\u8868\u793aprecedence\u5173\u7cfb\u3002precedence\u5173\u7cfb VS dependency\u5173\u7cfb\uff1f Precedence graph Concurrency control Serializability Serialization Conflict Serializability in DBMS NOTE: Wait-for graph vs Precedence graph ?\u4e24\u8005\u90fd\u5728concurrent control\u76f8\u5173\u95ee\u9898\u4e2d\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u6bd4\u8f83\u4e00\u4e0b\u5b83\u4eec\u3002","title":"Precedence graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#control-flow#graph","text":"NOTE: \u4f7f\u7528edge\u6765\u8868\u793acontrol\u7684flow \u5728\u7f16\u8bd1\u539f\u7406\u4e2d\u4f7f\u7528\u7684\u63a7\u5236\u6d41\u56fe","title":"Control-flow graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#dataflow","text":"NOTE:\u4f7f\u7528edge\u6765\u8868\u793adata\u7684flow Dataflow programming Data-flow analysis TensorFlow : A machine-learning library based on dataflow programming. \u5728tensorflow\u4e2d\uff0cnode\u8868\u793aoperation\uff0cedge\u8868\u793atensor\uff0c\u4e0e\u6b64\u7c7b\u4f3c\u7684\u6709\uff0c","title":"Dataflow"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#computational#graph","text":"https://www.codingame.com/playgrounds/9487/deep-learning-from-scratch---theory-and-implementation/computational-graphs http://colah.github.io/posts/2015-08-Backprop/ http://www.cs.columbia.edu/~mcollins/ff2.pdf","title":"computational graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#knowledge#graph","text":"","title":"Knowledge Graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#word#graph","text":"In jieba , based on a prefix dictionary structure to achieve efficient word graph scanning. Build a directed acyclic graph (DAG) for all possible word combinations. directed acyclic word graph","title":"word graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#probabilistic#graphical#model","text":"Tree diagram (probability theory)","title":"probabilistic graphical model"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#automata#theory","text":"","title":"Automata theory"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#finite-state#machine","text":"","title":"Finite-state machine"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#tournaments","text":"We now give some examples that show how graphs can also be used to model different kinds of tournaments.","title":"TOURNAMENTS"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#round-robin#tournaments","text":"A tournament where each team plays every other team exactly once and no ties are allowed is called a round-robin tournament . Such tournaments can be modeled using directed graphs where each team is represented by a vertex. Note that (a,b) is an edge if team a beats team b . This graph is a simple directed graph, containing no loops or multiple directed edges (because no two teams play each other more than once). Such a directed graph model is presented in Figure 13. We see that Team 1 is undefeated in this tournament, and Team 3 is winless.","title":"Round-Robin Tournaments"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#single-elimination#tournaments","text":"A tournament where each contestant is eliminated after one loss is called a single-elimination tournament. Single-elimination tournaments are often used in sports, including tennis championships and the yearly NCAA basketball championship. We can model such a tournament using a vertex to represent each game and a directed edge to connect a game to the next game the winner of this game played in. The graph in Figure 14 represents the games played by the final 16 teams in the 2010 NCAA women\u2019s basketball tournament. NOTE: Bracket (tournament)","title":"Single-Elimination Tournaments"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Graph-Model/#more","text":"\u66f4\u591a\u56fe\u6a21\u578b\u53c2\u89c1 Application-specific graphs","title":"More"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Dependency-graph/Dependency-graph/","text":"Dependency graph Dependency graph","title":"Dependency-graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Dependency-graph/Dependency-graph/#dependency#graph","text":"","title":"Dependency graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Dependency-graph/Dependency-resolving-algorithm/","text":"Dependency Resolving Algorithm Algorithm for dependency resolution","title":"Dependency-resolving-algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Dependency-graph/Dependency-resolving-algorithm/#dependency#resolving#algorithm","text":"","title":"Dependency Resolving Algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Dependency-graph/Dependency-resolving-algorithm/#algorithm#for#dependency#resolution","text":"","title":"Algorithm for dependency resolution"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Dependency-graph/Precedence-graph/","text":"Precedence graph Precedence graph","title":"Precedence-graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/02-Graph-Model/Dependency-graph/Precedence-graph/#precedence#graph","text":"","title":"Precedence graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/03-Graph-operation/Graph-operation/","text":"Graph operations Graph operations produce new graphs from initial ones. They may be separated into the following major categories. Unary operations Unary operations create a new graph from one initial one. Elementary operations Elementary operations or editing operations create a new graph from one initial one by a simple local change, such as addition or deletion of a vertex or of an edge, merging and splitting of vertices, edge contraction , etc. The graph edit distance between a pair of graphs is the minimum number of elementary operations required to transform one graph into the other. Advanced operations Advanced operations create a new graph from one initial one by a complex changes, such as: transpose graph ; complement graph ; line graph ; graph minor ; graph rewriting ; power of graph ; dual graph ; medial graph ; quotient graph ; Y-\u0394 transform ; Mycielskian . Binary operations Binary operations create a new graph from two initial ones *G*1 = (*V*1, *E*1) and *G*2 = (*V*2, *E*2), such as: graph union: *G*1 \u222a *G*2. There are two definitions. In the most common one, the disjoint union of graphs , the union is assumed to be disjoint. Less commonly (though more consistent with the general definition of union in mathematics) the union of two graphs is defined as the graph (*V*1 \u222a *V*2, *E*1 \u222a *E*2). \u56fe\u64cd\u4f5c\uff1a serialize/topological sorting\uff0c\u53c2\u89c1 Topological sorting C3 linearization shortest path Spanning tree see more: Computational problems in graph theory","title":"Graph-operation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/03-Graph-operation/Graph-operation/#graph#operations","text":"Graph operations produce new graphs from initial ones. They may be separated into the following major categories.","title":"Graph operations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/03-Graph-operation/Graph-operation/#unary#operations","text":"Unary operations create a new graph from one initial one.","title":"Unary operations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/03-Graph-operation/Graph-operation/#elementary#operations","text":"Elementary operations or editing operations create a new graph from one initial one by a simple local change, such as addition or deletion of a vertex or of an edge, merging and splitting of vertices, edge contraction , etc. The graph edit distance between a pair of graphs is the minimum number of elementary operations required to transform one graph into the other.","title":"Elementary operations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/03-Graph-operation/Graph-operation/#advanced#operations","text":"Advanced operations create a new graph from one initial one by a complex changes, such as: transpose graph ; complement graph ; line graph ; graph minor ; graph rewriting ; power of graph ; dual graph ; medial graph ; quotient graph ; Y-\u0394 transform ; Mycielskian .","title":"Advanced operations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/03-Graph-operation/Graph-operation/#binary#operations","text":"Binary operations create a new graph from two initial ones *G*1 = (*V*1, *E*1) and *G*2 = (*V*2, *E*2), such as: graph union: *G*1 \u222a *G*2. There are two definitions. In the most common one, the disjoint union of graphs , the union is assumed to be disjoint. Less commonly (though more consistent with the general definition of union in mathematics) the union of two graphs is defined as the graph (*V*1 \u222a *V*2, *E*1 \u222a *E*2). \u56fe\u64cd\u4f5c\uff1a serialize/topological sorting\uff0c\u53c2\u89c1 Topological sorting C3 linearization shortest path Spanning tree see more: Computational problems in graph theory","title":"Binary operations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/","text":"Computational problems in graph theory Spanning tree Topological sorting Pre-topological order C3 linearization Graph coloring path related problem Eulerian path Hamiltonian path Shortest path problem Longest path problem Tree decomposition Graph algorithms Reachability Connectivity (graph theory) TODO Computational problems in graph theory Spanning tree Topological sorting Pre-topological order C3 linearization Graph coloring path related problem Eulerian path Hamiltonian path Hamiltonian path problem Hamiltonian cycle polynomial Shortest path problem Longest path problem Tree decomposition Junction tree algorithm Graph algorithms Graph algorithms solve problems related to graph theory . Reachability Connectivity (graph theory) TODO Directed graph traversal, orderings and applications to data-flow analysis Control-flow graph","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#computational#problems#in#graph#theory","text":"","title":"Computational problems in graph theory"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#spanning#tree","text":"","title":"Spanning tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#topological#sorting","text":"","title":"Topological sorting"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#pre-topological#order","text":"","title":"Pre-topological order"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#c3#linearization","text":"","title":"C3 linearization"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#graph#coloring","text":"","title":"Graph coloring"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#path#related#problem","text":"","title":"path related problem"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#eulerian#path","text":"","title":"Eulerian path"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#hamiltonian#path","text":"Hamiltonian path problem Hamiltonian cycle polynomial","title":"Hamiltonian path"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#shortest#path#problem","text":"","title":"Shortest path problem"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#longest#path#problem","text":"","title":"Longest path problem"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#tree#decomposition","text":"Junction tree algorithm","title":"Tree decomposition"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#graph#algorithms","text":"Graph algorithms solve problems related to graph theory .","title":"Graph algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#reachability","text":"","title":"Reachability"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#connectivity#graph#theory","text":"","title":"Connectivity (graph theory)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/#todo","text":"Directed graph traversal, orderings and applications to data-flow analysis Control-flow graph","title":"TODO"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/TODO/","text":"TODO \u5982\u4f55\u5224\u65ad\u4e00\u4e2agraph\u662f\u5426\u662ftree\uff1f","title":"TODO"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/TODO/#todo","text":"\u5982\u4f55\u5224\u65ad\u4e00\u4e2agraph\u662f\u5426\u662ftree\uff1f","title":"TODO"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/","text":"Circular dependency Acyclic dependencies principle Resolve build errors due to circular dependency amongst classes Circular dependencies in C++ Circular dependency Acyclic dependencies principle Resolve build errors due to circular dependency amongst classes Circular dependencies in C++","title":"Circular-dependency"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/#circular#dependency","text":"","title":"Circular dependency"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/#acyclic#dependencies#principle","text":"","title":"Acyclic dependencies principle"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/#resolve#build#errors#due#to#circular#dependency#amongst#classes","text":"","title":"Resolve build errors due to circular dependency amongst classes"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/Circular-dependency/#circular#dependencies#in#c","text":"","title":"Circular dependencies in C++"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/Cycle-detection/","text":"Cycle detection Detect Cycle in a Directed Graph","title":"Cycle-detection"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/Cycle-detection/#cycle#detection","text":"","title":"Cycle detection"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/Cycle-detection/#detect#cycle#in#a#directed#graph","text":"","title":"Detect Cycle in a Directed Graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/TODO-Max-circle/","text":"Max circle in graph How to find max circle in a graph? \u53c2\u89c1: stackoverflow Longest circle in graphs wikipedia Travelling salesman problem","title":"TODO-Max-circle"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Circular/TODO-Max-circle/#max#circle#in#graph","text":"How to find max circle in a graph? \u53c2\u89c1: stackoverflow Longest circle in graphs wikipedia Travelling salesman problem","title":"Max circle in graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/","text":"Closure \u8fd9\u4e2a\u8bcd\u5728\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u78b0\u5230\uff0c\u672c\u6587\u5bf9\u5b83\u8fdb\u884c\u603b\u7ed3\u3002 \u201cclosure\"\u662f\u201dclose\u201c\u7684\u540d\u8bcd\u5f62\u5f0f\u3002 wikipedia Closure (mathematics) A set is closed under an operation if performance of that operation on members of the set always produces a member of that set. Examples Recursive definition wikipedia Closure (computer programming) wikipedia Kleene star Transitive_closure wikipedia Transitive_closure Draft \u6709\u5fc5\u8981\u5bf9closure\u8fdb\u884c\u603b\u7ed3\uff0c\u9700\u8981\u5c06\u5b83\u603b\u7ed3\u5230graph\u7ae0\u8282\u4e2d\u3002 TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems \u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\u63d0\u53ca\u4e86**closure\uff1a** TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems the TensorFlow implementation can compute the transitive closure of all nodes that must be executed in order to compute the outputs that were requested Draft 2 \u5728 https://cwiki.apache.org/confluence/display/ZOOKEEPER/PoweredBy \u4e2d\u4e5f\u662f\u7528\u4e86transitive closure\u8fd9\u4e2a\u8bcd\u8bed 20201123 \u91cd\u590d\u8fd0\u7528\u4e00\u4e2arelation\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u6761path\u3002 20201223 \u5728\u9f99\u4e66\u4e2d\u6709\u5173\u4e8e\u95ed\u5305\u7684\u63cf\u8ff0 \u5728\u79bb\u6563\u6570\u5b66\u7684\u5173\u7cfb\u7ae0\u8282\u4e2d\u6709\u95ed\u5305\u7684\u63cf\u8ff0 Kleene star Context-free grammar \u4e2d\u4e5f\u6709\u5173\u4e8eclosure\u7684\u63cf\u8ff0 \u4ece Formation rule / Production rule \u7684\u89d2\u5ea6\u6765\u770b\u5f85\u95ed\u5305 \u4ece\u903b\u8f91\u5b66\u63a8\u5bfc\u7684\u89d2\u5ea6\u6765\u770b\u5f85\u95ed\u5305 \u6211\u4e00\u76f4\u60f3\u8981\u641e\u6e05\u695a\u7684closure\u5c31\u662f\u57fa\u4e8e\u903b\u8f91\u63a8\u5bfc\u548c\u96c6\u5408\u8bba\u7684\u3002 Set theory \u662f\u6570\u5b66\u7684\u57fa\u77f3\u6240\u5728\uff0c\u5f88\u591a\u5176\u4ed6\u6570\u5b66\u5b66\u79d1\u90fd\u662f\u5efa\u7acb\u5728\u5b83\u7684\u57fa\u7840\u4e4b\u4e0a\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#closure","text":"\u8fd9\u4e2a\u8bcd\u5728\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u78b0\u5230\uff0c\u672c\u6587\u5bf9\u5b83\u8fdb\u884c\u603b\u7ed3\u3002 \u201cclosure\"\u662f\u201dclose\u201c\u7684\u540d\u8bcd\u5f62\u5f0f\u3002","title":"Closure"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#wikipedia#closure#mathematics","text":"A set is closed under an operation if performance of that operation on members of the set always produces a member of that set.","title":"wikipedia Closure (mathematics)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#examples","text":"Recursive definition","title":"Examples"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#wikipedia#closure#computer#programming","text":"","title":"wikipedia Closure (computer programming)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#wikipedia#kleene#star","text":"","title":"wikipedia Kleene star"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#transitive_closure","text":"wikipedia Transitive_closure","title":"Transitive_closure"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#draft","text":"\u6709\u5fc5\u8981\u5bf9closure\u8fdb\u884c\u603b\u7ed3\uff0c\u9700\u8981\u5c06\u5b83\u603b\u7ed3\u5230graph\u7ae0\u8282\u4e2d\u3002","title":"Draft"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#tensorflow#large-scale#machine#learning#on#heterogeneous#distributed#systems","text":"\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\u63d0\u53ca\u4e86**closure\uff1a** TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems the TensorFlow implementation can compute the transitive closure of all nodes that must be executed in order to compute the outputs that were requested","title":"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#draft#2","text":"\u5728 https://cwiki.apache.org/confluence/display/ZOOKEEPER/PoweredBy \u4e2d\u4e5f\u662f\u7528\u4e86transitive closure\u8fd9\u4e2a\u8bcd\u8bed","title":"Draft 2"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#20201123","text":"\u91cd\u590d\u8fd0\u7528\u4e00\u4e2arelation\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u6761path\u3002","title":"20201123"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Closure/#20201223","text":"\u5728\u9f99\u4e66\u4e2d\u6709\u5173\u4e8e\u95ed\u5305\u7684\u63cf\u8ff0 \u5728\u79bb\u6563\u6570\u5b66\u7684\u5173\u7cfb\u7ae0\u8282\u4e2d\u6709\u95ed\u5305\u7684\u63cf\u8ff0 Kleene star Context-free grammar \u4e2d\u4e5f\u6709\u5173\u4e8eclosure\u7684\u63cf\u8ff0 \u4ece Formation rule / Production rule \u7684\u89d2\u5ea6\u6765\u770b\u5f85\u95ed\u5305 \u4ece\u903b\u8f91\u5b66\u63a8\u5bfc\u7684\u89d2\u5ea6\u6765\u770b\u5f85\u95ed\u5305 \u6211\u4e00\u76f4\u60f3\u8981\u641e\u6e05\u695a\u7684closure\u5c31\u662f\u57fa\u4e8e\u903b\u8f91\u63a8\u5bfc\u548c\u96c6\u5408\u8bba\u7684\u3002 Set theory \u662f\u6570\u5b66\u7684\u57fa\u77f3\u6240\u5728\uff0c\u5f88\u591a\u5176\u4ed6\u6570\u5b66\u5b66\u79d1\u90fd\u662f\u5efa\u7acb\u5728\u5b83\u7684\u57fa\u7840\u4e4b\u4e0a\u3002","title":"20201223"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Dominator/Dominator%28graph-theory%29/","text":"Dominator (graph theory)","title":"Dominator"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Dominator/Dominator%28graph-theory%29/#dominator#graph#theory","text":"","title":"Dominator (graph theory)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Path/TODO-Max-path/","text":"Max path in a graph \u53c2\u89c1: wikipedia Longest path problem","title":"Max path in a graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Path/TODO-Max-path/#max#path#in#a#graph","text":"\u53c2\u89c1: wikipedia Longest path problem","title":"Max path in a graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Path/Dijkstra%27s-algorithm/Dijkstra%27s-algorithm/","text":"Dijkstra's algorithm \u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790 \u6309\u7167\u5728 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Search-algorithm\\Methods\\Summary\\Search-Algorithm.md \u4e2d\u63cf\u8ff0\u7684\u7b97\u6cd5\u6846\u67b6\u6765\u5206\u6790Dijkstra's algorithm\u3002 \u96c6\u5408S\u4e3a**CLOSED**\u96c6\uff0c\u5728\u8be5\u7b97\u6cd5\u4e2d\uff0c\u5e76\u6ca1\u6709\u663e\u5f0f\u5730\u7ef4\u62a4**OPEN\u96c6**\u3002 \u5bf9\u4e8eDijkstra's algorithm\uff0c\u5b83\u7684\u201cstrategies for selecting which node to expand next\u201d\u662f\u91c7\u7528\u7684greedy\u7b56\u7565\uff0c\u5373\u6bcf\u6b21\u9009\u62e9\u6700\u5c0f\u7684distance\u3002 dijkstra\u7684\u7b97\u6cd5\u601d\u60f3\uff1a greedy dynamic programming #define maxint 9999 /** * \u5355\u6e90\u6700\u77ed\u8def\u5f84\u7684Dijkstra\u7b97\u6cd5 * \u6b65\u9aa4: * 1. \u521d\u59cb\u5316 * 2. \u8d2a\u5fc3\u9009\u62e9 * 3. \u66f4\u65b0 * @param n \u9876\u70b9\u7684\u4e2a\u6570 * @param v \u6e90\u70b9\u7684index * @param dist \u6e90\u70b9\u5230\u5404\u8282\u70b9\u7684\u8ddd\u79bb * @param prev \u524d\u9a71\u8282\u70b9 * @param c \u56fe\u4e2d\u8282\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb */ template < typename Type > void Dijkstra ( int n , int v , Type dist [], int prev [], Type ** c ) { bool s [ maxint ]; // CLOSED\uff0c\u5f53\u6e90\u70b9\u5230\u8282\u70b9\u7684\u8ddd\u79bb\u5df2\u77e5\u7684\u65f6\u5019\uff0c\u5c31\u5c06\u8be5\u8282\u70b9\u52a0\u5165\u5230\u8fd9\u4e2a\u96c6\u5408\u4e2d // 1. \u521d\u59cb\u5316 for ( int i = 1 ; i <= n ; ++ i ) { dist [ i ] = c [ v ][ i ]; // \u6e90\u70b9\u5230\u8282\u70b9\u7684\u8ddd\u79bb s [ i ] = false ; // \u4e0e\u6e90\u70b9\u4e0d\u76f8\u90bb\u7684\u8282\u70b9 if ( dist [ i ] == maxint ) { prev [ i ] = 0 ; } // \u4e0e\u6e90\u70b9\u7684\u76f8\u90bb\u8282\u70b9 else { prev [ i ] = v ; } } dist [ v ] = 0 ; s [ v ] = true ; // \u5c06\u6e90\u70b9\u52a0\u5165\u5230CLOSED\u4e2d for ( int i = 10 ; i < n ; ++ i ) { // 2. \u8d2a\u5fc3\u9009\u62e9 // \u9009\u62e9dist\u4e2d\u7684\u6700\u5c0f\u503c int temp = maxint ; int u = v ; for ( int j = 1 ; i <= n ; j ++ ) { if (( ! s [ j ]) && ( dist [ j ] < temp )) { u = j ; temp = dist [ j ]; } } s [ u ] = true ; // \u5c06u\u52a0\u5165\u5230CLOSED\u4e2d // 3. \u66f4\u65b0 // \u4f7f\u7528u\u6765\u8fdb\u884c\u66f4\u65b0 for ( int j = 1 ; j <= n ; ++ j ) { if (( ! s [ j ]) && ( c [ u ][ j ] < maxint )) { Type newdist = dist [ u ] + c [ u ][ j ]; if ( newdist < dist [ j ]) { dist [ j ] = newdist ; prev [ j ] = u ; } } } } } int main () { } \u7ef4\u57fa\u767e\u79d1 Dijkstra's algorithm The algorithm exists in many variants. Dijkstra's original algorithm found the shortest path between two given nodes,[ 5] but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree . NOTE: single source The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered . It can be generalized to use any labels that are partially ordered , provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing. This generalization is called the generic Dijkstra shortest-path algorithm.[ 7] NOTE: label\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f","title":"Dijkstra's algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Path/Dijkstra%27s-algorithm/Dijkstra%27s-algorithm/#dijkstras#algorithm","text":"","title":"Dijkstra's algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Path/Dijkstra%27s-algorithm/Dijkstra%27s-algorithm/#_1","text":"\u6309\u7167\u5728 Relation-structure-computation\\Structure\\Data-structure\\Graph\\Search-algorithm\\Methods\\Summary\\Search-Algorithm.md \u4e2d\u63cf\u8ff0\u7684\u7b97\u6cd5\u6846\u67b6\u6765\u5206\u6790Dijkstra's algorithm\u3002 \u96c6\u5408S\u4e3a**CLOSED**\u96c6\uff0c\u5728\u8be5\u7b97\u6cd5\u4e2d\uff0c\u5e76\u6ca1\u6709\u663e\u5f0f\u5730\u7ef4\u62a4**OPEN\u96c6**\u3002 \u5bf9\u4e8eDijkstra's algorithm\uff0c\u5b83\u7684\u201cstrategies for selecting which node to expand next\u201d\u662f\u91c7\u7528\u7684greedy\u7b56\u7565\uff0c\u5373\u6bcf\u6b21\u9009\u62e9\u6700\u5c0f\u7684distance\u3002 dijkstra\u7684\u7b97\u6cd5\u601d\u60f3\uff1a greedy dynamic programming #define maxint 9999 /** * \u5355\u6e90\u6700\u77ed\u8def\u5f84\u7684Dijkstra\u7b97\u6cd5 * \u6b65\u9aa4: * 1. \u521d\u59cb\u5316 * 2. \u8d2a\u5fc3\u9009\u62e9 * 3. \u66f4\u65b0 * @param n \u9876\u70b9\u7684\u4e2a\u6570 * @param v \u6e90\u70b9\u7684index * @param dist \u6e90\u70b9\u5230\u5404\u8282\u70b9\u7684\u8ddd\u79bb * @param prev \u524d\u9a71\u8282\u70b9 * @param c \u56fe\u4e2d\u8282\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb */ template < typename Type > void Dijkstra ( int n , int v , Type dist [], int prev [], Type ** c ) { bool s [ maxint ]; // CLOSED\uff0c\u5f53\u6e90\u70b9\u5230\u8282\u70b9\u7684\u8ddd\u79bb\u5df2\u77e5\u7684\u65f6\u5019\uff0c\u5c31\u5c06\u8be5\u8282\u70b9\u52a0\u5165\u5230\u8fd9\u4e2a\u96c6\u5408\u4e2d // 1. \u521d\u59cb\u5316 for ( int i = 1 ; i <= n ; ++ i ) { dist [ i ] = c [ v ][ i ]; // \u6e90\u70b9\u5230\u8282\u70b9\u7684\u8ddd\u79bb s [ i ] = false ; // \u4e0e\u6e90\u70b9\u4e0d\u76f8\u90bb\u7684\u8282\u70b9 if ( dist [ i ] == maxint ) { prev [ i ] = 0 ; } // \u4e0e\u6e90\u70b9\u7684\u76f8\u90bb\u8282\u70b9 else { prev [ i ] = v ; } } dist [ v ] = 0 ; s [ v ] = true ; // \u5c06\u6e90\u70b9\u52a0\u5165\u5230CLOSED\u4e2d for ( int i = 10 ; i < n ; ++ i ) { // 2. \u8d2a\u5fc3\u9009\u62e9 // \u9009\u62e9dist\u4e2d\u7684\u6700\u5c0f\u503c int temp = maxint ; int u = v ; for ( int j = 1 ; i <= n ; j ++ ) { if (( ! s [ j ]) && ( dist [ j ] < temp )) { u = j ; temp = dist [ j ]; } } s [ u ] = true ; // \u5c06u\u52a0\u5165\u5230CLOSED\u4e2d // 3. \u66f4\u65b0 // \u4f7f\u7528u\u6765\u8fdb\u884c\u66f4\u65b0 for ( int j = 1 ; j <= n ; ++ j ) { if (( ! s [ j ]) && ( c [ u ][ j ] < maxint )) { Type newdist = dist [ u ] + c [ u ][ j ]; if ( newdist < dist [ j ]) { dist [ j ] = newdist ; prev [ j ] = u ; } } } } } int main () { }","title":"\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Path/Dijkstra%27s-algorithm/Dijkstra%27s-algorithm/#dijkstras#algorithm_1","text":"The algorithm exists in many variants. Dijkstra's original algorithm found the shortest path between two given nodes,[ 5] but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree . NOTE: single source The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered . It can be generalized to use any labels that are partially ordered , provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing. This generalization is called the generic Dijkstra shortest-path algorithm.[ 7] NOTE: label\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f","title":"\u7ef4\u57fa\u767e\u79d1Dijkstra's algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Sorting/Topological-sorting/","text":"Topological sorting","title":"Topological-sorting"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Sorting/Topological-sorting/#topological#sorting","text":"","title":"Topological sorting"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Spanning-tree/Bor%C5%AFvka%27s-algorithm/","text":"","title":"Bor\u016fvka's-algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Spanning-tree/Minimum-spanning-tree/","text":"","title":"Minimum-spanning-tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Spanning-tree/Spanning-tree/","text":"Spanning tree","title":"Spanning-tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/04-Computational-problems-in-graph-theory/Spanning-tree/Spanning-tree/#spanning#tree","text":"","title":"Spanning tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0graph\u7684\u5b9e\u73b0\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/#_1","text":"\u672c\u7ae0\u63cf\u8ff0graph\u7684\u5b9e\u73b0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/","text":"Cpp\u5f00\u6e90\u5e93 boost graph Boost.org graph module http://boost.org/libs/graph bjoern-andres/graph Graphs and Graph Algorithms in C++, including Minimum Cost (Lifted) Multicuts http://www.andres.sc/graph.html ogdf/ogdf This is a release mirror for OGDF, the Open Graph Drawing Framework/Open Graph algorithms and Data structure Framework. jinyuliao/Generic*Graph* Generic graph data structure plugin for ue4 shaolinbit/minisam_ lib Lightweighted graph optimization (Factor graph ) library. vincarlet/vf3*lib* VF3 Algorithm - The fastest sequential algorithm to solve subgraph isomorphism on large and dense graphs rdmpage/ graph -template-library bgauzere/ graph - lib C++ library for graph kernel and edit distance algorithm","title":"Cpp-library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#cpp","text":"","title":"Cpp\u5f00\u6e90\u5e93"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#boost#graph","text":"Boost.org graph module http://boost.org/libs/graph","title":"boost graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#bjoern-andresgraph","text":"Graphs and Graph Algorithms in C++, including Minimum Cost (Lifted) Multicuts http://www.andres.sc/graph.html","title":"bjoern-andres/graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#ogdfogdf","text":"This is a release mirror for OGDF, the Open Graph Drawing Framework/Open Graph algorithms and Data structure Framework.","title":"ogdf/ogdf"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#jinyuliaogenericgraph","text":"Generic graph data structure plugin for ue4","title":"jinyuliao/Generic*Graph*"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#shaolinbitminisam_lib","text":"Lightweighted graph optimization (Factor graph ) library.","title":"shaolinbit/minisam_lib"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#vincarletvf3lib","text":"VF3 Algorithm - The fastest sequential algorithm to solve subgraph isomorphism on large and dense graphs","title":"vincarlet/vf3*lib*"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#rdmpagegraph-template-library","text":"","title":"rdmpage/graph-template-library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Cpp-library/#bgauzeregraph-lib","text":"C++ library for graph kernel and edit distance algorithm","title":"bgauzere/graph-lib"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Python-library/","text":"python\u5f00\u6e90\u5e93 keon/ algorithms anubhavshrimal/Data-Structures- Algorithms Coursera-Stanford-Graph-Search-Shortest-Paths-and-Data-Structures","title":"Python-library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Python-library/#python","text":"","title":"python\u5f00\u6e90\u5e93"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Python-library/#keonalgorithms","text":"","title":"keon/algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Python-library/#anubhavshrimaldata-structures-algorithms","text":"","title":"anubhavshrimal/Data-Structures-Algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Python-library/#coursera-stanford-graph-search-shortest-paths-and-data-structures","text":"","title":"Coursera-Stanford-Graph-Search-Shortest-Paths-and-Data-Structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/","text":"A Quick Tour of the Boost Graph Library Constructing a Graph adjacency_list The adjacency_list class provides a generalized version of the classic \"adjacency list\" data structure. The first two template arguments ( vecS, vecS ) determine the data structure used to represent the out-edges for each vertex in the graph and the data structure used to represent the graph's vertex set (see section Choosing the Edgelist and VertexList for information about the tradeoffs of the different data structures). The third argument, bidirectionalS , selects a directed graph that provides access to both out and in-edges. The other options for the third argument are directedS which selects a directed graph with only out-edges, and undirectedS which selects an undirected graph. NOTE: bidirectionalS \u3001 directedS \u3001 undirectedS \u90fd\u662f\u524d\u4e09\u4e2atemplate parameter\u7684argument\u3002 NOTE: MutableGraph add_edge() add_vertex() remove_vertex() edge iterator constructor Accessing the Vertex Set NOTE: VertexListGraph vertices() Note that different graph classes can have different associated vertex iterator types, which is why we need the graph_traits class. NOTE: trait\u63d0\u4f9b\u4e86\u4e00\u5c42\u62bd\u8c61\u3002 Accessing the Edge Set NOTE: EdgeListGraph edges() source() target() The Adjacency Structure In the next few examples we will explore the adjacency structure of the graph from the point of view of a particular vertex. We will look at the vertices' in-edges, out-edges, and its adjacent vertices. NOTE: \u524d\u9762\u4e24\u8282\uff1aAccessing the Vertex Set\u3001Accessing the Edge Set\u90fd\u662f\u7740\u773c\u4e8e\u6574\u4e2agraph\uff0c\u5373\u5bf9\u6574\u4e2agraph\u8fdb\u884c\u64cd\u4f5c\uff0c\u800c\u672c\u8282\u5f00\u59cb\uff0c\u7740\u773c\u4e8egraph\u4e2d\u7684\u4e00\u4e2avertex\uff0c\u7ed3\u5408\u5404\u79cdgraph algorithm\u53ef\u77e5\uff0c\u5404\u79cdalgorithm\u90fd\u662f\u57fa\u4e8evertex\u7684\u3002\u6700\u6700\u5e38\u89c1\u7684\u4fbf\u662f\u83b7\u5f97\u4e00\u4e2avertex\u7684adjacent vertexes\u3002 We will encapsulate this in an \" exercise vertex \" function, and apply it to each vertex in the graph. To demonstrate the STL-interoperability of BGL, we will use the STL for_each() function to iterate through the vertices and apply the function. //... int main ( int , char * []) { //... std :: for_each ( vertices ( g ). first , vertices ( g ). second , exercise_vertex < Graph > ( g )); return 0 ; } We use a functor for exercise_vertex instead of just a function because the graph object will be needed when we access information about each vertex; using a functor gives us a place to keep a reference to the graph object during the execution of the std::for_each() . Also we template the functor on the graph type so that it is reusable with different graph classes. Here is the start of the exercise_vertex functor: NOTE: functor VS function template < class Graph > struct exercise_vertex { exercise_vertex ( Graph & g_ ) : g ( g_ ) {} //... Graph & g ; }; Vertex Descriptors NOTE: \u5728boost New Iterator Concepts \u4e2d\uff0c\u5bf9\u5b83\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 The first thing we need to know in order to write the operator() method of the functor is the type for the vertex objects of the graph. The vertex type will be the parameter to the operator() method. To be precise, we do not deal with actual vertex objects , but rather with vertex descriptors . Many graph representations (such as adjacency lists) do not store actual vertex objects , while others do (e.g., pointer-linked graphs). This difference is hidden underneath the \"black-box\" of the vertex descriptor object . The vertex descriptor is something provided by each graph type that can be used to access information about the graph via the out_edges() , in_edges() , adjacent_vertices() , and property map functions that are described in the following sections. NOTE: vertex descriptor\u662f\u5bf9vertex\u62bd\u8c61\u63cf\u8ff0\uff0c\u6216\u8005\u8bf4\u5b83\u662f\u4e00\u79cd\u534f\u8bae\u3002 The vertex_descriptor type is obtained through the graph_traits class. The typename keyword used below is necessary because the type on the left hand side of the scope :: operator (the graph_traits<Graph> type) is dependent on a template parameter (the Graph type). Here is how we define the functor's apply method: template < class Graph > struct exercise_vertex { //... typedef typename graph_traits < Graph > :: vertex_descriptor Vertex ; void operator ()( const Vertex & v ) const { //... } //... }; Out-Edges, In-Edges, and Edge Descriptors The out-edges of a vertex are accessed with the out_edges() function of the IncidenceGraph interface. The out_edges() function takes two arguments: the first argument is the vertex and the second is the graph object. The function returns a pair of iterators which provide access to all of the out-edges of a vertex (similar to how the vertices() function returned a pair of iterators). The iterators are called out-edge iterators and dereferencing one of these iterators gives an edge descriptor object. An edge descriptor plays the same kind of role as the vertex descriptor object , it is a \"black box\" provided by the graph type. The following code snippet prints the source-target pairs for each out-edge of vertex v . template < class Graph > struct exercise_vertex { //... void operator ()( const Vertex & v ) const { typedef graph_traits < Graph > GraphTraits ; typename property_map < Graph , vertex_index_t >:: type index = get ( vertex_index , g ); std :: cout << \"out-edges: \" ; typename GraphTraits :: out_edge_iterator out_i , out_end ; typename GraphTraits :: edge_descriptor e ; for ( boost :: tie ( out_i , out_end ) = out_edges ( v , g ); out_i != out_end ; ++ out_i ) { e = * out_i ; Vertex src = source ( e , g ), targ = target ( e , g ); std :: cout << \"(\" << index [ src ] << \",\" << index [ targ ] << \") \" ; } std :: cout << std :: endl ; //... } //... }; The in_edges() function of the BidirectionalGraph interface provides access to all the in-edges of a vertex through in-edge iterators . The in_edges() function is only available for the adjacency_list if bidirectionalS is supplied for the Directed template parameter. There is an extra cost in space when bidirectionalS is specified instead of directedS . template < class Graph > struct exercise_vertex { //... void operator ()( const Vertex & v ) const { //... std :: cout << \"in-edges: \" ; typedef typename graph_traits < Graph > GraphTraits ; typename GraphTraits :: in_edge_iterator in_i , in_end ; for ( boost :: tie ( in_i , in_end ) = in_edges ( v , g ); in_i != in_end ; ++ in_i ) { e = * in_i ; Vertex src = source ( e , g ), targ = target ( e , g ); std :: cout << \"(\" << index [ src ] << \",\" << index [ targ ] << \") \" ; } std :: cout << std :: endl ; //... } //... }; Adding Some Color to your Graph NOTE: internally stored property and externally stored property BGL uses a uniform mechanism to access both kinds of properties inside its graph algorithms called the property map interface, described in Section Property Map Concepts . In addition, the PropertyGraph concept defines the interface for obtaining a property map object for an internally stored property . The BGL adjacency_list class allows users to specify internally stored properties through plug-in template parameters of the graph class. How to do this is discussed in detail in Section Internal Properties . Externally stored properties can be created in many different ways, although they are ultimately passed as separate arguments to the graph algorithms . One straightforward way to store properties is to create an array indexed by vertex or edge index . In the adjacency_list with vecS \uff08vector\uff09 specified for the VertexList template parameter, vertices are automatically assigned indices, which can be accessed via the property map for the vertex_index_t . Edges are not automatically assigned indices. However the property mechanism can be used to attach indices to the edges which can be used to index into other externally stored properties. NOTE: \u5173\u4e8e vertex_index_t \uff0c\u53c2\u89c1 Using adjacency_list #Internal Properties \u7ae0\u8282\u3002 In the following example, we construct a graph and apply dijkstra_shortest_paths() . The complete source code for the example is in examples/dijkstra-example.cpp . Dijkstra's algorithm computes the shortest distance from the starting vertex to every other vertex in the graph. Dijkstra's algorithm requires that a weight property is associated with each edge and a distance property with each vertex. Here we use an internal property for the weight and an external property for the distance. For the weight property we use the property class and specify int as the type used to represent weight values and edge_weight_t for the property tag (which is one of the BGL predefined property tags). The weight property is then used as a template argument for adjacency_list . NOTE: distance property\u6240\u6307\u7684\u662fsource vertex\u5230\u8be5vertex\u7684distance\uff0c\u8fd9\u662fDijkstar\u7b97\u6cd5\u7684\u6700\u7ec8\u8f93\u51fa\u3002 The listS and vecS types are selectors that determine the data structure used inside the adjacency_list (see Section Choosing the Edgelist and VertexList ). The directedS type specifies that the graph should be directed (versus undirected). The following code shows the specification of the graph type and then the initialization of the graph. The edges and weights are passed to the graph constructor in the form of iterators (a pointer qualifies as a RandomAccessIterator ). typedef adjacency_list < listS , vecS , directedS , no_property , property < edge_weight_t , int > > Graph ; typedef graph_traits < Graph >:: vertex_descriptor Vertex ; typedef std :: pair < int , int > E ; const int num_nodes = 5 ; E edges [] = { E ( 0 , 2 ), E ( 1 , 1 ), E ( 1 , 3 ), E ( 1 , 4 ), E ( 2 , 1 ), E ( 2 , 3 ), E ( 3 , 4 ), E ( 4 , 0 ), E ( 4 , 1 ) }; int weights [] = { 1 , 2 , 1 , 2 , 7 , 3 , 1 , 1 , 1 }; Graph G ( edges , edges + sizeof ( edges ) / sizeof ( E ), weights , num_nodes );","title":"A-Quick-Tour-of-the-Boost-Graph-Library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/#a#quick#tour#of#the#boost#graph#library","text":"","title":"A Quick Tour of the Boost Graph Library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/#constructing#a#graph","text":"adjacency_list The adjacency_list class provides a generalized version of the classic \"adjacency list\" data structure. The first two template arguments ( vecS, vecS ) determine the data structure used to represent the out-edges for each vertex in the graph and the data structure used to represent the graph's vertex set (see section Choosing the Edgelist and VertexList for information about the tradeoffs of the different data structures). The third argument, bidirectionalS , selects a directed graph that provides access to both out and in-edges. The other options for the third argument are directedS which selects a directed graph with only out-edges, and undirectedS which selects an undirected graph. NOTE: bidirectionalS \u3001 directedS \u3001 undirectedS \u90fd\u662f\u524d\u4e09\u4e2atemplate parameter\u7684argument\u3002 NOTE: MutableGraph add_edge() add_vertex() remove_vertex() edge iterator constructor","title":"Constructing a Graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/#accessing#the#vertex#set","text":"NOTE: VertexListGraph vertices() Note that different graph classes can have different associated vertex iterator types, which is why we need the graph_traits class. NOTE: trait\u63d0\u4f9b\u4e86\u4e00\u5c42\u62bd\u8c61\u3002","title":"Accessing the Vertex Set"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/#accessing#the#edge#set","text":"NOTE: EdgeListGraph edges() source() target()","title":"Accessing the Edge Set"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/#the#adjacency#structure","text":"In the next few examples we will explore the adjacency structure of the graph from the point of view of a particular vertex. We will look at the vertices' in-edges, out-edges, and its adjacent vertices. NOTE: \u524d\u9762\u4e24\u8282\uff1aAccessing the Vertex Set\u3001Accessing the Edge Set\u90fd\u662f\u7740\u773c\u4e8e\u6574\u4e2agraph\uff0c\u5373\u5bf9\u6574\u4e2agraph\u8fdb\u884c\u64cd\u4f5c\uff0c\u800c\u672c\u8282\u5f00\u59cb\uff0c\u7740\u773c\u4e8egraph\u4e2d\u7684\u4e00\u4e2avertex\uff0c\u7ed3\u5408\u5404\u79cdgraph algorithm\u53ef\u77e5\uff0c\u5404\u79cdalgorithm\u90fd\u662f\u57fa\u4e8evertex\u7684\u3002\u6700\u6700\u5e38\u89c1\u7684\u4fbf\u662f\u83b7\u5f97\u4e00\u4e2avertex\u7684adjacent vertexes\u3002 We will encapsulate this in an \" exercise vertex \" function, and apply it to each vertex in the graph. To demonstrate the STL-interoperability of BGL, we will use the STL for_each() function to iterate through the vertices and apply the function. //... int main ( int , char * []) { //... std :: for_each ( vertices ( g ). first , vertices ( g ). second , exercise_vertex < Graph > ( g )); return 0 ; } We use a functor for exercise_vertex instead of just a function because the graph object will be needed when we access information about each vertex; using a functor gives us a place to keep a reference to the graph object during the execution of the std::for_each() . Also we template the functor on the graph type so that it is reusable with different graph classes. Here is the start of the exercise_vertex functor: NOTE: functor VS function template < class Graph > struct exercise_vertex { exercise_vertex ( Graph & g_ ) : g ( g_ ) {} //... Graph & g ; };","title":"The Adjacency Structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/#vertex#descriptors","text":"NOTE: \u5728boost New Iterator Concepts \u4e2d\uff0c\u5bf9\u5b83\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 The first thing we need to know in order to write the operator() method of the functor is the type for the vertex objects of the graph. The vertex type will be the parameter to the operator() method. To be precise, we do not deal with actual vertex objects , but rather with vertex descriptors . Many graph representations (such as adjacency lists) do not store actual vertex objects , while others do (e.g., pointer-linked graphs). This difference is hidden underneath the \"black-box\" of the vertex descriptor object . The vertex descriptor is something provided by each graph type that can be used to access information about the graph via the out_edges() , in_edges() , adjacent_vertices() , and property map functions that are described in the following sections. NOTE: vertex descriptor\u662f\u5bf9vertex\u62bd\u8c61\u63cf\u8ff0\uff0c\u6216\u8005\u8bf4\u5b83\u662f\u4e00\u79cd\u534f\u8bae\u3002 The vertex_descriptor type is obtained through the graph_traits class. The typename keyword used below is necessary because the type on the left hand side of the scope :: operator (the graph_traits<Graph> type) is dependent on a template parameter (the Graph type). Here is how we define the functor's apply method: template < class Graph > struct exercise_vertex { //... typedef typename graph_traits < Graph > :: vertex_descriptor Vertex ; void operator ()( const Vertex & v ) const { //... } //... };","title":"Vertex Descriptors"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/#out-edges#in-edges#and#edge#descriptors","text":"The out-edges of a vertex are accessed with the out_edges() function of the IncidenceGraph interface. The out_edges() function takes two arguments: the first argument is the vertex and the second is the graph object. The function returns a pair of iterators which provide access to all of the out-edges of a vertex (similar to how the vertices() function returned a pair of iterators). The iterators are called out-edge iterators and dereferencing one of these iterators gives an edge descriptor object. An edge descriptor plays the same kind of role as the vertex descriptor object , it is a \"black box\" provided by the graph type. The following code snippet prints the source-target pairs for each out-edge of vertex v . template < class Graph > struct exercise_vertex { //... void operator ()( const Vertex & v ) const { typedef graph_traits < Graph > GraphTraits ; typename property_map < Graph , vertex_index_t >:: type index = get ( vertex_index , g ); std :: cout << \"out-edges: \" ; typename GraphTraits :: out_edge_iterator out_i , out_end ; typename GraphTraits :: edge_descriptor e ; for ( boost :: tie ( out_i , out_end ) = out_edges ( v , g ); out_i != out_end ; ++ out_i ) { e = * out_i ; Vertex src = source ( e , g ), targ = target ( e , g ); std :: cout << \"(\" << index [ src ] << \",\" << index [ targ ] << \") \" ; } std :: cout << std :: endl ; //... } //... }; The in_edges() function of the BidirectionalGraph interface provides access to all the in-edges of a vertex through in-edge iterators . The in_edges() function is only available for the adjacency_list if bidirectionalS is supplied for the Directed template parameter. There is an extra cost in space when bidirectionalS is specified instead of directedS . template < class Graph > struct exercise_vertex { //... void operator ()( const Vertex & v ) const { //... std :: cout << \"in-edges: \" ; typedef typename graph_traits < Graph > GraphTraits ; typename GraphTraits :: in_edge_iterator in_i , in_end ; for ( boost :: tie ( in_i , in_end ) = in_edges ( v , g ); in_i != in_end ; ++ in_i ) { e = * in_i ; Vertex src = source ( e , g ), targ = target ( e , g ); std :: cout << \"(\" << index [ src ] << \",\" << index [ targ ] << \") \" ; } std :: cout << std :: endl ; //... } //... };","title":"Out-Edges, In-Edges, and Edge Descriptors"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/A-Quick-Tour-of-the-Boost-Graph-Library/#adding#some#color#to#your#graph","text":"NOTE: internally stored property and externally stored property BGL uses a uniform mechanism to access both kinds of properties inside its graph algorithms called the property map interface, described in Section Property Map Concepts . In addition, the PropertyGraph concept defines the interface for obtaining a property map object for an internally stored property . The BGL adjacency_list class allows users to specify internally stored properties through plug-in template parameters of the graph class. How to do this is discussed in detail in Section Internal Properties . Externally stored properties can be created in many different ways, although they are ultimately passed as separate arguments to the graph algorithms . One straightforward way to store properties is to create an array indexed by vertex or edge index . In the adjacency_list with vecS \uff08vector\uff09 specified for the VertexList template parameter, vertices are automatically assigned indices, which can be accessed via the property map for the vertex_index_t . Edges are not automatically assigned indices. However the property mechanism can be used to attach indices to the edges which can be used to index into other externally stored properties. NOTE: \u5173\u4e8e vertex_index_t \uff0c\u53c2\u89c1 Using adjacency_list #Internal Properties \u7ae0\u8282\u3002 In the following example, we construct a graph and apply dijkstra_shortest_paths() . The complete source code for the example is in examples/dijkstra-example.cpp . Dijkstra's algorithm computes the shortest distance from the starting vertex to every other vertex in the graph. Dijkstra's algorithm requires that a weight property is associated with each edge and a distance property with each vertex. Here we use an internal property for the weight and an external property for the distance. For the weight property we use the property class and specify int as the type used to represent weight values and edge_weight_t for the property tag (which is one of the BGL predefined property tags). The weight property is then used as a template argument for adjacency_list . NOTE: distance property\u6240\u6307\u7684\u662fsource vertex\u5230\u8be5vertex\u7684distance\uff0c\u8fd9\u662fDijkstar\u7b97\u6cd5\u7684\u6700\u7ec8\u8f93\u51fa\u3002 The listS and vecS types are selectors that determine the data structure used inside the adjacency_list (see Section Choosing the Edgelist and VertexList ). The directedS type specifies that the graph should be directed (versus undirected). The following code shows the specification of the graph type and then the initialization of the graph. The edges and weights are passed to the graph constructor in the form of iterators (a pointer qualifies as a RandomAccessIterator ). typedef adjacency_list < listS , vecS , directedS , no_property , property < edge_weight_t , int > > Graph ; typedef graph_traits < Graph >:: vertex_descriptor Vertex ; typedef std :: pair < int , int > E ; const int num_nodes = 5 ; E edges [] = { E ( 0 , 2 ), E ( 1 , 1 ), E ( 1 , 3 ), E ( 1 , 4 ), E ( 2 , 1 ), E ( 2 , 3 ), E ( 3 , 4 ), E ( 4 , 0 ), E ( 4 , 1 ) }; int weights [] = { 1 , 2 , 1 , 2 , 7 , 3 , 1 , 1 , 1 }; Graph G ( edges , edges + sizeof ( edges ) / sizeof ( E ), weights , num_nodes );","title":"Adding Some Color to your Graph"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/","text":"The Boost Graph Library \u5728\u5de5\u7a0bdata-structure\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86graph\u7684\u6982\u5ff5\uff0c\u672c\u7ae0\u4ecb\u7ecdc++\u4e2d\u7684\u5b9e\u73b0\uff1aboost\u7684graph library\u3002 Table of Contents: the Boost Graph Library NOTE: \u539f\u7f51\u7ad9\u5bf9\u6587\u6863\u7684\u7ec4\u7ec7\u5448\u73b0\u5730\u4e0d\u76f4\u89c2\uff0c\u4e0d\u4fbf\u4e8e\u76f4\u89c2\u5730\u4e86\u89e3\u6587\u6863\u7684\u6240\u6709\u5185\u5bb9\uff0c\u6240\u4ee5\u9996\u5148\u5c06TOC\u653e\u5230\u8fd9\u91cc\u3002 The Boost Graph Library (BGL) \u901a\u8fc7\u539f\u6587\u7684\u7b2c\u4e00\u6bb5\u6211\u4eec\u53ef\u4ee5\u770b\u51fa\uff0cBGL\u6240\u91c7\u7528\u7684\u8bbe\u8ba1\u601d\u60f3\u548cSTL\u7684\u8bbe\u8ba1\u601d\u60f3\u662f\u76f8\u540c\u7684\uff0c\u6838\u5fc3\u601d\u60f3\u662fgeneric-programming\uff0c\u5728\u539f\u6587\u5bf9\u4e24\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002 Genericity in the Boost Graph Library Like the STL, there are three ways in which the BGL is generic. Algorithm/Data-Structure Interoperability NOTE: \u7b80\u800c\u8a00\u4e4b\u5c31\u662f\u4f7f\u7528iterator\u3001adapter pattern Extension through Visitors NOTE: \u7b80\u800c\u8a00\u4e4b\u5c31\u662fvisitor pattern Vertex and Edge Property Multi-Parameterization NOTE: graph\u7ed3\u6784\u662f\u6bd4list\u7b49\u7ed3\u6784\u8981\u590d\u6742\u7684\uff0c\u5b83\u6709node\u3001edge\uff0c\u5e76\u4e14node\u548cedge\u6709\u5404\u81ea\u7684\u4e00\u4e9bproperty\uff08\u6bd4\u5982edge\u7684weight\u3001node\u7684label\u7b49\u7b49\uff09\uff0c\u5728BGL\u4e2d\uff0c\u8fd9\u4e9b\u90fdparameterization\u4e86\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7template parameter\u5bf9\u5b83\u4eec\u8fdb\u884c\u8c03\u6574\u3002 \u66f4\u591a\u7684parameter\uff0c\u80fd\u591f\u5e26\u6765BGL\u7684genericity\uff0c\u540c\u65f6\u5b83\u4e5f\u5e26\u6765\u4e86\u5b9e\u73b0\u7684\u590d\u6742\u3001\u4f7f\u7528\u7684\u590d\u6742\u3002 Algorithms NOTE: core algorithm pattern\u662f\u57fa\u7840\u7684algorithm\uff0c\u5b83\u662f\u5176\u4ed6\u51e0\u79cdalgorithm\u7684building block\u3002 Data Structures NOTE: \u672c\u6bb5\u6240\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u6765\u5b58\u50a8\u56fe","title":"Boost-Graph-Library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#the#boost#graph#library","text":"\u5728\u5de5\u7a0bdata-structure\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86graph\u7684\u6982\u5ff5\uff0c\u672c\u7ae0\u4ecb\u7ecdc++\u4e2d\u7684\u5b9e\u73b0\uff1aboost\u7684graph library\u3002","title":"The Boost Graph Library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#table#of#contents#the#boost#graph#library","text":"NOTE: \u539f\u7f51\u7ad9\u5bf9\u6587\u6863\u7684\u7ec4\u7ec7\u5448\u73b0\u5730\u4e0d\u76f4\u89c2\uff0c\u4e0d\u4fbf\u4e8e\u76f4\u89c2\u5730\u4e86\u89e3\u6587\u6863\u7684\u6240\u6709\u5185\u5bb9\uff0c\u6240\u4ee5\u9996\u5148\u5c06TOC\u653e\u5230\u8fd9\u91cc\u3002","title":"Table of Contents: the Boost Graph Library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#the#boost#graph#library#bgl","text":"\u901a\u8fc7\u539f\u6587\u7684\u7b2c\u4e00\u6bb5\u6211\u4eec\u53ef\u4ee5\u770b\u51fa\uff0cBGL\u6240\u91c7\u7528\u7684\u8bbe\u8ba1\u601d\u60f3\u548cSTL\u7684\u8bbe\u8ba1\u601d\u60f3\u662f\u76f8\u540c\u7684\uff0c\u6838\u5fc3\u601d\u60f3\u662fgeneric-programming\uff0c\u5728\u539f\u6587\u5bf9\u4e24\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002","title":"The Boost Graph Library (BGL)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#genericity#in#the#boost#graph#library","text":"Like the STL, there are three ways in which the BGL is generic.","title":"Genericity in the Boost Graph Library"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#algorithmdata-structure#interoperability","text":"NOTE: \u7b80\u800c\u8a00\u4e4b\u5c31\u662f\u4f7f\u7528iterator\u3001adapter pattern","title":"Algorithm/Data-Structure Interoperability"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#extension#through#visitors","text":"NOTE: \u7b80\u800c\u8a00\u4e4b\u5c31\u662fvisitor pattern","title":"Extension through Visitors"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#vertex#and#edge#property#multi-parameterization","text":"NOTE: graph\u7ed3\u6784\u662f\u6bd4list\u7b49\u7ed3\u6784\u8981\u590d\u6742\u7684\uff0c\u5b83\u6709node\u3001edge\uff0c\u5e76\u4e14node\u548cedge\u6709\u5404\u81ea\u7684\u4e00\u4e9bproperty\uff08\u6bd4\u5982edge\u7684weight\u3001node\u7684label\u7b49\u7b49\uff09\uff0c\u5728BGL\u4e2d\uff0c\u8fd9\u4e9b\u90fdparameterization\u4e86\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7template parameter\u5bf9\u5b83\u4eec\u8fdb\u884c\u8c03\u6574\u3002 \u66f4\u591a\u7684parameter\uff0c\u80fd\u591f\u5e26\u6765BGL\u7684genericity\uff0c\u540c\u65f6\u5b83\u4e5f\u5e26\u6765\u4e86\u5b9e\u73b0\u7684\u590d\u6742\u3001\u4f7f\u7528\u7684\u590d\u6742\u3002","title":"Vertex and Edge Property Multi-Parameterization"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#algorithms","text":"NOTE: core algorithm pattern\u662f\u57fa\u7840\u7684algorithm\uff0c\u5b83\u662f\u5176\u4ed6\u51e0\u79cdalgorithm\u7684building block\u3002","title":"Algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library/#data#structures","text":"NOTE: \u672c\u6bb5\u6240\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u6765\u5b58\u50a8\u56fe","title":"Data Structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Algorithms/Named-parameters/","text":"bgl_named_params<Param, Type, Rest>","title":"Named-parameters"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Algorithms/Named-parameters/#bgl_named_paramsparam#type#rest","text":"","title":"bgl_named_params&lt;Param, Type, Rest&gt;"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Algorithms/Core-Searches/breadth_first_search/","text":"breadth_first_search BFS uses two data structures to to implement the traversal: a color marker for each vertex and a queue . White vertices are undiscovered while gray vertices are discovered but have undiscovered adjacent vertices. Black vertices are discovered and are adjacent to only other black or gray vertices. The algorithm proceeds by removing a vertex u from the queue and examining each out-edge (u,v) . If an adjacent vertex v is not already discovered, it is colored gray and placed in the queue. After all of the out-edges are examined, vertex u is colored black and the process is repeated. Pseudo-code for the BFS algorithm is a listed below. BFS(G, s) for each vertex u in V[G] color[u] := WHITE d[u] := infinity p[u] := u end for color[s] := GRAY d[s] := 0 ENQUEUE(Q, s) while (Q != \u00d8) u := DEQUEUE(Q) for each vertex v in Adj[u] if (color[v] = WHITE) color[v] := GRAY d[v] := d[u] + 1 p[v] := u ENQUEUE(Q, v) else if (color[v] = GRAY) ... elsek ... end for color[u] := BLACK end while return (d, p) breadth_first_visit","title":"`breadth_first_search`"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Algorithms/Core-Searches/breadth_first_search/#breadth_first_search","text":"BFS uses two data structures to to implement the traversal: a color marker for each vertex and a queue . White vertices are undiscovered while gray vertices are discovered but have undiscovered adjacent vertices. Black vertices are discovered and are adjacent to only other black or gray vertices. The algorithm proceeds by removing a vertex u from the queue and examining each out-edge (u,v) . If an adjacent vertex v is not already discovered, it is colored gray and placed in the queue. After all of the out-edges are examined, vertex u is colored black and the process is repeated. Pseudo-code for the BFS algorithm is a listed below. BFS(G, s) for each vertex u in V[G] color[u] := WHITE d[u] := infinity p[u] := u end for color[s] := GRAY d[s] := 0 ENQUEUE(Q, s) while (Q != \u00d8) u := DEQUEUE(Q) for each vertex v in Adj[u] if (color[v] = WHITE) color[v] := GRAY d[v] := d[u] + 1 p[v] := u ENQUEUE(Q, v) else if (color[v] = GRAY) ... elsek ... end for color[u] := BLACK end while return (d, p)","title":"breadth_first_search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Algorithms/Core-Searches/breadth_first_search/#breadth_first_visit","text":"","title":"breadth_first_visit"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Algorithms/Core-Searches/depth_first_search/","text":"depth_first_search","title":"`depth_first_search`"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Algorithms/Core-Searches/depth_first_search/#depth_first_search","text":"","title":"depth_first_search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library-Tutorial/Using-adjacency_list/","text":"Using adjacency_list Choosing the Edgelist and VertexList Choosing the VertexList type Choosing the OutEdgeList type Directed and Undirected Adjacency Lists Internal Properties The template parameters VertexProperty and EdgeProperty of the adjacency_list class are meant to be filled by these interior properties. NOTE : The Boost Graph Library supports two interchangeable methods for specifying interior properties: bundled properties and property lists. The former is easier to use and requires less effort, whereas the latter is compatible with older, broken compilers and is backward-compatible with Boost versions prior to 1.32.0. One may specify internal properties via property lists , which are build from instances of the property class declared as follows. template < class PropertyTag , class T , class NextProperty = no_property > struct property ; The PropertyTag template parameter is a tag class that simply identifies or gives a unique name to the property. NOTE: tag class\u662fc++\u7ecf\u5e38\u4f7f\u7528\u7684\u6280\u5de7 The NextProperty parameter allows property types to be nested, so that an arbitrary number of properties can be attached to the same graph. NOTE: \u8fd9\u662f\u4e00\u4e2a\u6280\u5de7 Custom Edge Properties NOTE: \u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u81ea\u5b9a\u4e49edge property\u3002","title":"Using-adjacency_list"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library-Tutorial/Using-adjacency_list/#using#adjacency_list","text":"","title":"Using adjacency_list"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library-Tutorial/Using-adjacency_list/#choosing#the#edgelist#and#vertexlist","text":"","title":"Choosing the Edgelist and VertexList"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library-Tutorial/Using-adjacency_list/#choosing#the#vertexlist#type","text":"","title":"Choosing the VertexList type"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library-Tutorial/Using-adjacency_list/#choosing#the#outedgelist#type","text":"","title":"Choosing the OutEdgeList type"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library-Tutorial/Using-adjacency_list/#directed#and#undirected#adjacency#lists","text":"","title":"Directed and Undirected Adjacency Lists"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library-Tutorial/Using-adjacency_list/#internal#properties","text":"The template parameters VertexProperty and EdgeProperty of the adjacency_list class are meant to be filled by these interior properties. NOTE : The Boost Graph Library supports two interchangeable methods for specifying interior properties: bundled properties and property lists. The former is easier to use and requires less effort, whereas the latter is compatible with older, broken compilers and is backward-compatible with Boost versions prior to 1.32.0. One may specify internal properties via property lists , which are build from instances of the property class declared as follows. template < class PropertyTag , class T , class NextProperty = no_property > struct property ; The PropertyTag template parameter is a tag class that simply identifies or gives a unique name to the property. NOTE: tag class\u662fc++\u7ecf\u5e38\u4f7f\u7528\u7684\u6280\u5de7 The NextProperty parameter allows property types to be nested, so that an arbitrary number of properties can be attached to the same graph. NOTE: \u8fd9\u662f\u4e00\u4e2a\u6280\u5de7","title":"Internal Properties"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Boost-Graph-Library-Tutorial/Using-adjacency_list/#custom#edge#properties","text":"NOTE: \u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u81ea\u5b9a\u4e49edge property\u3002","title":"Custom Edge Properties"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Example/bfs-example/","text":"bfs-example \u4f8b\u5b50\u4e2d\u7684graph\u5982\u4e0b\uff1a","title":"bfs-example"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Example/bfs-example/#bfs-example","text":"\u4f8b\u5b50\u4e2d\u7684graph\u5982\u4e0b\uff1a","title":"bfs-example"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Implementation/Graph-representation/","text":"Graph representation Adjacency list \u7ef4\u57fa\u767e\u79d1 Adjacency list adjacency_list.hpp","title":"Graph representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Implementation/Graph-representation/#graph#representation","text":"","title":"Graph representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Implementation/Graph-representation/#adjacency#list","text":"\u7ef4\u57fa\u767e\u79d1 Adjacency list adjacency_list.hpp","title":"Adjacency list"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/The-Boost-Graph-Interface/The-Boost-Graph-Interface/","text":"Graph Concepts NOTE: \u539f\u6587\u7684\u524d\u4e24\u6bb5\u662f\u5fc5\u8bfb\u7684\u3002\u5176\u5b9e\u8fd9\u4e24\u6bb5\u6240\u63cf\u8ff0\u7684\u6838\u5fc3\u5185\u5bb9\u662f\uff1a \u4ec0\u4e48\u662finterface\uff1f\u201cdefine how a graph can be examined and manipulated in a data-structure neutral fashion\u201d\u3002 \u5982\u4f55\u6765\u5b9a\u4e49interface\uff1f\u201cThe reason for this is that the purpose of a concept is to summarize the requirements for particular algorithms. \u201d Graph Structure Concepts Overview Figure 1: The graph concepts and refinement relationships. concept explanation Graph The Graph concept contains a few requirements that are common to all the graph concepts. IncidenceGraph refines Graph The IncidenceGraph concept provides an interface for efficient access to the out-edges of each vertex in the graph. BidirectionalGraph refines IncidenceGraph The BidirectionalGraph concept refines IncidenceGraph and adds the requirement for efficient access to the in-edges of each vertex. AdjacencyGraph refines Graph The AdjacencyGraph concept provides and interface for efficient access of the adjacent vertices to a vertex in a graph. VertexListGraph refines Graph The VertexListGraph concept refines the Graph concept, and adds the requirement for efficient traversal of all the vertices in the graph. EdgeListGraph refines Graph The EdgeListGraph concept refines the Graph concept, and adds the requirement for efficient access to all the edges in the graph. AdjacencyMatrix refines Graph The AdjacencyMatrix concept refines Graph concept and adds the requirement for efficient access to any edge in the graph given the source and target vertices. MutableGraph refines Graph A MutableGraph can be changed via the addition or removal of edges and vertices. PropertyGraph refines Graph A PropertyGraph is a graph that has some property associated with each of the vertices or edges in the graph. NOTE: \u57fa\u4e8econcept\u7684\u8bbe\u8ba1\uff0cconcept\u662fbehavior-based\u7684\uff0c\u5b83\u662fduck-type\uff0c\u5b83\u6bd4\u8f83\u7c7b\u4f3c\u4e8einterface\u3002","title":"The-Boost-Graph-Interface"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/The-Boost-Graph-Interface/The-Boost-Graph-Interface/#graph#concepts","text":"NOTE: \u539f\u6587\u7684\u524d\u4e24\u6bb5\u662f\u5fc5\u8bfb\u7684\u3002\u5176\u5b9e\u8fd9\u4e24\u6bb5\u6240\u63cf\u8ff0\u7684\u6838\u5fc3\u5185\u5bb9\u662f\uff1a \u4ec0\u4e48\u662finterface\uff1f\u201cdefine how a graph can be examined and manipulated in a data-structure neutral fashion\u201d\u3002 \u5982\u4f55\u6765\u5b9a\u4e49interface\uff1f\u201cThe reason for this is that the purpose of a concept is to summarize the requirements for particular algorithms. \u201d","title":"Graph Concepts"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/The-Boost-Graph-Interface/The-Boost-Graph-Interface/#graph#structure#concepts#overview","text":"Figure 1: The graph concepts and refinement relationships. concept explanation Graph The Graph concept contains a few requirements that are common to all the graph concepts. IncidenceGraph refines Graph The IncidenceGraph concept provides an interface for efficient access to the out-edges of each vertex in the graph. BidirectionalGraph refines IncidenceGraph The BidirectionalGraph concept refines IncidenceGraph and adds the requirement for efficient access to the in-edges of each vertex. AdjacencyGraph refines Graph The AdjacencyGraph concept provides and interface for efficient access of the adjacent vertices to a vertex in a graph. VertexListGraph refines Graph The VertexListGraph concept refines the Graph concept, and adds the requirement for efficient traversal of all the vertices in the graph. EdgeListGraph refines Graph The EdgeListGraph concept refines the Graph concept, and adds the requirement for efficient access to all the edges in the graph. AdjacencyMatrix refines Graph The AdjacencyMatrix concept refines Graph concept and adds the requirement for efficient access to any edge in the graph given the source and target vertices. MutableGraph refines Graph A MutableGraph can be changed via the addition or removal of edges and vertices. PropertyGraph refines Graph A PropertyGraph is a graph that has some property associated with each of the vertices or edges in the graph. NOTE: \u57fa\u4e8econcept\u7684\u8bbe\u8ba1\uff0cconcept\u662fbehavior-based\u7684\uff0c\u5b83\u662fduck-type\uff0c\u5b83\u6bd4\u8f83\u7c7b\u4f3c\u4e8einterface\u3002","title":"Graph Structure Concepts Overview"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Visitor-Concepts/Visitor-Concepts/","text":"Visitor Concepts concept explanation BFS Visitor Concept This concept defines the visitor interface for breadth_first_search() . Users can define a class with the BFS Visitor interface and pass and object of the class to breadth_first_search() , thereby augmenting the actions taken during the graph search. DFS Visitor Concept This concept defines the visitor interface for depth_first_search() . Users can define a class with the DFS Visitor interface and pass an object of the class to depth_first_search() , thereby augmenting the actions taken during the graph search. Dijkstra Visitor Concept This concept defines the visitor interface for dijkstra_shortest_paths() and related algorithms. ...... Bellman Ford Visitor Concept This concept defines the visitor interface for bellman_ford_shortest_paths() . ...... AStar Visitor Concept This concept defines the visitor interface for astar_search() . ...... EventVisitor Concept This concept defines the interface for single-event visitors. An EventVisitor has an apply member function ( operator() ) which is invoked within the graph algorithm at the event-point specified by the event_filter typedef within the EventVisitor. EventVisitor's can be combined into an EventVistorList . Planar Face Visitor Concept This concept defines the visitor interface for planar_face_traversal ....... TSP Tour Visitor concept This concept defines the visitor interface for metric_tsp_approx() and related algorithms.","title":"Visitor-Concepts"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Graph/Implementation/Boost-Graph-Library/Visitor-Concepts/Visitor-Concepts/#visitor#concepts","text":"concept explanation BFS Visitor Concept This concept defines the visitor interface for breadth_first_search() . Users can define a class with the BFS Visitor interface and pass and object of the class to breadth_first_search() , thereby augmenting the actions taken during the graph search. DFS Visitor Concept This concept defines the visitor interface for depth_first_search() . Users can define a class with the DFS Visitor interface and pass an object of the class to depth_first_search() , thereby augmenting the actions taken during the graph search. Dijkstra Visitor Concept This concept defines the visitor interface for dijkstra_shortest_paths() and related algorithms. ...... Bellman Ford Visitor Concept This concept defines the visitor interface for bellman_ford_shortest_paths() . ...... AStar Visitor Concept This concept defines the visitor interface for astar_search() . ...... EventVisitor Concept This concept defines the interface for single-event visitors. An EventVisitor has an apply member function ( operator() ) which is invoked within the graph algorithm at the event-point specified by the event_filter typedef within the EventVisitor. EventVisitor's can be combined into an EventVistorList . Planar Face Visitor Concept This concept defines the visitor interface for planar_face_traversal ....... TSP Tour Visitor concept This concept defines the visitor interface for metric_tsp_approx() and related algorithms.","title":"Visitor Concepts"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Graph-traversal/Graph-traversal/","text":"Graph traversal \u7ef4\u57fa\u767e\u79d1 Graph traversal graph traversal and circle graph\u7684\u7ed3\u6784\u662f\u6bd4tree\u8981\u590d\u6742\u7684\uff0c\u6240\u4ee5\u76f8\u6bd4\u4e8etree\u5b83\u80fd\u591f\u8868\u8fbe\u66f4\u591a\u7684relation\uff1b graph\u662f\u5141\u8bb8circle\uff0c\u56e0\u6b64\u5728\u5404\u79cdalgorithm\u4e2d\uff0c\u9700\u8981\u5bf9circle\u8fdb\u884c\u7279\u6b8a\u5904\u7406\u3002 graph\u662f\u8fd0\u884cdisconnect\u3002 \u5728graph traversal\u4e2d\uff0c\u4e3a\u4e86\u907f\u514d\u7531\u4e8ecircle\u800c\u5bfc\u81f4\u7684dead loop\uff0cgraph traversal algorithm\u666e\u904d\u91c7\u7528\u7684\u662f\u201c\u6807\u8bb0\u5df2\u7ecfvisit\u8fc7\u7684vertex\uff0c\u5bf9\u4e8e\u5df2\u7ecfvisit\u8fc7\u7684vertex\uff0c\u518d\u6b21\u9047\u5230\u7684\u65f6\u5019\uff0c\u76f4\u63a5pass\u6389\u201d\u3002 Dijkstra's algorithm and breadth-first search Dijkstra's algorithm\u548cbreadth-first search\u975e\u5e38\u7c7b\u4f3c","title":"Graph-traversal"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Graph-traversal/Graph-traversal/#graph#traversal","text":"","title":"Graph traversal"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Graph-traversal/Graph-traversal/#graph#traversal_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Graph traversal"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Graph-traversal/Graph-traversal/#graph#traversal#and#circle","text":"graph\u7684\u7ed3\u6784\u662f\u6bd4tree\u8981\u590d\u6742\u7684\uff0c\u6240\u4ee5\u76f8\u6bd4\u4e8etree\u5b83\u80fd\u591f\u8868\u8fbe\u66f4\u591a\u7684relation\uff1b graph\u662f\u5141\u8bb8circle\uff0c\u56e0\u6b64\u5728\u5404\u79cdalgorithm\u4e2d\uff0c\u9700\u8981\u5bf9circle\u8fdb\u884c\u7279\u6b8a\u5904\u7406\u3002 graph\u662f\u8fd0\u884cdisconnect\u3002 \u5728graph traversal\u4e2d\uff0c\u4e3a\u4e86\u907f\u514d\u7531\u4e8ecircle\u800c\u5bfc\u81f4\u7684dead loop\uff0cgraph traversal algorithm\u666e\u904d\u91c7\u7528\u7684\u662f\u201c\u6807\u8bb0\u5df2\u7ecfvisit\u8fc7\u7684vertex\uff0c\u5bf9\u4e8e\u5df2\u7ecfvisit\u8fc7\u7684vertex\uff0c\u518d\u6b21\u9047\u5230\u7684\u65f6\u5019\uff0c\u76f4\u63a5pass\u6389\u201d\u3002","title":"graph traversal and circle"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Graph-traversal/Graph-traversal/#dijkstras#algorithm#and#breadth-first#search","text":"Dijkstra's algorithm\u548cbreadth-first search\u975e\u5e38\u7c7b\u4f3c","title":"Dijkstra's algorithm and breadth-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Graph-traversal/BFS-src/","text":"Depth First Search or DFS for a Graph","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/A-star-search-algorithm/","text":"","title":"A-star-search-algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/B-star-search-algorithm/","text":"","title":"B-star-search-algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Beam-search/","text":"","title":"Beam-search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Best-first-search/","text":"Best-first search \u7ef4\u57fa\u767e\u79d1 Best-first search","title":"Best-first-search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Best-first-search/#best-first#search","text":"","title":"Best-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Best-first-search/#best-first#search_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Best-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/DFS-and-BFS/","text":"DFS and BFS \u8fd9\u662f\u4e24\u79cd\u6700\u6700\u5e38\u89c1\u7684search policy\u3002 \u7ef4\u57fa\u767e\u79d1 Depth-first search \u7ef4\u57fa\u767e\u79d1 Breadth-first search BFS VS DFS What are the advantages of breadth-first search (BFS) over depth-first search (DFS)? Answer: BFS is complete and optimal, while DFS is not guaranteed to halt when there are loops. NOTE: DFS\u7684loop\u95ee\u9898\u662f\u53ef\u4ee5\u907f\u514d\u7684 What is the advantage of DFS over BFS? Answer: If m is the maximum path length and b is the branching factor, the space complexity for DFS is mb while for BFS it is b^m b^m .","title":"DFS-and-BFS"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/DFS-and-BFS/#dfs#and#bfs","text":"\u8fd9\u662f\u4e24\u79cd\u6700\u6700\u5e38\u89c1\u7684search policy\u3002","title":"DFS and BFS"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/DFS-and-BFS/#depth-first#search","text":"","title":"\u7ef4\u57fa\u767e\u79d1Depth-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/DFS-and-BFS/#breadth-first#search","text":"","title":"\u7ef4\u57fa\u767e\u79d1Breadth-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/DFS-and-BFS/#bfs#vs#dfs","text":"","title":"BFS VS DFS"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/DFS-and-BFS/#what#are#the#advantages#of#breadth-first#search#bfs#over#depth-first#search#dfs","text":"Answer: BFS is complete and optimal, while DFS is not guaranteed to halt when there are loops. NOTE: DFS\u7684loop\u95ee\u9898\u662f\u53ef\u4ee5\u907f\u514d\u7684","title":"What are the advantages of breadth-first search (BFS) over depth-first search (DFS)?"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/DFS-and-BFS/#what#is#the#advantage#of#dfs#over#bfs","text":"Answer: If m is the maximum path length and b is the branching factor, the space complexity for DFS is mb while for BFS it is b^m b^m .","title":"What is the advantage of DFS over BFS?"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Iterative-deepening-depth-first-search/","text":"Iterative deepening depth-first search","title":"Iterative-deepening-depth-first-search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Iterative-deepening-depth-first-search/#iterative#deepening#depth-first#search","text":"","title":"Iterative deepening depth-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/","text":"\u5173\u4e8e\u672c\u7ae0 graph search algorithm\u662f\u975e\u5e38\u4e4b\u591a\u7684\uff0c\u56e0\u6b64\u9700\u8981\u9996\u5148\u5efa\u7acb\u8d77\u4e00\u4e2a\u5b8c\u6574\u7684\u89c6\u56fe\uff0c\u672c\u7ae0\u4e3b\u8981\u53c2\u8003\u5982\u4e0b\u4e24\u7bc7\u6587\u7ae0: umn CSci 4511w: Class Notes on Search washington Search: Cost & Heuristics graph search algorithm\u662f\u5178\u578b\u7684Relation-based algorithm model\uff08\u53c2\u89c1 Relation-structure-computation\\Computation\\index.md \uff09\u3002 Classification \u5206\u7c7b\u65b9\u6cd5\u662f\u53c2\u8003\u7684\uff1aumn CSci 4511w: Class Notes on Search \u201c3. classes of search algorithms.\u201d\u3002 uninformed VS informed uninformed \u5373 Blind search informed \u5373 Heuristic search \u7c7b\u522b Example \u8bf4\u660e Blind search - Depth first search (DFS) - Breadth first search (BFS) - Iterative deepening depth-first search (IDS) Heuristic search - Best first search - Uniform cost search (UCS) - Greedy search - A* - Iterative Deepening A* ( IDA* ) - Beam search - Hill climbing \u4e0a\u8ff0example\u662f\u53c2\u8003\u7684washington Search: Cost & Heuristics \u3002 local VS global \u7c7b\u522b Example \u8bf4\u660e local - greedy - hill-climbing global - uniform cost - A* \u4e0e\u6b64\u76f8\u5173\u7684\u662f\uff1a\u5c40\u90e8\u6700\u4f18\u4e0e\u5168\u5c40\u6700\u4f18\u3002 systematic VS stochastic \u7c7b\u522b Example \u8bf4\u660e systematic - depth-first - A* stochastic - genetic algorithms\uff08\u9057\u4f20\u7b97\u6cd5\uff09","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/#_1","text":"graph search algorithm\u662f\u975e\u5e38\u4e4b\u591a\u7684\uff0c\u56e0\u6b64\u9700\u8981\u9996\u5148\u5efa\u7acb\u8d77\u4e00\u4e2a\u5b8c\u6574\u7684\u89c6\u56fe\uff0c\u672c\u7ae0\u4e3b\u8981\u53c2\u8003\u5982\u4e0b\u4e24\u7bc7\u6587\u7ae0: umn CSci 4511w: Class Notes on Search washington Search: Cost & Heuristics graph search algorithm\u662f\u5178\u578b\u7684Relation-based algorithm model\uff08\u53c2\u89c1 Relation-structure-computation\\Computation\\index.md \uff09\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/#classification","text":"\u5206\u7c7b\u65b9\u6cd5\u662f\u53c2\u8003\u7684\uff1aumn CSci 4511w: Class Notes on Search \u201c3. classes of search algorithms.\u201d\u3002","title":"Classification"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/#uninformed#vs#informed","text":"uninformed \u5373 Blind search informed \u5373 Heuristic search \u7c7b\u522b Example \u8bf4\u660e Blind search - Depth first search (DFS) - Breadth first search (BFS) - Iterative deepening depth-first search (IDS) Heuristic search - Best first search - Uniform cost search (UCS) - Greedy search - A* - Iterative Deepening A* ( IDA* ) - Beam search - Hill climbing \u4e0a\u8ff0example\u662f\u53c2\u8003\u7684washington Search: Cost & Heuristics \u3002","title":"uninformed VS informed"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/#local#vs#global","text":"\u7c7b\u522b Example \u8bf4\u660e local - greedy - hill-climbing global - uniform cost - A* \u4e0e\u6b64\u76f8\u5173\u7684\u662f\uff1a\u5c40\u90e8\u6700\u4f18\u4e0e\u5168\u5c40\u6700\u4f18\u3002","title":"local VS global"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/#systematic#vs#stochastic","text":"\u7c7b\u522b Example \u8bf4\u660e systematic - depth-first - A* stochastic - genetic algorithms\uff08\u9057\u4f20\u7b97\u6cd5\uff09","title":"systematic VS stochastic"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/","text":"umn CSci 4511w: Class Notes on Search NOTE: \u8fd9\u7bc7\u6587\u7ae0\uff0c\u603b\u7ed3\u5f97\u975e\u5e38\u597d The description of most of the search algorithms in these notes is taken from J. Pearl, \"Heuristics\", Addison-Wesley, 1984. Important issues about Search Algorithms We will address: 1. how to write search algorithms. In particular we will examine: the data structure to keep unexplored nodes. We use a queue (often called a list in many AI books) called OPEN . expansion of a node (or generation of its successors). All the successors of a node can be generated at once (method most commonly used) or they could be generated one at a time either in a systematic way or in a random way. The number of successors is called the branching factor . strategies for selecting which node to expand next. Different algorithms result from different choices (e.g. depth-first when successor nodes are added at the beginning of the queue, breadth-first when successor nodes are added at the end of the queue, etc), test for goal. We will assume the existence of a predicate that applied to a state will return true or false. bookkeeping (keeping track of visited nodes, keeping track of path to goal, etc). Keeping track of visited nodes is usually done by keeping them in a queue (or, better a hash-table) called CLOSED . This prevents getting trapped into loops or repeating work but results in a large space complexity. This is (most often) necessary when optimal solutions are sought\uff08\u5bfb\u627e\uff09, but can be (mostly) avoided in other cases. Keeping track of the path followed is not always necessary (when the problem is to find a goal state and knowing how to get there is not important). NOTE: \u4e0a\u8ff0\u8fc7\u7a0b\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8e\u4e00\u4e2aautomaton\uff0c\u5728\u5de5\u7a0b compiler-principle \u548c\u5de5\u7a0b automata-and-formal-language \u4e2d\u6709\u6d89\u53ca\uff0cparsing\u7684\u8fc7\u7a0b\u4e0e\u4e0a\u8ff0\u8fc7\u7a0b\u975e\u5e38\u7c7b\u4f3c\uff0c\u9700\u8981\u7ed3\u5408top-down parsing\u548cbottom-up parsing\u6765\u4e00\u8d77\u8fdb\u884c\u7406\u89e3\uff0c\u53e6\u5916\u53ef\u4ee5\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Pushdown automaton \u3002 2. properties of search algorithms and the solutions they find: Termination: the computation is guaranteed to terminate, no matter how large the search space is. NOTE: \u4fdd\u8bc1termination\u7684\u4e00\u79cd\u5e38\u89c1\u60c5\u51b5\u662f\uff1a\u907f\u514dloop\u3002\u4e0b\u9762\u7684\u201crepeated nodes are checked\u201d\u5c31\u662f\u4e3a\u4e86\u907f\u514dloop\u3002 Completeness: an algorithm is complete if it terminates with a solution when one exists. Admissibility: an algorithm is admissible if it is guaranteed to return an optimal solution whenever a solution exists. NOTE: Admissibility\u7684\u610f\u601d\u662f\u201c\u53ef\u5bb9\u8bb8\u6027\u201d\u3002 Space complexity and Time complexity: how the size of the memory and the time needed to run the algorithm grows depending on branching factor , depth of solution , number of nodes , etc. Let's briefly examine the properties of some commonly used uninformed search algorithms . Depth-First Search Termination: guaranteed for a finite space if repeated nodes are checked. Guaranteed when a depth bound\uff08\u6df1\u5ea6\u754c\u9650\uff09 is used. Not guaranteed otherwise. NOTE: \u201crepeated nodes are checked\u201d\u5c31\u662f\u4e3a\u4e86\u907f\u514d\u201cloop\u201d\u3002 Completeness: not guaranteed in general. Guaranteed if the search space is finite (exhaustive search) and repeated nodes are checked. Admissibility: not guaranteed. Breadth-First Search Termination: guaranteed for finite space. Guaranteed when a solution exists. NOTE: \u611f\u89c9\u6b64\u5904\u662f\u6709\u95ee\u9898\u7684\uff1a\u5982\u679c\u5b58\u5728loop\uff0c\u5e76\u4e14\u7b97\u6cd5\u6ca1\u6709\u907f\u514dloop\uff0c\u5219\u5b83\u65e0\u6cd5terminate\u3002 Completeness: guaranteed. Admissibility: the algorithm will always find the shortest path (it might not be the optimal path, if arcs have different costs). Depth-First Search Iterative-Deepening Termination: guaranteed for finite space. Guaranteed when a solution exists. Completeness: guaranteed. Admissibility: the algorithm will always find the shortest path (it might not be the optimal path, if arcs have different costs). 3. classes of search algorithms. uninformed (depth-first, breadth-first, uniform cost, depth-limited, iterative deepening) versus informed (greedy, A* , IDA* , SMA* ) NOTE: **uninformed**\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u65e0\u77e5\u7684\u201d\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u5b83\u6240\u8868\u793a\u7684\u662fblind search\uff0c\u6ca1\u6709heuristics\u7684 local (greedy, hill-climbing) versus global (uniform cost, A* , etc) systematic (depth-first, A* , etc) versus stochastic (simulated annealing, genetic algorithms\uff08\u9057\u4f20\u7b97\u6cd5\uff09) NOTE: \u5173\u4e8epointer\u7684\u89e3\u91ca \u5728\u4e0b\u9762\u7684\u5404\u79cdalgorithm\u4e2d\uff0c\u90fd\u4f1a\u770b\u5230**pointer**\uff0c\u539f\u6587\u4e2d\uff0c\u4f5c\u8005\u5e76\u6ca1\u6709\u5bf9pointer\u8fdb\u884c\u89e3\u91ca\u3002\u6839\u636e\u201cDepth-First Search with depth bound\u201d\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u63a8\u6d4b\u5f97\u77e5\uff1a \u6bcf\u4e2anode\uff0c\u90fd\u6709\u4e00\u4e2a\u6307\u5411\u5b83\u7684parent\u7684pointer\uff0c\u7b80\u79f0\u4e3a**parent pointer**\uff1a 2.2.4 put the successors on top of OPEN and provide for each a pointer back to n # \u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u6307\u5411\u5b83\u7684\u4e0a\u7ea7\u8282\u70b9\u7684\u6307\u9488 \u4f7f\u7528parent pointer\u7684\u539f\u56e0\u662f\u4e3a\u4e86\u80fd\u591f\u5f97\u5230\u5b8c\u6574\u7684solution\uff1a 3. If a goal node is found , then exit with the solution obtained by tracing back through its pointers Description of an Uninformed Search Algorithm: Depth-First Search with depth bound NOTE: \u5e26\u6df1\u5ea6\u9650\u5236\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22 1. Put the start node on OPEN. 2. until OPEN is empty or a goal node has been reached 2.1 remove the topmost node from OPEN and put it on CLOSED. Call this node n. 2.2 if the depth of n is less than the depth bound then 2.2.1 expand n, generating all successors (if any). 2.2.2 if any of these successors is already on the traversal path discard it 2.2.3 if any of these successors is a goal node, exit 2.2.4 put the successors on top of OPEN and provide for each a pointer back to n # \u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u6307\u5411\u5b83\u7684\u4e0a\u7ea7\u8282\u70b9\u7684\u6307\u9488 else 2.2.5 clean up CLOSED. 3. If a goal node is found, then exit with the solution obtained by tracing back through its pointers else announce failure. NOTE: \u4e0a\u8ff0algorithm\u4e2d\u7684\"2.2.4 put the successors on top of OPEN\"\u5bf9\u5e94\u7684\u662f\u201c1. how to write search algorithms\u201c\u4e2d\u5173\u4e8e\u201cstrategies for selecting which node to expand next\u201d\u7684\u8ba8\u8bba\uff1a depth-first when successor nodes are added at the beginning of the queue Notes: Nodes that have been expanded (i.e. their successors have been generated) are called closed , nodes that were generated and are awaiting expansion are called open . Two queues, CLOSED and OPEN keep track of these two sets of nodes. The depth of a node in a graph is defined recursively as 1 + depth of its shallowest parent. The start node has 0 depth. If the depth bound is less than the solution depth the algorithm terminates without finding a solution. If the depth bound is greater the algorithm might find a non optimal solution. This can be remedied\uff08\u5f25\u8865\uff09 by completing the search to the depth of the last solution found and then returning the best solution. To save memory CLOSED needs to be cleaned up by keeping in it only the nodes on the current traversal path. NOTE: \u6240\u641c\u7d22\u7684\u662f\u4ecestart\u5230goal\u7684\u4e00\u6761path\u3002 \u4e0a\u8ff0\u7b97\u6cd5\u5176\u5b9e\u662fbacktracking\uff0c\u5b83\u4f7f\u7528\u7684\u662f\u81ea\u5df1\u7ef4\u62a4\u7684stack\uff0c\u5373 OPEN \u3002 \u4e0a\u8ff0\u7b97\u6cd5\u4e2d\uff0c CLOSED \u4ec5\u4ec5\u4fdd\u5b58\u7684\u662fpath\uff0c\u800c\u975e\u6240\u6709\u7684\u5df2\u7ecfvisit\u8fc7\u7684node\u3002 \u6b65\u9aa42\u662f\u4e00\u4e2a while \u5faa\u73af\uff0c\u5b83\u4f1a\u4e0d\u65ad\u6267\u884c\uff0c\u76f4\u81f3\u6761\u4ef6\u6ee1\u8db3\u3002 \u6b65\u9aa42.2.1\u548c\u6b65\u9aa42.2.4\u662f\u9700\u8981\u6ce8\u610f\u7684\uff0c\u5b83\u4e00\u6b21\u6027\u4ea7\u751f\u6240\u6709\u7684successor\uff0c\u5e76\u4e14\u5c06\u6240\u6709\u7684successor\u90fd\u5165\u6808\uff0c\u5373\u52a0\u5165\u5230OPEN\u4e2d\u3002 \u6b65\u9aa42.2.2\u4e0d\u7406\u89e3\u3002 Description of an Informed Search Algorithm Best-First Search The term best-first search is used in different ways by different authors: The Russell and Norvig textbook calls best-first search any algorithm that sorts the nodes according to some function and selects the best each time. Judea Pearl has a somewhat different definition of best-first, which is given here. The major difference is in the specification of how to handle nodes already seen (Russell and Norvig do not specify what to do) and in when goal test is done (Russell and Norvig in Graph-Search check for goal only on the first element of OPEN , Pearl checks as soon as the successors of a node are generated). NOTE: \u4e3b\u8981\u5dee\u5f02\u5728\u4e8e\uff1a how to handle nodes already seen when goal test is done \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0b\u9762\u7684algorithm\u91c7\u7528\u7684\u662fPearl\u7684\u3002 1. Put the start node S on OPEN. 2. until OPEN is empty or a goal node has been reached 2.1 remove from OPEN a node at which f is minimum (break ties arbitrarily) and put it on CLOSED. Call this node n. 2.2 expand n, generating all successors (if any). 2.3 if any of the successors of n is a goal node, exit. 2.4 for each successor n' of n 2.4.1 calculate f(n') 2.4.2 if n' was neither on OPEN nor CLOSED, add it to OPEN. Attach the cost f(n') to it. 2.4.3 if n' was already on OPEN or on CLOSED compare the new value f(n') with the previously assigned value f(n'). If the old value is lower discard the newly generated node. If the new value is lower, substitute the new node for the old node. If the old node was on CLOSED move it back to OPEN. 3. If a goal node is found, then exit with the solution obtained by tracing back through pointers else announce failure. NOTE: \u4e0a\u8ff0algorithm\u4e2d\u7684\"2.4.2 if n' was neither on OPEN nor CLOSED, add it to OPEN.\"\u5bf9\u5e94\u7684\u662f\u201c1. how to write search algorithms\u201c\u4e2d\u5173\u4e8e\u201cstrategies for selecting which node to expand next\u201d\u7684\u8ba8\u8bba\uff1a breadth-first when successor nodes are added at the end of the queue Notes: Step 2.3 requires checking if any of the children of the node just expanded is a goal node . This allows terminating as soon as a goal is found, but the solution returned might have a larger value of f than the optimal solution . To guarantee an optimal solution we need to use a f function that understimates\uff08\u9884\u6d4b\uff09 the cost (as done in A* ) and to test for goal after a node has been selected for expansion. NOTE: \u4e5f\u5c31\u662f\u8bf4\uff0c\u4e0a\u8ff0algorithm\u4e0d\u4fdd\u8bc1optimal solution The computation required to do the tests is (that) step 2.4.3 can be substantial. We could eliminate this step, at the cost of having to maintain duplicate descriptions of identical nodes. NOTE: \u6b65\u9aa42.4.3\u9700\u8981\u8fdb\u884c\u5927\u91cf\u7684\u8ba1\u7b97 If a lower value has been found for a node n' that had already been visited (step 2.4.3) and the old node was on CLOSED , instead of moving it back to OPEN we could leave it on CLOSED , redirect the pointer to its parent node (so it will point to the new parent on the shorter path), and recompute the f values of its descendants. This requires keeping track of descendants of nodes and not only of their parent (so it makes the program more complex to write), but it saves reexpanding nodes that have already been expanded (so it can save time if expansion is an expensive process). NOTE: \u5bf9\u4e8e\u201cIf a lower value has been found for a node n' that had already been visited (step 2.4.3) and the old node was on CLOSED \u201d\uff0c\u4e0a\u8ff0\u7b97\u6cd5\u4e2d\u7ed9\u51fa\u7684\u5904\u7406\u65b9\u6cd5\u662f\uff1amove it back to OPEN . \u4e0a\u9762\u8fd9\u4e00\u6bb5\u7ed9\u51fa\u4e86\u53e6\u5916\u4e00\u79cd\u5904\u7406\u65b9\u6848\uff1a instead of moving it back to OPEN we could leave it on CLOSED , redirect the pointer to its parent node (so it will point to the new parent on the shorter path), and recompute the f values of its descendants \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\uff1a\u5bf9\u4e8e n' \uff0c\u5b83\u5df2\u7ecf\u6709\u4e86lower value\uff0c\u5219\u610f\u5473\u7740\uff1a\u6709\u4e00\u6761cost\u66f4\u4f4e\u7684path\u901a\u5411\u5b83\uff1b\u6b64\u65f6\u5df2\u7ecf\u4f4d\u4e8e CLOSED \u4e86\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u7684pointer\u6307\u5411\u65b0\u7684path\uff0c\u7136\u540e\u91cd\u65b0\u8ba1\u7b97\u5b83\u7684descendants\u7684 f values In general there are no constraints on the function f(n) . Best first always selects for expansion the node with the smallest f value, so f values must be smaller for better nodes. Also, best-first discards multiple paths leading to the same node and keeps the path with the smallest f value. Properties of Best-First Completeness: guaranteed on finite search spaces Admissibility: not guaranteed Time Complexity: depends on the accuracy of the heuristic function. See more later under A* . Space Complexity: O(B to the power of d) where B = branching factor, and d = solution depth NOTE: best-first search\u548c Dijkstra's algorithm \u662f\u6bd4\u8f83\u7c7b\u4f3c\u7684 A* 1. Put the start node S on OPEN . Attach to s the cost g ( s ) = 0. Initialize CLOSED to the empty queue . 2. until OPEN is empty or a goal node has been reached 2.1 remove from OPEN a node at which f is minimum ( break ties arbitrarily ) and put it on CLOSED . Call this node n . 2.2 if n is a goal node , exit . 2.3 expand n , generating all successors ( if any ). 2.4 for each successor n ' of n 2.4.1 calculate f ( n ' ) = g ( n ' ) + h ( n ' ) = g ( n ) + c ( n , n ' ) + h ( n ' ). 2.4.2 if n ' was neither on OPEN nor CLOSED , add it to OPEN . Attach the cost f ( n ' ) to it . 2.4.3 if n ' was already on OPEN or on CLOSED direct its pointers along the path yielding the lowest g ( n ' ) and keep the lowest f ( n ' ). 2.4.4 if n ' required pointer adjustment and was found on CLOSED , move it back to OPEN . 3. If a goal node is found , then exit with the solution obtained by tracing back through its pointers else announce failure . NOTE: \u4e0a\u8ff02.4.2\u30012.4.3\u30012.4.4\u548cBest-First Search algorithm\u76842.4.3\u7c7b\u4f3c\u3002 A* is a special case of best-first search where: The cost f(n) of a node is computed as f(n) = g(n) + h(n) , where g(n) is the cost of the current path from s to n , and h(n) is an estimate of the cost of the path from n to goal The g cost of a node n is computed recursively by adding the g cost of its parent node m to the cost of the arc from the parent m to the node n g(n) = g(m) + c(m,n) . We assume arc costs to be positive. The start node has a cost g(s) = 0 . h(n) is an underestimate of the cost of the optimal path from n to goal. If we call h*(n) the cost of the optimal path forall n h(n) < = h*(n) . This implies that a goal node has a cost h(goal) = 0 The check for the goal node (step 2.2) must be done after the node has been selected for expansion to guarantee an optimal solution NOTE: \u4e3a\u4ec0\u4e48\u80fd\u591f\u5b9e\u73b0\uff1f The redirection of the parent pointer is done using only the g value of the node (not the f value). This guarantees that the path used to any node is the best path. When the heuristic used is monotone\uff08\u5355\u8c03\u7684\uff09, A* never reopens nodes from CLOSED . This means that whenever a node is put into CLOSED , A* has already found the optimal path to it. If forall n h(n) = 0 the algorithm A* is the same as Branch&Bound (or Uniform Cost). If forall n h(n) = 0 and forall n, n' c(n,n')=1 the algorithm A* is the same as Breadth-First Search. Properties of A* Completeness: guaranteed even on infinite graphs provided that the branching factor is finite and that there is some positive constant delta such that every operator cost at least delta. Admissibility: guaranteed given an admissible heuristics (i.e. h(n) is an underestimate of the cost of the optimal path from n to the goal for every node) Time Complexity: depends on the accuracy of the heuristic function. If the heuristic function is exact A* runs in linear time, if the heuristic function is useless, the time complexity is exponential (as for uniform cost). Space Complexity: O(B to the power of d) Iterative-Deepening A* ( IDA* ) IDA* is similar to Depth-First Iterative-Deepening, the difference being the cutoff criteria. A path is cutoff when its total cots f(n) exceeds a cost threshold. IDA* starts with a threshold equal to f(s) (which is equal to h'(s) since g(s) = 0). Each iteration is a depth-first search, cutting off a branch when its f(n) value exceeds the threshold. If a solution is found, the algorithm terminates. Otherwise, the threshold is increased to the minimum f value that exceeded the previous threshold and another depth-first search is started from scratch. This continues until a solution is found. Time Complexity: depends on the accuracy of the heuristic function. If the heuristic function is exact IDA* runs in linear time, if the heuristic function is useless, the time complexity is exponential (as for uniform cost). Space Complexity: O(d)","title":"Search-Algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#umn#csci#4511w#class#notes#on#search","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\uff0c\u603b\u7ed3\u5f97\u975e\u5e38\u597d The description of most of the search algorithms in these notes is taken from J. Pearl, \"Heuristics\", Addison-Wesley, 1984.","title":"umn CSci 4511w: Class Notes on Search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#important#issues#about#search#algorithms","text":"We will address:","title":"Important issues about Search Algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#1#how#to#write#search#algorithms","text":"In particular we will examine: the data structure to keep unexplored nodes. We use a queue (often called a list in many AI books) called OPEN . expansion of a node (or generation of its successors). All the successors of a node can be generated at once (method most commonly used) or they could be generated one at a time either in a systematic way or in a random way. The number of successors is called the branching factor . strategies for selecting which node to expand next. Different algorithms result from different choices (e.g. depth-first when successor nodes are added at the beginning of the queue, breadth-first when successor nodes are added at the end of the queue, etc), test for goal. We will assume the existence of a predicate that applied to a state will return true or false. bookkeeping (keeping track of visited nodes, keeping track of path to goal, etc). Keeping track of visited nodes is usually done by keeping them in a queue (or, better a hash-table) called CLOSED . This prevents getting trapped into loops or repeating work but results in a large space complexity. This is (most often) necessary when optimal solutions are sought\uff08\u5bfb\u627e\uff09, but can be (mostly) avoided in other cases. Keeping track of the path followed is not always necessary (when the problem is to find a goal state and knowing how to get there is not important). NOTE: \u4e0a\u8ff0\u8fc7\u7a0b\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8e\u4e00\u4e2aautomaton\uff0c\u5728\u5de5\u7a0b compiler-principle \u548c\u5de5\u7a0b automata-and-formal-language \u4e2d\u6709\u6d89\u53ca\uff0cparsing\u7684\u8fc7\u7a0b\u4e0e\u4e0a\u8ff0\u8fc7\u7a0b\u975e\u5e38\u7c7b\u4f3c\uff0c\u9700\u8981\u7ed3\u5408top-down parsing\u548cbottom-up parsing\u6765\u4e00\u8d77\u8fdb\u884c\u7406\u89e3\uff0c\u53e6\u5916\u53ef\u4ee5\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Pushdown automaton \u3002","title":"1. how to write search algorithms."},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#2#properties#of#search#algorithms#and#the#solutions#they#find","text":"Termination: the computation is guaranteed to terminate, no matter how large the search space is. NOTE: \u4fdd\u8bc1termination\u7684\u4e00\u79cd\u5e38\u89c1\u60c5\u51b5\u662f\uff1a\u907f\u514dloop\u3002\u4e0b\u9762\u7684\u201crepeated nodes are checked\u201d\u5c31\u662f\u4e3a\u4e86\u907f\u514dloop\u3002 Completeness: an algorithm is complete if it terminates with a solution when one exists. Admissibility: an algorithm is admissible if it is guaranteed to return an optimal solution whenever a solution exists. NOTE: Admissibility\u7684\u610f\u601d\u662f\u201c\u53ef\u5bb9\u8bb8\u6027\u201d\u3002 Space complexity and Time complexity: how the size of the memory and the time needed to run the algorithm grows depending on branching factor , depth of solution , number of nodes , etc. Let's briefly examine the properties of some commonly used uninformed search algorithms .","title":"2. properties of search algorithms and the solutions they find:"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#depth-first#search","text":"Termination: guaranteed for a finite space if repeated nodes are checked. Guaranteed when a depth bound\uff08\u6df1\u5ea6\u754c\u9650\uff09 is used. Not guaranteed otherwise. NOTE: \u201crepeated nodes are checked\u201d\u5c31\u662f\u4e3a\u4e86\u907f\u514d\u201cloop\u201d\u3002 Completeness: not guaranteed in general. Guaranteed if the search space is finite (exhaustive search) and repeated nodes are checked. Admissibility: not guaranteed.","title":"Depth-First Search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#breadth-first#search","text":"Termination: guaranteed for finite space. Guaranteed when a solution exists. NOTE: \u611f\u89c9\u6b64\u5904\u662f\u6709\u95ee\u9898\u7684\uff1a\u5982\u679c\u5b58\u5728loop\uff0c\u5e76\u4e14\u7b97\u6cd5\u6ca1\u6709\u907f\u514dloop\uff0c\u5219\u5b83\u65e0\u6cd5terminate\u3002 Completeness: guaranteed. Admissibility: the algorithm will always find the shortest path (it might not be the optimal path, if arcs have different costs).","title":"Breadth-First Search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#depth-first#search#iterative-deepening","text":"Termination: guaranteed for finite space. Guaranteed when a solution exists. Completeness: guaranteed. Admissibility: the algorithm will always find the shortest path (it might not be the optimal path, if arcs have different costs).","title":"Depth-First Search Iterative-Deepening"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#3#classes#of#search#algorithms","text":"uninformed (depth-first, breadth-first, uniform cost, depth-limited, iterative deepening) versus informed (greedy, A* , IDA* , SMA* ) NOTE: **uninformed**\u7684\u4e2d\u6587\u610f\u601d\u662f\u201c\u65e0\u77e5\u7684\u201d\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u5b83\u6240\u8868\u793a\u7684\u662fblind search\uff0c\u6ca1\u6709heuristics\u7684 local (greedy, hill-climbing) versus global (uniform cost, A* , etc) systematic (depth-first, A* , etc) versus stochastic (simulated annealing, genetic algorithms\uff08\u9057\u4f20\u7b97\u6cd5\uff09) NOTE:","title":"3. classes of search algorithms."},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#pointer","text":"\u5728\u4e0b\u9762\u7684\u5404\u79cdalgorithm\u4e2d\uff0c\u90fd\u4f1a\u770b\u5230**pointer**\uff0c\u539f\u6587\u4e2d\uff0c\u4f5c\u8005\u5e76\u6ca1\u6709\u5bf9pointer\u8fdb\u884c\u89e3\u91ca\u3002\u6839\u636e\u201cDepth-First Search with depth bound\u201d\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u63a8\u6d4b\u5f97\u77e5\uff1a \u6bcf\u4e2anode\uff0c\u90fd\u6709\u4e00\u4e2a\u6307\u5411\u5b83\u7684parent\u7684pointer\uff0c\u7b80\u79f0\u4e3a**parent pointer**\uff1a 2.2.4 put the successors on top of OPEN and provide for each a pointer back to n # \u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u6307\u5411\u5b83\u7684\u4e0a\u7ea7\u8282\u70b9\u7684\u6307\u9488 \u4f7f\u7528parent pointer\u7684\u539f\u56e0\u662f\u4e3a\u4e86\u80fd\u591f\u5f97\u5230\u5b8c\u6574\u7684solution\uff1a 3. If a goal node is found , then exit with the solution obtained by tracing back through its pointers","title":"\u5173\u4e8epointer\u7684\u89e3\u91ca"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#description#of#an#uninformed#search#algorithm","text":"","title":"Description of an Uninformed Search Algorithm:"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#depth-first#search#with#depth#bound","text":"NOTE: \u5e26\u6df1\u5ea6\u9650\u5236\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22 1. Put the start node on OPEN. 2. until OPEN is empty or a goal node has been reached 2.1 remove the topmost node from OPEN and put it on CLOSED. Call this node n. 2.2 if the depth of n is less than the depth bound then 2.2.1 expand n, generating all successors (if any). 2.2.2 if any of these successors is already on the traversal path discard it 2.2.3 if any of these successors is a goal node, exit 2.2.4 put the successors on top of OPEN and provide for each a pointer back to n # \u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u6307\u5411\u5b83\u7684\u4e0a\u7ea7\u8282\u70b9\u7684\u6307\u9488 else 2.2.5 clean up CLOSED. 3. If a goal node is found, then exit with the solution obtained by tracing back through its pointers else announce failure. NOTE: \u4e0a\u8ff0algorithm\u4e2d\u7684\"2.2.4 put the successors on top of OPEN\"\u5bf9\u5e94\u7684\u662f\u201c1. how to write search algorithms\u201c\u4e2d\u5173\u4e8e\u201cstrategies for selecting which node to expand next\u201d\u7684\u8ba8\u8bba\uff1a depth-first when successor nodes are added at the beginning of the queue Notes: Nodes that have been expanded (i.e. their successors have been generated) are called closed , nodes that were generated and are awaiting expansion are called open . Two queues, CLOSED and OPEN keep track of these two sets of nodes. The depth of a node in a graph is defined recursively as 1 + depth of its shallowest parent. The start node has 0 depth. If the depth bound is less than the solution depth the algorithm terminates without finding a solution. If the depth bound is greater the algorithm might find a non optimal solution. This can be remedied\uff08\u5f25\u8865\uff09 by completing the search to the depth of the last solution found and then returning the best solution. To save memory CLOSED needs to be cleaned up by keeping in it only the nodes on the current traversal path. NOTE: \u6240\u641c\u7d22\u7684\u662f\u4ecestart\u5230goal\u7684\u4e00\u6761path\u3002 \u4e0a\u8ff0\u7b97\u6cd5\u5176\u5b9e\u662fbacktracking\uff0c\u5b83\u4f7f\u7528\u7684\u662f\u81ea\u5df1\u7ef4\u62a4\u7684stack\uff0c\u5373 OPEN \u3002 \u4e0a\u8ff0\u7b97\u6cd5\u4e2d\uff0c CLOSED \u4ec5\u4ec5\u4fdd\u5b58\u7684\u662fpath\uff0c\u800c\u975e\u6240\u6709\u7684\u5df2\u7ecfvisit\u8fc7\u7684node\u3002 \u6b65\u9aa42\u662f\u4e00\u4e2a while \u5faa\u73af\uff0c\u5b83\u4f1a\u4e0d\u65ad\u6267\u884c\uff0c\u76f4\u81f3\u6761\u4ef6\u6ee1\u8db3\u3002 \u6b65\u9aa42.2.1\u548c\u6b65\u9aa42.2.4\u662f\u9700\u8981\u6ce8\u610f\u7684\uff0c\u5b83\u4e00\u6b21\u6027\u4ea7\u751f\u6240\u6709\u7684successor\uff0c\u5e76\u4e14\u5c06\u6240\u6709\u7684successor\u90fd\u5165\u6808\uff0c\u5373\u52a0\u5165\u5230OPEN\u4e2d\u3002 \u6b65\u9aa42.2.2\u4e0d\u7406\u89e3\u3002","title":"Depth-First Search with depth bound"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#description#of#an#informed#search#algorithm","text":"","title":"Description of an Informed Search Algorithm"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#best-first#search","text":"The term best-first search is used in different ways by different authors: The Russell and Norvig textbook calls best-first search any algorithm that sorts the nodes according to some function and selects the best each time. Judea Pearl has a somewhat different definition of best-first, which is given here. The major difference is in the specification of how to handle nodes already seen (Russell and Norvig do not specify what to do) and in when goal test is done (Russell and Norvig in Graph-Search check for goal only on the first element of OPEN , Pearl checks as soon as the successors of a node are generated). NOTE: \u4e3b\u8981\u5dee\u5f02\u5728\u4e8e\uff1a how to handle nodes already seen when goal test is done \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0b\u9762\u7684algorithm\u91c7\u7528\u7684\u662fPearl\u7684\u3002 1. Put the start node S on OPEN. 2. until OPEN is empty or a goal node has been reached 2.1 remove from OPEN a node at which f is minimum (break ties arbitrarily) and put it on CLOSED. Call this node n. 2.2 expand n, generating all successors (if any). 2.3 if any of the successors of n is a goal node, exit. 2.4 for each successor n' of n 2.4.1 calculate f(n') 2.4.2 if n' was neither on OPEN nor CLOSED, add it to OPEN. Attach the cost f(n') to it. 2.4.3 if n' was already on OPEN or on CLOSED compare the new value f(n') with the previously assigned value f(n'). If the old value is lower discard the newly generated node. If the new value is lower, substitute the new node for the old node. If the old node was on CLOSED move it back to OPEN. 3. If a goal node is found, then exit with the solution obtained by tracing back through pointers else announce failure. NOTE: \u4e0a\u8ff0algorithm\u4e2d\u7684\"2.4.2 if n' was neither on OPEN nor CLOSED, add it to OPEN.\"\u5bf9\u5e94\u7684\u662f\u201c1. how to write search algorithms\u201c\u4e2d\u5173\u4e8e\u201cstrategies for selecting which node to expand next\u201d\u7684\u8ba8\u8bba\uff1a breadth-first when successor nodes are added at the end of the queue Notes: Step 2.3 requires checking if any of the children of the node just expanded is a goal node . This allows terminating as soon as a goal is found, but the solution returned might have a larger value of f than the optimal solution . To guarantee an optimal solution we need to use a f function that understimates\uff08\u9884\u6d4b\uff09 the cost (as done in A* ) and to test for goal after a node has been selected for expansion. NOTE: \u4e5f\u5c31\u662f\u8bf4\uff0c\u4e0a\u8ff0algorithm\u4e0d\u4fdd\u8bc1optimal solution The computation required to do the tests is (that) step 2.4.3 can be substantial. We could eliminate this step, at the cost of having to maintain duplicate descriptions of identical nodes. NOTE: \u6b65\u9aa42.4.3\u9700\u8981\u8fdb\u884c\u5927\u91cf\u7684\u8ba1\u7b97 If a lower value has been found for a node n' that had already been visited (step 2.4.3) and the old node was on CLOSED , instead of moving it back to OPEN we could leave it on CLOSED , redirect the pointer to its parent node (so it will point to the new parent on the shorter path), and recompute the f values of its descendants. This requires keeping track of descendants of nodes and not only of their parent (so it makes the program more complex to write), but it saves reexpanding nodes that have already been expanded (so it can save time if expansion is an expensive process). NOTE: \u5bf9\u4e8e\u201cIf a lower value has been found for a node n' that had already been visited (step 2.4.3) and the old node was on CLOSED \u201d\uff0c\u4e0a\u8ff0\u7b97\u6cd5\u4e2d\u7ed9\u51fa\u7684\u5904\u7406\u65b9\u6cd5\u662f\uff1amove it back to OPEN . \u4e0a\u9762\u8fd9\u4e00\u6bb5\u7ed9\u51fa\u4e86\u53e6\u5916\u4e00\u79cd\u5904\u7406\u65b9\u6848\uff1a instead of moving it back to OPEN we could leave it on CLOSED , redirect the pointer to its parent node (so it will point to the new parent on the shorter path), and recompute the f values of its descendants \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\uff1a\u5bf9\u4e8e n' \uff0c\u5b83\u5df2\u7ecf\u6709\u4e86lower value\uff0c\u5219\u610f\u5473\u7740\uff1a\u6709\u4e00\u6761cost\u66f4\u4f4e\u7684path\u901a\u5411\u5b83\uff1b\u6b64\u65f6\u5df2\u7ecf\u4f4d\u4e8e CLOSED \u4e86\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u7684pointer\u6307\u5411\u65b0\u7684path\uff0c\u7136\u540e\u91cd\u65b0\u8ba1\u7b97\u5b83\u7684descendants\u7684 f values In general there are no constraints on the function f(n) . Best first always selects for expansion the node with the smallest f value, so f values must be smaller for better nodes. Also, best-first discards multiple paths leading to the same node and keeps the path with the smallest f value. Properties of Best-First Completeness: guaranteed on finite search spaces Admissibility: not guaranteed Time Complexity: depends on the accuracy of the heuristic function. See more later under A* . Space Complexity: O(B to the power of d) where B = branching factor, and d = solution depth NOTE: best-first search\u548c Dijkstra's algorithm \u662f\u6bd4\u8f83\u7c7b\u4f3c\u7684","title":"Best-First Search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#a","text":"1. Put the start node S on OPEN . Attach to s the cost g ( s ) = 0. Initialize CLOSED to the empty queue . 2. until OPEN is empty or a goal node has been reached 2.1 remove from OPEN a node at which f is minimum ( break ties arbitrarily ) and put it on CLOSED . Call this node n . 2.2 if n is a goal node , exit . 2.3 expand n , generating all successors ( if any ). 2.4 for each successor n ' of n 2.4.1 calculate f ( n ' ) = g ( n ' ) + h ( n ' ) = g ( n ) + c ( n , n ' ) + h ( n ' ). 2.4.2 if n ' was neither on OPEN nor CLOSED , add it to OPEN . Attach the cost f ( n ' ) to it . 2.4.3 if n ' was already on OPEN or on CLOSED direct its pointers along the path yielding the lowest g ( n ' ) and keep the lowest f ( n ' ). 2.4.4 if n ' required pointer adjustment and was found on CLOSED , move it back to OPEN . 3. If a goal node is found , then exit with the solution obtained by tracing back through its pointers else announce failure . NOTE: \u4e0a\u8ff02.4.2\u30012.4.3\u30012.4.4\u548cBest-First Search algorithm\u76842.4.3\u7c7b\u4f3c\u3002 A* is a special case of best-first search where: The cost f(n) of a node is computed as f(n) = g(n) + h(n) , where g(n) is the cost of the current path from s to n , and h(n) is an estimate of the cost of the path from n to goal The g cost of a node n is computed recursively by adding the g cost of its parent node m to the cost of the arc from the parent m to the node n g(n) = g(m) + c(m,n) . We assume arc costs to be positive. The start node has a cost g(s) = 0 . h(n) is an underestimate of the cost of the optimal path from n to goal. If we call h*(n) the cost of the optimal path forall n h(n) < = h*(n) . This implies that a goal node has a cost h(goal) = 0 The check for the goal node (step 2.2) must be done after the node has been selected for expansion to guarantee an optimal solution NOTE: \u4e3a\u4ec0\u4e48\u80fd\u591f\u5b9e\u73b0\uff1f The redirection of the parent pointer is done using only the g value of the node (not the f value). This guarantees that the path used to any node is the best path. When the heuristic used is monotone\uff08\u5355\u8c03\u7684\uff09, A* never reopens nodes from CLOSED . This means that whenever a node is put into CLOSED , A* has already found the optimal path to it. If forall n h(n) = 0 the algorithm A* is the same as Branch&Bound (or Uniform Cost). If forall n h(n) = 0 and forall n, n' c(n,n')=1 the algorithm A* is the same as Breadth-First Search. Properties of A* Completeness: guaranteed even on infinite graphs provided that the branching factor is finite and that there is some positive constant delta such that every operator cost at least delta. Admissibility: guaranteed given an admissible heuristics (i.e. h(n) is an underestimate of the cost of the optimal path from n to the goal for every node) Time Complexity: depends on the accuracy of the heuristic function. If the heuristic function is exact A* runs in linear time, if the heuristic function is useless, the time complexity is exponential (as for uniform cost). Space Complexity: O(B to the power of d)","title":"A*"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Algorithm/#iterative-deepening#a#ida","text":"IDA* is similar to Depth-First Iterative-Deepening, the difference being the cutoff criteria. A path is cutoff when its total cots f(n) exceeds a cost threshold. IDA* starts with a threshold equal to f(s) (which is equal to h'(s) since g(s) = 0). Each iteration is a depth-first search, cutting off a branch when its f(n) value exceeds the threshold. If a solution is found, the algorithm terminates. Otherwise, the threshold is increased to the minimum f value that exceeded the previous threshold and another depth-first search is started from scratch. This continues until a solution is found. Time Complexity: depends on the accuracy of the heuristic function. If the heuristic function is exact IDA* runs in linear time, if the heuristic function is useless, the time complexity is exponential (as for uniform cost). Space Complexity: O(d)","title":"Iterative-Deepening A* (IDA*)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/","text":"washington Search: Cost & Heuristics Search thru a Problem Space / State Space Input Set of states Operators [and costs] Start state Goal state [test] Output Path: start \u21d2 a state satisfying goal test [May require shortest path] [Sometimes just need state passing test] Search Methods Blind search Depth first search (DFS) Breadth first search (BFS) Iterative deepening depth-first search (IDS) Heuristic search Best first search Uniform cost search (UCS) Greedy search A* Iterative Deepening A* ( IDA* ) Beam search Hill climbing Depth First Search","title":"Search-Cost-&-Heuristics"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/#washington#search#cost#heuristics","text":"","title":"washington Search: Cost &amp; Heuristics"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/#search#thru#a#problem#space#state#space","text":"","title":"Search thru a Problem Space / State Space"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/#input","text":"Set of states Operators [and costs] Start state Goal state [test]","title":"Input"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/#output","text":"Path: start \u21d2 a state satisfying goal test [May require shortest path] [Sometimes just need state passing test]","title":"Output"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/#search#methods","text":"","title":"Search Methods"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/#blind#search","text":"Depth first search (DFS) Breadth first search (BFS) Iterative deepening depth-first search (IDS)","title":"Blind search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/#heuristic#search","text":"Best first search Uniform cost search (UCS) Greedy search A* Iterative Deepening A* ( IDA* ) Beam search Hill climbing","title":"Heuristic search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Summary/Search-Cost-%26-Heuristics/#depth#first#search","text":"","title":"Depth First Search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Uniform-cost-search/Uniform-cost-search/","text":"Uniform-cost search How does the uniform-cost search algorithm work? Uniform-Cost Search (Dijkstra for large Graphs) https://algorithmicthoughts.wordpress.com/2012/12/15/artificial-intelligence-uniform-cost-searchucs/ https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm#Practical_optimizations_and_infinite_graphs","title":"Uniform-cost search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Uniform-cost-search/Uniform-cost-search/#uniform-cost#search","text":"","title":"Uniform-cost search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Uniform-cost-search/Uniform-cost-search/#how#does#the#uniform-cost#search#algorithm#work","text":"","title":"How does the uniform-cost search algorithm work?"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Methods/Uniform-cost-search/Uniform-cost-search/#uniform-cost#search#dijkstra#for#large#graphs","text":"https://algorithmicthoughts.wordpress.com/2012/12/15/artificial-intelligence-uniform-cost-searchucs/ https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm#Practical_optimizations_and_infinite_graphs","title":"Uniform-Cost Search (Dijkstra for large Graphs)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Iterative-deepening-depth-first-search/","text":"Iterative deepening depth-first search","title":"Iterative-deepening-depth-first-search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Iterative-deepening-depth-first-search/#iterative#deepening#depth-first#search","text":"","title":"Iterative deepening depth-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/","text":"Tree traversal In computer science , tree traversal (also known as tree search ) is a form of graph traversal and refers to the process of visiting (checking and/or updating) each node in a tree data structure , exactly once. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a binary tree , but they may be generalized to other trees as well. Types Unlike linked lists , one-dimensional arrays and other linear data structures , which are canonically traversed in linear order, trees may be traversed in multiple ways. They may be traversed in depth-first or breadth-first order . There are three common ways to traverse them in depth-first order : in-order, pre-order and post-order.[ 1] Beyond these basic traversals, various more complex or hybrid schemes are possible, such as depth-limited searches like iterative deepening depth-first search . Data structures for tree traversal Traversing a tree involves iterating over all nodes in some manner. Because from a given node there is more than one possible next node (it is not a linear data structure), then, assuming sequential computation (not parallel), some nodes must be deferred \u2014stored in some way for later visiting. This is often done via a stack (LIFO) or queue (FIFO). As a tree is a self-referential (recursively defined) data structure, traversal can be defined by recursion or, more subtly, corecursion , in a very natural and clear fashion; in these cases the deferred nodes are stored implicitly in the call stack . SUMMARY : data structure\u51b3\u5b9a\u4e86\u5bf9\u5b83\u64cd\u4f5c\u7684\u5404\u79cd\u5b9e\u73b0\uff0c\u56e0\u4e3atree\u662fself-referential\uff0c\u6240\u4ee5\u5bf9\u5b83\u7684\u5404\u79cd\u64cd\u4f5c\u4e5f\u662f\u53ef\u4ee5\u4ee5 recursion or corecursion \u7684\u65b9\u5f0f\u5b9e\u73b0\u7684\uff1b Depth-first search is easily implemented via a stack , including recursively (via the call stack ), while breadth-first search is easily implemented via a queue , including corecursively . Depth-first search Main article: Depth-first search These searches are referred to as depth-first search (DFS), as the search tree is deepened as much as possible on each child before going to the next sibling . For a binary tree, they are defined as display operations recursively at each node, starting with the root, whose algorithm is as follows:[ 2] [ 3] The general recursive pattern for traversing a (non-empty) binary tree is this: At node N do the following: (L) Recursively traverse its left subtree. This step is finished at the node N again. (R) Recursively traverse its right subtree. This step is finished at the node N again. (N) Process N itself. These steps can be done in any order . If (L) is done before (R), the process is called left-to-right traversal, otherwise it is called right-to-left traversal. The following methods show left-to-right traversal: Pre-order (NLR) \u5148\u5e8f Check if the current node is empty or null.\uff08\u5982\u679c\u4e3anull\uff0c\u5219return\uff09 Display the data part of the root (or current node). Traverse the left subtree by recursively calling the pre-order function. Traverse the right subtree by recursively calling the pre-order function. The pre-order traversal is a topologically sorted one, because a parent node is processed before any of its child nodes is done. Pre-order: F, B, A, D, C, E, G, I, H. In-order (LNR) \u4e2d\u5e8f Check if the current node is empty or null.\uff08\u5982\u679c\u4e3anull\uff0c\u5219return\uff09 Traverse the left subtree by recursively calling the in-order function. Display the data part of the root (or current node). Traverse the right subtree by recursively calling the in-order function. In a binary search tree , in-order traversal retrieves data in sorted order.[ 4] In-order: A, B, C, D, E, F, G, H, I. Out-order (RNL) SUMMARY : out\u548cin\u76f8\u53cd Check if the current node is empty or null.\uff08\u5982\u679c\u4e3anull\uff0c\u5219return\uff09 Traverse the right subtree by recursively calling the out-order function. Display the data part of the root (or current node). Traverse the left subtree by recursively calling the out-order function. In a binary search tree , out-order traversal retrieves data in reverse sorted order . Post-order (LRN) Check if the current node is empty or null.\uff08\u5982\u679c\u4e3anull\uff0c\u5219return\uff09 Traverse the left subtree by recursively calling the post-order function. Traverse the right subtree by recursively calling the post-order function. Display the data part of the root (or current node). The trace of a traversal is called a sequentialisation of the tree. The traversal trace is a list of each visited root. No one sequentialisation according to pre-, in- or post-order describes the underlying tree uniquely. Given a tree with distinct elements, either pre-order or post-order paired with in-order is sufficient to describe the tree uniquely. However, pre-order with post-order leaves some ambiguity in the tree structure.[ 5] Post-order: A, C, E, D, B, H, I, G, F. Generic tree To traverse any tree with depth-first search , perform the following operations recursively at each node: Perform pre-order operation. For each i from 1 to the number of children do: Visit i -th, if present. Perform in-order operation. Perform post-order operation. Depending on the problem at hand, the pre-order, in-order or post-order operations may be void, or you may only want to visit a specific child, so these operations are optional. Also, in practice more than one of pre-order, in-order and post-order operations may be required. For example, when inserting into a ternary tree, a pre-order operation is performed by comparing items. A post-order operation may be needed afterwards to re-balance the tree. Breadth-first search / level order Main article: Breadth-first search Trees can also be traversed in level-order , where we visit every node on a level before going to a lower level. This search is referred to as breadth-first search (BFS), as the search tree is broadened as much as possible on each depth before going to the next depth. Other types There are also tree traversal algorithms that classify as neither depth-first search nor breadth-first search. One such algorithm is Monte Carlo tree search , which concentrates on analyzing the most promising moves, basing the expansion of the search tree on random sampling of the search space. Applications Pre-order traversal while duplicating nodes and edges can make a complete duplicate of a binary tree . It can also be used to make a prefix expression ( Polish notation ) from expression trees : traverse the expression tree pre-orderly. For example, traversing the depicted arithmetic expression in pre-order yields \"+ * 1 - 2 3 + 4 5\". In-order traversal is very commonly used on binary search trees because it returns values from the underlying set in order, according to the comparator that set up the binary search tree (hence the name). Post-order traversal while deleting or freeing nodes and values can delete or free an entire binary tree. It can also generate a postfix representation ( Reverse Polish notation ) of a binary tree. Traversing the depicted arithmetic expression in post-order yields \"1 2 3 - * 4 5 + +\". SUMMARY : \u5982\u4f55\u6839\u636e\u5b57\u7b26\u4e32\u6784\u9020\u51faexpression tree Implementations THINKING : tree\u662f\u6ee1\u8db3\u9012\u5f52\u7ed3\u6784\u7684\uff0c\u90a3\u4e3a\u4ec0\u4e48\u5e7f\u5ea6\u4f18\u5148\u904d\u5386\u4e0d\u80fd\u591f\u4f7f\u7528\u9012\u5f52\u800c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u80fd\u591f\u4f7f\u7528\u9012\u5f52\u5462\uff1f\u56e0\u4e3a\u5e7f\u5ea6\u4f18\u5148\u904d\u5386\u5e76\u6ca1\u6709\u6309\u7167tree\u7684\u9012\u5f52\u7ed3\u6784\u6765\u8fdb\u884c\u904d\u5386\uff0c\u800c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u5219\u662f\u6309\u7167\u6811\u7684\u9012\u5f52\u7ed3\u6784\u6765\u8fdb\u884c\u904d\u5386\u7684\uff1b\u6240\u4ee5\uff0c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u5c31\u80fd\u591f\u6309\u7167\u9012\u5f52\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u5b9e\u73b0\uff1b Depth-first search SUMMARY : \u9012\u5f52\u51fd\u6570\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5176\u5b9e\u662f\u6709\u4e00\u4e2acall stack\u7684\uff0c\u4e0b\u9762\u7684\u4f2a\u4ee3\u7801\u90fd\u63d0\u4f9b\u4e86\u4e24\u4e2a\u7248\u672c\uff1a \u9012\u5f52\u7248\u672c \u8fed\u4ee3\u7248\u672c Pre-order preorder(node) if (node == null) return visit(node) preorder(node.left) preorder(node.right) iterativePreorder(node) if (node == null) return s \u2190 empty stack s.push(node) while (not s.isEmpty()) node \u2190 s.pop() visit(node) //right child is pushed first so that left is processed first if (node.right \u2260 null) s.push(node.right) if (node.left \u2260 null) s.push(node.left) SUMMARY : \u4ee3\u7801\u7406\u89e3\uff0cstack\u4e2d\u5b58\u653e\u7684\u662f\u5f85visit\u7684node\uff0c\u6bcf\u6b21visit\u6808\u9876\u7684node\uff0c\u4e00\u65e6\u8be5node\u88abvisit\u540e\uff0c\u5c31\u8981\u5c06\u5176pop\uff1b\u7531\u4e8epre-order\u662f\u4e00\u65e6\u9047\u5230\u8be5\u8282\u70b9\uff0c\u5c31\u8981\u8fdb\u884cvisit\uff0c\u6240\u4ee5\u5728 while \u4e2d\u5c31\u4f1a\u9996\u5148\u6267\u884c node \u2190 s.pop() \u3001 visit(node) \uff0c\u7136\u540e\u518d\u5148stack\u4e2d\u538b\u5165\u65b0\u7684node\uff1b In-order inorder(node) if (node == null) return inorder(node.left) visit(node) inorder(node.right) iterativeInorder(node) s \u2190 empty stack while (not s.isEmpty() or node \u2260 null) if (node \u2260 null) s.push(node) node \u2190 node.left else node \u2190 s.pop() visit(node) node \u2190 node.right SUMMARY : in-order\u7684\u4e00\u4e2a\u7279\u70b9\u662f\uff1a\u53ea\u6709\u5f53\u4e00\u4e2anode\u6ca1\u6709left node\u6216\u8005\u5b83\u7684left node\u5df2\u7ecfvisit\u8fc7\u4e86\uff0c\u5b83\u624d\u80fd\u591f\u88abvisit\uff0c\u7136\u540e\u518d\u53bbvisit\u5b83\u7684right node\uff1b\u5426\u5219\u5b83\u53ea\u80fd\u591f\u5728stack\u4e2d\u7b49\u5f85\uff0c\u65e2\u7136node\u5728stack\u4e2d\uff0c\u90a3\u4e48\u5c31\u53ef\u4ee5\u83b7\u5f97\u5230\u5b83\u7684\u5b50\u8282\u70b9\u7684\u4fe1\u606f\uff1b Post-order postorder(node) if (node == null) return postorder(node.left) postorder(node.right) visit(node) iterativePostorder(node) s \u2190 empty stack lastNodeVisited \u2190 null while (not s.isEmpty() or node \u2260 null) if (node \u2260 null) s.push(node) node \u2190 node.left else peekNode \u2190 s.peek() // if right child exists and traversing node // from left child, then move right if (peekNode.right \u2260 null and lastNodeVisited \u2260 peekNode.right) node \u2190 peekNode.right else visit(peekNode) lastNodeVisited \u2190 s.pop() SUMMARY : post-order\u548cin-order\u5176\u5b9e\u662f\u6709\u70b9\u7c7b\u4f3c\u7684\uff1b All the above implementations require stack space proportional to the height of the tree which is a call stack for the recursive and a parent stack for the iterative ones. In a poorly balanced tree, this can be considerable. With the iterative implementations we can remove the stack requirement by maintaining parent pointers in each node, or by threading the tree (next section). Morris in-order traversal using threading A binary tree is threaded by making every left child pointer (that would otherwise be null) point to the in-order predecessor of the node (if it exists) and every right child pointer (that would otherwise be null) point to the in-order successor of the node (if it exists). Advantages: Avoids recursion, which uses a call stack and consumes memory and time. The node keeps a record of its parent. Disadvantages: The tree is more complex. We can make only one traversal at a time. It is more prone to errors when both the children are not present and both values of nodes point to their ancestors. Morris traversal is an implementation of in-order traversal that uses threading:[ 6] Create links to the in-order successor. Print the data using these links. Revert the changes to restore original tree. Breadth-first search Also, listed below is pseudocode for a simple queue based level-order traversal, and will require space proportional to the maximum number of nodes at a given depth. This can be as much as the total number of nodes / 2. A more space-efficient approach for this type of traversal can be implemented using an iterative deepening depth-first search . levelorder(root) q \u2190 empty queue q.enqueue(root) while (not q.isEmpty()) node \u2190 q.dequeue() visit(node) if (node.left \u2260 null) q.enqueue(node.left) if (node.right \u2260 null) q.enqueue(node.right) Infinite trees","title":"Tree-traversal"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#tree#traversal","text":"In computer science , tree traversal (also known as tree search ) is a form of graph traversal and refers to the process of visiting (checking and/or updating) each node in a tree data structure , exactly once. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a binary tree , but they may be generalized to other trees as well.","title":"Tree traversal"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#types","text":"Unlike linked lists , one-dimensional arrays and other linear data structures , which are canonically traversed in linear order, trees may be traversed in multiple ways. They may be traversed in depth-first or breadth-first order . There are three common ways to traverse them in depth-first order : in-order, pre-order and post-order.[ 1] Beyond these basic traversals, various more complex or hybrid schemes are possible, such as depth-limited searches like iterative deepening depth-first search .","title":"Types"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#data#structures#for#tree#traversal","text":"Traversing a tree involves iterating over all nodes in some manner. Because from a given node there is more than one possible next node (it is not a linear data structure), then, assuming sequential computation (not parallel), some nodes must be deferred \u2014stored in some way for later visiting. This is often done via a stack (LIFO) or queue (FIFO). As a tree is a self-referential (recursively defined) data structure, traversal can be defined by recursion or, more subtly, corecursion , in a very natural and clear fashion; in these cases the deferred nodes are stored implicitly in the call stack . SUMMARY : data structure\u51b3\u5b9a\u4e86\u5bf9\u5b83\u64cd\u4f5c\u7684\u5404\u79cd\u5b9e\u73b0\uff0c\u56e0\u4e3atree\u662fself-referential\uff0c\u6240\u4ee5\u5bf9\u5b83\u7684\u5404\u79cd\u64cd\u4f5c\u4e5f\u662f\u53ef\u4ee5\u4ee5 recursion or corecursion \u7684\u65b9\u5f0f\u5b9e\u73b0\u7684\uff1b Depth-first search is easily implemented via a stack , including recursively (via the call stack ), while breadth-first search is easily implemented via a queue , including corecursively .","title":"Data structures for tree traversal"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#depth-first#search","text":"Main article: Depth-first search These searches are referred to as depth-first search (DFS), as the search tree is deepened as much as possible on each child before going to the next sibling . For a binary tree, they are defined as display operations recursively at each node, starting with the root, whose algorithm is as follows:[ 2] [ 3] The general recursive pattern for traversing a (non-empty) binary tree is this: At node N do the following: (L) Recursively traverse its left subtree. This step is finished at the node N again. (R) Recursively traverse its right subtree. This step is finished at the node N again. (N) Process N itself. These steps can be done in any order . If (L) is done before (R), the process is called left-to-right traversal, otherwise it is called right-to-left traversal. The following methods show left-to-right traversal:","title":"Depth-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#pre-order#nlr","text":"\u5148\u5e8f Check if the current node is empty or null.\uff08\u5982\u679c\u4e3anull\uff0c\u5219return\uff09 Display the data part of the root (or current node). Traverse the left subtree by recursively calling the pre-order function. Traverse the right subtree by recursively calling the pre-order function. The pre-order traversal is a topologically sorted one, because a parent node is processed before any of its child nodes is done. Pre-order: F, B, A, D, C, E, G, I, H.","title":"Pre-order (NLR)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#in-order#lnr","text":"\u4e2d\u5e8f Check if the current node is empty or null.\uff08\u5982\u679c\u4e3anull\uff0c\u5219return\uff09 Traverse the left subtree by recursively calling the in-order function. Display the data part of the root (or current node). Traverse the right subtree by recursively calling the in-order function. In a binary search tree , in-order traversal retrieves data in sorted order.[ 4] In-order: A, B, C, D, E, F, G, H, I.","title":"In-order (LNR)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#out-order#rnl","text":"SUMMARY : out\u548cin\u76f8\u53cd Check if the current node is empty or null.\uff08\u5982\u679c\u4e3anull\uff0c\u5219return\uff09 Traverse the right subtree by recursively calling the out-order function. Display the data part of the root (or current node). Traverse the left subtree by recursively calling the out-order function. In a binary search tree , out-order traversal retrieves data in reverse sorted order .","title":"Out-order (RNL)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#post-order#lrn","text":"Check if the current node is empty or null.\uff08\u5982\u679c\u4e3anull\uff0c\u5219return\uff09 Traverse the left subtree by recursively calling the post-order function. Traverse the right subtree by recursively calling the post-order function. Display the data part of the root (or current node). The trace of a traversal is called a sequentialisation of the tree. The traversal trace is a list of each visited root. No one sequentialisation according to pre-, in- or post-order describes the underlying tree uniquely. Given a tree with distinct elements, either pre-order or post-order paired with in-order is sufficient to describe the tree uniquely. However, pre-order with post-order leaves some ambiguity in the tree structure.[ 5] Post-order: A, C, E, D, B, H, I, G, F.","title":"Post-order (LRN)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#generic#tree","text":"To traverse any tree with depth-first search , perform the following operations recursively at each node: Perform pre-order operation. For each i from 1 to the number of children do: Visit i -th, if present. Perform in-order operation. Perform post-order operation. Depending on the problem at hand, the pre-order, in-order or post-order operations may be void, or you may only want to visit a specific child, so these operations are optional. Also, in practice more than one of pre-order, in-order and post-order operations may be required. For example, when inserting into a ternary tree, a pre-order operation is performed by comparing items. A post-order operation may be needed afterwards to re-balance the tree.","title":"Generic tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#breadth-first#search#level#order","text":"Main article: Breadth-first search Trees can also be traversed in level-order , where we visit every node on a level before going to a lower level. This search is referred to as breadth-first search (BFS), as the search tree is broadened as much as possible on each depth before going to the next depth.","title":"Breadth-first search / level order"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#other#types","text":"There are also tree traversal algorithms that classify as neither depth-first search nor breadth-first search. One such algorithm is Monte Carlo tree search , which concentrates on analyzing the most promising moves, basing the expansion of the search tree on random sampling of the search space.","title":"Other types"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#applications","text":"Pre-order traversal while duplicating nodes and edges can make a complete duplicate of a binary tree . It can also be used to make a prefix expression ( Polish notation ) from expression trees : traverse the expression tree pre-orderly. For example, traversing the depicted arithmetic expression in pre-order yields \"+ * 1 - 2 3 + 4 5\". In-order traversal is very commonly used on binary search trees because it returns values from the underlying set in order, according to the comparator that set up the binary search tree (hence the name). Post-order traversal while deleting or freeing nodes and values can delete or free an entire binary tree. It can also generate a postfix representation ( Reverse Polish notation ) of a binary tree. Traversing the depicted arithmetic expression in post-order yields \"1 2 3 - * 4 5 + +\". SUMMARY : \u5982\u4f55\u6839\u636e\u5b57\u7b26\u4e32\u6784\u9020\u51faexpression tree","title":"Applications"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#implementations","text":"THINKING : tree\u662f\u6ee1\u8db3\u9012\u5f52\u7ed3\u6784\u7684\uff0c\u90a3\u4e3a\u4ec0\u4e48\u5e7f\u5ea6\u4f18\u5148\u904d\u5386\u4e0d\u80fd\u591f\u4f7f\u7528\u9012\u5f52\u800c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u80fd\u591f\u4f7f\u7528\u9012\u5f52\u5462\uff1f\u56e0\u4e3a\u5e7f\u5ea6\u4f18\u5148\u904d\u5386\u5e76\u6ca1\u6709\u6309\u7167tree\u7684\u9012\u5f52\u7ed3\u6784\u6765\u8fdb\u884c\u904d\u5386\uff0c\u800c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u5219\u662f\u6309\u7167\u6811\u7684\u9012\u5f52\u7ed3\u6784\u6765\u8fdb\u884c\u904d\u5386\u7684\uff1b\u6240\u4ee5\uff0c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u5c31\u80fd\u591f\u6309\u7167\u9012\u5f52\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u5b9e\u73b0\uff1b","title":"Implementations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#depth-first#search_1","text":"SUMMARY : \u9012\u5f52\u51fd\u6570\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5176\u5b9e\u662f\u6709\u4e00\u4e2acall stack\u7684\uff0c\u4e0b\u9762\u7684\u4f2a\u4ee3\u7801\u90fd\u63d0\u4f9b\u4e86\u4e24\u4e2a\u7248\u672c\uff1a \u9012\u5f52\u7248\u672c \u8fed\u4ee3\u7248\u672c","title":"Depth-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#pre-order","text":"preorder(node) if (node == null) return visit(node) preorder(node.left) preorder(node.right) iterativePreorder(node) if (node == null) return s \u2190 empty stack s.push(node) while (not s.isEmpty()) node \u2190 s.pop() visit(node) //right child is pushed first so that left is processed first if (node.right \u2260 null) s.push(node.right) if (node.left \u2260 null) s.push(node.left) SUMMARY : \u4ee3\u7801\u7406\u89e3\uff0cstack\u4e2d\u5b58\u653e\u7684\u662f\u5f85visit\u7684node\uff0c\u6bcf\u6b21visit\u6808\u9876\u7684node\uff0c\u4e00\u65e6\u8be5node\u88abvisit\u540e\uff0c\u5c31\u8981\u5c06\u5176pop\uff1b\u7531\u4e8epre-order\u662f\u4e00\u65e6\u9047\u5230\u8be5\u8282\u70b9\uff0c\u5c31\u8981\u8fdb\u884cvisit\uff0c\u6240\u4ee5\u5728 while \u4e2d\u5c31\u4f1a\u9996\u5148\u6267\u884c node \u2190 s.pop() \u3001 visit(node) \uff0c\u7136\u540e\u518d\u5148stack\u4e2d\u538b\u5165\u65b0\u7684node\uff1b","title":"Pre-order"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#in-order","text":"inorder(node) if (node == null) return inorder(node.left) visit(node) inorder(node.right) iterativeInorder(node) s \u2190 empty stack while (not s.isEmpty() or node \u2260 null) if (node \u2260 null) s.push(node) node \u2190 node.left else node \u2190 s.pop() visit(node) node \u2190 node.right SUMMARY : in-order\u7684\u4e00\u4e2a\u7279\u70b9\u662f\uff1a\u53ea\u6709\u5f53\u4e00\u4e2anode\u6ca1\u6709left node\u6216\u8005\u5b83\u7684left node\u5df2\u7ecfvisit\u8fc7\u4e86\uff0c\u5b83\u624d\u80fd\u591f\u88abvisit\uff0c\u7136\u540e\u518d\u53bbvisit\u5b83\u7684right node\uff1b\u5426\u5219\u5b83\u53ea\u80fd\u591f\u5728stack\u4e2d\u7b49\u5f85\uff0c\u65e2\u7136node\u5728stack\u4e2d\uff0c\u90a3\u4e48\u5c31\u53ef\u4ee5\u83b7\u5f97\u5230\u5b83\u7684\u5b50\u8282\u70b9\u7684\u4fe1\u606f\uff1b","title":"In-order"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#post-order","text":"postorder(node) if (node == null) return postorder(node.left) postorder(node.right) visit(node) iterativePostorder(node) s \u2190 empty stack lastNodeVisited \u2190 null while (not s.isEmpty() or node \u2260 null) if (node \u2260 null) s.push(node) node \u2190 node.left else peekNode \u2190 s.peek() // if right child exists and traversing node // from left child, then move right if (peekNode.right \u2260 null and lastNodeVisited \u2260 peekNode.right) node \u2190 peekNode.right else visit(peekNode) lastNodeVisited \u2190 s.pop() SUMMARY : post-order\u548cin-order\u5176\u5b9e\u662f\u6709\u70b9\u7c7b\u4f3c\u7684\uff1b All the above implementations require stack space proportional to the height of the tree which is a call stack for the recursive and a parent stack for the iterative ones. In a poorly balanced tree, this can be considerable. With the iterative implementations we can remove the stack requirement by maintaining parent pointers in each node, or by threading the tree (next section).","title":"Post-order"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#morris#in-order#traversal#using#threading","text":"A binary tree is threaded by making every left child pointer (that would otherwise be null) point to the in-order predecessor of the node (if it exists) and every right child pointer (that would otherwise be null) point to the in-order successor of the node (if it exists). Advantages: Avoids recursion, which uses a call stack and consumes memory and time. The node keeps a record of its parent. Disadvantages: The tree is more complex. We can make only one traversal at a time. It is more prone to errors when both the children are not present and both values of nodes point to their ancestors. Morris traversal is an implementation of in-order traversal that uses threading:[ 6] Create links to the in-order successor. Print the data using these links. Revert the changes to restore original tree.","title":"Morris in-order traversal using threading"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#breadth-first#search","text":"Also, listed below is pseudocode for a simple queue based level-order traversal, and will require space proportional to the maximum number of nodes at a given depth. This can be as much as the total number of nodes / 2. A more space-efficient approach for this type of traversal can be implemented using an iterative deepening depth-first search . levelorder(root) q \u2190 empty queue q.enqueue(root) while (not q.isEmpty()) node \u2190 q.dequeue() visit(node) if (node.left \u2260 null) q.enqueue(node.left) if (node.right \u2260 null) q.enqueue(node.right)","title":"Breadth-first search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Search-algorithm/Tree-traversal/Tree-traversal/#infinite#trees","text":"","title":"Infinite trees"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0tree\uff0c\u5b83\u5728computer science\u548c\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002 \u672c\u7ae0\u4f9d\u7167\u672c\u5de5\u7a0b\u7684\u60ef\u4f8b\uff0c\u4ecestructure\u5165\u624b\uff0c\u5373\u5148\u63cf\u8ff0tree structure\uff08\u6811\u5f62\u7ed3\u6784\uff09\uff0c\u7136\u540e\u63cf\u8ff0tree in data structure\uff08\u6570\u636e\u7ed3\u6784\u4e2d\u7684\u6811\uff09\uff0c\u56e0\u4e3atree in data structure\u672c\u8d28\u4e0a\u662f\u5bf9tree structure\u7684\u8ba1\u7b97\u673a\u5b9e\u73b0\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/#_1","text":"\u672c\u7ae0\u63cf\u8ff0tree\uff0c\u5b83\u5728computer science\u548c\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002 \u672c\u7ae0\u4f9d\u7167\u672c\u5de5\u7a0b\u7684\u60ef\u4f8b\uff0c\u4ecestructure\u5165\u624b\uff0c\u5373\u5148\u63cf\u8ff0tree structure\uff08\u6811\u5f62\u7ed3\u6784\uff09\uff0c\u7136\u540e\u63cf\u8ff0tree in data structure\uff08\u6570\u636e\u7ed3\u6784\u4e2d\u7684\u6811\uff09\uff0c\u56e0\u4e3atree in data structure\u672c\u8d28\u4e0a\u662f\u5bf9tree structure\u7684\u8ba1\u7b97\u673a\u5b9e\u73b0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Application-of-tree/","text":"Application of tree data structure NOTE: The content is concluded from the following post: Tree structure Tree (data structure) Representing nesting structure Decision tree Representing sorted lists of data Tree automaton Tree model \u5728Examples of tree structures\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0ctree structure\u7684\u5e7f\u6cdb\u5b58\u5728\uff1b\u5728Tree structure\u4e2d\uff0c\u6211\u4eec\u603b\u7ed3\u4e86tree structure\u6240\u5177\u5907\u7684**nesting\u5173\u7cfb**\uff0c\u540e\u9762\u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0\uff0c\u6211\u4eec\u521b\u5efa\u201ctree model\u201d\u6982\u5ff5\uff0c\u5b83\u8868\u793a\u5177\u5907nesting\u5173\u7cfb\u7684\u7ed3\u6784\u3002\u663e\u7136\uff0c\u80fd\u591f\u4f7f\u7528tree model\u63cf\u8ff0\u7684\u95ee\u9898\uff0c\u90fd\u80fd\u591f\u8f6c\u6362\u4e3a\u4e0etree\u76f8\u5173\u7684\u64cd\u4f5c\uff0c\u6bd4\u5982\u6784\u5efa\u4e00\u68f5\u6811\u3001\u904d\u5386\u6811\u3002\u6709\u7684\u65f6\u5019\uff0c\u9700\u8981\u663e\u5f0f\u5730\u6784\u9020\u4e00\u68f5\u6811\uff0c\u6709\u7684\u65f6\u5019\uff0c\u65e0\u9700\u663e\u5f0f\u5730\u6784\u9020\u6811\u3002 \u5f88\u591a\u95ee\u9898\u6700\u7ec8\u90fd\u8f6c\u6362\u4e3a\u6811\u64cd\u4f5c parsing\u3001\u62ec\u53f7\u5339\u914d\u3001\u7ec4\u5408\u5206\u6790\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528**nesting\u5173\u7cfb**\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u5bf9\u5b83\u4eec\u7684\u76f8\u5173\u95ee\u9898\u7684\u6c42\u89e3\u6700\u7ec8\u90fd\u53ef\u4ee5\u8f6c\u6362\u4e3a\u6811\u7684\u6784\u5efa\u3001\u6811\u7684\u904d\u5386\u3002","title":"Application-of-tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Application-of-tree/#application#of#tree#data#structure","text":"NOTE: The content is concluded from the following post: Tree structure Tree (data structure)","title":"Application of tree data structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Application-of-tree/#representing#nesting#structure","text":"","title":"Representing  nesting structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Application-of-tree/#decision#tree","text":"","title":"Decision tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Application-of-tree/#representing#sorted#lists#of#data","text":"","title":"Representing sorted lists of data"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Application-of-tree/#tree#automaton","text":"","title":"Tree automaton"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Application-of-tree/#tree#model","text":"\u5728Examples of tree structures\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0ctree structure\u7684\u5e7f\u6cdb\u5b58\u5728\uff1b\u5728Tree structure\u4e2d\uff0c\u6211\u4eec\u603b\u7ed3\u4e86tree structure\u6240\u5177\u5907\u7684**nesting\u5173\u7cfb**\uff0c\u540e\u9762\u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0\uff0c\u6211\u4eec\u521b\u5efa\u201ctree model\u201d\u6982\u5ff5\uff0c\u5b83\u8868\u793a\u5177\u5907nesting\u5173\u7cfb\u7684\u7ed3\u6784\u3002\u663e\u7136\uff0c\u80fd\u591f\u4f7f\u7528tree model\u63cf\u8ff0\u7684\u95ee\u9898\uff0c\u90fd\u80fd\u591f\u8f6c\u6362\u4e3a\u4e0etree\u76f8\u5173\u7684\u64cd\u4f5c\uff0c\u6bd4\u5982\u6784\u5efa\u4e00\u68f5\u6811\u3001\u904d\u5386\u6811\u3002\u6709\u7684\u65f6\u5019\uff0c\u9700\u8981\u663e\u5f0f\u5730\u6784\u9020\u4e00\u68f5\u6811\uff0c\u6709\u7684\u65f6\u5019\uff0c\u65e0\u9700\u663e\u5f0f\u5730\u6784\u9020\u6811\u3002","title":"Tree model"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Application-of-tree/#_1","text":"parsing\u3001\u62ec\u53f7\u5339\u914d\u3001\u7ec4\u5408\u5206\u6790\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528**nesting\u5173\u7cfb**\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u5bf9\u5b83\u4eec\u7684\u76f8\u5173\u95ee\u9898\u7684\u6c42\u89e3\u6700\u7ec8\u90fd\u53ef\u4ee5\u8f6c\u6362\u4e3a\u6811\u7684\u6784\u5efa\u3001\u6811\u7684\u904d\u5386\u3002","title":"\u5f88\u591a\u95ee\u9898\u6700\u7ec8\u90fd\u8f6c\u6362\u4e3a\u6811\u64cd\u4f5c"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Implicit-tree/","text":"Implicit tree https://en.wikipedia.org/wiki/Bottom-up_parsing Tree is then merely implicit in the parser's actions. \u7c7b\u4f3c\u7684\u6709\u9012\u5f52\u8c03\u7528\u6811\u3002 https://opendatastructures.org/ods-cpp/10_1_Implicit_Binary_Tree.html An Implicit Binary Tree Implicit data structure \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u4f7f\u7528\u7684\u662f\u6808\uff0c\u6b63\u5982\u5728 Implicit data structure \u4e2d\u6240\u4ecb\u7ecd\u7684\uff0c\u53ef\u4ee5\u5c06tree\u4fdd\u5b58\u5230\u4e00\u4e2aarray\u4e2d\uff0c\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u4e2d\u6240\u4f7f\u7528\u7684call stack\u5176\u5b9e\u5c31\u7c7b\u4f3c\u4e8e\u5c06\u4e00\u68f5\u6811\u4fdd\u5b58\u5230\u6808\u4e2d\u3002 Implicit k -d tree","title":"Implicit-tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Implicit-tree/#implicit#tree","text":"https://en.wikipedia.org/wiki/Bottom-up_parsing Tree is then merely implicit in the parser's actions. \u7c7b\u4f3c\u7684\u6709\u9012\u5f52\u8c03\u7528\u6811\u3002 https://opendatastructures.org/ods-cpp/10_1_Implicit_Binary_Tree.html An Implicit Binary Tree Implicit data structure \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u4f7f\u7528\u7684\u662f\u6808\uff0c\u6b63\u5982\u5728 Implicit data structure \u4e2d\u6240\u4ecb\u7ecd\u7684\uff0c\u53ef\u4ee5\u5c06tree\u4fdd\u5b58\u5230\u4e00\u4e2aarray\u4e2d\uff0c\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u4e2d\u6240\u4f7f\u7528\u7684call stack\u5176\u5b9e\u5c31\u7c7b\u4f3c\u4e8e\u5c06\u4e00\u68f5\u6811\u4fdd\u5b58\u5230\u6808\u4e2d\u3002 Implicit k -d tree","title":"Implicit  tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/TODO/","text":"radix tree vs trie https://stackoverflow.com/questions/14708134/what-is-the-difference-between-trie-and-radix-trie-data-structures","title":"TODO"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/TODO/#radix#tree#vs#trie","text":"https://stackoverflow.com/questions/14708134/what-is-the-difference-between-trie-and-radix-trie-data-structures","title":"radix tree vs trie"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/","text":"Tree (data structure) In computer science , a tree is a widely used abstract data type (ADT)\u2014or data structure implementing this ADT\u2014that simulates a hierarchical tree structure , with a root value and subtrees of children with a parent node , represented as a set of linked nodes . A tree data structure can be defined recursively as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the \"children\"), with the constraints that no reference is duplicated, and none points to the root. NOTE: \u4e0a\u8ff0\u5b9a\u4e49\u65b9\u6cd5\u91c7\u7528\u7684\u662f Recursive definition \u3002 Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree , with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a set of nodes and an adjacency list of edges between nodes, as one may represent a digraph , for instance). For example, looking at a tree as a whole, one can talk about \"the parent node\" of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any). Preliminary definition A tree is a nonlinear data structure, compared to arrays, linked lists, stacks and queues which are linear data structures. A tree can be empty with no nodes or a tree is a structure consisting of one node called the root and zero or one or more subtrees. NOTE: \u4e0a\u8ff0\u5b9a\u4e49\u65b9\u6cd5\u91c7\u7528\u7684\u662f Recursive definition \u3002 Mathematical definition NOTE: \u8fd9\u4e00\u8282\u662f\u539f\u6587\u4e2d\u6700\u6700\u6666\u6da9\u96be\u61c2\u7684\u7ae0\u8282\u4e86\uff0c\u5b83\u9700\u8981set theory\u7684\u77e5\u8bc6\u4f5c\u4e3a\u57fa\u7840\u3002\u90a3\u8fd9\u5c31 \u5176\u5b9e\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\uff0c\u4f7f\u7528tree\u6765\u8868\u793a\u96c6\u5408\u7684\u5305\u542b\u5173\u7cfb\uff0c\u8fd9\u5c31\u597d\u6bd4\u662f\u62ec\u53f7\u4e86\u3002 Unordered tree Mathematically, an unordered tree [ 1] (or \"algebraic tree\"[ 2] ) can be defined as an algebraic structure $ (X,parent) $ where X is the non-empty carrier set of nodes and parent is a function on X which assigns each node x its \"parent\" node, parent ( x ). The structure is subject to the condition that every non-empty subalgebra must have the same fixed point . That is, there must be a unique \"root\" node r , such that parent ( r ) = r and for every node x , some iterative application parent ( parent (\u2026 parent ( x )\u2026)) equals r . NOTE :\u6570\u5b66\u7684\u5b9a\u4e49\u5f3a\u8c03\u7684\u662f\u6bcf\u4e2anode\u9700\u8981\u6709\u4e00\u4e2a\u4e14\u53ea\u6709\u4e00\u4e2aparent\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u662f\u6570\u5b66\u4e0a\u7684\u5b9a\u4e49\uff0c\u8ba1\u7b97\u673a\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u4e0d\u662f\u5b9a\u4e49parent\uff0c\u800c\u662f\u53cd\u5411\u5b9a\u4e49children\u3002 There are several equivalent definitions. As the closest alternative, one can define unordered trees as partial algebras ( X , parent ) which are obtained from the total algebras described above by letting parent ( r ) be undefined. That is, the root r is the only node on which the parent function is not defined and for every node x , the root is reachable from x in the directed graph ( X , parent ). This definition is in fact coincident with that of an anti-arborescence . The TAoCP book uses the term oriented tree .[ 3] Another equivalent definition is that of a set-theoretic tree that is singly-rooted and whose height is at most \u03c9 (a finite-ish tree[ 4] ). That is, the algebraic structures ( X , parent ) are equivalent to partial orders $ (X,\\leq ) $ that have a top element r and whose every principal upset (aka principal filter ) is a finite chain . To be precise, we should speak about an inverse set-theoretic tree since the set-theoretic definition usually employs opposite ordering. The correspondence between ( X , parent ) and ( X , \u2264) is established via reflexive transitive closure / reduction , with the reduction resulting in the \"partial\" version without the root cycle. We can refer to the four equivalent characterizations as to tree as an algebra , tree as a partial algebra , tree as a partial order , and tree as a prefix order . There is also a fifth equivalent definition \u2013 that of a graph-theoretic rooted tree which is just a connected acyclic rooted graph . Ordered tree The structures introduced in the previous subsection form just the core \"hierarchical\" part of tree data structures that appear in computing. In most cases, there is also an additional \"horizontal\" ordering between siblings. In search trees the order is commonly established by the \"key\" or value associated with each sibling, but in many trees that is not the case. For example, XML documents, lists within JSON files, and many other structures have order that does not depend on the values in the nodes, but is itself data \u2014 sorting the paragraphs of a novel alphabetically would lose information. The correspondent expansion of the previously described tree structures ( X , \u2264) can be defined by endowing each sibling set with a linear order as follows. An alternative definition according to Kuboyama is presented in the next subsection. Terminology used in trees NOTE: \u539f\u6587\u672c\u8282\u63cf\u8ff0tree\u4e2d\u7684\u5404\u79cd\u672f\u8bed\u3002 Representations There are many different ways to represent trees; common representations represent the nodes as dynamically allocated records with pointers to their children, their parents, or both, or as items in an array , with relationships between them determined by their positions in the array (e.g., binary heap ). Indeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp S-expressions , where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child. In general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a threaded binary tree . Common operations Enumerating all the items Enumerating a section of a tree Searching for an item Adding a new item at a certain position on the tree Deleting an item Pruning : Removing a whole section of a tree Grafting : Adding a whole section to a tree Finding the root for any node Finding the lowest common ancestor of two nodes Common uses Representing hierarchical data such as syntax trees Storing data in a way that makes it efficiently searchable (see binary search tree and tree traversal ) Representing sorted lists of data As a workflow for compositing digital images for visual effects Storing Barnes-Hut trees used to simulate galaxies. Recursive definition of tree \u672c\u6587\u4e2d\uff0c\u5173\u4e8e\u6811\u7684Recursive definition\u90fd\u6709\u6807\u6ce8\u51fa\u6765\u4e86\u3002 https://cgi.csc.liv.ac.uk/~michele/TEACHING/COMP102/2006/5.4.pdf http://www.montefiore.ulg.ac.be/~piater/Cours/INFO0902/notes/tree/foil04.xhtml \u6811\u7684\u7ed3\u6784\u662f\u5177\u5907\u9012\u5f52\u6027\u7684\uff1a\u4e00\u4e2a\u8282\u70b9\u7684\u5de6\u8282\u70b9\u53ef\u80fd\u662f\u4e00\u68f5\u6811\uff0c\u53f3\u8282\u70b9\u4e5f\u53ef\u80fd\u662f\u4e00\u68f5\u6811\uff0c\u663e\u7136\u6811\u7684\u5b9a\u4e49\u662f\u7531\u5b83\u81ea\u8eab\u5b9a\u4e49\u7684\uff1b\u6240\u4ee5\u5bf9\u6811\u7684\u64cd\u4f5c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u6811\u7ed3\u6784\u7684\u9012\u5f52\u6027\u800c\u5199\u51fa\u9012\u5f52\u51fd\u6570\uff1b","title":"Tree(data-structure)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#tree#data#structure","text":"In computer science , a tree is a widely used abstract data type (ADT)\u2014or data structure implementing this ADT\u2014that simulates a hierarchical tree structure , with a root value and subtrees of children with a parent node , represented as a set of linked nodes . A tree data structure can be defined recursively as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the \"children\"), with the constraints that no reference is duplicated, and none points to the root. NOTE: \u4e0a\u8ff0\u5b9a\u4e49\u65b9\u6cd5\u91c7\u7528\u7684\u662f Recursive definition \u3002 Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree , with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a set of nodes and an adjacency list of edges between nodes, as one may represent a digraph , for instance). For example, looking at a tree as a whole, one can talk about \"the parent node\" of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any).","title":"Tree (data structure)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#preliminary#definition","text":"A tree is a nonlinear data structure, compared to arrays, linked lists, stacks and queues which are linear data structures. A tree can be empty with no nodes or a tree is a structure consisting of one node called the root and zero or one or more subtrees. NOTE: \u4e0a\u8ff0\u5b9a\u4e49\u65b9\u6cd5\u91c7\u7528\u7684\u662f Recursive definition \u3002","title":"Preliminary definition"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#mathematical#definition","text":"NOTE: \u8fd9\u4e00\u8282\u662f\u539f\u6587\u4e2d\u6700\u6700\u6666\u6da9\u96be\u61c2\u7684\u7ae0\u8282\u4e86\uff0c\u5b83\u9700\u8981set theory\u7684\u77e5\u8bc6\u4f5c\u4e3a\u57fa\u7840\u3002\u90a3\u8fd9\u5c31 \u5176\u5b9e\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\uff0c\u4f7f\u7528tree\u6765\u8868\u793a\u96c6\u5408\u7684\u5305\u542b\u5173\u7cfb\uff0c\u8fd9\u5c31\u597d\u6bd4\u662f\u62ec\u53f7\u4e86\u3002","title":"Mathematical definition"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#unordered#tree","text":"Mathematically, an unordered tree [ 1] (or \"algebraic tree\"[ 2] ) can be defined as an algebraic structure $ (X,parent) $ where X is the non-empty carrier set of nodes and parent is a function on X which assigns each node x its \"parent\" node, parent ( x ). The structure is subject to the condition that every non-empty subalgebra must have the same fixed point . That is, there must be a unique \"root\" node r , such that parent ( r ) = r and for every node x , some iterative application parent ( parent (\u2026 parent ( x )\u2026)) equals r . NOTE :\u6570\u5b66\u7684\u5b9a\u4e49\u5f3a\u8c03\u7684\u662f\u6bcf\u4e2anode\u9700\u8981\u6709\u4e00\u4e2a\u4e14\u53ea\u6709\u4e00\u4e2aparent\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u662f\u6570\u5b66\u4e0a\u7684\u5b9a\u4e49\uff0c\u8ba1\u7b97\u673a\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u4e0d\u662f\u5b9a\u4e49parent\uff0c\u800c\u662f\u53cd\u5411\u5b9a\u4e49children\u3002 There are several equivalent definitions. As the closest alternative, one can define unordered trees as partial algebras ( X , parent ) which are obtained from the total algebras described above by letting parent ( r ) be undefined. That is, the root r is the only node on which the parent function is not defined and for every node x , the root is reachable from x in the directed graph ( X , parent ). This definition is in fact coincident with that of an anti-arborescence . The TAoCP book uses the term oriented tree .[ 3] Another equivalent definition is that of a set-theoretic tree that is singly-rooted and whose height is at most \u03c9 (a finite-ish tree[ 4] ). That is, the algebraic structures ( X , parent ) are equivalent to partial orders $ (X,\\leq ) $ that have a top element r and whose every principal upset (aka principal filter ) is a finite chain . To be precise, we should speak about an inverse set-theoretic tree since the set-theoretic definition usually employs opposite ordering. The correspondence between ( X , parent ) and ( X , \u2264) is established via reflexive transitive closure / reduction , with the reduction resulting in the \"partial\" version without the root cycle. We can refer to the four equivalent characterizations as to tree as an algebra , tree as a partial algebra , tree as a partial order , and tree as a prefix order . There is also a fifth equivalent definition \u2013 that of a graph-theoretic rooted tree which is just a connected acyclic rooted graph .","title":"Unordered tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#ordered#tree","text":"The structures introduced in the previous subsection form just the core \"hierarchical\" part of tree data structures that appear in computing. In most cases, there is also an additional \"horizontal\" ordering between siblings. In search trees the order is commonly established by the \"key\" or value associated with each sibling, but in many trees that is not the case. For example, XML documents, lists within JSON files, and many other structures have order that does not depend on the values in the nodes, but is itself data \u2014 sorting the paragraphs of a novel alphabetically would lose information. The correspondent expansion of the previously described tree structures ( X , \u2264) can be defined by endowing each sibling set with a linear order as follows. An alternative definition according to Kuboyama is presented in the next subsection.","title":"Ordered tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#terminology#used#in#trees","text":"NOTE: \u539f\u6587\u672c\u8282\u63cf\u8ff0tree\u4e2d\u7684\u5404\u79cd\u672f\u8bed\u3002","title":"Terminology used in trees"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#representations","text":"There are many different ways to represent trees; common representations represent the nodes as dynamically allocated records with pointers to their children, their parents, or both, or as items in an array , with relationships between them determined by their positions in the array (e.g., binary heap ). Indeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp S-expressions , where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child. In general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a threaded binary tree .","title":"Representations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#common#operations","text":"Enumerating all the items Enumerating a section of a tree Searching for an item Adding a new item at a certain position on the tree Deleting an item Pruning : Removing a whole section of a tree Grafting : Adding a whole section to a tree Finding the root for any node Finding the lowest common ancestor of two nodes","title":"Common operations"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#common#uses","text":"Representing hierarchical data such as syntax trees Storing data in a way that makes it efficiently searchable (see binary search tree and tree traversal ) Representing sorted lists of data As a workflow for compositing digital images for visual effects Storing Barnes-Hut trees used to simulate galaxies.","title":"Common uses"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree%28data-structure%29/#recursive#definition#of#tree","text":"\u672c\u6587\u4e2d\uff0c\u5173\u4e8e\u6811\u7684Recursive definition\u90fd\u6709\u6807\u6ce8\u51fa\u6765\u4e86\u3002 https://cgi.csc.liv.ac.uk/~michele/TEACHING/COMP102/2006/5.4.pdf http://www.montefiore.ulg.ac.be/~piater/Cours/INFO0902/notes/tree/foil04.xhtml \u6811\u7684\u7ed3\u6784\u662f\u5177\u5907\u9012\u5f52\u6027\u7684\uff1a\u4e00\u4e2a\u8282\u70b9\u7684\u5de6\u8282\u70b9\u53ef\u80fd\u662f\u4e00\u68f5\u6811\uff0c\u53f3\u8282\u70b9\u4e5f\u53ef\u80fd\u662f\u4e00\u68f5\u6811\uff0c\u663e\u7136\u6811\u7684\u5b9a\u4e49\u662f\u7531\u5b83\u81ea\u8eab\u5b9a\u4e49\u7684\uff1b\u6240\u4ee5\u5bf9\u6811\u7684\u64cd\u4f5c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u6811\u7ed3\u6784\u7684\u9012\u5f52\u6027\u800c\u5199\u51fa\u9012\u5f52\u51fd\u6570\uff1b","title":"Recursive definition of tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/VS-of-tree/","text":"B-tree VS self-balancing trees \u5728 Search tree \u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u8bdd\uff1a B-trees are generalizations of binary search trees in that they can have a variable number of subtrees at each node. While child-nodes have a pre-defined range, they will not necessarily be filled with data, meaning B-trees can potentially waste some space. The advantage is that B-trees do not need to be re-balanced as frequently as other self-balancing trees .","title":"[B-tree](https://en.wikipedia.org/wiki/B-tree) VS [self-balancing trees](https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/VS-of-tree/#b-tree#vs#self-balancing#trees","text":"\u5728 Search tree \u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u8bdd\uff1a B-trees are generalizations of binary search trees in that they can have a variable number of subtrees at each node. While child-nodes have a pre-defined range, they will not necessarily be filled with data, meaning B-trees can potentially waste some space. The advantage is that B-trees do not need to be re-balanced as frequently as other self-balancing trees .","title":"B-tree VS self-balancing trees"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/","text":"TODO","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Build-tree/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u6784\u9020tree\u3002\u4e3b\u8981\u63cf\u8ff0\u4e24\u79cd\u5b9e\u73b0\u601d\u8def\uff1a \u81ea\u5e95\u5411\u4e0a\uff08reduce\uff09 \u81ea\u9876\u5411\u4e0b\uff08expand\uff09 \u7406\u89e3\u6811\u6784\u9020\u7b97\u6cd5\u7684\u6700\u4f73\u6848\u4f8b\u5c31\u662fparsing\uff0c\u5728parsing\u4e2d\uff0c\u5206\u522b\u6709\u81ea\u5e95\u5411\u4e0a\u548c\u81ea\u9876\u5411\u4e0b\u4e24\u79cd\u7b56\u7565\u3002 \u5173\u4e8eparsing\uff0c\u53ef\u4ee5\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Parsing \u5de5\u7a0b compiler-principle \u7684 Chapter-4-Syntax-Analysis \u6811\u6784\u9020\u6a21\u578b\u975e\u5e38\u91cd\u8981\uff0c\u5728\u5f88\u591a\u5730\u65b9\u90fd\u6709\u5e7f\u6cdb\u8fd0\u7528\u3002 TODO: \u7ed3\u5408\u6848\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff1a \u81ea\u9876\u5411\u4e0b\u7684\u6848\u4f8b\u5305\u62ec\uff1a \u751f\u6210nav tree \u81ea\u5e95\u5411\u4e0a\u7684\u6848\u4f8b\u5305\u62ec\uff1a shift-reduce parsing","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Build-tree/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u6784\u9020tree\u3002\u4e3b\u8981\u63cf\u8ff0\u4e24\u79cd\u5b9e\u73b0\u601d\u8def\uff1a \u81ea\u5e95\u5411\u4e0a\uff08reduce\uff09 \u81ea\u9876\u5411\u4e0b\uff08expand\uff09 \u7406\u89e3\u6811\u6784\u9020\u7b97\u6cd5\u7684\u6700\u4f73\u6848\u4f8b\u5c31\u662fparsing\uff0c\u5728parsing\u4e2d\uff0c\u5206\u522b\u6709\u81ea\u5e95\u5411\u4e0a\u548c\u81ea\u9876\u5411\u4e0b\u4e24\u79cd\u7b56\u7565\u3002 \u5173\u4e8eparsing\uff0c\u53ef\u4ee5\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Parsing \u5de5\u7a0b compiler-principle \u7684 Chapter-4-Syntax-Analysis \u6811\u6784\u9020\u6a21\u578b\u975e\u5e38\u91cd\u8981\uff0c\u5728\u5f88\u591a\u5730\u65b9\u90fd\u6709\u5e7f\u6cdb\u8fd0\u7528\u3002 TODO: \u7ed3\u5408\u6848\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff1a \u81ea\u9876\u5411\u4e0b\u7684\u6848\u4f8b\u5305\u62ec\uff1a \u751f\u6210nav tree \u81ea\u5e95\u5411\u4e0a\u7684\u6848\u4f8b\u5305\u62ec\uff1a shift-reduce parsing","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0\u5982\u4f55\u6765\u5b9e\u73b0tree\u548c\u5176\u4e00\u7cfb\u5217\u64cd\u4f5c\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u5982\u4f55\u6765\u5b9e\u73b0tree\u548c\u5176\u4e00\u7cfb\u5217\u64cd\u4f5c\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Cpp-library/","text":"Cpp\u5f00\u6e90\u5e93 fbuihuu/ libtree CastXML/CastXML C-family Abstract Syntax Tree XML Output","title":"Cpp\u5f00\u6e90\u5e93"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Cpp-library/#cpp","text":"","title":"Cpp\u5f00\u6e90\u5e93"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Cpp-library/#fbuihuulibtree","text":"","title":"fbuihuu/libtree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Cpp-library/#castxmlcastxml","text":"C-family Abstract Syntax Tree XML Output","title":"CastXML/CastXML"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/","text":"python\u5f00\u6e90\u5e93 treelib \u8bed\u8a00\uff1apython An efficient implementation of tree data structure in python \u2154. http://treelib.readthedocs.io/en/latest/ algorithms \u8bed\u8a00\uff1apython Minimal examples of data structures and algorithms in Python binarytree \u8bed\u8a00\uff1apython Python Library for Studying Binary Trees http://binarytree.readthedocs.io bplustree An on-disk B+tree for Python 3 ete Python package for building, comparing, annotating, manipulating and visualising trees. It provides a comprehensive API and a collection of command line tools, including utilities to work with the NCBI taxonomy tree. http://etetoolkit.org LibCST A concrete syntax tree parser and serializer library for Python that preserves many aspects of Python's abstract syntax tree https://libcst.readthedocs.io/ anytree Python tree data library","title":"python\u5f00\u6e90\u5e93"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/#python","text":"","title":"python\u5f00\u6e90\u5e93"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/#treelib","text":"\u8bed\u8a00\uff1apython An efficient implementation of tree data structure in python \u2154. http://treelib.readthedocs.io/en/latest/","title":"treelib"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/#algorithms","text":"\u8bed\u8a00\uff1apython Minimal examples of data structures and algorithms in Python","title":"algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/#binarytree","text":"\u8bed\u8a00\uff1apython Python Library for Studying Binary Trees http://binarytree.readthedocs.io","title":"binarytree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/#bplustree","text":"An on-disk B+tree for Python 3","title":"bplustree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/#ete","text":"Python package for building, comparing, annotating, manipulating and visualising trees. It provides a comprehensive API and a collection of command line tools, including utilities to work with the NCBI taxonomy tree. http://etetoolkit.org","title":"ete"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/#libcst","text":"A concrete syntax tree parser and serializer library for Python that preserves many aspects of Python's abstract syntax tree https://libcst.readthedocs.io/","title":"LibCST"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/Python-library/#anytree","text":"Python tree data library","title":"anytree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/nltk-tree/","text":"tree Module Unit tests for nltk.tree.Tree","title":"Nltk tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Implementation/nltk-tree/#tree#module","text":"Unit tests for nltk.tree.Tree","title":"tree Module"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/TODO-Tree-traversal/","text":"","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbatree \u548c stack\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u76ee\u524d\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002 \u8349\u7a3f1 \u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793a\u6811\uff0c\u8fd9\u662f\u6e90\u4e8e Newick format and Dyck language \uff0c\u5982\u4e0b\u662f\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\uff1a [[[]]] \u5bf9\u5e94\u7684tree\u5982\u4e0b\uff1a [] / / [] / / [] \u53ef\u4ee5\u770b\u5230\uff0c\u5b83\u5df2\u7ecf\u9000\u5316\u6210\u4e86\u4e00\u4e2alist\uff0c\u8fd9\u79cd\u5c5e\u4e8e\u4f7f\u7528tree\u6765\u63cf\u8ff0\u5177\u6709hierarchy structure\u7684\u6570\u636e\u3002 \u5bf9\u4e8e Dyck language \uff0c\u5728\u5bf9\u5176\u8fdb\u884cparsing\u7684\u65f6\u5019\uff0c\u9700\u8981\u4f7f\u7528stack\uff0c\u5176\u5b9e\u6211\u4eec\u8fdb\u884cparsing\u7684\u65f6\u5019\uff0c\u662f\u6cbf\u7740\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\uff0c\u4e00\u822c\u6211\u4eec\u8fdb\u884cparsing\u65f6\u5019\uff0c\u9047\u5230\u5f00\u62ec\u53f7 [ \u662f\u8981\u7ee7\u7eed\u538b\u6808\uff0c\u5176\u5b9e\u770b\u4e0a\u56fe\u5c31\u53ef\u4ee5\u77e5\u9053\uff0c\u538b\u6808\u5bf9\u5e94\u7740\u662f\u6cbf\u7740\u6811\u8def\u5f84\u5411\u4e0b\uff0c\u5373\u4e0d\u65ad\u5730\u5411\u4e0b\u904d\u5386\u3002\u9047\u5230\u95ed\u62ec\u53f7 ] \uff0c\u5176\u5b9e\u662f\u51fa\u6808\uff0c\u5f00\u59cb\u5411\u4e0a\u4e86\u3002 \u8349\u7a3f2 call stack and stack SUMMARY :\u5173\u4e8e call stack \uff0c\u5728ABI\u4e2d\u5df2\u7ecf\u6536\u5f55\u4e86 \u51fd\u6570\u8c03\u7528 \u5165\u6808 \u51fd\u6570\u8fd4\u56de \u51fa\u6808 parenthesis and stack \u6b63\u62ec\u53f7 \u5165\u6808 \u53cd\u62ec\u53f7 \u51fa\u6808 SUMMARY : stack\u7684\u5165\u6808\u4e0e\u51fa\u6808\u662f\u4e00\u5bf9\u6237\u9006\u7684\u64cd\u4f5c\uff0c\u6240\u4ee5stack\u975e\u5e38\u9002\u5408\u4e8e\u89e3\u51b3\u54ea\u4e9b\u5b58\u5728\u7740\u4e92\u9006\u64cd\u4f5c\u7684\u95ee\u9898\uff1b \u8349\u7a3f3 activation tree\uff0c Parse tree \uff0c\u5b83\u4eec\u90fd\u662f\u5448\u73b0\u7684tree\u7ed3\u6784\uff0c\u4f46\u662f\u51fd\u6570\u7684\u6267\u884c\u4ec5\u4ec5\u9700\u8981\u4e00\u4e2acall stack\uff0cparsing\u7684\u8fc7\u7a0b\u4e5f\u4ec5\u4ec5\u53ea\u9700\u8981\u4e00\u4e2a pushdown automata \uff08\u672c\u8d28\u4e0a\u662f\u4e00\u4e2astack\uff09\uff0c\u4e24\u8005\u5b58\u5728\u7740\u975e\u5e38\u7c7b\u4f3c\u7684\u73b0\u8c61\uff0c\u6211\u4eec\u9700\u8981\u53d6\u601d\u8003\u73b0\u8c61\u80cc\u540e\u6240\u8574\u542b\u7684\u9053\u7406\u3002\u4e24\u4e2a\u8fc7\u7a0b\u90fd\u5177\u6709nesting\u7279\u6027\uff0c\u6240\u4ee5\u5b83\u4eec\u7684\u8fc7\u7a0b\u90fd\u5448\u73b0tree structure\u3002\u5728 4.6 Introduction to LR Parsing: Simple LR \u4e2d\u6211\u5bf9\u6b64\u6709\u8fc7\u5206\u6790\u3002 \u5728 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/#_1","text":"\u672c\u7ae0\u8ba8\u8bbatree \u548c stack\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u76ee\u524d\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/#1","text":"\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793a\u6811\uff0c\u8fd9\u662f\u6e90\u4e8e Newick format and Dyck language \uff0c\u5982\u4e0b\u662f\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\uff1a [[[]]] \u5bf9\u5e94\u7684tree\u5982\u4e0b\uff1a [] / / [] / / [] \u53ef\u4ee5\u770b\u5230\uff0c\u5b83\u5df2\u7ecf\u9000\u5316\u6210\u4e86\u4e00\u4e2alist\uff0c\u8fd9\u79cd\u5c5e\u4e8e\u4f7f\u7528tree\u6765\u63cf\u8ff0\u5177\u6709hierarchy structure\u7684\u6570\u636e\u3002 \u5bf9\u4e8e Dyck language \uff0c\u5728\u5bf9\u5176\u8fdb\u884cparsing\u7684\u65f6\u5019\uff0c\u9700\u8981\u4f7f\u7528stack\uff0c\u5176\u5b9e\u6211\u4eec\u8fdb\u884cparsing\u7684\u65f6\u5019\uff0c\u662f\u6cbf\u7740\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\uff0c\u4e00\u822c\u6211\u4eec\u8fdb\u884cparsing\u65f6\u5019\uff0c\u9047\u5230\u5f00\u62ec\u53f7 [ \u662f\u8981\u7ee7\u7eed\u538b\u6808\uff0c\u5176\u5b9e\u770b\u4e0a\u56fe\u5c31\u53ef\u4ee5\u77e5\u9053\uff0c\u538b\u6808\u5bf9\u5e94\u7740\u662f\u6cbf\u7740\u6811\u8def\u5f84\u5411\u4e0b\uff0c\u5373\u4e0d\u65ad\u5730\u5411\u4e0b\u904d\u5386\u3002\u9047\u5230\u95ed\u62ec\u53f7 ] \uff0c\u5176\u5b9e\u662f\u51fa\u6808\uff0c\u5f00\u59cb\u5411\u4e0a\u4e86\u3002","title":"\u8349\u7a3f1"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/#2","text":"","title":"\u8349\u7a3f2"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/#call#stack#and#stack","text":"SUMMARY :\u5173\u4e8e call stack \uff0c\u5728ABI\u4e2d\u5df2\u7ecf\u6536\u5f55\u4e86 \u51fd\u6570\u8c03\u7528 \u5165\u6808 \u51fd\u6570\u8fd4\u56de \u51fa\u6808","title":"call stack and stack"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/#parenthesis#and#stack","text":"\u6b63\u62ec\u53f7 \u5165\u6808 \u53cd\u62ec\u53f7 \u51fa\u6808 SUMMARY : stack\u7684\u5165\u6808\u4e0e\u51fa\u6808\u662f\u4e00\u5bf9\u6237\u9006\u7684\u64cd\u4f5c\uff0c\u6240\u4ee5stack\u975e\u5e38\u9002\u5408\u4e8e\u89e3\u51b3\u54ea\u4e9b\u5b58\u5728\u7740\u4e92\u9006\u64cd\u4f5c\u7684\u95ee\u9898\uff1b","title":"parenthesis and stack"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/#3","text":"activation tree\uff0c Parse tree \uff0c\u5b83\u4eec\u90fd\u662f\u5448\u73b0\u7684tree\u7ed3\u6784\uff0c\u4f46\u662f\u51fd\u6570\u7684\u6267\u884c\u4ec5\u4ec5\u9700\u8981\u4e00\u4e2acall stack\uff0cparsing\u7684\u8fc7\u7a0b\u4e5f\u4ec5\u4ec5\u53ea\u9700\u8981\u4e00\u4e2a pushdown automata \uff08\u672c\u8d28\u4e0a\u662f\u4e00\u4e2astack\uff09\uff0c\u4e24\u8005\u5b58\u5728\u7740\u975e\u5e38\u7c7b\u4f3c\u7684\u73b0\u8c61\uff0c\u6211\u4eec\u9700\u8981\u53d6\u601d\u8003\u73b0\u8c61\u80cc\u540e\u6240\u8574\u542b\u7684\u9053\u7406\u3002\u4e24\u4e2a\u8fc7\u7a0b\u90fd\u5177\u6709nesting\u7279\u6027\uff0c\u6240\u4ee5\u5b83\u4eec\u7684\u8fc7\u7a0b\u90fd\u5448\u73b0tree structure\u3002\u5728 4.6 Introduction to LR Parsing: Simple LR \u4e2d\u6211\u5bf9\u6b64\u6709\u8fc7\u5206\u6790\u3002 \u5728 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002","title":"\u8349\u7a3f3"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/VS-bracket-VS-tree/","text":"Bracket and tree \u672c\u6587\u76ee\u524d\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002 \u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree \u5df2\u7ecf\u770b\u5230\u4e86\u6709\u4e24\u4e2a\u9879\u76ee\u90fd\u662f\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree\u7684\uff1a nltk\u7684 Noun Phrase Chunking python\u7684 Python Language Services \u4e5f\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree \u603b\u7684\u6765\u8bf4\uff0c\u5728\u5b57\u7b26\u7ec8\u7aef\u4e2d\uff0c\u4f7f\u7528\u62ec\u53f7\u662f\u975e\u5e38\u9002\u5408\u6765\u8868\u793atree\u7684\u3002 Binary tree to string with brackets Construct Binary Tree from String with bracket representation Parentheses Are Trees Parentheses are at the heart of programming. Understand parentheses and you can rule the earth. No, seriously! Parentheses, trees and stacks are all interconnected in a very deep and fundamental way. Back in the early days of programming, long before Google started asking why manhole covers are round, one of the very common programming aptitude tests was to take a long mess of parentheses and ask the candidate if they were valid. Something like: Is: (((((()))((()))))))((())))))) a valid expression? So parentheses are fundamental to programming and being able to handle parentheses is a fundamental programming skill. But why? The simple reason why the humble parenthesis is so great is that it forms a tree structure as soon as you give it a chance. Hence the title - Parentheses Are Trees. The fact that parentheses are trees might not seem obvious so let's see how this works. A single parenthesis is a useless thing. It only comes into its own when paired with a parenthesis of the opposite handedness. The reason that a ( goes with a ) is simply that together they form a container . Put simply you can put something in between the parenthesis - (data) Usually the data that you put in the parenthesis has some sort of structure but this isn't really relevant to the key property of parenthesis. However even at this stage you can see that parenthesis can be used to represent an array: (first) (second) (third) (fourth) What makes parenthesis really useful is that the data that you place between a pair of parenthesis can be another pair of parenthesis. This is always the moment when a simple data structure turns into a complex one - let a one dimensional array element store another array and the result is a two dimensional and more array. Letting a data structure store other data structures is one way of getting brand new things. If you allow a parenthesis pair to store another pair of parenthesis then the result is a tree. For example: (()()) corresponds to a binary tree with two terminal nodes: | /\\ (()()) You can now construct examples as complicated as you care to make them. For example: | / \\ /\\ \\ / \\ \\ / \\ \\ /\\ /\\ \\ (((()())(()()))()) The nesting of parenthesis simply gives the structure of the tree that the parenthesis correspond to. Now you can see why many computer languages use a lot of parenthesis. Probably none more so than the infamous Lisp and at this point who could resist quoting the well known xkcd cartoon: For more xkcd cartoons click here . Lisp can get away with adding very little additional machinery to the parenthesis to create a complete and powerful language. The fact that parenthesis form a tree structure also explains the strange and arbitrary rules that you had to learn in school concerning arithmetic. The rule of arithmetic expressions form a \"little\" language the grammar of which can be expressed as a very simple tree structure with the rule that you always work out the expression by doing a left to right depth first walk. Consider the expression 3+2*5. To make sense of this and evaluate it correctly we have to invoke the idea of operator precedence. In particular we have to say that multiply has a higher precedence than addition so that the expression is 2 times 5 plus 3. However if this expression had been written as ((3)+((2)*(5))) then no operator precedence rules need to be used. The parenthesised expression corresponds to the tree: + / \\ / * / / \\ ((3)+((2)*(5))) NOTE: binary expression tree can also be used to demostrate the relavance between tree and parenthesis. and you can see that walking the tree in depth first either right to left or left to right and performing the operations indicated at each of the nodes gives you the correct evaluation. Here is a depth first left-to-right evaluation: In other words if you are prepared to put all the parenthesis needed into the expression to make the syntax tree of the expression clear and unambiguous you don't need to introduce the idea of operator precedence. Of course we prefer to leave out parenthesis and complicate things by claiming that multiplication has to be done before addition. In fact we leave out so many parenthesis that we have to use the \"My Dear Aunt Sally\" rule - i.e. Multiplication and Division before Addition and Subtraction. Perhaps the biggest use of parenthesis today is in the form of all those markup languages that generate hierarchies i.e. trees of visual objects. For example what is HTML other than a bracketing syntax - <open tab>content</close tag> Just think of each open tag as a \"(\" and each closing tag as a \")\". The same is true of XAML and all the other object instantiation languages. They all create tree structures. In a more general context XML is a bracketing system that generates general tree structures consisting of arbitrary data. For another example consider the nesting of control structures such as for , if , do while and so on. These too are a bracketing language, often using curly brackets {} , and they generate a tree structure which the compiler has to work out to successfully parse the language. Once you notice the way bracketing generates hierarchies and general tree structures you start noticing them more or less everywhere. Perhaps now you will agree that parenthesis are fundamental to programming and testing the ability to work with them is probably a good way to see if some one is going to make the grade as a programmer. We have one question remaining - what arrangements of parenthesis are legal? The simple answer is only those arrangements that correspond to complete tree structures and there are only two ways in which a set of parenthesis can fail to do so. The first is just not having the same number of opening and closing parenthesis. You can't have half a container and so all valid bracketing structures have to match numbers of opening and closing parenthesis. This is a minimal condition for legal parenthesis. The second condition is that the pairs of parenthesis always occur in the right order - that is you always have () and never )(. Put as simply as this you would think that this condition is trivial but it is very easy to hide a pair of parenthesis in the wrong order. For example, ())(()() This structure is clearly wrong but there is no single answer to exactly what is wrong with it. For example you might say that it corresponds to any of the following groupings: () )( ()() () )( ( )( ) ( ))(()( ) In other words there is no single correct way to parse an incorrect structure. So how do we check for a valid bracketing structure? There is more than one answer to this but the simplest is to see if it is possible to walk the tree that the parenthesis describe. To do this you need a stack . All you have to do is scan the expression from left-to-right. Each time you encounter an opening parenthesis push it on the stack. Each time you encounter a closing parenthesis pop the top of the stack - if there is nothing on the top of the stack to pop then you have an invalid set of parenthesis. If you reach the end of the scan without trying to pop an empty stack then the stack will be empty if the expression was valid. For example in the case of: ((()())) the stack contents are: stack remainder 0 . ((()())). 1 .( (()())). 2 .(( ()())). 3 .((( )())). 4 .(( ())). 5 .((( ))). 6 .(( )). 7 .( ). 8 . . As the stack is empty when the scan is complete the expression was valid. The real question is can you see that this algorithm works? The answer is that the stack algorithm performs a depth-first left-to-right tree walk and this can only be completed if the parenthesis really do specify a tree. Notice that we are only considering parenthesis that are unlabeled - that is any closing parenthesis will pair with any opening parenthesis . Things become a little more difficult if you allow named parenthesis as in HTML, XML or program language structures. Then there are other ways to get things wrong such as <div><span></div></span> . You can easily modify the stack algorithm to detect such errors - all you have to do is make sure that the popped parenthesis matches the closing parenthesis that caused the pop operation. Parenthesis , stacks and trees go together perfectly. The final word however should go to xkcd and the observation in its blog of a halcyon time when Wikipedia had a sense of humor. The blog quotes from the Wikipedia article on parenthesis: Parentheses may also be nested (with one set (such as this) inside another set). This is not commonly used in formal writing [though sometimes other brackets (especially parentheses) will be used for one or more inner set of parentheses (in other words, secondary {or even tertiary } phrases can be found within the main sentence)] Sadly the Wikipedia entry no longer contains this paragraph... Yes parenthesises are fundamental to programming so much so that some programmers can spot a malformed structure instinctively. It is as if their eyes had a developed a co-processor for walking the tree structure. This perhaps the source for the final final word from parenthesis obsessed xkcd: ![ ( ]( https://imgs.xkcd.com/comics/(.png ) Only if you are a programmer... \u5b8c\u5168\u52a0\u62ec\u53f7\u95ee\u9898\u5176\u5b9e\u662f\u53ef\u4ee5\u4f7f\u7528\u6811\u6765\u8fdb\u884c\u8868\u793a\u7684 Bracket (tournament) A bracket or tournament bracket is a tree diagram that represents the series of games played during a knockout tournament . Different knockout tournament formats have different brackets; the simplest and most common is that of the single-elimination tournament . The name \"bracket\" is American English , derived from the resemblance of the links in the tree diagram to the bracket punctuation symbol ] or [ (called a \"square bracket\" in British English ). The closest British term is draw , although this implies an element of chance, whereas some brackets are determined entirely by seeding . Abstract syntax tree and bracket For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches. Reverse Polish Notation and bracket","title":"Bracket and tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/VS-bracket-VS-tree/#bracket#and#tree","text":"\u672c\u6587\u76ee\u524d\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002","title":"Bracket and tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/VS-bracket-VS-tree/#tree","text":"\u5df2\u7ecf\u770b\u5230\u4e86\u6709\u4e24\u4e2a\u9879\u76ee\u90fd\u662f\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree\u7684\uff1a nltk\u7684 Noun Phrase Chunking python\u7684 Python Language Services \u4e5f\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree \u603b\u7684\u6765\u8bf4\uff0c\u5728\u5b57\u7b26\u7ec8\u7aef\u4e2d\uff0c\u4f7f\u7528\u62ec\u53f7\u662f\u975e\u5e38\u9002\u5408\u6765\u8868\u793atree\u7684\u3002","title":"\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793atree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/VS-bracket-VS-tree/#binary#tree#to#string#with#brackets","text":"","title":"Binary tree to string with brackets"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/VS-bracket-VS-tree/#construct#binary#tree#from#string#with#bracket#representation","text":"","title":"Construct Binary Tree from String with bracket representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/VS-bracket-VS-tree/#parentheses#are#trees","text":"Parentheses are at the heart of programming. Understand parentheses and you can rule the earth. No, seriously! Parentheses, trees and stacks are all interconnected in a very deep and fundamental way. Back in the early days of programming, long before Google started asking why manhole covers are round, one of the very common programming aptitude tests was to take a long mess of parentheses and ask the candidate if they were valid. Something like: Is: (((((()))((()))))))((())))))) a valid expression? So parentheses are fundamental to programming and being able to handle parentheses is a fundamental programming skill. But why? The simple reason why the humble parenthesis is so great is that it forms a tree structure as soon as you give it a chance. Hence the title - Parentheses Are Trees. The fact that parentheses are trees might not seem obvious so let's see how this works. A single parenthesis is a useless thing. It only comes into its own when paired with a parenthesis of the opposite handedness. The reason that a ( goes with a ) is simply that together they form a container . Put simply you can put something in between the parenthesis - (data) Usually the data that you put in the parenthesis has some sort of structure but this isn't really relevant to the key property of parenthesis. However even at this stage you can see that parenthesis can be used to represent an array: (first) (second) (third) (fourth) What makes parenthesis really useful is that the data that you place between a pair of parenthesis can be another pair of parenthesis. This is always the moment when a simple data structure turns into a complex one - let a one dimensional array element store another array and the result is a two dimensional and more array. Letting a data structure store other data structures is one way of getting brand new things. If you allow a parenthesis pair to store another pair of parenthesis then the result is a tree. For example: (()()) corresponds to a binary tree with two terminal nodes: | /\\ (()()) You can now construct examples as complicated as you care to make them. For example: | / \\ /\\ \\ / \\ \\ / \\ \\ /\\ /\\ \\ (((()())(()()))()) The nesting of parenthesis simply gives the structure of the tree that the parenthesis correspond to. Now you can see why many computer languages use a lot of parenthesis. Probably none more so than the infamous Lisp and at this point who could resist quoting the well known xkcd cartoon: For more xkcd cartoons click here . Lisp can get away with adding very little additional machinery to the parenthesis to create a complete and powerful language. The fact that parenthesis form a tree structure also explains the strange and arbitrary rules that you had to learn in school concerning arithmetic. The rule of arithmetic expressions form a \"little\" language the grammar of which can be expressed as a very simple tree structure with the rule that you always work out the expression by doing a left to right depth first walk. Consider the expression 3+2*5. To make sense of this and evaluate it correctly we have to invoke the idea of operator precedence. In particular we have to say that multiply has a higher precedence than addition so that the expression is 2 times 5 plus 3. However if this expression had been written as ((3)+((2)*(5))) then no operator precedence rules need to be used. The parenthesised expression corresponds to the tree: + / \\ / * / / \\ ((3)+((2)*(5))) NOTE: binary expression tree can also be used to demostrate the relavance between tree and parenthesis. and you can see that walking the tree in depth first either right to left or left to right and performing the operations indicated at each of the nodes gives you the correct evaluation. Here is a depth first left-to-right evaluation: In other words if you are prepared to put all the parenthesis needed into the expression to make the syntax tree of the expression clear and unambiguous you don't need to introduce the idea of operator precedence. Of course we prefer to leave out parenthesis and complicate things by claiming that multiplication has to be done before addition. In fact we leave out so many parenthesis that we have to use the \"My Dear Aunt Sally\" rule - i.e. Multiplication and Division before Addition and Subtraction. Perhaps the biggest use of parenthesis today is in the form of all those markup languages that generate hierarchies i.e. trees of visual objects. For example what is HTML other than a bracketing syntax - <open tab>content</close tag> Just think of each open tag as a \"(\" and each closing tag as a \")\". The same is true of XAML and all the other object instantiation languages. They all create tree structures. In a more general context XML is a bracketing system that generates general tree structures consisting of arbitrary data. For another example consider the nesting of control structures such as for , if , do while and so on. These too are a bracketing language, often using curly brackets {} , and they generate a tree structure which the compiler has to work out to successfully parse the language. Once you notice the way bracketing generates hierarchies and general tree structures you start noticing them more or less everywhere. Perhaps now you will agree that parenthesis are fundamental to programming and testing the ability to work with them is probably a good way to see if some one is going to make the grade as a programmer. We have one question remaining - what arrangements of parenthesis are legal? The simple answer is only those arrangements that correspond to complete tree structures and there are only two ways in which a set of parenthesis can fail to do so. The first is just not having the same number of opening and closing parenthesis. You can't have half a container and so all valid bracketing structures have to match numbers of opening and closing parenthesis. This is a minimal condition for legal parenthesis. The second condition is that the pairs of parenthesis always occur in the right order - that is you always have () and never )(. Put as simply as this you would think that this condition is trivial but it is very easy to hide a pair of parenthesis in the wrong order. For example, ())(()() This structure is clearly wrong but there is no single answer to exactly what is wrong with it. For example you might say that it corresponds to any of the following groupings: () )( ()() () )( ( )( ) ( ))(()( ) In other words there is no single correct way to parse an incorrect structure. So how do we check for a valid bracketing structure? There is more than one answer to this but the simplest is to see if it is possible to walk the tree that the parenthesis describe. To do this you need a stack . All you have to do is scan the expression from left-to-right. Each time you encounter an opening parenthesis push it on the stack. Each time you encounter a closing parenthesis pop the top of the stack - if there is nothing on the top of the stack to pop then you have an invalid set of parenthesis. If you reach the end of the scan without trying to pop an empty stack then the stack will be empty if the expression was valid. For example in the case of: ((()())) the stack contents are: stack remainder 0 . ((()())). 1 .( (()())). 2 .(( ()())). 3 .((( )())). 4 .(( ())). 5 .((( ))). 6 .(( )). 7 .( ). 8 . . As the stack is empty when the scan is complete the expression was valid. The real question is can you see that this algorithm works? The answer is that the stack algorithm performs a depth-first left-to-right tree walk and this can only be completed if the parenthesis really do specify a tree. Notice that we are only considering parenthesis that are unlabeled - that is any closing parenthesis will pair with any opening parenthesis . Things become a little more difficult if you allow named parenthesis as in HTML, XML or program language structures. Then there are other ways to get things wrong such as <div><span></div></span> . You can easily modify the stack algorithm to detect such errors - all you have to do is make sure that the popped parenthesis matches the closing parenthesis that caused the pop operation. Parenthesis , stacks and trees go together perfectly. The final word however should go to xkcd and the observation in its blog of a halcyon time when Wikipedia had a sense of humor. The blog quotes from the Wikipedia article on parenthesis: Parentheses may also be nested (with one set (such as this) inside another set). This is not commonly used in formal writing [though sometimes other brackets (especially parentheses) will be used for one or more inner set of parentheses (in other words, secondary {or even tertiary } phrases can be found within the main sentence)] Sadly the Wikipedia entry no longer contains this paragraph... Yes parenthesises are fundamental to programming so much so that some programmers can spot a malformed structure instinctively. It is as if their eyes had a developed a co-processor for walking the tree structure. This perhaps the source for the final final word from parenthesis obsessed xkcd: ![ ( ]( https://imgs.xkcd.com/comics/(.png ) Only if you are a programmer... \u5b8c\u5168\u52a0\u62ec\u53f7\u95ee\u9898\u5176\u5b9e\u662f\u53ef\u4ee5\u4f7f\u7528\u6811\u6765\u8fdb\u884c\u8868\u793a\u7684","title":"Parentheses Are Trees"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/VS-bracket-VS-tree/#bracket#tournament","text":"A bracket or tournament bracket is a tree diagram that represents the series of games played during a knockout tournament . Different knockout tournament formats have different brackets; the simplest and most common is that of the single-elimination tournament . The name \"bracket\" is American English , derived from the resemblance of the links in the tree diagram to the bracket punctuation symbol ] or [ (called a \"square bracket\" in British English ). The closest British term is draw , although this implies an element of chance, whereas some brackets are determined entirely by seeding .","title":"Bracket (tournament)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-operation/Tree-and-stack/VS-bracket-VS-tree/#abstract#syntax#tree#and#bracket","text":"For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches. Reverse Polish Notation and bracket","title":"Abstract syntax tree and bracket"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/","text":"Examples of tree structures \u5728\u4e0a\u4e00\u7bc7\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\uff0c\u5177\u5907nesting\u5173\u7cfb\u7684\u7ed3\u6784\u90fd\u53ef\u4ee5\u8868\u793a\u6210tree structure\uff0c\u672c\u6587\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684tree structure\u8fdb\u884c\u679a\u4e3e\u3002 Directory structure ( directory ) nesting\u5173\u7cfb\u3002 See also: Tree (command) Path (computing) Process tree parent-children\u5173\u7cfb\u3002 File format nesting\u5173\u7cfb\u3002 Document Object Model \uff08 XML \uff09 json yaml Namespace nesting\u5173\u7cfb\u3002 Namespace\u7684\u5e94\u7528\u573a\u666f\u5b9e\u5728\u592a\u591a\uff0c\u5728\u7ef4\u57fa\u767e\u79d1\u7684 Namespace \u5bf9\u5b83\u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002\u5728\u5bf9\u5b83\u8fdb\u884c\u601d\u8003\u7684\u65f6\u5019\uff0c\u53d1\u89c9\u4f7f\u7528namespace\u6765\u7ec4\u7ec7\u7684\u6570\u636e\u6700\u7ec8\u5c31\u662fhierarchy\u7ed3\u6784\u3002\u5176\u5b9e\u4e5f\u53ef\u4ee5\u7b80\u5355\u5730\u5c06namespace\u770b\u505a\u662f\u62ec\u53f7\u3002 Expression binary expression tree Source code Parse tree \u3001 Abstract syntax tree \u3002 nesting\u5173\u7cfb\u3002 Activation tree nesting\u5173\u7cfb\u3002 \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u4e5f\u662f\u53ef\u4ee5\u4f7f\u7528tree\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u53c2\u89c1 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u3002 Linguistics \u5728\u8bed\u8a00\u5b66\u4e2d\uff0c\u57fa\u672c\u4e0a\u662f\u4f7f\u7528tree\u6765\u63cf\u8ff0\u8bed\u8a00\u7684\u7ed3\u6784\u3002 If you have read book describing the compiler technology , for example the classic definitive book Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman, you will be aware how important the tree structure is to the compiler. As described in chapter 2.2.3 Parse Trees: Tree data structures figure prominently in compiling. There are some many tree in the book, such as Parse tree , abstract syntax tree , activation tree(in chapter 7.2.1 Activation Trees), expression tree . Essentially Speaking, a programming language is a formal language , what have been concluded in article structure is that Tree can be used to describe the structure of sentences that is syntax and formal grammar can be convert to tree . So it's natural to use trees in the compiling. Regular expression , algebraic expression can be described using formal grammar , so given an expression, it can be converted to an equivalent parse tree . Nesting\u7ed3\u6784\u7684\u4e00\u4e9b\u4f8b\u5b50 Nested function Scope (computer science) \u4e0enesting\u7d27\u5bc6\u76f8\u5173\u7684\u8bcd\u6709\uff1a enclosing\uff08\u53c2\u89c1\u4e0b\u9762\u7684enclosing function\uff09\u3001 Closure (computer programming) block Nested function and enclosing function Counting problems can be solved using tree diagrams \u5728 Discrete Mathematics and Its Applications \u7684 6.1 The Basics of Counting \u4e2d\u4f7f\u7528tree diagram\u6765\u63cf\u8ff0**Counting problems**\uff0c\u8fd9\u662f\u4e00\u79cd\u5178\u578b\u7684\u903b\u8f91\u7ed3\u6784\uff08\u5e76\u975e\u8089\u773c\u53ef\u89c1\u7684\u6811\u7ed3\u6784\uff09\u3002","title":"Examples-of-tree-structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#examples#of#tree#structures","text":"\u5728\u4e0a\u4e00\u7bc7\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\uff0c\u5177\u5907nesting\u5173\u7cfb\u7684\u7ed3\u6784\u90fd\u53ef\u4ee5\u8868\u793a\u6210tree structure\uff0c\u672c\u6587\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684tree structure\u8fdb\u884c\u679a\u4e3e\u3002","title":"Examples of tree structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#directory#structure#directory","text":"nesting\u5173\u7cfb\u3002 See also: Tree (command) Path (computing)","title":"Directory structure (directory)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#process#tree","text":"parent-children\u5173\u7cfb\u3002","title":"Process tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#file#format","text":"nesting\u5173\u7cfb\u3002 Document Object Model \uff08 XML \uff09 json yaml","title":"File format"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#namespace","text":"nesting\u5173\u7cfb\u3002 Namespace\u7684\u5e94\u7528\u573a\u666f\u5b9e\u5728\u592a\u591a\uff0c\u5728\u7ef4\u57fa\u767e\u79d1\u7684 Namespace \u5bf9\u5b83\u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002\u5728\u5bf9\u5b83\u8fdb\u884c\u601d\u8003\u7684\u65f6\u5019\uff0c\u53d1\u89c9\u4f7f\u7528namespace\u6765\u7ec4\u7ec7\u7684\u6570\u636e\u6700\u7ec8\u5c31\u662fhierarchy\u7ed3\u6784\u3002\u5176\u5b9e\u4e5f\u53ef\u4ee5\u7b80\u5355\u5730\u5c06namespace\u770b\u505a\u662f\u62ec\u53f7\u3002","title":"Namespace"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#expression","text":"binary expression tree","title":"Expression"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#source#code","text":"Parse tree \u3001 Abstract syntax tree \u3002 nesting\u5173\u7cfb\u3002","title":"Source code"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#activation#tree","text":"nesting\u5173\u7cfb\u3002 \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u4e5f\u662f\u53ef\u4ee5\u4f7f\u7528tree\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u53c2\u89c1 Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u3002","title":"Activation tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#linguistics","text":"\u5728\u8bed\u8a00\u5b66\u4e2d\uff0c\u57fa\u672c\u4e0a\u662f\u4f7f\u7528tree\u6765\u63cf\u8ff0\u8bed\u8a00\u7684\u7ed3\u6784\u3002 If you have read book describing the compiler technology , for example the classic definitive book Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman, you will be aware how important the tree structure is to the compiler. As described in chapter 2.2.3 Parse Trees: Tree data structures figure prominently in compiling. There are some many tree in the book, such as Parse tree , abstract syntax tree , activation tree(in chapter 7.2.1 Activation Trees), expression tree . Essentially Speaking, a programming language is a formal language , what have been concluded in article structure is that Tree can be used to describe the structure of sentences that is syntax and formal grammar can be convert to tree . So it's natural to use trees in the compiling. Regular expression , algebraic expression can be described using formal grammar , so given an expression, it can be converted to an equivalent parse tree .","title":"Linguistics"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#nesting","text":"Nested function Scope (computer science) \u4e0enesting\u7d27\u5bc6\u76f8\u5173\u7684\u8bcd\u6709\uff1a enclosing\uff08\u53c2\u89c1\u4e0b\u9762\u7684enclosing function\uff09\u3001 Closure (computer programming) block Nested function and enclosing function","title":"Nesting\u7ed3\u6784\u7684\u4e00\u4e9b\u4f8b\u5b50"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Examples-of-tree-structures/#counting#problems#can#be#solved#using#tree#diagrams","text":"\u5728 Discrete Mathematics and Its Applications \u7684 6.1 The Basics of Counting \u4e2d\u4f7f\u7528tree diagram\u6765\u63cf\u8ff0**Counting problems**\uff0c\u8fd9\u662f\u4e00\u79cd\u5178\u578b\u7684\u903b\u8f91\u7ed3\u6784\uff08\u5e76\u975e\u8089\u773c\u53ef\u89c1\u7684\u6811\u7ed3\u6784\uff09\u3002","title":"Counting problems can be solved using tree diagrams"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/","text":"Represent tree Visually Representing Trees There are many ways of visually representing tree structures. Almost always, these boil down to variations, or combinations, of a few basic styles: Classical node-link diagrams Classical node-link diagrams, that connect nodes together with line segments: encyclopedia / \\ culture science / \\ art craft Nested sets Nested sets that use enclosure/containment to show parenthood, examples include TreeMaps and fractal maps : Layered \"icicle\" diagrams Layered \"icicle\" diagrams that use alignment/adjacency. Outlines and tree views Lists or diagrams that use indentation, sometimes called \" outlines \" or \" tree views \". A tree view: encyclopedia culture art craft science Nested parentheses See also: Newick format and Dyck language A correspondence to nested parentheses was first noticed by Sir Arthur Cayley : ((art,craft)culture,science)encyclopedia or encyclopedia(culture(art,craft),science) Radial trees See also: Radial tree Trees can also be represented radially : art craft \\ / culture | encyclopedia | science Note: The following content comes from the the dragon book 6.2 Three-Address Code: Three-address code is a linearized representation of a syntax tree or a DAG in which explicit names correspond to the interior nodes of the graph. Three-address code Reverse Polish notation Representing Tree in Memory/Implementation The following is a partial enumeration: represent the nodes as dynamically allocated records with pointers to their children, their parents, or both represent the nodes as items in an array , with relationships between them determined by their positions in the array (e.g., binary heap ). For a more complete enumeration, click the link above.","title":"Represent-tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#represent#tree","text":"","title":"Represent tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#visually#representing#trees","text":"There are many ways of visually representing tree structures. Almost always, these boil down to variations, or combinations, of a few basic styles:","title":"Visually  Representing Trees"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#classical#node-link#diagrams","text":"Classical node-link diagrams, that connect nodes together with line segments: encyclopedia / \\ culture science / \\ art craft","title":"Classical node-link diagrams"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#nested#sets","text":"Nested sets that use enclosure/containment to show parenthood, examples include TreeMaps and fractal maps :","title":"Nested sets"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#layered#icicle#diagrams","text":"Layered \"icicle\" diagrams that use alignment/adjacency.","title":"Layered \"icicle\" diagrams"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#outlines#and#tree#views","text":"Lists or diagrams that use indentation, sometimes called \" outlines \" or \" tree views \". A tree view: encyclopedia culture art craft science","title":"Outlines and tree views"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#nested#parentheses","text":"See also: Newick format and Dyck language A correspondence to nested parentheses was first noticed by Sir Arthur Cayley : ((art,craft)culture,science)encyclopedia or encyclopedia(culture(art,craft),science)","title":"Nested parentheses"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#radial#trees","text":"See also: Radial tree Trees can also be represented radially : art craft \\ / culture | encyclopedia | science Note: The following content comes from the the dragon book 6.2 Three-Address Code: Three-address code is a linearized representation of a syntax tree or a DAG in which explicit names correspond to the interior nodes of the graph.","title":"Radial trees"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#three-address#code","text":"","title":"Three-address code"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#reverse#polish#notation","text":"","title":"Reverse Polish notation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Represent-tree/#representing#tree#in#memoryimplementation","text":"The following is a partial enumeration: represent the nodes as dynamically allocated records with pointers to their children, their parents, or both represent the nodes as items in an array , with relationships between them determined by their positions in the array (e.g., binary heap ). For a more complete enumeration, click the link above.","title":"Representing Tree in Memory/Implementation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree%28set-theory%29/","text":"Tree (set theory) In set theory , a tree is a partially ordered set ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. Frequently trees are assumed to have only one root (i.e. minimal element ), as the typical questions investigated in this field are easily reduced to questions about single-rooted trees. Definition A tree is a partially ordered set (poset) ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. In particular, each well-ordered set ( T , <) is a tree. For each t \u2208 T , the order type of { s \u2208 T : s < t } is called the height of t (denoted ht( t , T )). The height of T itself is the least ordinal greater than the height of each element of T . A root of a tree T is an element of height 0. Frequently trees are assumed to have only one root. Note that trees in set theory are often defined to grow downward making the root the greatest node. Trees with a single root may be viewed as rooted trees in the sense of graph theory in one of two ways: either as a tree (graph theory) or as a trivially perfect graph . In the first case, the graph is the undirected Hasse Diagram of the partially ordered set, and in the second case, the graph is simply the underlying (undirected) graph of the partially ordered set. However, if T is a tree of height > \u03c9, then the Hasse diagram definition does not work. For example, the partially ordered set $ \\omega +1=\\left{0,1,2,\\dots ,\\omega \\right} $ does not have a Hasse Diagram, as there is no predecessor to \u03c9. Hence we require height at most omega in this case. A branch of a tree is a maximal chain in the tree (that is, any two elements of the branch are comparable, and any element of the tree not in the branch is incomparable with at least one element of the branch). The length of a branch is the ordinal that is order isomorphic to the branch. For each ordinal \u03b1, the \u03b1-th level of T is the set of all elements of T of height \u03b1. A tree is a \u03ba-tree, for an ordinal number \u03ba, if and only if it has height \u03ba and every level has size less than the cardinality of \u03ba. The width of a tree is the supremum of the cardinalities of its levels. Any single-rooted tree of height $ \\leq \\omega $ forms a meet-semilattice, where meet (common ancestor) is given by maximal element of intersection of ancestors, which exists as the set of ancestors is non-empty and finite well-ordered, hence has a maximal element. Without a single root, the intersection of parents can be empty (two elements need not have common ancestors), for example $ \\left{a,b\\right} $ where the elements are not comparable; while if there are an infinite number of ancestors there need not be a maximal element \u2013 for example, $ \\left{0,1,2,\\dots ,\\omega _{0},\\omega _{0}'\\right} $ where $ \\omega _{0},\\omega _{0}' $ are not comparable. A subtree of a tree $ (T,<) $ is a tree $ (T',<) $ where $ T'\\subseteq T $ and $ T' $ is downward closed under $ < $, i.e., if $ s,t\\in T $ and $ s<t $ then $ t\\in T'\\implies s\\in T' $.","title":"Tree(set-theory)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree%28set-theory%29/#tree#set#theory","text":"In set theory , a tree is a partially ordered set ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. Frequently trees are assumed to have only one root (i.e. minimal element ), as the typical questions investigated in this field are easily reduced to questions about single-rooted trees.","title":"Tree (set theory)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree%28set-theory%29/#definition","text":"A tree is a partially ordered set (poset) ( T , <) such that for each t \u2208 T , the set { s \u2208 T : s < t } is well-ordered by the relation <. In particular, each well-ordered set ( T , <) is a tree. For each t \u2208 T , the order type of { s \u2208 T : s < t } is called the height of t (denoted ht( t , T )). The height of T itself is the least ordinal greater than the height of each element of T . A root of a tree T is an element of height 0. Frequently trees are assumed to have only one root. Note that trees in set theory are often defined to grow downward making the root the greatest node. Trees with a single root may be viewed as rooted trees in the sense of graph theory in one of two ways: either as a tree (graph theory) or as a trivially perfect graph . In the first case, the graph is the undirected Hasse Diagram of the partially ordered set, and in the second case, the graph is simply the underlying (undirected) graph of the partially ordered set. However, if T is a tree of height > \u03c9, then the Hasse diagram definition does not work. For example, the partially ordered set $ \\omega +1=\\left{0,1,2,\\dots ,\\omega \\right} $ does not have a Hasse Diagram, as there is no predecessor to \u03c9. Hence we require height at most omega in this case. A branch of a tree is a maximal chain in the tree (that is, any two elements of the branch are comparable, and any element of the tree not in the branch is incomparable with at least one element of the branch). The length of a branch is the ordinal that is order isomorphic to the branch. For each ordinal \u03b1, the \u03b1-th level of T is the set of all elements of T of height \u03b1. A tree is a \u03ba-tree, for an ordinal number \u03ba, if and only if it has height \u03ba and every level has size less than the cardinality of \u03ba. The width of a tree is the supremum of the cardinalities of its levels. Any single-rooted tree of height $ \\leq \\omega $ forms a meet-semilattice, where meet (common ancestor) is given by maximal element of intersection of ancestors, which exists as the set of ancestors is non-empty and finite well-ordered, hence has a maximal element. Without a single root, the intersection of parents can be empty (two elements need not have common ancestors), for example $ \\left{a,b\\right} $ where the elements are not comparable; while if there are an infinite number of ancestors there need not be a maximal element \u2013 for example, $ \\left{0,1,2,\\dots ,\\omega _{0},\\omega _{0}'\\right} $ where $ \\omega _{0},\\omega _{0}' $ are not comparable. A subtree of a tree $ (T,<) $ is a tree $ (T',<) $ where $ T'\\subseteq T $ and $ T' $ is downward closed under $ < $, i.e., if $ s,t\\in T $ and $ s<t $ then $ t\\in T'\\implies s\\in T' $.","title":"Definition"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/","text":"Tree structure \u672c\u6587\u6240\u8981\u8ba8\u8bba\u7684\u662f\u201c\u6811\u5f62\u5f62\u72b6\u201d\uff0c\u6807\u9898\u4e2d\u7684\u201cstructure\u201d\u6240\u8981\u8868\u8fbe\u7684\u542b\u4e49\u662f\u201c\u5f62\u72b6\u201d\u3002 \u672c\u6587\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1 Tree structure \u3002 A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. It is named a \"tree structure\" because the classic representation resembles a tree . \u5728 Hierarchy \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86**tree structure**\u5bf9\u5e94\u7684\u662f nested hierarchy structure\u3002 tree structure \u7684\u6700\u6700\u5178\u578b\u7684\u7279\u6027\u662f\uff1a \u4e00\u4e2a\u8282\u70b9\u53ef\u4ee5\u6709\uff08\u5305\u542b\uff09\u591a\u4e2a\u5b50\u8282\u70b9 \u4e00\u4e2a\u5b50\u8282\u70b9\u53ea\u80fd\u591f\u6709\u4e00\u4e2a\u7236\u8282\u70b9 root\u8282\u70b9\u6ca1\u6709\u7236\u8282\u70b9\u3002 tree structure \u7684 \u201c\u4e00\u4e2a\u5b50\u8282\u70b9\u53ea\u80fd\u591f\u6709\u4e00\u4e2a\u7236\u8282\u70b9\u201d \u7684\u8981\u6c42\uff0c\u5c06\u5b83\u548cgraph\u533a\u5206\u5f00\u6765\u4e86\uff08\u53c2\u89c1 Discrete Mathematics and Its Applications \u7684Tree\u7ae0\u8282\uff09\u3002\u5728\u672c\u6587\u7684\u540e\u9762\u6211\u4eec\u4f7f\u7528\u201c nesting \u201d\u8fd9\u4e2a\u8bcd\u6765\u63cf\u8ff0**tree structure**\u7684\u8fd9\u4e2a\u7279\u6027\u3002\u201c nesting \u201d\u7684\u4e2d\u6587\u542b\u4e49\u662f\u201c\u5d4c\u5957\u201d\uff0c\u663e\u7136\uff0c\u5b83\u80fd\u591f\u63cf\u8ff0\u5143\u7d20\u4e4b\u95f4\u7684\u5d4c\u5957\u5173\u7cfb\uff1b\u8fd9\u4e2a\u8bcd\u7684\u542b\u4e49\u662f\u4e30\u5bcc\u7684\uff0c\u5b83\u7684\u8868\u9762\u610f\u601d\u662f\u201c\u5d4c\u5957\u201d\uff0c\u201c\u5d4c\u5957\u201d\u8574\u542b\u7740\u201c\u5305\u542b\u201d\u7684\u542b\u4e49\uff0c\u201c\u5d4c\u5957\u201d\u8574\u542b\u7740\u201c\u9012\u5f52\u201d\u7684\u542b\u4e49\uff1b\u663e\u7136\u5177\u5907nesting\u7279\u6027\uff0c\u5c31\u5177\u5907\u4e86\u5982\u4e0b\u7279\u6027\uff1a hierarchical \uff0c\u5373\u6811\u7ed3\u6784\u662f\u5c42\u6b21\u7684 recursive \uff0cnesting\u7684\u9012\u5f52\u6027\uff1a\u6211\u89c9\u5f97nesting\u7684\u9012\u5f52\u6027\u53ef\u4ee5\u4f7f\u7528\u5173\u7cfb\u7684transitive\u7279\u6027\u6765\u8fdb\u884c\u89e3\u91ca\u3002\u6bd4\u5982 recursive scope rules \uff0c\u8fd9\u79cd\u5305\u542b\u5173\u7cfb\u4e00\u79cdtransitive relation\u3002\u4e0enesting\u76f8\u5173\u7684\u53e6\u5916\u4e00\u4e2a\u8bcd\u662f\uff1alevel\uff0c\u5176\u5b9e\u5b83\u5c31\u548c\u6811\u7684\u6df1\u5ea6\u76f8\u5173\u3002 \u5177\u5907nesting\u7279\u6027\u7684\u7ed3\u6784\u90fd\u53ef\u4ee5\u4f7f\u7528tree structure\u6765\u8fdb\u884c\u8868\u793a\u3002 \u6211\u7b2c\u4e00\u6b21\u78b0\u5230\u8fd9\u4e2a\u8bcd\u662f\u5728\u9605\u8bfb Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u8282\u65f6\uff1a Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time . The following example illustrates nesting of procedure calls. \u6b63\u662f\u201cactivations of procedures\u201d\u7684\u201c nest in time \u201d\u7279\u6027\uff0c\u4f7f\u5f97\u201cStack allocation\u201d\u53d8\u5f97\u53ef\u884c\uff0c\u5e76\u4e14activations of procedures\u7684\u8fc7\u7a0b\u662ftree structure\u7684\uff08\u539f\u6587\u4e2d\u5173\u4e8e\u6b64\u662f\u6709\u5206\u6790\u7684\uff09\u3002 \u7b2c\u4e8c\u6b21\u78b0\u5230\u8fd9\u4e2a\u8bcd\u662f\u5728\u9605\u8bfb Hierarchy \u7684\u201cNested hierarchy\u201d\u8282\u65f6\uff0c\u81f3\u6b64\u624d\u66f4\u52a0\u89c9\u5f97nesting\u8fd9\u4e2a\u8bcd\u975e\u5e38\u80fd\u591f\u4f53\u73b0**tree structure**\u7684\u672c\u8d28\u3002 \u66f4\u591a\u5173\u4e8enesting\u7684\u63cf\u8ff0\uff0c\u53c2\u89c1\uff1a Nesting (computing) \u548c Nested sets \u3002 \u54ea\u4e9b\u5173\u7cfb\u80fd\u591f\u5f62\u6210\u6811\u5f62\u72b6 \u8fd9\u4e2a\u95ee\u9898\u5728 Hierarchy \u4e2d\u540c\u6837\u63d0\u95ee\u8fc7\uff0c\u672c\u6bb5\u4ece\u4e00\u4e9b\u5177\u4f53\u7684\u5173\u7cfb\u7684\u4f8b\u5b50\u51fa\u53d1\u6765\u8fdb\u884c\u603b\u7ed3\u3002 Example: nesting\u5173\u7cfb \u5728\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u8bf4\u660e\u4e86tree structure\u7684\u6700\u6700\u6839\u672c\u7684\u7279\u5f81\u662fnesting\uff0c\u6240\u6709\u7684\u5177\u5907nesting\u5173\u7cfb\u7684\u6570\u636e\uff0c\u6309\u7167\u8be5\u5173\u7cfb\u8fdb\u884c\u7ec4\u7ec7\uff0c\u90fd\u80fd\u591f\u5f62\u6210tree structure\u3002\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5b58\u5728\u7740\u592a\u591a\u592a\u591a\u5177\u5907nesting\u5173\u7cfb\u7684\u6570\u636e\u4e86\uff0c\u5728 Examples-of-tree-structures \u4e2d\u4f1a\u679a\u4e3e\u5177\u5907\u8fd9\u79cd\u5173\u7cfb\u7684\u7ed3\u6784\u3002 \u5176\u5b9e\u6709\u5f88\u591a\u7684\u5173\u7cfb\u672c\u8d28\u4e0a\u90fd\u662fnesting\u5173\u7cfb\uff1a \u63a8\u5bfc\u5173\u7cfb Formal grammar\u7684production\u7684head\u53ef\u4ee5derive\u5f97\u5230body\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\u4e8eexpand\uff0c\u5176\u5b9eexpand\u5c31\u662f\u5305\u542b\uff0c\u4e5f\u5c31\u662fnesting \u5173\u7cfb\u3002\u66f4\u591a\u5173\u4e8e\u63a8\u5bfc\u5173\u7cfb\u53c2\u89c1 \u63a8\u5bfc\u5173\u7cfb\u5206\u6790 \u3002 Constituency relation \u8fd9\u662f Phrase structure grammar \u5728\u63cf\u8ff0\u8bed\u8a00\u7ed3\u6784\u65f6\u6240\u91c7\u7528\u7684\u5173\u7cfb\uff0c\u8fd9\u79cd\u5173\u7cfb\u672c\u8d28\u4e0a\u4e5f\u662fnesting\u5173\u7cfb\u3002\u5b83\u63cf\u8ff0\u7684\u8bed\u8a00\u7684\u7ed3\u6784\u662f\u4e00\u68f5\u6811\uff0c\u5982\u4e0b\uff1a Example: Parent-child\u5173\u7cfb \u4e00\u4e2aparent\u53ef\u4ee5\u6709\u591a\u4e2achildren\uff0c\u4e00\u4e2achild\u53ea\u80fd\u591f\u6709\u4e00\u4e2aparent\u3002\u5176\u5b9eParent-child\u5173\u7cfb\u4e5f\u53ef\u4ee5\u5f52\u5165\u5230nesting\u5173\u7cfb\u4e2d\u3002 \u6700\u7ec8\u7b54\u6848 \u8fd9\u79cd\u5173\u7cfb\u9700\u8981\u662f N:1 \u7684\uff08\u8bb0\u5f97\u5728\u5927\u5b66\u7684\u6570\u636e\u5e93\u8bfe\u7a0b\u6240\u4f7f\u7528\u7684\u6559\u6750\u4e2d\u6709\u8fc7\u8fd9\u6837\u7684\u7406\u8bba\uff0c\u8fd9\u4e2a\u7406\u8bba\u5e94\u8be5\u662f\u5c5e\u4e8e Relational algebra \uff0c\u53c2\u89c1\uff1a Relational model \u3001 Database normalization \uff09\u3002 \u8fd9\u79cd\u5173\u7cfb\u5e94\u8be5\u662f Transitive relation \u3002 \u8fd9\u4e2a\u5173\u7cfb\u4e0d\u80fd\u662f Reflexive relation \u3002 \u53ef\u4ee5\u8fd9\u6837\u4e0d\u4e25\u8c28\u5730\u8fdb\u884c\u63cf\u8ff0\uff1a \u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb \u3002 \u540e\u9762\u6211\u4eec\u4e3a\u4e86\u63cf\u8ff0\u7684\u4fbf\u5229\uff0c\u7edf\u4e00\u5c06\u8fd9\u79cd\u5173\u7cfb\u79f0\u4e3a\u201c nesting\u5173\u7cfb \u201d\uff0c\u540e\u9762\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u6709\u65f6\u5019\u4e5f\u4f1a\u5c06\u201ctree structure\u201d\u8868\u8ff0\u6210\u201cnesting structure\u201d\u3002 \u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb\u4f8b\u5b50 \u4e00\u4e9b\u5305\u542b\u5173\u7cfb\u5177\u6709\u4f20\u9012\u6027\uff08\u56de\u53bb\u770b\u79bb\u6563\u6570\u5b66\u5bf9\u8fd9\u7684\u63cf\u8ff0\uff09\uff0c\u6bd4\u5982A\u5305\u542bB\uff0cB\u53c8\u5305\u542bC\uff0c\u5219A\u5e94\u8be5\u5305\u542b\u6240\u6709\u7684C\u3002 \u5982\u4e0b\u662f\u4e00\u4e2a\u4f8b\u5b50\uff1a \u6709\u4ef7\u8bc1\u5238:\u80a1\u7968,\u503a\u5238,\u6743\u8bc1,\u8d44\u4ea7\u652f\u6301\u8bc1\u5238,\u4e70\u5165\u8fd4\u552e\u91d1\u878d\u8d44\u4ea7 \u503a\u5238:\u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \u6709\u4ef7\u8bc1\u5238 \u5305\u542b \u503a\u5238 \uff0c\u503a\u5238\u53c8\u5305\u542b \u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \uff0c\u6240\u4ee5 \u6709\u4ef7\u8bc1\u5238 \u5305\u62ec \u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \u3002 \u4e0a\u8ff0\u5173\u7cfb\u662f\u53ef\u4ee5\u4f7f\u7528tree\u6765\u8fdb\u884c\u63cf\u8ff0\u7684 \u4e0a\u8ff0\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2adict\u6765\u8fdb\u884c\u4fdd\u5b58\uff1a\u6240\u6709\u4f5c\u4e3akey\u7684\u90fd\u662fnon-terminal\uff0c\u90fd\u9700\u8981\u8fdb\u884c\u6269\u5c55\uff1b Nesting \u4e0e hierarchy Nesting\u7ed3\u6784\u5177\u5907hierarchy\u7279\u6027\uff0c\u53ef\u4ee5\u8fd9\u6837\u6765\u8fdb\u884c\u89e3\u91ca\uff1a \u6700\u5916\u5c42\u662f\u662f\u7b2c\u4e00\u5c42\u3001\u5b83\u6240\u76f4\u63a5\u5305\u542b\u7684\u5143\u7d20\u90fd\u5c5e\u4e8e\u7b2c\u4e8c\u5c42\u3001\u4f9d\u6b21\u9012\u5f52\u3002 \u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793a\u6811 Nesting \u7ed3\u6784\u5728computer science\u662f\u975e\u5e38\u5e38\u89c1\uff0c\u5b83\u662f\u4e00\u79cd\u5178\u578b\u7684hierarchy\u7ed3\u6784\uff0cnesting\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u8868\u793a\uff1a ( () () ( ( ) ) ) \u4e0a\u9762\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793anesting\u7ed3\u6784\uff0c\u56e0\u4e3a\u62ec\u53f7\u6240\u80fd\u591f\u8868\u8fbe\u7684\u201c\u5305\u542b\u201d\u5173\u7cfb\u548c\u201c\u5d4c\u5957\u201d\u5173\u7cfb\u662f\u57fa\u672c\u7c7b\u4f3c\u7684\u3002 \u4e0a\u8ff0\u7ed3\u6784\u662f\u53ef\u4ee5\u8868\u793a\u6210\u6811\u7684\uff0c\u5982\u4e0b\uff1a ( ) ( ) ( ) ( ) ( ) \u4f8b\u5b50\u5305\u62ec\uff1a C\u548cC++\u4e2d\uff0c\u4f7f\u7528 {} \u6765\u5b9a\u4e49block\uff0cblock\u4e2d\u53ef\u4ee5\u518d\u5305\u542bblock\uff0c\u4ece\u800c\u5f62\u6210nesting\u7ed3\u6784 \u9f99\u4e667.2.1 Activation Trees\uff1a Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time. \u5373\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u4ece\u65f6\u95f4\u4e0a\u6765\u770b\u4e5f\u662f\u5d4c\u5957\u7684\u3002 \u63a8\u5bfc\u5173\u7cfb\u5206\u6790 \u6269\u5c55\u4e00\u4e2a\u4f7f\u7528tree\u63cf\u8ff0\u7684\u5173\u7cfb\u7684\u6700\u7ec8\u76ee\u6807\u662f\u83b7\u5f97\u6240\u6709\u7684\u53f6\u5b50\u8282\u70b9\uff0c\u5b83\u7684\u57fa\u672c\u7b97\u6cd5\u662f\uff1a\u4e00\u4e2a\u8282\u70b9\uff0c\u53ea\u8981\u662fnon-terminal\u5143\u7d20\uff0c\u5c31\u9700\u8981\u5bf9\u5b83\u8fdb\u884cexpand\uff0c\u5176\u5b9e\u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u662f Parse tree \u7684\u751f\u6210\u8fc7\u7a0b\uff1b \u6240\u4ee5\u5176\u5b9e\uff0c\u6211\u4e0a\u8ff0\u6240\u63cf\u8ff0\u7684\u90fd\u662f Parse tree \u7684\u751f\u6210\u8fc7\u7a0b\u8fc7\u7a0b\uff1b \u4e0b\u9762\u662f\u4e00\u6bb5\u63cf\u8ff0\u4e0a\u8ff0**\u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb**\u7684\u83b7\u53d6\u6240\u6709\u7684\u53ef\u80fd\u7684\u53f6\u5b50\u8282\u70b9\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u5b83\u9700\u8981\u5c06\u6240\u6709\u7684\u5185\u8282\u70b9\u8fdb\u884c\u6269\u5c55\uff0c\u6700\u7ec8\u7684\u7ed3\u679c\u53ea\u80fd\u591f\u5305\u542b\u53f6\u5b50\u8282\u70b9\u800c\u4e0d\u80fd\u5305\u542b\u53f6\u5b50\u8282\u70b9 self.expanded_fen_zi_dict[fen_zi_word_info] = list() to_expand_words = list(retriever_context.fen_zi_detail_dict[fen_zi_word_str]) # \u5f85\u6269\u5c55\u8bcd\u5217\u8868 while len(to_expand_words): word = to_expand_words.pop() # \u4e00\u6b21\u53ea\u5904\u7406\u4e00\u4e2a\u8bcd if word in retriever_context.fen_zi_detail_dict: # \u5f53\u524d\u8bcd\u76f8\u5f53\u4e8e\u4e00\u4e2a\u5185\u8282\u70b9 to_expand_words.extend(retriever_context.fen_zi_detail_dict[word]) # \u6269\u5c55\u5f53\u524d\u8bcd\uff0c\u5e76\u4e14\u5c06\u5b83\u6dfb\u52a0\u5230\u5f85\u6269\u5c55\u8bcd\u5217\u8868\u4e2d else: # \u5f53\u524d\u8bcd\u662f\u4e00\u4e2a\u9875\u8282\u70b9 self.expanded_fen_zi_dict[fen_zi_word_info].append(word) # \u5c06\u8be5\u8bcd\u8fdb\u884c\u8f93\u51fa See also Hierarchy Nesting (computing) Nested set model Nested set Hereditary property","title":"Tree-structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#tree#structure","text":"\u672c\u6587\u6240\u8981\u8ba8\u8bba\u7684\u662f\u201c\u6811\u5f62\u5f62\u72b6\u201d\uff0c\u6807\u9898\u4e2d\u7684\u201cstructure\u201d\u6240\u8981\u8868\u8fbe\u7684\u542b\u4e49\u662f\u201c\u5f62\u72b6\u201d\u3002 \u672c\u6587\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1 Tree structure \u3002 A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. It is named a \"tree structure\" because the classic representation resembles a tree . \u5728 Hierarchy \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86**tree structure**\u5bf9\u5e94\u7684\u662f nested hierarchy structure\u3002 tree structure \u7684\u6700\u6700\u5178\u578b\u7684\u7279\u6027\u662f\uff1a \u4e00\u4e2a\u8282\u70b9\u53ef\u4ee5\u6709\uff08\u5305\u542b\uff09\u591a\u4e2a\u5b50\u8282\u70b9 \u4e00\u4e2a\u5b50\u8282\u70b9\u53ea\u80fd\u591f\u6709\u4e00\u4e2a\u7236\u8282\u70b9 root\u8282\u70b9\u6ca1\u6709\u7236\u8282\u70b9\u3002 tree structure \u7684 \u201c\u4e00\u4e2a\u5b50\u8282\u70b9\u53ea\u80fd\u591f\u6709\u4e00\u4e2a\u7236\u8282\u70b9\u201d \u7684\u8981\u6c42\uff0c\u5c06\u5b83\u548cgraph\u533a\u5206\u5f00\u6765\u4e86\uff08\u53c2\u89c1 Discrete Mathematics and Its Applications \u7684Tree\u7ae0\u8282\uff09\u3002\u5728\u672c\u6587\u7684\u540e\u9762\u6211\u4eec\u4f7f\u7528\u201c nesting \u201d\u8fd9\u4e2a\u8bcd\u6765\u63cf\u8ff0**tree structure**\u7684\u8fd9\u4e2a\u7279\u6027\u3002\u201c nesting \u201d\u7684\u4e2d\u6587\u542b\u4e49\u662f\u201c\u5d4c\u5957\u201d\uff0c\u663e\u7136\uff0c\u5b83\u80fd\u591f\u63cf\u8ff0\u5143\u7d20\u4e4b\u95f4\u7684\u5d4c\u5957\u5173\u7cfb\uff1b\u8fd9\u4e2a\u8bcd\u7684\u542b\u4e49\u662f\u4e30\u5bcc\u7684\uff0c\u5b83\u7684\u8868\u9762\u610f\u601d\u662f\u201c\u5d4c\u5957\u201d\uff0c\u201c\u5d4c\u5957\u201d\u8574\u542b\u7740\u201c\u5305\u542b\u201d\u7684\u542b\u4e49\uff0c\u201c\u5d4c\u5957\u201d\u8574\u542b\u7740\u201c\u9012\u5f52\u201d\u7684\u542b\u4e49\uff1b\u663e\u7136\u5177\u5907nesting\u7279\u6027\uff0c\u5c31\u5177\u5907\u4e86\u5982\u4e0b\u7279\u6027\uff1a hierarchical \uff0c\u5373\u6811\u7ed3\u6784\u662f\u5c42\u6b21\u7684 recursive \uff0cnesting\u7684\u9012\u5f52\u6027\uff1a\u6211\u89c9\u5f97nesting\u7684\u9012\u5f52\u6027\u53ef\u4ee5\u4f7f\u7528\u5173\u7cfb\u7684transitive\u7279\u6027\u6765\u8fdb\u884c\u89e3\u91ca\u3002\u6bd4\u5982 recursive scope rules \uff0c\u8fd9\u79cd\u5305\u542b\u5173\u7cfb\u4e00\u79cdtransitive relation\u3002\u4e0enesting\u76f8\u5173\u7684\u53e6\u5916\u4e00\u4e2a\u8bcd\u662f\uff1alevel\uff0c\u5176\u5b9e\u5b83\u5c31\u548c\u6811\u7684\u6df1\u5ea6\u76f8\u5173\u3002 \u5177\u5907nesting\u7279\u6027\u7684\u7ed3\u6784\u90fd\u53ef\u4ee5\u4f7f\u7528tree structure\u6765\u8fdb\u884c\u8868\u793a\u3002 \u6211\u7b2c\u4e00\u6b21\u78b0\u5230\u8fd9\u4e2a\u8bcd\u662f\u5728\u9605\u8bfb Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) \u7684 7.2.1 Activation Trees \u8282\u65f6\uff1a Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time . The following example illustrates nesting of procedure calls. \u6b63\u662f\u201cactivations of procedures\u201d\u7684\u201c nest in time \u201d\u7279\u6027\uff0c\u4f7f\u5f97\u201cStack allocation\u201d\u53d8\u5f97\u53ef\u884c\uff0c\u5e76\u4e14activations of procedures\u7684\u8fc7\u7a0b\u662ftree structure\u7684\uff08\u539f\u6587\u4e2d\u5173\u4e8e\u6b64\u662f\u6709\u5206\u6790\u7684\uff09\u3002 \u7b2c\u4e8c\u6b21\u78b0\u5230\u8fd9\u4e2a\u8bcd\u662f\u5728\u9605\u8bfb Hierarchy \u7684\u201cNested hierarchy\u201d\u8282\u65f6\uff0c\u81f3\u6b64\u624d\u66f4\u52a0\u89c9\u5f97nesting\u8fd9\u4e2a\u8bcd\u975e\u5e38\u80fd\u591f\u4f53\u73b0**tree structure**\u7684\u672c\u8d28\u3002 \u66f4\u591a\u5173\u4e8enesting\u7684\u63cf\u8ff0\uff0c\u53c2\u89c1\uff1a Nesting (computing) \u548c Nested sets \u3002","title":"Tree structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#_1","text":"\u8fd9\u4e2a\u95ee\u9898\u5728 Hierarchy \u4e2d\u540c\u6837\u63d0\u95ee\u8fc7\uff0c\u672c\u6bb5\u4ece\u4e00\u4e9b\u5177\u4f53\u7684\u5173\u7cfb\u7684\u4f8b\u5b50\u51fa\u53d1\u6765\u8fdb\u884c\u603b\u7ed3\u3002","title":"\u54ea\u4e9b\u5173\u7cfb\u80fd\u591f\u5f62\u6210\u6811\u5f62\u72b6"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#example#nesting","text":"\u5728\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u8bf4\u660e\u4e86tree structure\u7684\u6700\u6700\u6839\u672c\u7684\u7279\u5f81\u662fnesting\uff0c\u6240\u6709\u7684\u5177\u5907nesting\u5173\u7cfb\u7684\u6570\u636e\uff0c\u6309\u7167\u8be5\u5173\u7cfb\u8fdb\u884c\u7ec4\u7ec7\uff0c\u90fd\u80fd\u591f\u5f62\u6210tree structure\u3002\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5b58\u5728\u7740\u592a\u591a\u592a\u591a\u5177\u5907nesting\u5173\u7cfb\u7684\u6570\u636e\u4e86\uff0c\u5728 Examples-of-tree-structures \u4e2d\u4f1a\u679a\u4e3e\u5177\u5907\u8fd9\u79cd\u5173\u7cfb\u7684\u7ed3\u6784\u3002 \u5176\u5b9e\u6709\u5f88\u591a\u7684\u5173\u7cfb\u672c\u8d28\u4e0a\u90fd\u662fnesting\u5173\u7cfb\uff1a","title":"Example: nesting\u5173\u7cfb"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#_2","text":"Formal grammar\u7684production\u7684head\u53ef\u4ee5derive\u5f97\u5230body\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\u4e8eexpand\uff0c\u5176\u5b9eexpand\u5c31\u662f\u5305\u542b\uff0c\u4e5f\u5c31\u662fnesting \u5173\u7cfb\u3002\u66f4\u591a\u5173\u4e8e\u63a8\u5bfc\u5173\u7cfb\u53c2\u89c1 \u63a8\u5bfc\u5173\u7cfb\u5206\u6790 \u3002","title":"\u63a8\u5bfc\u5173\u7cfb"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#constituency#relation","text":"\u8fd9\u662f Phrase structure grammar \u5728\u63cf\u8ff0\u8bed\u8a00\u7ed3\u6784\u65f6\u6240\u91c7\u7528\u7684\u5173\u7cfb\uff0c\u8fd9\u79cd\u5173\u7cfb\u672c\u8d28\u4e0a\u4e5f\u662fnesting\u5173\u7cfb\u3002\u5b83\u63cf\u8ff0\u7684\u8bed\u8a00\u7684\u7ed3\u6784\u662f\u4e00\u68f5\u6811\uff0c\u5982\u4e0b\uff1a","title":"Constituency relation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#example#parent-child","text":"\u4e00\u4e2aparent\u53ef\u4ee5\u6709\u591a\u4e2achildren\uff0c\u4e00\u4e2achild\u53ea\u80fd\u591f\u6709\u4e00\u4e2aparent\u3002\u5176\u5b9eParent-child\u5173\u7cfb\u4e5f\u53ef\u4ee5\u5f52\u5165\u5230nesting\u5173\u7cfb\u4e2d\u3002","title":"Example: Parent-child\u5173\u7cfb"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#_3","text":"\u8fd9\u79cd\u5173\u7cfb\u9700\u8981\u662f N:1 \u7684\uff08\u8bb0\u5f97\u5728\u5927\u5b66\u7684\u6570\u636e\u5e93\u8bfe\u7a0b\u6240\u4f7f\u7528\u7684\u6559\u6750\u4e2d\u6709\u8fc7\u8fd9\u6837\u7684\u7406\u8bba\uff0c\u8fd9\u4e2a\u7406\u8bba\u5e94\u8be5\u662f\u5c5e\u4e8e Relational algebra \uff0c\u53c2\u89c1\uff1a Relational model \u3001 Database normalization \uff09\u3002 \u8fd9\u79cd\u5173\u7cfb\u5e94\u8be5\u662f Transitive relation \u3002 \u8fd9\u4e2a\u5173\u7cfb\u4e0d\u80fd\u662f Reflexive relation \u3002 \u53ef\u4ee5\u8fd9\u6837\u4e0d\u4e25\u8c28\u5730\u8fdb\u884c\u63cf\u8ff0\uff1a \u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb \u3002 \u540e\u9762\u6211\u4eec\u4e3a\u4e86\u63cf\u8ff0\u7684\u4fbf\u5229\uff0c\u7edf\u4e00\u5c06\u8fd9\u79cd\u5173\u7cfb\u79f0\u4e3a\u201c nesting\u5173\u7cfb \u201d\uff0c\u540e\u9762\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u6709\u65f6\u5019\u4e5f\u4f1a\u5c06\u201ctree structure\u201d\u8868\u8ff0\u6210\u201cnesting structure\u201d\u3002","title":"\u6700\u7ec8\u7b54\u6848"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#_4","text":"\u4e00\u4e9b\u5305\u542b\u5173\u7cfb\u5177\u6709\u4f20\u9012\u6027\uff08\u56de\u53bb\u770b\u79bb\u6563\u6570\u5b66\u5bf9\u8fd9\u7684\u63cf\u8ff0\uff09\uff0c\u6bd4\u5982A\u5305\u542bB\uff0cB\u53c8\u5305\u542bC\uff0c\u5219A\u5e94\u8be5\u5305\u542b\u6240\u6709\u7684C\u3002 \u5982\u4e0b\u662f\u4e00\u4e2a\u4f8b\u5b50\uff1a \u6709\u4ef7\u8bc1\u5238:\u80a1\u7968,\u503a\u5238,\u6743\u8bc1,\u8d44\u4ea7\u652f\u6301\u8bc1\u5238,\u4e70\u5165\u8fd4\u552e\u91d1\u878d\u8d44\u4ea7 \u503a\u5238:\u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \u6709\u4ef7\u8bc1\u5238 \u5305\u542b \u503a\u5238 \uff0c\u503a\u5238\u53c8\u5305\u542b \u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \uff0c\u6240\u4ee5 \u6709\u4ef7\u8bc1\u5238 \u5305\u62ec \u56fd\u503a,\u4f01\u503a,\u975e\u653f\u7b56\u578b\u91d1\u878d\u503a,\u5730\u65b9\u503a,\u53ef\u8f6c\u503a,\u653f\u7b56\u6027\u91d1\u878d\u503a,\u516c\u53f8\u503a,\u592e\u884c\u7968\u636e,\u6b21\u7ea7\u503a \u3002 \u4e0a\u8ff0\u5173\u7cfb\u662f\u53ef\u4ee5\u4f7f\u7528tree\u6765\u8fdb\u884c\u63cf\u8ff0\u7684 \u4e0a\u8ff0\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2adict\u6765\u8fdb\u884c\u4fdd\u5b58\uff1a\u6240\u6709\u4f5c\u4e3akey\u7684\u90fd\u662fnon-terminal\uff0c\u90fd\u9700\u8981\u8fdb\u884c\u6269\u5c55\uff1b","title":"\u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb\u4f8b\u5b50"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#nesting#hierarchy","text":"Nesting\u7ed3\u6784\u5177\u5907hierarchy\u7279\u6027\uff0c\u53ef\u4ee5\u8fd9\u6837\u6765\u8fdb\u884c\u89e3\u91ca\uff1a \u6700\u5916\u5c42\u662f\u662f\u7b2c\u4e00\u5c42\u3001\u5b83\u6240\u76f4\u63a5\u5305\u542b\u7684\u5143\u7d20\u90fd\u5c5e\u4e8e\u7b2c\u4e8c\u5c42\u3001\u4f9d\u6b21\u9012\u5f52\u3002","title":"Nesting \u4e0e hierarchy"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#_5","text":"Nesting \u7ed3\u6784\u5728computer science\u662f\u975e\u5e38\u5e38\u89c1\uff0c\u5b83\u662f\u4e00\u79cd\u5178\u578b\u7684hierarchy\u7ed3\u6784\uff0cnesting\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u8868\u793a\uff1a ( () () ( ( ) ) ) \u4e0a\u9762\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793anesting\u7ed3\u6784\uff0c\u56e0\u4e3a\u62ec\u53f7\u6240\u80fd\u591f\u8868\u8fbe\u7684\u201c\u5305\u542b\u201d\u5173\u7cfb\u548c\u201c\u5d4c\u5957\u201d\u5173\u7cfb\u662f\u57fa\u672c\u7c7b\u4f3c\u7684\u3002 \u4e0a\u8ff0\u7ed3\u6784\u662f\u53ef\u4ee5\u8868\u793a\u6210\u6811\u7684\uff0c\u5982\u4e0b\uff1a ( ) ( ) ( ) ( ) ( ) \u4f8b\u5b50\u5305\u62ec\uff1a C\u548cC++\u4e2d\uff0c\u4f7f\u7528 {} \u6765\u5b9a\u4e49block\uff0cblock\u4e2d\u53ef\u4ee5\u518d\u5305\u542bblock\uff0c\u4ece\u800c\u5f62\u6210nesting\u7ed3\u6784 \u9f99\u4e667.2.1 Activation Trees\uff1a Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time. \u5373\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u4ece\u65f6\u95f4\u4e0a\u6765\u770b\u4e5f\u662f\u5d4c\u5957\u7684\u3002","title":"\u4f7f\u7528\u62ec\u53f7\u6765\u8868\u793a\u6811"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#_6","text":"\u6269\u5c55\u4e00\u4e2a\u4f7f\u7528tree\u63cf\u8ff0\u7684\u5173\u7cfb\u7684\u6700\u7ec8\u76ee\u6807\u662f\u83b7\u5f97\u6240\u6709\u7684\u53f6\u5b50\u8282\u70b9\uff0c\u5b83\u7684\u57fa\u672c\u7b97\u6cd5\u662f\uff1a\u4e00\u4e2a\u8282\u70b9\uff0c\u53ea\u8981\u662fnon-terminal\u5143\u7d20\uff0c\u5c31\u9700\u8981\u5bf9\u5b83\u8fdb\u884cexpand\uff0c\u5176\u5b9e\u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u662f Parse tree \u7684\u751f\u6210\u8fc7\u7a0b\uff1b \u6240\u4ee5\u5176\u5b9e\uff0c\u6211\u4e0a\u8ff0\u6240\u63cf\u8ff0\u7684\u90fd\u662f Parse tree \u7684\u751f\u6210\u8fc7\u7a0b\u8fc7\u7a0b\uff1b \u4e0b\u9762\u662f\u4e00\u6bb5\u63cf\u8ff0\u4e0a\u8ff0**\u5177\u5907\u4f20\u9012\u6027\u7684\u5305\u542b\u5173\u7cfb**\u7684\u83b7\u53d6\u6240\u6709\u7684\u53ef\u80fd\u7684\u53f6\u5b50\u8282\u70b9\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u5b83\u9700\u8981\u5c06\u6240\u6709\u7684\u5185\u8282\u70b9\u8fdb\u884c\u6269\u5c55\uff0c\u6700\u7ec8\u7684\u7ed3\u679c\u53ea\u80fd\u591f\u5305\u542b\u53f6\u5b50\u8282\u70b9\u800c\u4e0d\u80fd\u5305\u542b\u53f6\u5b50\u8282\u70b9 self.expanded_fen_zi_dict[fen_zi_word_info] = list() to_expand_words = list(retriever_context.fen_zi_detail_dict[fen_zi_word_str]) # \u5f85\u6269\u5c55\u8bcd\u5217\u8868 while len(to_expand_words): word = to_expand_words.pop() # \u4e00\u6b21\u53ea\u5904\u7406\u4e00\u4e2a\u8bcd if word in retriever_context.fen_zi_detail_dict: # \u5f53\u524d\u8bcd\u76f8\u5f53\u4e8e\u4e00\u4e2a\u5185\u8282\u70b9 to_expand_words.extend(retriever_context.fen_zi_detail_dict[word]) # \u6269\u5c55\u5f53\u524d\u8bcd\uff0c\u5e76\u4e14\u5c06\u5b83\u6dfb\u52a0\u5230\u5f85\u6269\u5c55\u8bcd\u5217\u8868\u4e2d else: # \u5f53\u524d\u8bcd\u662f\u4e00\u4e2a\u9875\u8282\u70b9 self.expanded_fen_zi_dict[fen_zi_word_info].append(word) # \u5c06\u8be5\u8bcd\u8fdb\u884c\u8f93\u51fa","title":"\u63a8\u5bfc\u5173\u7cfb\u5206\u6790"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/Tree-structure/Tree-structure/#see#also","text":"Hierarchy Nesting (computing) Nested set model Nested set Hereditary property","title":"See also"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0trie\u4ee5\u53ca\u5b83\u7684\u5404\u79cd\u5e94\u7528\uff0c\u53d8\u4f53\u7b49\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/#_1","text":"\u672c\u7ae0\u63cf\u8ff0trie\u4ee5\u53ca\u5b83\u7684\u5404\u79cd\u5e94\u7528\uff0c\u53d8\u4f53\u7b49\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/","text":"Trie NOTE: trie\u4e5f\u53ef\u4ee5\u79f0\u4e4b\u4e3a\u524d\u7f00\u6811 In computer science , a trie , also called digital tree , radix tree or prefix tree , is a kind of search tree \u2014an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings . Unlike a binary search tree , no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string . Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node. For the space-optimized presentation of prefix tree, see compact prefix tree . In the example shown, keys are listed in the nodes and values below them. Each complete English word has an arbitrary integer value associated with it. A trie can be seen as a tree-shaped deterministic finite automaton \uff08\u5373DFA\uff0c\u786e\u5b9a\u6709\u7a77\u81ea\u52a8\u673a\uff09. Each finite language is generated by a trie automaton , and each trie can be compressed into a deterministic acyclic finite state automaton . SUMMARY : \u5173\u4e8efinite automata\u7684\u5185\u5bb9\u53c2\u89c1\u300aAutomata theory-formal languages and formal grammars\u300b\u76ee\u5f55\uff0c\u5176\u4e2d\u6536\u5f55\u5173\u4e8efinite automata\u7684\u77e5\u8bc6\u3002 SUMMARY : \u5176\u5b9e\u611f\u89c9trie\u66f4\u52a0\u7c7b\u4f3c\u4e8egraph\uff0c\u56e0\u4e3atrie\u7684edge\u6709label\uff0c\u7c7b\u4f3c\u4e8eweighted graph\uff0c\u6240\u4ee5\u5b83\u7684\u5b9e\u73b0\u9664\u4e86\u9700\u8981\u4fdd\u5b58\u8282\u70b9\u4e4b\u95f4\u7684edge\uff0c\u8fd8\u9700\u8981\u4fdd\u5b58\u6bcf\u6761edge\u7684label\uff0c\u5176\u5b9e\u8fd9\u5c31\u975e\u5e38\u7c7b\u4f3cfinite automata\uff1b Though tries can be keyed by character strings, they need not be. The same algorithms can be adapted to serve similar functions on ordered lists of any construct, e.g. permutations on a list of digits or shapes. In particular, a bitwise trie is keyed on the individual bits making up any fixed-length binary datum, such as an integer or memory address. A trie for keys \"A\", \"to\", \"tea\", \"ted\", \"ten\", \"i\", \"in\", and \"inn\". Note that this example does not have all the children alphabetically sorted from left to right as it should be (the root and node 't'). Applications As a replacement for other data structures As discussed below, a trie has a number of advantages over binary search trees. A trie can also be used to replace a hash table , over which it has the following advantages: Looking up data in a trie is faster in the worst case, O(m) time (where m is the length of a search string), compared to an imperfect hash table. An imperfect hash table can have key collisions. A key collision is the hash function mapping of different keys to the same position in a hash table. The worst-case lookup speed in an imperfect hash table is O(N) time, but far more typically is O(1), with O(m) time spent evaluating the hash. There are no collisions of different keys in a trie. Buckets in a trie, which are analogous to hash table buckets that store key collisions, are necessary only if a single key is associated with more than one value. There is no need to provide a hash function or to change hash functions as more keys are added to a trie. A trie can provide an alphabetical ordering of the entries by key. However, a trie also has some drawbacks compared to a hash table: Trie lookup can be slower than hash table lookup, especially if the data is directly accessed on a hard disk drive or some other secondary storage device where the random-access time is high compared to main memory.[ 7] Some keys, such as floating point numbers, can lead to long chains and prefixes that are not particularly meaningful. Nevertheless, a bitwise trie can handle standard IEEE single and double format floating point numbers.[ citation needed ] Some tries can require more space than a hash table, as memory may be allocated for each character in the search string, rather than a single chunk of memory for the whole entry, as in most hash tables. Dictionary representation A common application of a trie is storing a predictive text or autocomplete dictionary, such as found on a mobile telephone . Such applications take advantage of a trie's ability to quickly search for, insert, and delete entries; however, if storing dictionary words is all that is required (i.e., storage of information auxiliary to each word is not required), a minimal deterministic acyclic finite state automaton (DAFSA) would use less space than a trie. This is because a DAFSA can compress identical branches from the trie which correspond to the same suffixes (or parts) of different words being stored. Tries are also well suited for implementing approximate matching algorithms,[ 8] including those used in spell checking and hyphenation [ 4] software. Term indexing A discrimination tree term index stores its information in a trie data structure.[ 9] Algorithms The trie is a tree of nodes which supports Find and Insert operations. Find returns the value for a key string, and Insert inserts a string (the key) and a value into the trie. Both Insert and Find run in O( n ) time, where n is the length of the key. A simple Node class can be used to represent nodes in the trie: class Node (): def __init__ ( self ): # Note that using dictionary for children (as in this implementation) would not allow lexicographic sorting mentioned in the next section (Sorting), # because ordinary dictionary would not preserve the order of the keys self . children : Dict [ str , Node ] = {} # mapping from character ==> Node self . value : Any = None Note that children is a dictionary of characters to a node's children; and it is said that a \"terminal\" node is one which represents a complete string. A trie's value can be looked up as follows: def find ( node : Node , key : str ) -> Any : for char in key : if char in node . children : node = node . children [ char ] else : return None return node . value Insertion proceeds by walking the trie according to the string to be inserted, then appending new nodes for the suffix of the string that is not contained in the trie: def insert ( node : Node , key : str , value : Any ) -> None : for char in key : if char not in node . children : node . children [ char ] = Node () node = node . children [ char ] node . value = value Sorting Lexicographic sorting of a set of keys can be accomplished by building a trie from them, and traversing it in pre-order , printing only the leaves' values. This algorithm is a form of radix sort .[ 10] A trie forms the fundamental data structure of Burstsort , which (in 2007) was the fastest known string sorting algorithm.[ 11] However, now there are faster string sorting algorithms.[ 12] Full text search A special kind of trie, called a suffix tree , can be used to index all suffixes in a text in order to carry out fast full text searches. SUMMARY : suffix tree \u7684\u786e\u5f88\u5f3a\u5927\uff1b Implementation strategies There are several ways to represent tries, corresponding to different trade-offs between memory use and speed of the operations. The basic form is that of a linked set of nodes, where each node contains an array of child pointers, one for each symbol in the alphabet (so for the English alphabet , one would store 26 child pointers and for the alphabet of bytes, 256 pointers). This is simple but wasteful in terms of memory: using the alphabet of bytes (size 256) and four-byte pointers, each node requires a kilobyte of storage, and when there is little overlap in the strings' prefixes, the number of required nodes is roughly the combined length of the stored strings.[ 2] :341 Put another way, the nodes near the bottom of the tree tend to have few children and there are many of them, so the structure wastes space storing null pointers.[ 13] The storage problem can be alleviated by an implementation technique called alphabet reduction , whereby the original strings are reinterpreted as longer strings over a smaller alphabet. E.g., a string of n bytes can alternatively be regarded as a string of 2*n* four-bit units and stored in a trie with sixteen pointers per node. Lookups need to visit twice as many nodes in the worst case, but the storage requirements go down by a factor of eight.[ 2] :347\u2013352 An alternative implementation represents a node as a triple (symbol, child, next) and links the children of a node together as a singly linked list : child points to the node's first child, next to the parent node's next child.[ 13] [ 14] The set of children can also be represented as a binary search tree ; one instance of this idea is the ternary search tree developed by Bentley and Sedgewick .[ 2] :353 Another alternative in order to avoid the use of an array of 256 pointers (ASCII), as suggested before, is to store the alphabet array as a bitmap of 256 bits representing the ASCII alphabet, reducing dramatically the size of the nodes.[ 15] Bitwise tries Bitwise tries are much the same as a normal character-based trie except that individual bits are used to traverse what effectively becomes a form of binary tree. Generally, implementations use a special CPU instruction to very quickly find the first set bit in a fixed length key (e.g., GCC's __builtin_clz() intrinsic). This value is then used to index a 32- or 64-entry table which points to the first item in the bitwise trie with that number of leading zero bits. The search then proceeds by testing each subsequent bit in the key and choosing child[0] or child[1] appropriately until the item is found. Although this process might sound slow, it is very cache-local and highly parallelizable due to the lack of register dependencies and therefore in fact has excellent performance on modern out-of-order execution CPUs. A red-black tree for example performs much better on paper, but is highly cache-unfriendly and causes multiple pipeline and TLB stalls on modern CPUs which makes that algorithm bound by memory latency rather than CPU speed. In comparison, a bitwise trie rarely accesses memory, and when it does, it does so only to read, thus avoiding SMP cache coherency overhead. Hence, it is increasingly becoming the algorithm of choice for code that performs many rapid insertions and deletions, such as memory allocators (e.g., recent versions of the famous Doug Lea's allocator (dlmalloc) and its descendants ). Compressing tries Compressing the trie and merging the common branches can sometimes yield large performance gains. This works best under the following conditions: The trie is mostly static (key insertions to or deletions from a pre-filled trie are disabled).[ citation needed ] Only lookups are needed. The trie nodes are not keyed by node-specific data, or the nodes' data are common.[ 16] The total set of stored keys is very sparse within their representation space.[ citation needed ] For example, it may be used to represent sparse bitsets , i.e., subsets of a much larger, fixed enumerable set. In such a case, the trie is keyed by the bit element position within the full set. The key is created from the string of bits needed to encode the integral position of each element. Such tries have a very degenerate form with many missing branches. After detecting the repetition of common patterns or filling the unused gaps, the unique leaf nodes (bit strings) can be stored and compressed easily, reducing the overall size of the trie. Such compression is also used in the implementation of the various fast lookup tables for retrieving Unicode character properties. These could include case-mapping tables (e.g. for the Greek letter pi , from \u03a0 to \u03c0), or lookup tables normalizing the combination of base and combining characters (like the a- umlaut in German , \u00e4, or the dalet - patah - dagesh - ole in Biblical Hebrew , \u05d3\u05b7\u05bc\u05ab). For such applications, the representation is similar to transforming a very large, unidimensional, sparse table (e.g. Unicode code points) into a multidimensional matrix of their combinations, and then using the coordinates in the hyper-matrix as the string key of an uncompressed trie to represent the resulting character. The compression will then consist of detecting and merging the common columns within the hyper-matrix to compress the last dimension in the key. For example, to avoid storing the full, multibyte Unicode code point of each element forming a matrix column, the groupings of similar code points can be exploited. Each dimension of the hyper-matrix stores the start position of the next dimension, so that only the offset (typically a single byte) need be stored. The resulting vector is itself compressible when it is also sparse, so each dimension (associated to a layer level in the trie) can be compressed separately. Some implementations do support such data compression within dynamic sparse tries and allow insertions and deletions in compressed tries. However, this usually has a significant cost when compressed segments need to be split or merged. Some tradeoff has to be made between data compression and update speed. A typical strategy is to limit the range of global lookups for comparing the common branches in the sparse trie.[ citation needed ] The result of such compression may look similar to trying to transform the trie into a directed acyclic graph (DAG), because the reverse transform from a DAG to a trie is obvious and always possible. However, the shape of the DAG is determined by the form of the key chosen to index the nodes, in turn constraining the compression possible. Another compression strategy is to \"unravel\" the data structure into a single byte array.[ 17] This approach eliminates the need for node pointers, substantially reducing the memory requirements. This in turn permits memory mapping and the use of virtual memory to efficiently load the data from disk. One more approach is to \"pack\" the trie.[ 4] Liang describes a space-efficient implementation of a sparse packed trie applied to automatic hyphenation , in which the descendants of each node may be interleaved in memory.","title":"[Trie](https://en.wikipedia.org/wiki/Trie)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#trie","text":"NOTE: trie\u4e5f\u53ef\u4ee5\u79f0\u4e4b\u4e3a\u524d\u7f00\u6811 In computer science , a trie , also called digital tree , radix tree or prefix tree , is a kind of search tree \u2014an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings . Unlike a binary search tree , no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string . Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node. For the space-optimized presentation of prefix tree, see compact prefix tree . In the example shown, keys are listed in the nodes and values below them. Each complete English word has an arbitrary integer value associated with it. A trie can be seen as a tree-shaped deterministic finite automaton \uff08\u5373DFA\uff0c\u786e\u5b9a\u6709\u7a77\u81ea\u52a8\u673a\uff09. Each finite language is generated by a trie automaton , and each trie can be compressed into a deterministic acyclic finite state automaton . SUMMARY : \u5173\u4e8efinite automata\u7684\u5185\u5bb9\u53c2\u89c1\u300aAutomata theory-formal languages and formal grammars\u300b\u76ee\u5f55\uff0c\u5176\u4e2d\u6536\u5f55\u5173\u4e8efinite automata\u7684\u77e5\u8bc6\u3002 SUMMARY : \u5176\u5b9e\u611f\u89c9trie\u66f4\u52a0\u7c7b\u4f3c\u4e8egraph\uff0c\u56e0\u4e3atrie\u7684edge\u6709label\uff0c\u7c7b\u4f3c\u4e8eweighted graph\uff0c\u6240\u4ee5\u5b83\u7684\u5b9e\u73b0\u9664\u4e86\u9700\u8981\u4fdd\u5b58\u8282\u70b9\u4e4b\u95f4\u7684edge\uff0c\u8fd8\u9700\u8981\u4fdd\u5b58\u6bcf\u6761edge\u7684label\uff0c\u5176\u5b9e\u8fd9\u5c31\u975e\u5e38\u7c7b\u4f3cfinite automata\uff1b Though tries can be keyed by character strings, they need not be. The same algorithms can be adapted to serve similar functions on ordered lists of any construct, e.g. permutations on a list of digits or shapes. In particular, a bitwise trie is keyed on the individual bits making up any fixed-length binary datum, such as an integer or memory address. A trie for keys \"A\", \"to\", \"tea\", \"ted\", \"ten\", \"i\", \"in\", and \"inn\". Note that this example does not have all the children alphabetically sorted from left to right as it should be (the root and node 't').","title":"Trie"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#applications","text":"","title":"Applications"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#as#a#replacement#for#other#data#structures","text":"As discussed below, a trie has a number of advantages over binary search trees. A trie can also be used to replace a hash table , over which it has the following advantages: Looking up data in a trie is faster in the worst case, O(m) time (where m is the length of a search string), compared to an imperfect hash table. An imperfect hash table can have key collisions. A key collision is the hash function mapping of different keys to the same position in a hash table. The worst-case lookup speed in an imperfect hash table is O(N) time, but far more typically is O(1), with O(m) time spent evaluating the hash. There are no collisions of different keys in a trie. Buckets in a trie, which are analogous to hash table buckets that store key collisions, are necessary only if a single key is associated with more than one value. There is no need to provide a hash function or to change hash functions as more keys are added to a trie. A trie can provide an alphabetical ordering of the entries by key. However, a trie also has some drawbacks compared to a hash table: Trie lookup can be slower than hash table lookup, especially if the data is directly accessed on a hard disk drive or some other secondary storage device where the random-access time is high compared to main memory.[ 7] Some keys, such as floating point numbers, can lead to long chains and prefixes that are not particularly meaningful. Nevertheless, a bitwise trie can handle standard IEEE single and double format floating point numbers.[ citation needed ] Some tries can require more space than a hash table, as memory may be allocated for each character in the search string, rather than a single chunk of memory for the whole entry, as in most hash tables.","title":"As a replacement for other data structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#dictionary#representation","text":"A common application of a trie is storing a predictive text or autocomplete dictionary, such as found on a mobile telephone . Such applications take advantage of a trie's ability to quickly search for, insert, and delete entries; however, if storing dictionary words is all that is required (i.e., storage of information auxiliary to each word is not required), a minimal deterministic acyclic finite state automaton (DAFSA) would use less space than a trie. This is because a DAFSA can compress identical branches from the trie which correspond to the same suffixes (or parts) of different words being stored. Tries are also well suited for implementing approximate matching algorithms,[ 8] including those used in spell checking and hyphenation [ 4] software.","title":"Dictionary representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#term#indexing","text":"A discrimination tree term index stores its information in a trie data structure.[ 9]","title":"Term indexing"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#algorithms","text":"The trie is a tree of nodes which supports Find and Insert operations. Find returns the value for a key string, and Insert inserts a string (the key) and a value into the trie. Both Insert and Find run in O( n ) time, where n is the length of the key. A simple Node class can be used to represent nodes in the trie: class Node (): def __init__ ( self ): # Note that using dictionary for children (as in this implementation) would not allow lexicographic sorting mentioned in the next section (Sorting), # because ordinary dictionary would not preserve the order of the keys self . children : Dict [ str , Node ] = {} # mapping from character ==> Node self . value : Any = None Note that children is a dictionary of characters to a node's children; and it is said that a \"terminal\" node is one which represents a complete string. A trie's value can be looked up as follows: def find ( node : Node , key : str ) -> Any : for char in key : if char in node . children : node = node . children [ char ] else : return None return node . value Insertion proceeds by walking the trie according to the string to be inserted, then appending new nodes for the suffix of the string that is not contained in the trie: def insert ( node : Node , key : str , value : Any ) -> None : for char in key : if char not in node . children : node . children [ char ] = Node () node = node . children [ char ] node . value = value","title":"Algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#sorting","text":"Lexicographic sorting of a set of keys can be accomplished by building a trie from them, and traversing it in pre-order , printing only the leaves' values. This algorithm is a form of radix sort .[ 10] A trie forms the fundamental data structure of Burstsort , which (in 2007) was the fastest known string sorting algorithm.[ 11] However, now there are faster string sorting algorithms.[ 12]","title":"Sorting"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#full#text#search","text":"A special kind of trie, called a suffix tree , can be used to index all suffixes in a text in order to carry out fast full text searches. SUMMARY : suffix tree \u7684\u786e\u5f88\u5f3a\u5927\uff1b","title":"Full text search"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#implementation#strategies","text":"There are several ways to represent tries, corresponding to different trade-offs between memory use and speed of the operations. The basic form is that of a linked set of nodes, where each node contains an array of child pointers, one for each symbol in the alphabet (so for the English alphabet , one would store 26 child pointers and for the alphabet of bytes, 256 pointers). This is simple but wasteful in terms of memory: using the alphabet of bytes (size 256) and four-byte pointers, each node requires a kilobyte of storage, and when there is little overlap in the strings' prefixes, the number of required nodes is roughly the combined length of the stored strings.[ 2] :341 Put another way, the nodes near the bottom of the tree tend to have few children and there are many of them, so the structure wastes space storing null pointers.[ 13] The storage problem can be alleviated by an implementation technique called alphabet reduction , whereby the original strings are reinterpreted as longer strings over a smaller alphabet. E.g., a string of n bytes can alternatively be regarded as a string of 2*n* four-bit units and stored in a trie with sixteen pointers per node. Lookups need to visit twice as many nodes in the worst case, but the storage requirements go down by a factor of eight.[ 2] :347\u2013352 An alternative implementation represents a node as a triple (symbol, child, next) and links the children of a node together as a singly linked list : child points to the node's first child, next to the parent node's next child.[ 13] [ 14] The set of children can also be represented as a binary search tree ; one instance of this idea is the ternary search tree developed by Bentley and Sedgewick .[ 2] :353 Another alternative in order to avoid the use of an array of 256 pointers (ASCII), as suggested before, is to store the alphabet array as a bitmap of 256 bits representing the ASCII alphabet, reducing dramatically the size of the nodes.[ 15]","title":"Implementation strategies"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#bitwise#tries","text":"Bitwise tries are much the same as a normal character-based trie except that individual bits are used to traverse what effectively becomes a form of binary tree. Generally, implementations use a special CPU instruction to very quickly find the first set bit in a fixed length key (e.g., GCC's __builtin_clz() intrinsic). This value is then used to index a 32- or 64-entry table which points to the first item in the bitwise trie with that number of leading zero bits. The search then proceeds by testing each subsequent bit in the key and choosing child[0] or child[1] appropriately until the item is found. Although this process might sound slow, it is very cache-local and highly parallelizable due to the lack of register dependencies and therefore in fact has excellent performance on modern out-of-order execution CPUs. A red-black tree for example performs much better on paper, but is highly cache-unfriendly and causes multiple pipeline and TLB stalls on modern CPUs which makes that algorithm bound by memory latency rather than CPU speed. In comparison, a bitwise trie rarely accesses memory, and when it does, it does so only to read, thus avoiding SMP cache coherency overhead. Hence, it is increasingly becoming the algorithm of choice for code that performs many rapid insertions and deletions, such as memory allocators (e.g., recent versions of the famous Doug Lea's allocator (dlmalloc) and its descendants ).","title":"Bitwise tries"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/Trie/#compressing#tries","text":"Compressing the trie and merging the common branches can sometimes yield large performance gains. This works best under the following conditions: The trie is mostly static (key insertions to or deletions from a pre-filled trie are disabled).[ citation needed ] Only lookups are needed. The trie nodes are not keyed by node-specific data, or the nodes' data are common.[ 16] The total set of stored keys is very sparse within their representation space.[ citation needed ] For example, it may be used to represent sparse bitsets , i.e., subsets of a much larger, fixed enumerable set. In such a case, the trie is keyed by the bit element position within the full set. The key is created from the string of bits needed to encode the integral position of each element. Such tries have a very degenerate form with many missing branches. After detecting the repetition of common patterns or filling the unused gaps, the unique leaf nodes (bit strings) can be stored and compressed easily, reducing the overall size of the trie. Such compression is also used in the implementation of the various fast lookup tables for retrieving Unicode character properties. These could include case-mapping tables (e.g. for the Greek letter pi , from \u03a0 to \u03c0), or lookup tables normalizing the combination of base and combining characters (like the a- umlaut in German , \u00e4, or the dalet - patah - dagesh - ole in Biblical Hebrew , \u05d3\u05b7\u05bc\u05ab). For such applications, the representation is similar to transforming a very large, unidimensional, sparse table (e.g. Unicode code points) into a multidimensional matrix of their combinations, and then using the coordinates in the hyper-matrix as the string key of an uncompressed trie to represent the resulting character. The compression will then consist of detecting and merging the common columns within the hyper-matrix to compress the last dimension in the key. For example, to avoid storing the full, multibyte Unicode code point of each element forming a matrix column, the groupings of similar code points can be exploited. Each dimension of the hyper-matrix stores the start position of the next dimension, so that only the offset (typically a single byte) need be stored. The resulting vector is itself compressible when it is also sparse, so each dimension (associated to a layer level in the trie) can be compressed separately. Some implementations do support such data compression within dynamic sparse tries and allow insertions and deletions in compressed tries. However, this usually has a significant cost when compressed segments need to be split or merged. Some tradeoff has to be made between data compression and update speed. A typical strategy is to limit the range of global lookups for comparing the common branches in the sparse trie.[ citation needed ] The result of such compression may look similar to trying to transform the trie into a directed acyclic graph (DAG), because the reverse transform from a DAG to a trie is obvious and always possible. However, the shape of the DAG is determined by the form of the key chosen to index the nodes, in turn constraining the compression possible. Another compression strategy is to \"unravel\" the data structure into a single byte array.[ 17] This approach eliminates the need for node pointers, substantially reducing the memory requirements. This in turn permits memory mapping and the use of virtual memory to efficiently load the data from disk. One more approach is to \"pack\" the trie.[ 4] Liang describes a space-efficient implementation of a sparse packed trie applied to automatic hyphenation , in which the descendants of each node may be interleaved in memory.","title":"Compressing tries"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/geeksforgeeks-trie/","text":"Trie | (Insert and Search) Trie is an efficient information re**Trie**val data structure. Using Trie, search complexities can be brought to optimal limit (key length). If we store keys in binary search tree, a well balanced BST will need time proportional to M * log N , where M is maximum string length and N is number of keys in tree. Using Trie, we can search the key in O(M) time. However the penalty is on Trie storage requirements (Please refer Applications of Trie for more details) Every node of Trie consists of multiple branches. Each branch represents a possible character of keys. We need to mark the last node of every key as end of word node. A Trie node field isEndOfWord is used to distinguish the node as end of word node. A simple structure to represent nodes of the English alphabet can be as following, // Trie node struct TrieNode { struct TrieNode * children [ ALPHABET_SIZE ]; // isEndOfWord is true if the node // represents end of a word bool isEndOfWord ; }; Inserting Inserting a key into Trie is a simple approach. Every character of the input key is inserted as an individual Trie node. Note that the children is an array of pointers (or references) to next level trie nodes. The key character acts as an index into the array children . If the input key is new or an extension of the existing key, we need to construct non-existing nodes of the key, and mark end of the word for the last node. If the input key is a prefix of the existing key in Trie, we simply mark the last node of the key as the end of a word. The key length determines Trie depth. Searching Searching for a key is similar to insert operation, however, we only compare the characters and move down. The search can terminate due to the end of a string or lack of key in the trie. In the former case, if the isEndofWord field of the last node is true, then the key exists in the trie. In the second case, the search terminates without examining all the characters of the key, since the key is not present in the trie. The following picture explains construction of trie using keys given in the example below, root / \\ \\ t a b | | | h n y | | \\ | e s y e / | | i r w | | | r e e | r In the picture, every character is of type trie_node_t . For example, the root is of type trie_node_t , and it\u2019s children a , b and t are filled, all other nodes of root will be NULL. Similarly, \u201ca\u201d at the next level is having only one child (\u201cn\u201d), all other children are NULL. The leaf nodes are in blue. Insert and search costs O(key_length) , however the memory requirements of Trie is O(ALPHABET_SIZE * key_length * N) where N is number of keys in Trie. There are efficient representation of trie nodes (e.g. compressed trie, ternary search tree , etc.) to minimize memory requirements of trie.","title":"[Trie | (Insert and Search)](https://www.geeksforgeeks.org/tag/trie/)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/geeksforgeeks-trie/#trie#insert#and#search","text":"Trie is an efficient information re**Trie**val data structure. Using Trie, search complexities can be brought to optimal limit (key length). If we store keys in binary search tree, a well balanced BST will need time proportional to M * log N , where M is maximum string length and N is number of keys in tree. Using Trie, we can search the key in O(M) time. However the penalty is on Trie storage requirements (Please refer Applications of Trie for more details) Every node of Trie consists of multiple branches. Each branch represents a possible character of keys. We need to mark the last node of every key as end of word node. A Trie node field isEndOfWord is used to distinguish the node as end of word node. A simple structure to represent nodes of the English alphabet can be as following, // Trie node struct TrieNode { struct TrieNode * children [ ALPHABET_SIZE ]; // isEndOfWord is true if the node // represents end of a word bool isEndOfWord ; };","title":"Trie | (Insert and Search)"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/geeksforgeeks-trie/#inserting","text":"Inserting a key into Trie is a simple approach. Every character of the input key is inserted as an individual Trie node. Note that the children is an array of pointers (or references) to next level trie nodes. The key character acts as an index into the array children . If the input key is new or an extension of the existing key, we need to construct non-existing nodes of the key, and mark end of the word for the last node. If the input key is a prefix of the existing key in Trie, we simply mark the last node of the key as the end of a word. The key length determines Trie depth.","title":"Inserting"},{"location":"Relation-structure-computation/Structure/Data-structure/Graph/Tree/trie/geeksforgeeks-trie/#searching","text":"Searching for a key is similar to insert operation, however, we only compare the characters and move down. The search can terminate due to the end of a string or lack of key in the trie. In the former case, if the isEndofWord field of the last node is true, then the key exists in the trie. In the second case, the search terminates without examining all the characters of the key, since the key is not present in the trie. The following picture explains construction of trie using keys given in the example below, root / \\ \\ t a b | | | h n y | | \\ | e s y e / | | i r w | | | r e e | r In the picture, every character is of type trie_node_t . For example, the root is of type trie_node_t , and it\u2019s children a , b and t are filled, all other nodes of root will be NULL. Similarly, \u201ca\u201d at the next level is having only one child (\u201cn\u201d), all other children are NULL. The leaf nodes are in blue. Insert and search costs O(key_length) , however the memory requirements of Trie is O(ALPHABET_SIZE * key_length * N) where N is number of keys in Trie. There are efficient representation of trie nodes (e.g. compressed trie, ternary search tree , etc.) to minimize memory requirements of trie.","title":"Searching"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0hash\u76f8\u5173\u5185\u5bb9\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/#_1","text":"\u672c\u7ae0\u63cf\u8ff0hash\u76f8\u5173\u5185\u5bb9\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/application-of-hash/","text":"liblex Symbol table \u5728python\u4e2d\u6709hashable\u7684\u6982\u5ff5 TODO https://leetcode-cn.com/problems/permutations-ii/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liwe-2/ hash\u7528\u4e8e\u53bb\u91cd https://stackoverflow.com/questions/3435616/c-string-compare-vs-hash-compare https://stackoverflow.com/questions/10534937/comparing-long-strings-by-their-hashes https://en.wikipedia.org/wiki/Rolling_hash https://cs.stackexchange.com/questions/51933/string-comparison-vs-hashing https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm Hashing - The Greatest Idea In Programming https://www.i-programmer.info/babbages-bag/479-hashing.html https://www.i-programmer.info/babbages-bag/479-hashing.html?start=1 Applications of Hashing https://www.geeksforgeeks.org/applications-of-hashing/ python\u4e2d\u7684symbol table\u662f\u4f7f\u7528\u7684hash \u5417\uff1f","title":"Application of hash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/application-of-hash/#todo","text":"https://leetcode-cn.com/problems/permutations-ii/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liwe-2/ hash\u7528\u4e8e\u53bb\u91cd https://stackoverflow.com/questions/3435616/c-string-compare-vs-hash-compare https://stackoverflow.com/questions/10534937/comparing-long-strings-by-their-hashes https://en.wikipedia.org/wiki/Rolling_hash https://cs.stackexchange.com/questions/51933/string-comparison-vs-hashing https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm","title":"TODO"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/application-of-hash/#hashing#-#the#greatest#idea#in#programming","text":"https://www.i-programmer.info/babbages-bag/479-hashing.html https://www.i-programmer.info/babbages-bag/479-hashing.html?start=1","title":"Hashing - The Greatest Idea In Programming"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/application-of-hash/#applications#of#hashing","text":"https://www.geeksforgeeks.org/applications-of-hashing/","title":"Applications of Hashing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/application-of-hash/#pythonsymbol#tablehash","text":"","title":"python\u4e2d\u7684symbol table\u662f\u4f7f\u7528\u7684hash \u5417\uff1f"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/geeksforgeeks-Top-20-Hashing-Technique-based-Interview-Questions/","text":"","title":"geeksforgeeks Top 20 Hashing Technique based Interview Questions"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Checksum/VS-check-sum-VS-cryptographic-hash/","text":"","title":"VS check sum VS cryptographic hash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Checksum/wikipedia-Checksum/","text":"Checksum A checksum is a small-sized datum derived from a block of digital data for the purpose of detecting errors that may have been introduced during its transmission or storage . By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity . The procedure which generates this checksum is called a checksum function or checksum algorithm . Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions , which may be used to detect many data corruption errors and verify overall data integrity ; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted. NOTE: Hash code can be used as checksum. Check digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers , bank account numbers, computer words , single bytes , etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases. \u8865\u5145 What Is a Checksum (and Why Should You Care)? Git Has Integrity \u5728 Pro Git book \u7684 1.3 Getting Started - What is Git? \u8282\u4e2d\u63cf\u8ff0\u4e86git\u4e2dchecksum\u7684\u4f7f\u7528\u60c5\u51b5\u3002 Practical Application of Cryptographic Checksums \u603b\u7ed3\u7684\u975e\u5e38\u597d","title":"[Checksum](https://en.wikipedia.org/wiki/Checksum)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Checksum/wikipedia-Checksum/#checksum","text":"A checksum is a small-sized datum derived from a block of digital data for the purpose of detecting errors that may have been introduced during its transmission or storage . By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity . The procedure which generates this checksum is called a checksum function or checksum algorithm . Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions , which may be used to detect many data corruption errors and verify overall data integrity ; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted. NOTE: Hash code can be used as checksum. Check digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers , bank account numbers, computer words , single bytes , etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases.","title":"Checksum"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Checksum/wikipedia-Checksum/#_1","text":"","title":"\u8865\u5145"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Checksum/wikipedia-Checksum/#what#is#a#checksum#and#why#should#you#care","text":"","title":"What Is a Checksum (and Why Should You Care)?"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Checksum/wikipedia-Checksum/#git#has#integrity","text":"\u5728 Pro Git book \u7684 1.3 Getting Started - What is Git? \u8282\u4e2d\u63cf\u8ff0\u4e86git\u4e2dchecksum\u7684\u4f7f\u7528\u60c5\u51b5\u3002","title":"Git Has Integrity"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Checksum/wikipedia-Checksum/#practical#application#of#cryptographic#checksums","text":"\u603b\u7ed3\u7684\u975e\u5e38\u597d","title":"Practical Application of Cryptographic Checksums"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Error-detection-and-correction/Error-detection-and-correction/","text":"Error detection and correction","title":"[Error detection and correction](https://en.wikipedia.org/wiki/Category:Error_detection_and_correction)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Error-detection-and-correction/Error-detection-and-correction/#error#detection#and#correction","text":"","title":"Error detection and correction"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/","text":"Hashing Algorithms to Find Duplicates Finding duplicate files is a hectic task when you have millions of files spread all over your computer. To check if two files are duplicates of each other, we should do a one-to-one check for the suspect files. Even though this is simple for smaller files, performing a one-to-one (byte-to-byte) comparison for large files is a massively time-consuming task. Imagine comparing two gigantic files to check whether they are duplicates, the time and the effort you need to spare for undertaking such a task is probably enough to simply make you give up. An easy way to remove duplicates would be to utilize an all-purpose duplicate cleaner such as the Clone Files Checker . It works on the basis of highly advanced algorithms and saves your time in organizing files. Let\u2019s imagine that we are downloading a large file over the Internet and we need to make sure that the file was not modified by a third-party in the midst of the transmission, which may cause a man-in-the-middle attack. Checking each byte over the Internet would take years to accomplish and also eat up a significant portion of your Internet bandwidth. So, how can we easily compare two files in such manner that is credible yet not at all laborious? There are multiple ways to do so, including Hash Comparison which is turning out to be a highly popular and reliable. What is a hash? Hash is a mathematical function which takes objects as inputs and produces as output a number or a string. These input objects can vary from numbers and strings (text) to large byte streams (files or network data). The output is usually known as the hash of the input. The main properties of a hash function are: Easy to compute Relatively small output Example We can define a hash function to get the hash of the numbers between 0 to 9999. h(x) = x mod 100 In the above-mentioned hash function, you can see that there is a significant probability of getting the same hash (collision) for two different inputs. (i.e. 1 and 1001). So, it is always good to have a better hash function with fewer collisions, which makes it difficult to find two inputs which give the same output. While hashing is used in many applications such as hash-tables (data structure), compression and encryption, it is also used to generate checksums of files, network data and many other input types. Checksum The checksum is a small sized datum, generated by applying a hash function on a large chunk of data. This hash function should have a minimum rate of collisions such that the hashes for different inputs are almost unique. That means, getting the same hash for different inputs is nearly impossible in practice. These checksums are used to verify if a segment of data has been modified. The checksum (from a known hash function) of received data can be compared with the checksum provided by the sender to verify the purity of the segment. That is how all the data is verified in TCP/IP protocols. In this way, if we generate two checksums for two files, we can declare that the two files aren\u2019t duplicates if the checksums are different. If the checksums are equal, we can claim that the files are identical, considering the fact that getting the same hash for two different files is almost impossible. And many websites provide hashes of the files at the download pages, especially when the files are located on different servers. In such a scenario, the user can verify the authenticity of a file by comparing the provided hash with the one he generated using the downloaded file. There are various hashing functions that are used to generate checksums. Here are some popular ones. Name Output size MD5 128bits SHA-1 160bits SHA-256 256bits SHA-512 512bits MD5 MD5 is a popular hashing function which was initially used for cryptographic purposes. Even though this generates 128-bit output, many people no longer use it due to a host of vulnerabilities that later surfaced. However, it still can be used as a checksum to verify the authenticity of a file (or a data segment) to detect unintentional changes/corruptions. SHA-1 SHA 1 ( Secure Hash Algorithm 1 ) is a cryptographic hashing function which generates a 160-bit output. This is no longer considered as secure after many attacks, web browsers already have stopped accepting SLL Certificates based on SHA-1 . The later versions of SHA checksums ( SHA-256 and SHA-512 ) are more secure as they incorporate vast improvements and hence don\u2019t contain any vulnerabilities as oft he time this article was published. Features of a strong hash function Should be open \u2013 Everybody should know how the hashing is performed Easy to generate \u2013 Should not take too much time to generate the output Fewer collisions \u2013 The probability of getting the same hash for two inputs should be near to zero Hard to back-trace \u2013 Given a hash, recovering the original input should not be possible Hash MD5 Check \u2013 The Best Method to Find Duplicate Files One of the biggest problems of every computer user is the piling up of junk in their system. This happens due to many factors, and the creation of duplicate files is one of them. Duplicate files decrease the performance of a system drastically. They aren\u2019t created by the user intentionally, but nevertheless, they will clutter the hard drive and cause disorganization of data without the user even knowing about them. Luckily enough, some genius people have worked endlessly to come up with unique state-of-the-art methods that make scanning for and getting rid of duplicate data of all kinds a very straightforward matter. So let\u2019s jump into some detail of the mechanism that is employed to scan for duplicates and then we shall move on to a genius solution of its own kind. But let\u2019s just introduce it to you before we go into more details. Clone Files Checker is its name, and its job is to go after those pesky duplicates on Windows as well as Mac systems! Why Use MD5 Check to find Duplicate Files? The history of MD5 starts from 1991 when the systems had already been upgraded to 128-bit data transfer rates and a security feature that could encrypt 128-bit data slot was very much needed. MD5 was initially used for cryptographic purposes only, but the discovery of several vulnerabilities meant it was put out of service soon after. However, the algorithm is still used to verify the authenticity of a file, and to know if it has been through an event of data corruption. The benefits of MD5 check over other cryptic algorithms is that it can be implemented faster than the other cryptic algorithms and provides an impressive performance increase for verifying data in comparison with SHA1 , SHA2 and SHA3 cryptic software. However, MD5 is vulnerable to collision resistance. But, the good point is that in finding duplicate files, collision resistance is not really an issue. Collision resistance refers to two inputs providing the same outputs when hashed. But A good MD5 hash program will work in unison with the file size, type, and the last byte value. This is because it is common for various parameters of some files to be similar/ identical/ different (size, name etc) but the hash value will always be the same (provided these files are duplicates). This absolutely does away with the chances of deleting a file which isn\u2019t a duplicate while cleaning up duplicate data. Therefore, MD5 is a huge blessing while on the lookout for duplicates. Now let\u2019s take a brief look at a software that uses MD5 to weed out duplicates swiftly and accurately. Clone Files Checker This software is simply put, a death sentence for duplicate data. Be it duplicate data of any kind, located on your local computer, external hard drive or even your cloud account, this software will dive into great details and make exceptional use of MD5 Check to bring up a list of all the duplicate data that had eaten into your previous hard drive and was beginning to slow down your system. From then onwards, it is pretty simple for you to choose between deleting the duplicates permanently or moving them to a separate folder. But it is all due to the goodness offered by MD5 Check and the genius brains behind Clone Files Checker that you can be assured to a swift and accurate scan that will help free up a big quantum of your hard drive. C++ Coding Example, Using Hash Algorithm to Remove Duplicate Number by O(n)","title":"[Hashing Algorithms to Find Duplicates](https://www.clonefileschecker.com/blog/hashing-algorithms-to-find-duplicates/)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#hashing#algorithms#to#find#duplicates","text":"Finding duplicate files is a hectic task when you have millions of files spread all over your computer. To check if two files are duplicates of each other, we should do a one-to-one check for the suspect files. Even though this is simple for smaller files, performing a one-to-one (byte-to-byte) comparison for large files is a massively time-consuming task. Imagine comparing two gigantic files to check whether they are duplicates, the time and the effort you need to spare for undertaking such a task is probably enough to simply make you give up. An easy way to remove duplicates would be to utilize an all-purpose duplicate cleaner such as the Clone Files Checker . It works on the basis of highly advanced algorithms and saves your time in organizing files. Let\u2019s imagine that we are downloading a large file over the Internet and we need to make sure that the file was not modified by a third-party in the midst of the transmission, which may cause a man-in-the-middle attack. Checking each byte over the Internet would take years to accomplish and also eat up a significant portion of your Internet bandwidth. So, how can we easily compare two files in such manner that is credible yet not at all laborious? There are multiple ways to do so, including Hash Comparison which is turning out to be a highly popular and reliable.","title":"Hashing Algorithms to Find Duplicates"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#what#is#a#hash","text":"Hash is a mathematical function which takes objects as inputs and produces as output a number or a string. These input objects can vary from numbers and strings (text) to large byte streams (files or network data). The output is usually known as the hash of the input. The main properties of a hash function are: Easy to compute Relatively small output Example We can define a hash function to get the hash of the numbers between 0 to 9999. h(x) = x mod 100 In the above-mentioned hash function, you can see that there is a significant probability of getting the same hash (collision) for two different inputs. (i.e. 1 and 1001). So, it is always good to have a better hash function with fewer collisions, which makes it difficult to find two inputs which give the same output. While hashing is used in many applications such as hash-tables (data structure), compression and encryption, it is also used to generate checksums of files, network data and many other input types.","title":"What is a hash?"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#checksum","text":"The checksum is a small sized datum, generated by applying a hash function on a large chunk of data. This hash function should have a minimum rate of collisions such that the hashes for different inputs are almost unique. That means, getting the same hash for different inputs is nearly impossible in practice. These checksums are used to verify if a segment of data has been modified. The checksum (from a known hash function) of received data can be compared with the checksum provided by the sender to verify the purity of the segment. That is how all the data is verified in TCP/IP protocols. In this way, if we generate two checksums for two files, we can declare that the two files aren\u2019t duplicates if the checksums are different. If the checksums are equal, we can claim that the files are identical, considering the fact that getting the same hash for two different files is almost impossible. And many websites provide hashes of the files at the download pages, especially when the files are located on different servers. In such a scenario, the user can verify the authenticity of a file by comparing the provided hash with the one he generated using the downloaded file. There are various hashing functions that are used to generate checksums. Here are some popular ones. Name Output size MD5 128bits SHA-1 160bits SHA-256 256bits SHA-512 512bits","title":"Checksum"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#md5","text":"MD5 is a popular hashing function which was initially used for cryptographic purposes. Even though this generates 128-bit output, many people no longer use it due to a host of vulnerabilities that later surfaced. However, it still can be used as a checksum to verify the authenticity of a file (or a data segment) to detect unintentional changes/corruptions.","title":"MD5"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#sha-1","text":"SHA 1 ( Secure Hash Algorithm 1 ) is a cryptographic hashing function which generates a 160-bit output. This is no longer considered as secure after many attacks, web browsers already have stopped accepting SLL Certificates based on SHA-1 . The later versions of SHA checksums ( SHA-256 and SHA-512 ) are more secure as they incorporate vast improvements and hence don\u2019t contain any vulnerabilities as oft he time this article was published.","title":"SHA-1"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#features#of#a#strong#hash#function","text":"Should be open \u2013 Everybody should know how the hashing is performed Easy to generate \u2013 Should not take too much time to generate the output Fewer collisions \u2013 The probability of getting the same hash for two inputs should be near to zero Hard to back-trace \u2013 Given a hash, recovering the original input should not be possible","title":"Features of a strong hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#hash#md5#check#the#best#method#to#find#duplicate#files","text":"One of the biggest problems of every computer user is the piling up of junk in their system. This happens due to many factors, and the creation of duplicate files is one of them. Duplicate files decrease the performance of a system drastically. They aren\u2019t created by the user intentionally, but nevertheless, they will clutter the hard drive and cause disorganization of data without the user even knowing about them. Luckily enough, some genius people have worked endlessly to come up with unique state-of-the-art methods that make scanning for and getting rid of duplicate data of all kinds a very straightforward matter. So let\u2019s jump into some detail of the mechanism that is employed to scan for duplicates and then we shall move on to a genius solution of its own kind. But let\u2019s just introduce it to you before we go into more details. Clone Files Checker is its name, and its job is to go after those pesky duplicates on Windows as well as Mac systems!","title":"Hash MD5 Check \u2013 The Best Method to Find Duplicate Files"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#why#use#md5#check#to#find#duplicate#files","text":"The history of MD5 starts from 1991 when the systems had already been upgraded to 128-bit data transfer rates and a security feature that could encrypt 128-bit data slot was very much needed. MD5 was initially used for cryptographic purposes only, but the discovery of several vulnerabilities meant it was put out of service soon after. However, the algorithm is still used to verify the authenticity of a file, and to know if it has been through an event of data corruption. The benefits of MD5 check over other cryptic algorithms is that it can be implemented faster than the other cryptic algorithms and provides an impressive performance increase for verifying data in comparison with SHA1 , SHA2 and SHA3 cryptic software. However, MD5 is vulnerable to collision resistance. But, the good point is that in finding duplicate files, collision resistance is not really an issue. Collision resistance refers to two inputs providing the same outputs when hashed. But A good MD5 hash program will work in unison with the file size, type, and the last byte value. This is because it is common for various parameters of some files to be similar/ identical/ different (size, name etc) but the hash value will always be the same (provided these files are duplicates). This absolutely does away with the chances of deleting a file which isn\u2019t a duplicate while cleaning up duplicate data. Therefore, MD5 is a huge blessing while on the lookout for duplicates. Now let\u2019s take a brief look at a software that uses MD5 to weed out duplicates swiftly and accurately.","title":"Why Use MD5 Check to find Duplicate Files?"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#clone#files#checker","text":"This software is simply put, a death sentence for duplicate data. Be it duplicate data of any kind, located on your local computer, external hard drive or even your cloud account, this software will dive into great details and make exceptional use of MD5 Check to bring up a list of all the duplicate data that had eaten into your previous hard drive and was beginning to slow down your system. From then onwards, it is pretty simple for you to choose between deleting the duplicates permanently or moving them to a separate folder. But it is all due to the goodness offered by MD5 Check and the genius brains behind Clone Files Checker that you can be assured to a swift and accurate scan that will help free up a big quantum of your hard drive.","title":"Clone Files Checker"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/Hashing-Algorithms-to-Find-Duplicates/#c#coding#example#using#hash#algorithm#to#remove#duplicate#number#by#on","text":"","title":"C++ Coding Example, Using Hash Algorithm to Remove Duplicate Number by O(n)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/wikipedia-Fingerprint%28computing%29/","text":"","title":"wikipedia Fingerprint(computing)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/wikipedia-Rabin-fingerprint/","text":"Rabin fingerprint","title":"[Rabin fingerprint](https://en.wikipedia.org/wiki/Rabin_fingerprint)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Fingerprint/wikipedia-Rabin-fingerprint/#rabin#fingerprint","text":"","title":"Rabin fingerprint"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Summary-of-Hash-based-data-structures/","text":"Category:Hash based data structures","title":"[Category:Hash based data structures](https://en.wikipedia.org/wiki/Category:Hash_based_data_structures)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Summary-of-Hash-based-data-structures/#categoryhash#based#data#structures","text":"","title":"Category:Hash based data structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table-Q%26A/","text":"Hash table Q&A Why array size is a power of two ? \u5728\u7ef4\u57fa\u767e\u79d1 Hash table#Hashing \u4e2d\u7ed9\u51fa\u4e86\u89e3\u7b54\u3002","title":"Hash table Q&A"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table-Q%26A/#hash#table#qa","text":"","title":"Hash table Q&amp;A"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table-Q%26A/#why#array#size#is#a#power#of#two","text":"\u5728\u7ef4\u57fa\u767e\u79d1 Hash table#Hashing \u4e2d\u7ed9\u51fa\u4e86\u89e3\u7b54\u3002","title":"Why  array size is a power of two?"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/","text":"Hash table Hash table\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7\u6587\u7ae0\u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002 \u7ef4\u57fa\u767e\u79d1 Hash table In computing , a hash table ( hash map ) is a data structure that implements an associative array abstract data type , a structure that can map keys to values . A hash table uses a hash function to compute an index into an array of buckets or slots , from which the desired value can be found. NOTE: \u4e0a\u8ff0\u672f\u8bed\u975e\u5e38\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u7ecf\u5e38\u88ab\u63d0\u53ca\u3002 Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key. Such collisions must be accommodated in some way. NOTE: \u5173\u4e8eperfect hash function\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Perfect hash function \u3002 In a well-dimensioned hash table, the average cost (number of instructions ) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at ( amortized ) constant average cost per operation. NOTE: \u5982\u4f55\u7406\u89e3well-dimensioned \uff1f\u6211\u89c9\u5f97\u5b83\u7684\u610f\u601d\u662f \u201c\u5927\u5c0f\u5408\u9002\u7684\u201d In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software , particularly for associative arrays , database indexing , caches , and sets . Hashing Main article: Hash function NOTE: \u8fd9\u4e00\u6bb5\u5173\u4e8ehash\u7684\u4ecb\u7ecd\u662f\u6bd4\u8f83\u7cbe\u7b80\u7684 Choosing a hash function A basic requirement is that the function should provide a uniform distribution \uff08\u79bb\u6563\u5747\u5300\u5206\u5e03\uff09of hash values. The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two . Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number .[ 8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function. SUMMARY : \u5728\u8bbe\u8ba1hash function\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u8fd8\u9700\u8981\u8003\u8651\u7684\u662fhash\u503c\u662f\u5426\u9700\u8981\u5728table size\u8303\u56f4\u5185\u5747\u5300\u5206\u5e03\uff1b\u4ee5\u53ca\u5f53table size\u53d8\u66f4\u7684\u65f6\u5019\u6240\u9700\u8981\u8003\u8651\u7684\u4e00\u7cfb\u5217\u95ee\u9898\uff1b For open addressing schemes, the hash function should also avoid clustering , the mapping of two or more keys to consecutive\uff08\u8fde\u7eed\u7684\uff09 slots. Such clustering may cause the lookup cost to skyrocket\uff08\u98de\u6da8\uff09, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[ 3] is claimed to have particularly poor clustering behavior.[ 8] Cryptographic hash functions are believed to provide good hash functions for any table size , either by modulo reduction or by bit masking [ citation needed ]. They may also be appropriate if there is a risk of malicious\uff08\u6076\u6bd2\u7684\uff09 users trying to sabotage \uff08\u84c4\u610f\u7834\u574f\uff09a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function ). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[ citation needed ] SUMMARY : \u663e\u7136\uff0c Cryptographic hash functions are believed to provide good hash functions for any table size , either by modulo reduction or by bit masking [ citation needed ]\u7684\u8fd9\u4e2a\u7279\u6027\u662f\u975e\u5e38\u597d\u7684\uff0c\u5b83\u5141\u8bb8\u4f7f\u7528\u6237\u65e0\u9700\u8003\u8651\u6539\u53d8table size\u6240\u5e26\u6765\u7684\u5404\u79cd\u95ee\u9898\uff1b Key statistics A critical statistic for a hash table is the load factor , defined as $ {\\text{load factor}}={\\frac {n}{k}}, $ where n is the number of entries occupied in the hash table. k is the number of buckets\uff08\u5176\u5b9e\u5c31\u662ftable size\uff09. NOTE: load factor\u662f\u4e00\u4e2a\u975e\u5e38\u4e3b\u8981\u7684\u6982\u5ff5 Collision resolution Hash collisions are practically unavoidable when hashing a random subset of a large set of possible keys. For example, if 2,450 keys are hashed into a million buckets, even with a perfectly uniform random distribution, according to the birthday problem there is approximately a 95% chance of at least two of the keys being hashed to the same slot. Therefore, almost all hash table implementations have some collision resolution strategy to handle such events. Some common strategies are described below. All these methods require that the keys (or pointers to them) be stored in the table, together with the associated values. Separate chaining In the method known as separate chaining , each bucket is independent, and has some sort of list of entries with the same index. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation. In a good hash table, each bucket has zero or one entries, and sometimes two or three, but rarely more than that. Therefore, structures that are efficient in time and space for these cases are preferred. Structures that are efficient for a fairly large number of entries per bucket are not needed or desirable. If these cases happen often, the hashing function needs to be fixed.[ citation needed ] Separate chaining with linked lists Chained hash tables with linked lists are popular because they require only basic data structures with simple algorithms, and can use simple hash functions that are unsuitable for other methods.[ citation needed ] The cost of a table operation is that of scanning the entries of the selected bucket for the desired key. If the distribution of keys is sufficiently uniform , the average cost of a lookup depends only on the average number of keys per bucket\u2014that is, it is roughly proportional to the load factor. For this reason, chained hash tables remain effective even when the number of table entries n is much higher than the number of slots. For example, a chained hash table with 1000 slots and 10,000 stored keys (load factor 10) is five to ten times slower than a 10,000-slot table (load factor 1); but still 1000 times faster than a plain sequential list. For separate-chaining, the worst-case scenario is when all entries are inserted into the same bucket, in which case the hash table is ineffective and the cost is that of searching the bucket data structure. If the latter is a linear list, the lookup procedure may have to scan all its entries, so the worst-case cost is proportional to the number n of entries in the table. The bucket chains are often searched sequentially using the order the entries were added to the bucket. If the load factor is large and some keys are more likely to come up than others, then rearranging the chain with a move-to-front heuristic may be effective. More sophisticated data structures, such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance even in a worst-case scenario. However, using a larger table and/or a better hash function may be even more effective in those cases.[ citation needed ] Chained hash tables also inherit the disadvantages of linked lists. When storing small keys and values, the space overhead of the next pointer in each entry record can be significant. An additional disadvantage is that traversing a linked list has poor cache performance , making the processor cache ineffective. Separate chaining with list head cells Separate chaining with other structures Open addressing Main article: Open addressing Dynamic resizing When an insert is made such that the number of entries in a hash table exceeds the product of the load factor and the current capacity then the hash table will need to be rehashed .[ 9] Rehashing includes increasing the size of the underlying data structure[ 9] and mapping existing items to new bucket locations. In some implementations, if the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.[ 9] To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table\u2014followed by a rehash\u2014when items are deleted. From the point of space\u2013time tradeoffs, this operation is similar to the deallocation in dynamic arrays. Resizing by copying all entries A common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold r*max. Then a new larger table is allocated , each entry is removed from the old table, and inserted into the new table. When all entries have been removed from the old table then the old table is returned to the free storage pool. Likewise, when the load factor falls below a second threshold *r min, all entries are moved to a new smaller table. For hash tables that shrink and grow frequently, the resizing downward can be skipped entirely. In this case, the table size is proportional to the maximum number of entries that ever were in the hash table at one time, rather than the current number. The disadvantage is that memory usage will be higher, and thus cache behavior may be worse. For best control, a \"shrink-to-fit\" operation can be provided that does this only on request. If the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, amortized over all insert and delete operations, is still a constant, independent of the number of entries n and of the number m of operations performed. For example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If m elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most m \u2212 1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation. Alternatives to all-at-once rehashing Some hash table implementations, notably in real-time systems , cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually \uff08\u6e10\u8fdb\u5f0f\uff09. Disk-based hash tables almost always use some alternative to all-at-once rehashing, since the cost of rebuilding the entire table on disk would be too high. Incremental resizing One alternative to enlarging the table all at once is to perform the rehashing gradually: During the resize, allocate the new hash table, but keep the old table unchanged. In each lookup or delete operation, check both tables. Perform insertion operations only in the new table. At each insertion also move r elements from the old table to the new table. When all elements are removed from the old table, deallocate it. To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it is necessary to increase the size of the table by a factor of at least ( r + 1)/ r during resizing. Monotonic keys If it is known that key values will always increase (or decrease) monotonically , then a variation of consistent hashing can be achieved by keeping a list of the single most recent key value at each hash table resize operation. Upon lookup, keys that fall in the ranges defined by these list entries are directed to the appropriate hash function\u2014and indeed hash table\u2014both of which can be different for each range. Since it is common to grow the overall number of entries by doubling, there will only be O(log( N )) ranges to check, and binary search time for the redirection would be O(log(log( N ))). As with consistent hashing, this approach guarantees that any key's hash, once issued, will never change, even when the hash table is later grown. Linear hashing Linear hashing [ 25] is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible lookup functions. Hashing for distributed hash tables Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. Such hash functions are prevalent\uff08\u6d41\u884c\u7684\uff09 in disk-based and distributed hash tables , where rehashing is prohibitively costly. The problem of designing a hash such that most values do not change when the table is resized is known as the distributed hash table problem. The four most popular approaches are rendezvous hashing , consistent hashing , the content addressable network algorithm, and Kademlia distance. Performance analysis In the simplest model, the hash function is completely unspecified and the table does not resize. With an ideal hash function, a table of size $ k $ with open addressing has no collisions and holds up to $ k $ elements with a single comparison for successful lookup, while a table of size $ k $ with chaining and $ n $ keys has the minimum $ max(0,n-k) $ collisions and $ \\Theta (1+{\\frac {n}{k}}) $ comparisons for lookup. With the worst possible hash function, every insertion causes a collision, and hash tables degenerate to linear search, with $ \\Theta (n) $ amortized comparisons per insertion and up to $ n $ comparisons for a successful lookup. Adding rehashing to this model is straightforward. As in a dynamic array , geometric resizing by a factor of $ b $ implies that only $ {\\frac {n}{b^{i}}} $keys are inserted $ i $ or more times, so that the total number of insertions is bounded above by $ {\\frac {bn}{b-1}} $, which is $ \\Theta (n) $. By using rehashing to maintain $ n<k $, tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function. In more realistic models, the hash function is a random variable over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is uniform , the assumption is called \"simple uniform hashing\" and it can be shown that hashing with chaining requires $ \\Theta (1+{\\frac {n}{k}}) $ comparisons on average for an unsuccessful lookup, and hashing with open addressing requires $ \\Theta \\left({\\frac {1}{1-n/k}}\\right) .[[26\\]](https://en.wikipedia.org/wiki/Hash_table#cite_note-26) Both these bounds are constant, if we maintain ' .[[26\\]](https://en.wikipedia.org/wiki/Hash_table#cite_note-26) Both these bounds are constant, if we maintain ' {\\frac {n}{k}}<c $ using table resizing, where $ c $ is a fixed constant less than 1. Features Advantages The main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted in advance, so that the bucket array can be allocated once with the optimum size and never resized. If the set of key-value pairs is fixed and known ahead of time (so insertions and deletions are not allowed), one may reduce the average lookup cost by a careful choice of the hash function, bucket table size, and internal data structures. In particular, one may be able to devise a hash function that is collision-free, or even perfect. In this case the keys need not be stored in the table. Drawbacks Although operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small. (However, in some cases the high cost of computing the hash function can be mitigated by saving the hash value together with the key.) For certain string processing applications, such as spell-checking , hash tables may be less efficient than tries , finite automata , or Judy arrays . Also, if there are not too many possible keys to store\u2014that is, if each key can be represented by a small enough number of bits\u2014then, instead of a hash table, one may use the key directly as the index into an array of values. Note that there are no collisions in this case. The entries stored in a hash table can be enumerated efficiently (at constant cost per entry), but only in some pseudo-random order. Therefore, there is no efficient way to locate an entry whose key is nearest to a given key. Listing all n entries in some specific order generally requires a separate sorting step, whose cost is proportional to log( n ) per entry. In comparison, ordered search trees have lookup and insertion cost proportional to log( n ), but allow finding the nearest key at about the same cost, and ordered enumeration of all entries at constant cost per entry. If the keys are not stored (because the hash function is collision-free), there may be no easy way to enumerate the keys that are present in the table at any given moment. Although the average cost per operation is constant and fairly small, the cost of a single operation may be quite high. In particular, if the hash table uses dynamic resizing , an insertion or deletion operation may occasionally take time proportional to the number of entries. This may be a serious drawback in real-time or interactive applications. Hash tables in general exhibit poor locality of reference \u2014that is, the data to be accessed is distributed seemingly at random in memory. Because hash tables cause access patterns that jump around, this can trigger microprocessor cache misses that cause long delays. Compact data structures such as arrays searched with linear search may be faster, if the table is relatively small and keys are compact. The optimal performance point varies from system to system. Hash tables become quite inefficient when there are many collisions. While extremely uneven hash distributions are extremely unlikely to arise by chance, a malicious adversary with knowledge of the hash function may be able to supply information to a hash that creates worst-case behavior by causing excessive collisions, resulting in very poor performance, e.g., a denial of service attack .[ 27] [ 28] [ 29] In critical applications, a data structure with better worst-case guarantees can be used; however, universal hashing \u2014a randomized algorithm that prevents the attacker from predicting which inputs cause worst-case behavior\u2014may be preferable.[ 30] The hash function used by the hash table in the Linux routing table cache was changed with Linux version 2.4.2 as a countermeasure against such attacks.[ 31] Uses Associative arrays Main article: Associative array Database indexing Hash tables may also be used as disk -based data structures and database indices (such as in dbm ) although B-trees are more popular in these applications. In multi-node database systems, hash tables are commonly used to distribute rows amongst nodes, reducing network traffic for hash joins. Caches Main article: Cache (computing) Hash tables can be used to implement caches , auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries\u2014usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value. Sets Besides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not. Those structures can therefore be used to implement a set data structure , which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets. Object representation Several dynamic languages, such as Perl , Python , JavaScript , Lua , and Ruby , use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method. Unique data representation Main article: String interning Hash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single string pool implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in Lisp interpreters under the name hash consing , and can be used with many other kinds of data ( expression trees in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.). Transposition table Main article: Transposition table Implemtentation An Analysis of Hash Map Implementations in Popular Languages CPython dict https://github.com/zpoint/CPython-Internals/blob/master/BasicObject/dict/dict.md https://github.com/python/cpython/blob/master/Objects/dictnotes.txt","title":"Hash table"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#hash#table","text":"Hash table\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7\u6587\u7ae0\u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002","title":"Hash table"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#hash#table_1","text":"In computing , a hash table ( hash map ) is a data structure that implements an associative array abstract data type , a structure that can map keys to values . A hash table uses a hash function to compute an index into an array of buckets or slots , from which the desired value can be found. NOTE: \u4e0a\u8ff0\u672f\u8bed\u975e\u5e38\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u7ecf\u5e38\u88ab\u63d0\u53ca\u3002 Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key. Such collisions must be accommodated in some way. NOTE: \u5173\u4e8eperfect hash function\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Perfect hash function \u3002 In a well-dimensioned hash table, the average cost (number of instructions ) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at ( amortized ) constant average cost per operation. NOTE: \u5982\u4f55\u7406\u89e3well-dimensioned \uff1f\u6211\u89c9\u5f97\u5b83\u7684\u610f\u601d\u662f \u201c\u5927\u5c0f\u5408\u9002\u7684\u201d In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software , particularly for associative arrays , database indexing , caches , and sets .","title":"\u7ef4\u57fa\u767e\u79d1Hash table"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#hashing","text":"Main article: Hash function NOTE: \u8fd9\u4e00\u6bb5\u5173\u4e8ehash\u7684\u4ecb\u7ecd\u662f\u6bd4\u8f83\u7cbe\u7b80\u7684","title":"Hashing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#choosing#a#hash#function","text":"A basic requirement is that the function should provide a uniform distribution \uff08\u79bb\u6563\u5747\u5300\u5206\u5e03\uff09of hash values. The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two . Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number .[ 8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function. SUMMARY : \u5728\u8bbe\u8ba1hash function\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u8fd8\u9700\u8981\u8003\u8651\u7684\u662fhash\u503c\u662f\u5426\u9700\u8981\u5728table size\u8303\u56f4\u5185\u5747\u5300\u5206\u5e03\uff1b\u4ee5\u53ca\u5f53table size\u53d8\u66f4\u7684\u65f6\u5019\u6240\u9700\u8981\u8003\u8651\u7684\u4e00\u7cfb\u5217\u95ee\u9898\uff1b For open addressing schemes, the hash function should also avoid clustering , the mapping of two or more keys to consecutive\uff08\u8fde\u7eed\u7684\uff09 slots. Such clustering may cause the lookup cost to skyrocket\uff08\u98de\u6da8\uff09, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[ 3] is claimed to have particularly poor clustering behavior.[ 8] Cryptographic hash functions are believed to provide good hash functions for any table size , either by modulo reduction or by bit masking [ citation needed ]. They may also be appropriate if there is a risk of malicious\uff08\u6076\u6bd2\u7684\uff09 users trying to sabotage \uff08\u84c4\u610f\u7834\u574f\uff09a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function ). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[ citation needed ] SUMMARY : \u663e\u7136\uff0c Cryptographic hash functions are believed to provide good hash functions for any table size , either by modulo reduction or by bit masking [ citation needed ]\u7684\u8fd9\u4e2a\u7279\u6027\u662f\u975e\u5e38\u597d\u7684\uff0c\u5b83\u5141\u8bb8\u4f7f\u7528\u6237\u65e0\u9700\u8003\u8651\u6539\u53d8table size\u6240\u5e26\u6765\u7684\u5404\u79cd\u95ee\u9898\uff1b","title":"Choosing a hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#key#statistics","text":"A critical statistic for a hash table is the load factor , defined as $ {\\text{load factor}}={\\frac {n}{k}}, $ where n is the number of entries occupied in the hash table. k is the number of buckets\uff08\u5176\u5b9e\u5c31\u662ftable size\uff09. NOTE: load factor\u662f\u4e00\u4e2a\u975e\u5e38\u4e3b\u8981\u7684\u6982\u5ff5","title":"Key statistics"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#collision#resolution","text":"Hash collisions are practically unavoidable when hashing a random subset of a large set of possible keys. For example, if 2,450 keys are hashed into a million buckets, even with a perfectly uniform random distribution, according to the birthday problem there is approximately a 95% chance of at least two of the keys being hashed to the same slot. Therefore, almost all hash table implementations have some collision resolution strategy to handle such events. Some common strategies are described below. All these methods require that the keys (or pointers to them) be stored in the table, together with the associated values.","title":"Collision resolution"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#separate#chaining","text":"In the method known as separate chaining , each bucket is independent, and has some sort of list of entries with the same index. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation. In a good hash table, each bucket has zero or one entries, and sometimes two or three, but rarely more than that. Therefore, structures that are efficient in time and space for these cases are preferred. Structures that are efficient for a fairly large number of entries per bucket are not needed or desirable. If these cases happen often, the hashing function needs to be fixed.[ citation needed ]","title":"Separate chaining"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#separate#chaining#with#linked#lists","text":"Chained hash tables with linked lists are popular because they require only basic data structures with simple algorithms, and can use simple hash functions that are unsuitable for other methods.[ citation needed ] The cost of a table operation is that of scanning the entries of the selected bucket for the desired key. If the distribution of keys is sufficiently uniform , the average cost of a lookup depends only on the average number of keys per bucket\u2014that is, it is roughly proportional to the load factor. For this reason, chained hash tables remain effective even when the number of table entries n is much higher than the number of slots. For example, a chained hash table with 1000 slots and 10,000 stored keys (load factor 10) is five to ten times slower than a 10,000-slot table (load factor 1); but still 1000 times faster than a plain sequential list. For separate-chaining, the worst-case scenario is when all entries are inserted into the same bucket, in which case the hash table is ineffective and the cost is that of searching the bucket data structure. If the latter is a linear list, the lookup procedure may have to scan all its entries, so the worst-case cost is proportional to the number n of entries in the table. The bucket chains are often searched sequentially using the order the entries were added to the bucket. If the load factor is large and some keys are more likely to come up than others, then rearranging the chain with a move-to-front heuristic may be effective. More sophisticated data structures, such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance even in a worst-case scenario. However, using a larger table and/or a better hash function may be even more effective in those cases.[ citation needed ] Chained hash tables also inherit the disadvantages of linked lists. When storing small keys and values, the space overhead of the next pointer in each entry record can be significant. An additional disadvantage is that traversing a linked list has poor cache performance , making the processor cache ineffective.","title":"Separate chaining with linked lists"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#separate#chaining#with#list#head#cells","text":"","title":"Separate chaining with list head cells"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#separate#chaining#with#other#structures","text":"","title":"Separate chaining with other structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#open#addressing","text":"Main article: Open addressing","title":"Open addressing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#dynamic#resizing","text":"When an insert is made such that the number of entries in a hash table exceeds the product of the load factor and the current capacity then the hash table will need to be rehashed .[ 9] Rehashing includes increasing the size of the underlying data structure[ 9] and mapping existing items to new bucket locations. In some implementations, if the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.[ 9] To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table\u2014followed by a rehash\u2014when items are deleted. From the point of space\u2013time tradeoffs, this operation is similar to the deallocation in dynamic arrays.","title":"Dynamic resizing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#resizing#by#copying#all#entries","text":"A common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold r*max. Then a new larger table is allocated , each entry is removed from the old table, and inserted into the new table. When all entries have been removed from the old table then the old table is returned to the free storage pool. Likewise, when the load factor falls below a second threshold *r min, all entries are moved to a new smaller table. For hash tables that shrink and grow frequently, the resizing downward can be skipped entirely. In this case, the table size is proportional to the maximum number of entries that ever were in the hash table at one time, rather than the current number. The disadvantage is that memory usage will be higher, and thus cache behavior may be worse. For best control, a \"shrink-to-fit\" operation can be provided that does this only on request. If the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, amortized over all insert and delete operations, is still a constant, independent of the number of entries n and of the number m of operations performed. For example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If m elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most m \u2212 1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation.","title":"Resizing by copying all entries"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#alternatives#to#all-at-once#rehashing","text":"Some hash table implementations, notably in real-time systems , cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually \uff08\u6e10\u8fdb\u5f0f\uff09. Disk-based hash tables almost always use some alternative to all-at-once rehashing, since the cost of rebuilding the entire table on disk would be too high.","title":"Alternatives to all-at-once rehashing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#incremental#resizing","text":"One alternative to enlarging the table all at once is to perform the rehashing gradually: During the resize, allocate the new hash table, but keep the old table unchanged. In each lookup or delete operation, check both tables. Perform insertion operations only in the new table. At each insertion also move r elements from the old table to the new table. When all elements are removed from the old table, deallocate it. To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it is necessary to increase the size of the table by a factor of at least ( r + 1)/ r during resizing.","title":"Incremental resizing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#monotonic#keys","text":"If it is known that key values will always increase (or decrease) monotonically , then a variation of consistent hashing can be achieved by keeping a list of the single most recent key value at each hash table resize operation. Upon lookup, keys that fall in the ranges defined by these list entries are directed to the appropriate hash function\u2014and indeed hash table\u2014both of which can be different for each range. Since it is common to grow the overall number of entries by doubling, there will only be O(log( N )) ranges to check, and binary search time for the redirection would be O(log(log( N ))). As with consistent hashing, this approach guarantees that any key's hash, once issued, will never change, even when the hash table is later grown.","title":"Monotonic keys"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#linear#hashing","text":"Linear hashing [ 25] is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible lookup functions.","title":"Linear hashing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#hashing#for#distributed#hash#tables","text":"Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. Such hash functions are prevalent\uff08\u6d41\u884c\u7684\uff09 in disk-based and distributed hash tables , where rehashing is prohibitively costly. The problem of designing a hash such that most values do not change when the table is resized is known as the distributed hash table problem. The four most popular approaches are rendezvous hashing , consistent hashing , the content addressable network algorithm, and Kademlia distance.","title":"Hashing for distributed hash tables"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#performance#analysis","text":"In the simplest model, the hash function is completely unspecified and the table does not resize. With an ideal hash function, a table of size $ k $ with open addressing has no collisions and holds up to $ k $ elements with a single comparison for successful lookup, while a table of size $ k $ with chaining and $ n $ keys has the minimum $ max(0,n-k) $ collisions and $ \\Theta (1+{\\frac {n}{k}}) $ comparisons for lookup. With the worst possible hash function, every insertion causes a collision, and hash tables degenerate to linear search, with $ \\Theta (n) $ amortized comparisons per insertion and up to $ n $ comparisons for a successful lookup. Adding rehashing to this model is straightforward. As in a dynamic array , geometric resizing by a factor of $ b $ implies that only $ {\\frac {n}{b^{i}}} $keys are inserted $ i $ or more times, so that the total number of insertions is bounded above by $ {\\frac {bn}{b-1}} $, which is $ \\Theta (n) $. By using rehashing to maintain $ n<k $, tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function. In more realistic models, the hash function is a random variable over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is uniform , the assumption is called \"simple uniform hashing\" and it can be shown that hashing with chaining requires $ \\Theta (1+{\\frac {n}{k}}) $ comparisons on average for an unsuccessful lookup, and hashing with open addressing requires $ \\Theta \\left({\\frac {1}{1-n/k}}\\right) .[[26\\]](https://en.wikipedia.org/wiki/Hash_table#cite_note-26) Both these bounds are constant, if we maintain ' .[[26\\]](https://en.wikipedia.org/wiki/Hash_table#cite_note-26) Both these bounds are constant, if we maintain ' {\\frac {n}{k}}<c $ using table resizing, where $ c $ is a fixed constant less than 1.","title":"Performance analysis"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#features","text":"","title":"Features"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#advantages","text":"The main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted in advance, so that the bucket array can be allocated once with the optimum size and never resized. If the set of key-value pairs is fixed and known ahead of time (so insertions and deletions are not allowed), one may reduce the average lookup cost by a careful choice of the hash function, bucket table size, and internal data structures. In particular, one may be able to devise a hash function that is collision-free, or even perfect. In this case the keys need not be stored in the table.","title":"Advantages"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#drawbacks","text":"Although operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small. (However, in some cases the high cost of computing the hash function can be mitigated by saving the hash value together with the key.) For certain string processing applications, such as spell-checking , hash tables may be less efficient than tries , finite automata , or Judy arrays . Also, if there are not too many possible keys to store\u2014that is, if each key can be represented by a small enough number of bits\u2014then, instead of a hash table, one may use the key directly as the index into an array of values. Note that there are no collisions in this case. The entries stored in a hash table can be enumerated efficiently (at constant cost per entry), but only in some pseudo-random order. Therefore, there is no efficient way to locate an entry whose key is nearest to a given key. Listing all n entries in some specific order generally requires a separate sorting step, whose cost is proportional to log( n ) per entry. In comparison, ordered search trees have lookup and insertion cost proportional to log( n ), but allow finding the nearest key at about the same cost, and ordered enumeration of all entries at constant cost per entry. If the keys are not stored (because the hash function is collision-free), there may be no easy way to enumerate the keys that are present in the table at any given moment. Although the average cost per operation is constant and fairly small, the cost of a single operation may be quite high. In particular, if the hash table uses dynamic resizing , an insertion or deletion operation may occasionally take time proportional to the number of entries. This may be a serious drawback in real-time or interactive applications. Hash tables in general exhibit poor locality of reference \u2014that is, the data to be accessed is distributed seemingly at random in memory. Because hash tables cause access patterns that jump around, this can trigger microprocessor cache misses that cause long delays. Compact data structures such as arrays searched with linear search may be faster, if the table is relatively small and keys are compact. The optimal performance point varies from system to system. Hash tables become quite inefficient when there are many collisions. While extremely uneven hash distributions are extremely unlikely to arise by chance, a malicious adversary with knowledge of the hash function may be able to supply information to a hash that creates worst-case behavior by causing excessive collisions, resulting in very poor performance, e.g., a denial of service attack .[ 27] [ 28] [ 29] In critical applications, a data structure with better worst-case guarantees can be used; however, universal hashing \u2014a randomized algorithm that prevents the attacker from predicting which inputs cause worst-case behavior\u2014may be preferable.[ 30] The hash function used by the hash table in the Linux routing table cache was changed with Linux version 2.4.2 as a countermeasure against such attacks.[ 31]","title":"Drawbacks"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#uses","text":"","title":"Uses"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#associative#arrays","text":"Main article: Associative array","title":"Associative arrays"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#database#indexing","text":"Hash tables may also be used as disk -based data structures and database indices (such as in dbm ) although B-trees are more popular in these applications. In multi-node database systems, hash tables are commonly used to distribute rows amongst nodes, reducing network traffic for hash joins.","title":"Database indexing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#caches","text":"Main article: Cache (computing) Hash tables can be used to implement caches , auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries\u2014usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value.","title":"Caches"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#sets","text":"Besides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not. Those structures can therefore be used to implement a set data structure , which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets.","title":"Sets"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#object#representation","text":"Several dynamic languages, such as Perl , Python , JavaScript , Lua , and Ruby , use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method.","title":"Object representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#unique#data#representation","text":"Main article: String interning Hash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single string pool implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in Lisp interpreters under the name hash consing , and can be used with many other kinds of data ( expression trees in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.).","title":"Unique data representation"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#transposition#table","text":"Main article: Transposition table","title":"Transposition table"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#implemtentation","text":"","title":"Implemtentation"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#an#analysis#of#hash#map#implementations#in#popular#languages","text":"","title":"An Analysis of Hash Map Implementations in Popular Languages"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Hash-table/Hash-table/#cpython#dict","text":"https://github.com/zpoint/CPython-Internals/blob/master/BasicObject/dict/dict.md https://github.com/python/cpython/blob/master/Objects/dictnotes.txt","title":"CPython dict"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Authenticated-Data-Structures/","text":"Authenticated Data Structures, Generically \u8bba\u6587\u5176\u4ed6\u5730\u5740\uff1a Authenticated data structures, generically authenticated-data-structures An implementation of generic authenticated data structures in OCaml Authenticated Data Structures, as a Library, for Free!","title":"[Authenticated Data Structures, Generically](http://soc1024.ece.illinois.edu/gpads/)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Authenticated-Data-Structures/#authenticated#data#structures#generically","text":"\u8bba\u6587\u5176\u4ed6\u5730\u5740\uff1a Authenticated data structures, generically","title":"Authenticated Data Structures, Generically"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Authenticated-Data-Structures/#authenticated-data-structures","text":"An implementation of generic authenticated data structures in OCaml","title":"authenticated-data-structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Authenticated-Data-Structures/#authenticated#data#structures#as#a#library#for#free","text":"","title":"Authenticated Data Structures, as a Library, for Free!"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/","text":"\u524d\u8a00 \u662f\u5728\u5b66\u4e60 git \u7684\u65f6\u5019\uff0c\u53d1\u73b0 Merkle tree \u7684\u3002\u5728\u5b66\u4e60 Merkle tree \u4e4b\u524d\uff0c\u5148\u5b66\u4e60 Hash list \u548c Hash chain \uff0c\u4e24\u8005\u662f\u6df1\u523b\u638c\u63e1 Merkle tree \u7684\u57fa\u7840\u3002 Hash list In computer science , a hash list is typically a list of hashes of the data blocks in a file or set of files. NOTE: hash list\u7684\u542b\u4e49\u5176\u5b9e\u975e\u5e38\u7b80\u5355\uff0c\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8bf4\u7684list of hash\uff0c\u4ec5\u6b64\u800c\u5df2\u3002\u5f53\u7136\u53ef\u4ee5\u5728\u5b83\u4e0a\u9762\u518d\u8fdb\u884c\u6269\u5c55\uff0c\u6bd4\u5982\u4e0b\u9762\u5c06\u8981\u4ecb\u7ecd\u7684Root hash\u3002 Root hash Often, an additional hash of the hash list itself (a top hash , also called root hash or master hash ) is used. A hash list with a top hash \u4e0a\u8ff0\u5c31\u662f\u4e00\u4e2a\u5e26\u6709root hash\u7684hash list\u3002 NOTE: \u601d\u8003\uff1aroot hash\u7684\u4ef7\u503c\u4f55\u5728\u5462\uff1f\u5982\u679c\u5c06hash list\u4e2d\u7684\u6bcf\u4e2ahash\u90fd\u5bf9\u5e94\u7684\u662f\u4e00\u4e2adata block\uff0c\u800croot hash\u5219\u5bf9\u5e94\u7684\u662f\u6240\u6709\u7684data block\uff0c\u4e5f\u5c31\u662f\u8bf4\u5b83\u5176\u5b9e\u6240\u5bf9\u5e94\u7684\u662f\u6574\u4f53data\uff0c\u800c\u975e\u5c40\u90e8\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u5c31\u63cf\u8ff0\u7684\u662f\u8fd9\u79cd\u542b\u4e49\u3002 Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash list can be received from any non-trusted source, like any peer in the p2p network. Then the received hash list is checked against the trusted top hash, and if the hash list is damaged or fake, another hash list from another source will be tried until the program finds one that matches the top hash. Applications Hash lists can be used to protect any kind of data stored, handled and transferred in and between computers. An important use of hash lists is to make sure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and to check that the other peers do not \"lie\" and send fake blocks. Usually a cryptographic hash function such as SHA-256 is used for the hashing. If the hash list only needs to protect against unintentional damage unsecured checksums such as CRCs can be used. NOTE: \u4e0a\u8ff0\u5176\u5b9e\u662fhash\u7684\u5e38\u89c1\u7528\u6cd5\uff0c\u5e76\u65e0\u4ec0\u4e48\u7279\u6b8a\u4e4b\u5904\u3002\u4e0b\u9762\u5219\u5f3a\u8c03hash list\u7684\u7279\u6b8a\u4e4b\u5904\u3002 Hash lists are better than a simple hash of the entire file since, in the case of a data block being damaged, this is noticed, and only the damaged block needs to be redownloaded. With only a hash of the file, many undamaged blocks would have to be redownloaded, and the file reconstructed and tested until the correct hash of the entire file is obtained. Hash lists also protect against nodes that try to sabotage by sending fake blocks, since in such a case the damaged block can be acquired from some other source. Hash chain A hash chain is the successive application of a cryptographic hash function to a piece of data. Definition A hash chain is a successive application of a cryptographic hash function $ h $ to a string $ x $. For example, $ h(h(h(h(x)))) $ gives a hash chain of length 4, often denoted $ h^{4}(x) $ NOTE: \u663e\u7136\uff0chash chain\u7684\u5b9a\u4e49\u6240\u5f3a\u8c03\u7684\u662f\u91cd\u590d\u8fed\u4ee3\u5730\u4f7f\u7528hash function\u3002\u663e\u7136\u5b83\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u4e00\u4e2a\u6570\u5b66\u8fc7\u7a0b\u3002 Features \u672c\u6587\u7ed9\u51fa\u7684hash chain\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c \u8fd0\u7528hash chain\u8fd8\u80fd\u591f\u5e26\u6765\u5176\u4ed6\u7684\u975e\u5e38\u4f18\u826f\u7684\u7279\u6027\uff0c\u8fd9\u5c31\u4f7f\u5f97hash chain\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u4e0b\u9762\u5c31\u6765\u63a2\u8ba8\u8fd0\u7528hash chain\u53ef\u4ee5\u5e26\u6765\u7684\u4f18\u826f\u7279\u6027\uff1a Pre-image resistance / one-way \u7531\u4e8ehash chain\u6240\u4f7f\u7528\u7684\u662f cryptographic hash function \uff0c cryptographic hash function \u662f one-way function ,\u6240\u6709hash chain\u5c31\u5177\u5907\u4e86 cryptographic hash function \u7684 Pre-image resistance \u7279\u6027\uff0c\u5373\u6839\u636e\u51fd\u6570\u503c\u65e0\u6cd5\u9006\u5411\u8ba1\u7b97\u51fa\u5b83\u7684\u81ea\u53d8\u91cf\u7684\u503c\uff0c\u8bf4\u7684\u66f4\u52a0\u5f62\u8c61\u4e00\u4e9b\u5c31\u662fhash chain\u53ea\u80fd\u591f\u987a\u63a8\u800c\u4e0d\u80fd\u591f\u9006\u63a8\u3002 \u4e0b\u9762\u8fd9\u6bb5\u8bdd\u662f\u6458\u81ea Hash Chain \u9879\u76ee\u7684\u6587\u6863\uff0c\u76ee\u7684\u662f\u5e2e\u52a9\u7406\u89e3\uff1a The idea of a hash chain is simple: you start with a base (could be a password, or just a number, or some other data) and hash ( cryptographic hash function ) it. You then take the result and hash that too. You continue hashing the results repeatedly, till you have a series of hashes like this: Base -> Hash0 = H(Base) -> Hash1 = H(Hash0) -> ... -> HashN = H(HashN-1) The exciting property of these hash chains is that given the last hash in the chain, HashN, it is very difficult to determine any of the previous hashes, or the base. However, given the last hash, it is trivial to verify whether another hash is part of the chain. This means that a hash chain has the potential to be a limited source of authentication. You can deploy a resource in public along with the last hash of the chain. Then you can give commands to this resource, passing along each previous hash as authentication of your identity. Immutable \u7be1\u6539 Hash chain \u4e2d\u7684\u4efb\u4f55\u4e00\u4e2anode\uff08\u8282\u70b9\uff09\u7684hash value\u90fd\u4f1a\u5bfc\u81f4\u8fd9\u4e2a\u8282\u70b9\u540e\u7684\u6240\u6709\u7684\u8282\u70b9\u7684hash value\u90fd\u5fc5\u987b\u91cd\u65b0\u8ba1\u7b97\u4e00\u904d\u624d\u80fd\u591f\u91cd\u65b0\u7ec4\u6210\u94fe\uff0c\u5426\u5219\u5c31\u65e0\u6cd5\u5f62\u6210hash chain\u4e86\u3002\u8fd9\u610f\u5473\u4e2d\u6574\u4e2a Hash chain \u662fimmutable\u7684\u3002 \u4e0b\u9762\u662f\u5bfb\u627e\u5230\u7684\u4e00\u4e9b\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\uff1a Where did the idea of blockchain come from? Git was already using it since 2005 With bitcoin's blockchain, each block's hash is computed with the previous block's hash. That makes all blocks an immutable chain . This is not the first time people used a hash-chain. As far as I know, Git was already used it since 2005 for the commit-hash computing. So, what is the earliest use of hash chains? Where did the idea of blockchain come from? \u8fd9\u79cdimmutable\u7684\u7279\u6027\u6709\u7740\u975e\u5e38\u91cd\u8981\u7684\u4ef7\u503c\uff0c\u6bd4\u5982\uff1a \u539f\u6587\u94fe\u63a5 According to Bitcoin and Cryptocurrency Technologies (BaCT) , the Princeton Bitcoin textbook, the block chain dates back to a \"paper by Haber and Stornetta in 1991. Their proposal was a method for secure timestamping of digitaldocuments, rather than a digital money scheme.\" (BaCT p.15) The chaining of Merkle trees instead of single documents was proposed in a later paper. (BaCT p.16) Applications one-time keys In computer security , a hash chain is a method to produce many one-time keys from a single key or password . For non-repudiation \uff08\u4e0d\u53ef\u62b5\u8d56\uff09 a hash function can be applied successively to additional pieces of data in order to record the chronology of data's existence. see also: S/KEY Binary hash chains Main article: Merkle tree Hash chain vs. blockchain A hash chain is similar to a blockchain , as they both utilize a cryptographic hash function for creating a link between two nodes. However, a blockchain (as used by Bitcoin and related systems) is generally intended to support distributed consensus around a public ledger (data), and incorporates a set of rules for encapsulation of data and associated data permissions. NOTE: Blockchain \u80af\u5b9a\u8fdc\u6bd4Hash chain\u590d\u6742\uff0c\u5173\u4e8e\u4e24\u8005\u7684\u5173\u8054\uff0c\u6709\u592a\u591a\u592a\u591a\u7684\u6587\u7ae0\u8fdb\u884c\u5256\u6790\u4e86\u3002 Blockchain \u8fd0\u7528\u4e86\u591a\u79cd\u6280\u672f\uff0c\u5176\u4e2d\u4e4b\u4e00\u5c31\u662fHash chain\uff0c Blockchain \u4f7f\u7528Hash chain\u6765\u5c06\u5b83\u7684block\u94fe\u63a5\u8d77\u6765\u3002\u5173\u4e8e Blockchain \u4e2dHash chain\u7684\u5e94\u7528\uff0c\u8fd8\u9700\u8981\u53c2\u89c1 Blockchain \u6587\u7ae0\u3002 \u4e0b\u9762\u662f\u68c0\u7d22\u5230\u7684\u5173\u4e8e\u4e24\u8005\u7684\u6587\u7ae0\uff1a https://www.researchgate.net/figure/The-Bitcoin-blockchain-is-a-hash-chain-of-blocks-Each-block-has-a-Merkle-tree-of_fig1_316789505 The Bitcoin blockchain is a hash chain of blocks. Each block has a Merkle tree of transactions. Efficient membership proofs of transactions can be constructed with respect to the Merkle root. \u8fd9\u7bc7\u6587\u7ae0\u5c06block chain\u7684\u7b80\u5355\u6a21\u578b\uff0c\u901a\u8fc7\u8fd9\u4e2a\u7b80\u5355\u6a21\u578b\u53ef\u4ee5\u770b\u51fahash chain\u5728block chain\u4e2d\u7684\u4f7f\u7528\u3002 https://crypto.stackexchange.com/questions/68290/when-was-hash-chain-first-used Hash linking is used to prove the integrity of a blockchain, or similar systems. When was that technique first used? I would guess it was early, maybe 1950s/1960s? Hash list VS Hash chain \u5728 Hash chain \u4e2d\u5bf9\u4e24\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff1a In contrast to the recursive structure of hash chains, the elements of a hash list are independent of each other. TO READ When was hash chain first used? https://www.techopedia.com/definition/32920/hash-chain https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-5906-5_780 Merkle tree In cryptography and computer science , a hash tree or Merkle tree is a tree in which every leaf node is labelled with the cryptographic hash of a data block, and every non-leaf node is labelled with the hash of the labels of its child nodes. Hash trees are a generalization of hash lists and hash chains . NOTE: \u6709\u4e86\u524d\u9762 Hash list \u548c Hash chain \u7684\u57fa\u7840\u73b0\u5728\u6765\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5176\u5b9e\u5c31\u76f8\u5bf9\u6bd4\u8f83\u5bb9\u6613\u4e86\u3002hash chain\u662f\u7ebf\u6027\u7ed3\u6784\uff0chash tree\u5219\u662fhierarchy\u7ed3\u6784\u3002 An example of a binary hash tree. Hashes 0-0 and 0-1 are the hash values of data blocks L1 and L2, respectively, and hash 0 is the hash of the concatenation of hashes 0-0 and 0-1. Uses Hash trees can be used to verify any kind of data stored, handled and transferred in and between computers. They can help ensure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and even to check that the other peers do not lie and send fake blocks. Hash trees are used in hash-based cryptography . Hash trees are also used in file systems: IPFS , Btrfs and ZFS distributed revision control systems: Git and Mercurial peer-to-peer network: Bitcoin NoSQL systems such as Apache Cassandra NOTE: \u901a\u8fc7\u4e0a\u8ff0\u4f8b\u5b50\u53ef\u4ee5\u770b\u51fa Merkle tree \u5728distributed application\u4e2d\u6709\u7740\u5e7f\u6cdb\u8fd0\u7528\u3002 Overview A hash tree is a tree of hashes in which the leaves are hashes of data blocks in, for instance, a file or set of files. Nodes further up in the tree are the hashes of their respective children. For example, in the picture hash 0 is the result of hashing the concatenation of hash 0-0 and hash 0-1 . That is, hash 0 = hash( hash(0-0) + hash(0-1) ) where + denotes concatenation . Usually, a cryptographic hash function such as SHA-2 is used for the hashing. If the hash tree only needs to protect against unintentional damage, unsecured checksums such as CRCs can be used. In the top of a hash tree there is a top hash (or root hash or master hash ). Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash tree can be received from any non-trusted source, like any peer in the p2p network. Then, the received hash tree is checked against the trusted top hash , and if the hash tree is damaged or fake, another hash tree from another source will be tried until the program finds one that matches the top hash. Merkel DAG DAG\u7684\u542b\u4e49\u662fdirected acyclic graph\u3002Merkel DAG\u662f\u6211\u5728\u9605\u8bfb Is Git a Block Chain? \u53d1\u73b0\u5b83\u4eec\u63d0\u51fa\u7684\u4e00\u4e2a\u6982\u5ff5\u3002 TO READ https://blockchainlabs.ai/the-merkle-tree/ Linked timestamping https://en.wikipedia.org/wiki/Linked_timestamping","title":"\u524d\u8a00"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#_1","text":"\u662f\u5728\u5b66\u4e60 git \u7684\u65f6\u5019\uff0c\u53d1\u73b0 Merkle tree \u7684\u3002\u5728\u5b66\u4e60 Merkle tree \u4e4b\u524d\uff0c\u5148\u5b66\u4e60 Hash list \u548c Hash chain \uff0c\u4e24\u8005\u662f\u6df1\u523b\u638c\u63e1 Merkle tree \u7684\u57fa\u7840\u3002","title":"\u524d\u8a00"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#hash#list","text":"In computer science , a hash list is typically a list of hashes of the data blocks in a file or set of files. NOTE: hash list\u7684\u542b\u4e49\u5176\u5b9e\u975e\u5e38\u7b80\u5355\uff0c\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8bf4\u7684list of hash\uff0c\u4ec5\u6b64\u800c\u5df2\u3002\u5f53\u7136\u53ef\u4ee5\u5728\u5b83\u4e0a\u9762\u518d\u8fdb\u884c\u6269\u5c55\uff0c\u6bd4\u5982\u4e0b\u9762\u5c06\u8981\u4ecb\u7ecd\u7684Root hash\u3002","title":"Hash list"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#root#hash","text":"Often, an additional hash of the hash list itself (a top hash , also called root hash or master hash ) is used. A hash list with a top hash \u4e0a\u8ff0\u5c31\u662f\u4e00\u4e2a\u5e26\u6709root hash\u7684hash list\u3002 NOTE: \u601d\u8003\uff1aroot hash\u7684\u4ef7\u503c\u4f55\u5728\u5462\uff1f\u5982\u679c\u5c06hash list\u4e2d\u7684\u6bcf\u4e2ahash\u90fd\u5bf9\u5e94\u7684\u662f\u4e00\u4e2adata block\uff0c\u800croot hash\u5219\u5bf9\u5e94\u7684\u662f\u6240\u6709\u7684data block\uff0c\u4e5f\u5c31\u662f\u8bf4\u5b83\u5176\u5b9e\u6240\u5bf9\u5e94\u7684\u662f\u6574\u4f53data\uff0c\u800c\u975e\u5c40\u90e8\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u5c31\u63cf\u8ff0\u7684\u662f\u8fd9\u79cd\u542b\u4e49\u3002 Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash list can be received from any non-trusted source, like any peer in the p2p network. Then the received hash list is checked against the trusted top hash, and if the hash list is damaged or fake, another hash list from another source will be tried until the program finds one that matches the top hash.","title":"Root hash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#applications","text":"Hash lists can be used to protect any kind of data stored, handled and transferred in and between computers. An important use of hash lists is to make sure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and to check that the other peers do not \"lie\" and send fake blocks. Usually a cryptographic hash function such as SHA-256 is used for the hashing. If the hash list only needs to protect against unintentional damage unsecured checksums such as CRCs can be used. NOTE: \u4e0a\u8ff0\u5176\u5b9e\u662fhash\u7684\u5e38\u89c1\u7528\u6cd5\uff0c\u5e76\u65e0\u4ec0\u4e48\u7279\u6b8a\u4e4b\u5904\u3002\u4e0b\u9762\u5219\u5f3a\u8c03hash list\u7684\u7279\u6b8a\u4e4b\u5904\u3002 Hash lists are better than a simple hash of the entire file since, in the case of a data block being damaged, this is noticed, and only the damaged block needs to be redownloaded. With only a hash of the file, many undamaged blocks would have to be redownloaded, and the file reconstructed and tested until the correct hash of the entire file is obtained. Hash lists also protect against nodes that try to sabotage by sending fake blocks, since in such a case the damaged block can be acquired from some other source.","title":"Applications"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#hash#chain","text":"A hash chain is the successive application of a cryptographic hash function to a piece of data.","title":"Hash chain"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#definition","text":"A hash chain is a successive application of a cryptographic hash function $ h $ to a string $ x $. For example, $ h(h(h(h(x)))) $ gives a hash chain of length 4, often denoted $ h^{4}(x) $ NOTE: \u663e\u7136\uff0chash chain\u7684\u5b9a\u4e49\u6240\u5f3a\u8c03\u7684\u662f\u91cd\u590d\u8fed\u4ee3\u5730\u4f7f\u7528hash function\u3002\u663e\u7136\u5b83\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u4e00\u4e2a\u6570\u5b66\u8fc7\u7a0b\u3002","title":"Definition"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#features","text":"\u672c\u6587\u7ed9\u51fa\u7684hash chain\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c \u8fd0\u7528hash chain\u8fd8\u80fd\u591f\u5e26\u6765\u5176\u4ed6\u7684\u975e\u5e38\u4f18\u826f\u7684\u7279\u6027\uff0c\u8fd9\u5c31\u4f7f\u5f97hash chain\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u4e0b\u9762\u5c31\u6765\u63a2\u8ba8\u8fd0\u7528hash chain\u53ef\u4ee5\u5e26\u6765\u7684\u4f18\u826f\u7279\u6027\uff1a","title":"Features"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#pre-image#resistance#one-way","text":"\u7531\u4e8ehash chain\u6240\u4f7f\u7528\u7684\u662f cryptographic hash function \uff0c cryptographic hash function \u662f one-way function ,\u6240\u6709hash chain\u5c31\u5177\u5907\u4e86 cryptographic hash function \u7684 Pre-image resistance \u7279\u6027\uff0c\u5373\u6839\u636e\u51fd\u6570\u503c\u65e0\u6cd5\u9006\u5411\u8ba1\u7b97\u51fa\u5b83\u7684\u81ea\u53d8\u91cf\u7684\u503c\uff0c\u8bf4\u7684\u66f4\u52a0\u5f62\u8c61\u4e00\u4e9b\u5c31\u662fhash chain\u53ea\u80fd\u591f\u987a\u63a8\u800c\u4e0d\u80fd\u591f\u9006\u63a8\u3002 \u4e0b\u9762\u8fd9\u6bb5\u8bdd\u662f\u6458\u81ea Hash Chain \u9879\u76ee\u7684\u6587\u6863\uff0c\u76ee\u7684\u662f\u5e2e\u52a9\u7406\u89e3\uff1a The idea of a hash chain is simple: you start with a base (could be a password, or just a number, or some other data) and hash ( cryptographic hash function ) it. You then take the result and hash that too. You continue hashing the results repeatedly, till you have a series of hashes like this: Base -> Hash0 = H(Base) -> Hash1 = H(Hash0) -> ... -> HashN = H(HashN-1) The exciting property of these hash chains is that given the last hash in the chain, HashN, it is very difficult to determine any of the previous hashes, or the base. However, given the last hash, it is trivial to verify whether another hash is part of the chain. This means that a hash chain has the potential to be a limited source of authentication. You can deploy a resource in public along with the last hash of the chain. Then you can give commands to this resource, passing along each previous hash as authentication of your identity.","title":"Pre-image resistance / one-way"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#immutable","text":"\u7be1\u6539 Hash chain \u4e2d\u7684\u4efb\u4f55\u4e00\u4e2anode\uff08\u8282\u70b9\uff09\u7684hash value\u90fd\u4f1a\u5bfc\u81f4\u8fd9\u4e2a\u8282\u70b9\u540e\u7684\u6240\u6709\u7684\u8282\u70b9\u7684hash value\u90fd\u5fc5\u987b\u91cd\u65b0\u8ba1\u7b97\u4e00\u904d\u624d\u80fd\u591f\u91cd\u65b0\u7ec4\u6210\u94fe\uff0c\u5426\u5219\u5c31\u65e0\u6cd5\u5f62\u6210hash chain\u4e86\u3002\u8fd9\u610f\u5473\u4e2d\u6574\u4e2a Hash chain \u662fimmutable\u7684\u3002 \u4e0b\u9762\u662f\u5bfb\u627e\u5230\u7684\u4e00\u4e9b\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\uff1a Where did the idea of blockchain come from? Git was already using it since 2005 With bitcoin's blockchain, each block's hash is computed with the previous block's hash. That makes all blocks an immutable chain . This is not the first time people used a hash-chain. As far as I know, Git was already used it since 2005 for the commit-hash computing. So, what is the earliest use of hash chains? Where did the idea of blockchain come from? \u8fd9\u79cdimmutable\u7684\u7279\u6027\u6709\u7740\u975e\u5e38\u91cd\u8981\u7684\u4ef7\u503c\uff0c\u6bd4\u5982\uff1a \u539f\u6587\u94fe\u63a5 According to Bitcoin and Cryptocurrency Technologies (BaCT) , the Princeton Bitcoin textbook, the block chain dates back to a \"paper by Haber and Stornetta in 1991. Their proposal was a method for secure timestamping of digitaldocuments, rather than a digital money scheme.\" (BaCT p.15) The chaining of Merkle trees instead of single documents was proposed in a later paper. (BaCT p.16)","title":"Immutable"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#applications_1","text":"","title":"Applications"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#one-time#keys","text":"In computer security , a hash chain is a method to produce many one-time keys from a single key or password . For non-repudiation \uff08\u4e0d\u53ef\u62b5\u8d56\uff09 a hash function can be applied successively to additional pieces of data in order to record the chronology of data's existence. see also: S/KEY","title":"one-time keys"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#binary#hash#chains","text":"Main article: Merkle tree","title":"Binary hash chains"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#hash#chain#vs#blockchain","text":"A hash chain is similar to a blockchain , as they both utilize a cryptographic hash function for creating a link between two nodes. However, a blockchain (as used by Bitcoin and related systems) is generally intended to support distributed consensus around a public ledger (data), and incorporates a set of rules for encapsulation of data and associated data permissions. NOTE: Blockchain \u80af\u5b9a\u8fdc\u6bd4Hash chain\u590d\u6742\uff0c\u5173\u4e8e\u4e24\u8005\u7684\u5173\u8054\uff0c\u6709\u592a\u591a\u592a\u591a\u7684\u6587\u7ae0\u8fdb\u884c\u5256\u6790\u4e86\u3002 Blockchain \u8fd0\u7528\u4e86\u591a\u79cd\u6280\u672f\uff0c\u5176\u4e2d\u4e4b\u4e00\u5c31\u662fHash chain\uff0c Blockchain \u4f7f\u7528Hash chain\u6765\u5c06\u5b83\u7684block\u94fe\u63a5\u8d77\u6765\u3002\u5173\u4e8e Blockchain \u4e2dHash chain\u7684\u5e94\u7528\uff0c\u8fd8\u9700\u8981\u53c2\u89c1 Blockchain \u6587\u7ae0\u3002 \u4e0b\u9762\u662f\u68c0\u7d22\u5230\u7684\u5173\u4e8e\u4e24\u8005\u7684\u6587\u7ae0\uff1a https://www.researchgate.net/figure/The-Bitcoin-blockchain-is-a-hash-chain-of-blocks-Each-block-has-a-Merkle-tree-of_fig1_316789505 The Bitcoin blockchain is a hash chain of blocks. Each block has a Merkle tree of transactions. Efficient membership proofs of transactions can be constructed with respect to the Merkle root. \u8fd9\u7bc7\u6587\u7ae0\u5c06block chain\u7684\u7b80\u5355\u6a21\u578b\uff0c\u901a\u8fc7\u8fd9\u4e2a\u7b80\u5355\u6a21\u578b\u53ef\u4ee5\u770b\u51fahash chain\u5728block chain\u4e2d\u7684\u4f7f\u7528\u3002 https://crypto.stackexchange.com/questions/68290/when-was-hash-chain-first-used Hash linking is used to prove the integrity of a blockchain, or similar systems. When was that technique first used? I would guess it was early, maybe 1950s/1960s?","title":"Hash chain vs. blockchain"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#hash#list#vs#hash#chain","text":"\u5728 Hash chain \u4e2d\u5bf9\u4e24\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff1a In contrast to the recursive structure of hash chains, the elements of a hash list are independent of each other. TO READ When was hash chain first used? https://www.techopedia.com/definition/32920/hash-chain https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-5906-5_780","title":"Hash list VS  Hash chain"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#merkle#tree","text":"In cryptography and computer science , a hash tree or Merkle tree is a tree in which every leaf node is labelled with the cryptographic hash of a data block, and every non-leaf node is labelled with the hash of the labels of its child nodes. Hash trees are a generalization of hash lists and hash chains . NOTE: \u6709\u4e86\u524d\u9762 Hash list \u548c Hash chain \u7684\u57fa\u7840\u73b0\u5728\u6765\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5176\u5b9e\u5c31\u76f8\u5bf9\u6bd4\u8f83\u5bb9\u6613\u4e86\u3002hash chain\u662f\u7ebf\u6027\u7ed3\u6784\uff0chash tree\u5219\u662fhierarchy\u7ed3\u6784\u3002 An example of a binary hash tree. Hashes 0-0 and 0-1 are the hash values of data blocks L1 and L2, respectively, and hash 0 is the hash of the concatenation of hashes 0-0 and 0-1.","title":"Merkle tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#uses","text":"Hash trees can be used to verify any kind of data stored, handled and transferred in and between computers. They can help ensure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and even to check that the other peers do not lie and send fake blocks. Hash trees are used in hash-based cryptography . Hash trees are also used in file systems: IPFS , Btrfs and ZFS distributed revision control systems: Git and Mercurial peer-to-peer network: Bitcoin NoSQL systems such as Apache Cassandra NOTE: \u901a\u8fc7\u4e0a\u8ff0\u4f8b\u5b50\u53ef\u4ee5\u770b\u51fa Merkle tree \u5728distributed application\u4e2d\u6709\u7740\u5e7f\u6cdb\u8fd0\u7528\u3002","title":"Uses"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#overview","text":"A hash tree is a tree of hashes in which the leaves are hashes of data blocks in, for instance, a file or set of files. Nodes further up in the tree are the hashes of their respective children. For example, in the picture hash 0 is the result of hashing the concatenation of hash 0-0 and hash 0-1 . That is, hash 0 = hash( hash(0-0) + hash(0-1) ) where + denotes concatenation . Usually, a cryptographic hash function such as SHA-2 is used for the hashing. If the hash tree only needs to protect against unintentional damage, unsecured checksums such as CRCs can be used. In the top of a hash tree there is a top hash (or root hash or master hash ). Before downloading a file on a p2p network, in most cases the top hash is acquired from a trusted source, for instance a friend or a web site that is known to have good recommendations of files to download. When the top hash is available, the hash tree can be received from any non-trusted source, like any peer in the p2p network. Then, the received hash tree is checked against the trusted top hash , and if the hash tree is damaged or fake, another hash tree from another source will be tried until the program finds one that matches the top hash.","title":"Overview"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#merkel#dag","text":"DAG\u7684\u542b\u4e49\u662fdirected acyclic graph\u3002Merkel DAG\u662f\u6211\u5728\u9605\u8bfb Is Git a Block Chain? \u53d1\u73b0\u5b83\u4eec\u63d0\u51fa\u7684\u4e00\u4e2a\u6982\u5ff5\u3002","title":"Merkel DAG"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#to#read","text":"https://blockchainlabs.ai/the-merkle-tree/","title":"TO READ"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-based-data-structures/Merkle-tree/Merkle-tree/#linked#timestamping","text":"https://en.wikipedia.org/wiki/Linked_timestamping","title":"Linked timestamping"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Category-Hash-functions/","text":"Category:Hash functions List of hash functions Application Property of hash function length Hash table Cryptographic hash function \u56fa\u5b9a Distributed hash tables VS-check-sum-VS-cryptographic-hash https://security.stackexchange.com/a/194602 A checksum (such as CRC32) is to prevent accidental changes. If one byte changes, the checksum changes. The checksum is not safe to protect against malicious changes: it is pretty easy to create a file with a particular checksum. A hash function maps some data to other data. It is often used to speed up comparisons or create a hash table. Not all hash functions are secure and the hash does not necessarily changes when the data changes. A cryptographic hash function (such as SHA1) is a checksum that is secure against malicious changes. It is pretty hard to create a file with a specific cryptographic hash. To make things more complicated, cryptographic hash functions are sometimes simply referred to as hash functions. VS-cryptographic-hash-function-VS-perfect hash function? Are cryptographic hash functions perfect hash functions? TODO https://security.stackexchange.com/questions/11839/what-is-the-difference-between-a-hash-function-and-a-cryptographic-hash-function https://crypto.stackexchange.com/questions/879/what-is-the-random-oracle-model-and-why-is-it-controversial/880#880 https://computer.howstuffworks.com/encryption7.htm https://en.wikipedia.org/wiki/Hash","title":"[Category:Hash functions](https://en.wikipedia.org/wiki/Category:Hash_functions)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Category-Hash-functions/#categoryhash#functions","text":"","title":"Category:Hash functions"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Category-Hash-functions/#list#of#hash#functions","text":"Application Property of hash function length Hash table Cryptographic hash function \u56fa\u5b9a Distributed hash tables","title":"List of hash functions"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Category-Hash-functions/#vs-check-sum-vs-cryptographic-hash","text":"https://security.stackexchange.com/a/194602 A checksum (such as CRC32) is to prevent accidental changes. If one byte changes, the checksum changes. The checksum is not safe to protect against malicious changes: it is pretty easy to create a file with a particular checksum. A hash function maps some data to other data. It is often used to speed up comparisons or create a hash table. Not all hash functions are secure and the hash does not necessarily changes when the data changes. A cryptographic hash function (such as SHA1) is a checksum that is secure against malicious changes. It is pretty hard to create a file with a specific cryptographic hash. To make things more complicated, cryptographic hash functions are sometimes simply referred to as hash functions.","title":"VS-check-sum-VS-cryptographic-hash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Category-Hash-functions/#vs-cryptographic-hash-function-vs-perfect#hash#function","text":"Are cryptographic hash functions perfect hash functions?","title":"VS-cryptographic-hash-function-VS-perfect hash function?"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Category-Hash-functions/#todo","text":"https://security.stackexchange.com/questions/11839/what-is-the-difference-between-a-hash-function-and-a-cryptographic-hash-function https://crypto.stackexchange.com/questions/879/what-is-the-random-oracle-model-and-why-is-it-controversial/880#880 https://computer.howstuffworks.com/encryption7.htm https://en.wikipedia.org/wiki/Hash","title":"TODO"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/","text":"Hash function \u7efc\u8ff0 \u6b63\u5982\u5728 List of hash functions \u4e2d\u6240\u7f57\u5217\u7684\uff0c\u6709\u7740\u5f62\u5f62\u8272\u8272\u7684hash function\u3002\u90a3\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u6765\u628a\u6211hash function\u5462\uff1f\u6211\u89c9\u5f97\u4ece\u6570\u5b66\u4e0a\u5bf9 function \u7684\u5b9a\u4e49\u5165\u624b\u8f83\u597d\u3002 hash function\u548c\u666e\u901a\u7684function\u4e00\u6837\uff0c\u6709 domain \u548c codomain \u3002\u5728hash function\u4e2d\uff0cdomain\u7684\u540c\u4e49\u8bcd\u6709\uff1akey space \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u8981\u5bb9\u7eb3\u54ea\u4e9b\u6570\u636e\u3002\u6bd4\u5982\uff1a \u5bf9\u4e8e\u4f7f\u7528 cryptographic hash function \u6765\u52a0\u5bc6\u5bc6\u7801\u7684function\uff0c\u5b83\u7684 domain \u8981\u5bb9\u7eb3\u5c31\u662f\u6240\u6709\u53ef\u4ee5\u7528\u4e8e\u5bc6\u7801\u7684\u5b57\u7b26\u7684\u7ec4\u5408\u800c\u6210\u7684\u5b57\u7b26\u4e32 \u53ef\u80fdhash\u6570\u5b57 \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u4e2d\u6570\u636e\u7684\u7279\u5f81\uff0c\u6bd4\u5982\uff1a identity hash function \u5c31\u662f\u4f7f\u7528\u7684 domain \u4e2d\u6570\u636e\u662fidentity\u7684\u7279\u5f81 trivial hash function \u5c31\u662f\u4f7f\u7528domain\u4e2d\u7684\u6570\u636e\u662funiformly or sufficiently uniformly distributed \u7684\u7279\u5f81 modulo division hash function \u4e00\u822c\u9009\u62e9\u4e00\u4e2abig prime\u6765\u4f5c\u4e3a modulo \uff0c\u5177\u4f53\u539f\u56e0\uff0c\u53c2\u89c1 Why is it best to use a prime number as a mod in a hashing function? \u3002 domain \u548c codomain \u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff1a Injective function \uff0c\u663e\u7136 perfect hash function \u5177\u5907\u8fd9\u79cd\u6027\u8d28\u3002 \u7ef4\u57fa\u767e\u79d1 Hash table#Hashing NOTE: Hash table#Hashing \u4e2d\u5173\u4e8ehash\u7684\u4ecb\u7ecd\u6bd4\u8f83\u7cbe\u7b80\u3002 \u7ef4\u57fa\u767e\u79d1 Hash function A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values , hash codes , digests , or simply hashes . NOTE: \"digests\" means \u6458\u8981 in Chinese. Overview NOTE: Although this article is named Hash function , what this chapter talks about is hash table. A hash function takes as input a key, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed length , like an integer, or variable length , like a name. In some cases, the key is the datum itself. The output is a hash code used to index a hash table holding the data or records, or pointers to them. A hash function may be considered to perform three functions: Convert variable length keys into fixed length (usually machine word length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR. NOTE: Folding means bitwise operation. NOTE: What is parity-preserving operator? Parity bit Scramble the bits of the key so that the resulting values are uniformly distributed over the key space . NOTE: What is the key space? ASCII code? Map the key values into ones less than or equal to the size of the table NOTE: This function will be used only when it is used as index of the table in hash table . In other applications, this function may be omitted. A good hash function satisfies two basic properties: 1) it should be very fast to compute; NOTE: This is efficiency 2) it should minimize duplication of output values (collisions). NOTE: This is uniformity Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant. High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists , or systematic probing of the table to find an empty slot. Properties Note: Properties determine their usage, it is necessary to know the properties a hash function possess. Uniformity Note: \u5747\u5300\u6027\uff0c\u6309\u7167\u7ef4\u57fa\u767e\u79d1 Hash table#Choosing a hash function \u7684\u8bf4\u6cd5\uff1a A basic requirement is that the function should provide a uniform distribution of hash values. A good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability . The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of collisions \u2014pairs of inputs that are mapped to the same hash value \u2014increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries. Note that this criterion only requires the value to be uniformly distributed , not random in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function , but the converse need not be true. NOTE: hash function VS randomizing function Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries. NOTE: Typical subset VS global set Efficiency In most applications, it is highly desirable that the hash function be computable with minimum latency and secondarily in a minimum number of instructions. Universality Universal hashing Applicability A hash function should be applicable to all situations in which a hash function might be used. A hash function that allows only certain table sizes, strings only up to a certain length, or can't accept a seed (i.e. allow double hashing) isn't as useful as one that does. NOTE: The seed in the paragraph is salt (cryptography) . Reference the Deterministic to see the value of the seed. Deterministic A hash procedure must be deterministic \u2014meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection ), although sometimes rehashing of the item is possible. The determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions make use of a randomized seed that is generated once when the Python process starts in addition to the input to be hashed. The Python hash is still a valid hash function when used within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ. Note: Reference python doc: object.__hash__ ( self ) \u00b6 Defined range Fixed-length hash It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. Producing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value. Variable range In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters\u2014the input data z , and the number n of allowed hash values. NOTE: n \u662f codomain \u7684 cardinality A common solution is to compute a fixed hash function with a very large range (say, 0 to 2^{32} \u2212 1 2^{32} \u2212 1 ), divide the result by n , and use the division's remainder . If n is itself a power of 2, this can be done by bit masking and bit shifting . When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and n \u2212 1, for any value of n that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of n , e.g. odd or prime numbers . Variable range with minimal movement (dynamic hash function) When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table . A hash function that will relocate the minimum number of records when the table is resized is desirable. What is needed is a hash function H ( z , n ) \u2013 where z is the key being hashed and n is the number of allowed hash values \u2013 such that H ( z , n + 1) = H ( z , n ) with probability close to n /( n + 1). Linear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property. Extendible hashing uses a dynamic hash function that requires space proportional to n to compute the hash function, and it becomes a function of the previous keys that have been inserted. Several algorithms that preserve the uniformity property but require time proportional to n to compute the value of H ( z , n ) have been invented. A hash function with minimal movement is especially useful in distributed hash tables . Hashing integer data types Identity hash function identity function Trivial hash function Folding NOTE: \u5230\u5e95\u5728\u54ea\u4e2a\u7ea7\u522b\u8fdb\u884cfold\uff1f\u662f\u5728bit\u7ea7\u522b\u8fd8\u662fdigit\u7ea7\u522b\uff1f\u5728 What is Folding technique in hashing and how to implement it? \u4e2d\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u5728digit\u7ea7\u522b\u8fdb\u884cfold\u7684\u3002\u4f46\u662fWikipedia\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u5728bit\u7ea7\u522b\u8fdb\u884cfold\u7684\u3002 \u8fd9\u5176\u5b9e\u662f\u7531hash function\u7684domain\u51b3\u5b9a\u7684\uff0c\u5982\u679c\u8981hash\u7684\u662f\u6570\u5b57\uff0c\u5219\u5c31\u5728digit\u7ea7\u522b\u8fdb\u884cfold\uff1b\u5982\u679c\u8981hash\u7684\u662f\u5b57\u7b26\u4e32\uff0c\u5219\u5c31\u5728bit\u7ea7\u522b\u8fdb\u884cfold\u3002 Hashing variable-length dat","title":"Hash-function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#hash#function","text":"","title":"Hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#_1","text":"\u6b63\u5982\u5728 List of hash functions \u4e2d\u6240\u7f57\u5217\u7684\uff0c\u6709\u7740\u5f62\u5f62\u8272\u8272\u7684hash function\u3002\u90a3\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u6765\u628a\u6211hash function\u5462\uff1f\u6211\u89c9\u5f97\u4ece\u6570\u5b66\u4e0a\u5bf9 function \u7684\u5b9a\u4e49\u5165\u624b\u8f83\u597d\u3002 hash function\u548c\u666e\u901a\u7684function\u4e00\u6837\uff0c\u6709 domain \u548c codomain \u3002\u5728hash function\u4e2d\uff0cdomain\u7684\u540c\u4e49\u8bcd\u6709\uff1akey space \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u8981\u5bb9\u7eb3\u54ea\u4e9b\u6570\u636e\u3002\u6bd4\u5982\uff1a \u5bf9\u4e8e\u4f7f\u7528 cryptographic hash function \u6765\u52a0\u5bc6\u5bc6\u7801\u7684function\uff0c\u5b83\u7684 domain \u8981\u5bb9\u7eb3\u5c31\u662f\u6240\u6709\u53ef\u4ee5\u7528\u4e8e\u5bc6\u7801\u7684\u5b57\u7b26\u7684\u7ec4\u5408\u800c\u6210\u7684\u5b57\u7b26\u4e32 \u53ef\u80fdhash\u6570\u5b57 \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u4e2d\u6570\u636e\u7684\u7279\u5f81\uff0c\u6bd4\u5982\uff1a identity hash function \u5c31\u662f\u4f7f\u7528\u7684 domain \u4e2d\u6570\u636e\u662fidentity\u7684\u7279\u5f81 trivial hash function \u5c31\u662f\u4f7f\u7528domain\u4e2d\u7684\u6570\u636e\u662funiformly or sufficiently uniformly distributed \u7684\u7279\u5f81 modulo division hash function \u4e00\u822c\u9009\u62e9\u4e00\u4e2abig prime\u6765\u4f5c\u4e3a modulo \uff0c\u5177\u4f53\u539f\u56e0\uff0c\u53c2\u89c1 Why is it best to use a prime number as a mod in a hashing function? \u3002 domain \u548c codomain \u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff1a Injective function \uff0c\u663e\u7136 perfect hash function \u5177\u5907\u8fd9\u79cd\u6027\u8d28\u3002","title":"\u7efc\u8ff0"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#hash#tablehashing","text":"NOTE: Hash table#Hashing \u4e2d\u5173\u4e8ehash\u7684\u4ecb\u7ecd\u6bd4\u8f83\u7cbe\u7b80\u3002","title":"\u7ef4\u57fa\u767e\u79d1Hash table#Hashing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#hash#function_1","text":"A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values , hash codes , digests , or simply hashes . NOTE: \"digests\" means \u6458\u8981 in Chinese.","title":"\u7ef4\u57fa\u767e\u79d1Hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#overview","text":"NOTE: Although this article is named Hash function , what this chapter talks about is hash table. A hash function takes as input a key, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed length , like an integer, or variable length , like a name. In some cases, the key is the datum itself. The output is a hash code used to index a hash table holding the data or records, or pointers to them. A hash function may be considered to perform three functions: Convert variable length keys into fixed length (usually machine word length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR. NOTE: Folding means bitwise operation. NOTE: What is parity-preserving operator? Parity bit Scramble the bits of the key so that the resulting values are uniformly distributed over the key space . NOTE: What is the key space? ASCII code? Map the key values into ones less than or equal to the size of the table NOTE: This function will be used only when it is used as index of the table in hash table . In other applications, this function may be omitted. A good hash function satisfies two basic properties: 1) it should be very fast to compute; NOTE: This is efficiency 2) it should minimize duplication of output values (collisions). NOTE: This is uniformity Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant. High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists , or systematic probing of the table to find an empty slot.","title":"Overview"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#properties","text":"Note: Properties determine their usage, it is necessary to know the properties a hash function possess.","title":"Properties"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#uniformity","text":"Note: \u5747\u5300\u6027\uff0c\u6309\u7167\u7ef4\u57fa\u767e\u79d1 Hash table#Choosing a hash function \u7684\u8bf4\u6cd5\uff1a A basic requirement is that the function should provide a uniform distribution of hash values. A good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability . The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of collisions \u2014pairs of inputs that are mapped to the same hash value \u2014increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries. Note that this criterion only requires the value to be uniformly distributed , not random in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function , but the converse need not be true. NOTE: hash function VS randomizing function Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries. NOTE: Typical subset VS global set","title":"Uniformity"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#efficiency","text":"In most applications, it is highly desirable that the hash function be computable with minimum latency and secondarily in a minimum number of instructions.","title":"Efficiency"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#universality","text":"Universal hashing","title":"Universality"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#applicability","text":"A hash function should be applicable to all situations in which a hash function might be used. A hash function that allows only certain table sizes, strings only up to a certain length, or can't accept a seed (i.e. allow double hashing) isn't as useful as one that does. NOTE: The seed in the paragraph is salt (cryptography) . Reference the Deterministic to see the value of the seed.","title":"Applicability"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#deterministic","text":"A hash procedure must be deterministic \u2014meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection ), although sometimes rehashing of the item is possible. The determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions make use of a randomized seed that is generated once when the Python process starts in addition to the input to be hashed. The Python hash is still a valid hash function when used within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ. Note: Reference python doc: object.__hash__ ( self ) \u00b6","title":"Deterministic"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#defined#range","text":"","title":"Defined range"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#fixed-length#hash","text":"It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. Producing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value.","title":"Fixed-length hash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#variable#range","text":"In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters\u2014the input data z , and the number n of allowed hash values. NOTE: n \u662f codomain \u7684 cardinality A common solution is to compute a fixed hash function with a very large range (say, 0 to 2^{32} \u2212 1 2^{32} \u2212 1 ), divide the result by n , and use the division's remainder . If n is itself a power of 2, this can be done by bit masking and bit shifting . When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and n \u2212 1, for any value of n that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of n , e.g. odd or prime numbers .","title":"Variable range"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#variable#range#with#minimal#movement#dynamic#hash#function","text":"When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table . A hash function that will relocate the minimum number of records when the table is resized is desirable. What is needed is a hash function H ( z , n ) \u2013 where z is the key being hashed and n is the number of allowed hash values \u2013 such that H ( z , n + 1) = H ( z , n ) with probability close to n /( n + 1). Linear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property. Extendible hashing uses a dynamic hash function that requires space proportional to n to compute the hash function, and it becomes a function of the previous keys that have been inserted. Several algorithms that preserve the uniformity property but require time proportional to n to compute the value of H ( z , n ) have been invented. A hash function with minimal movement is especially useful in distributed hash tables .","title":"Variable range with minimal movement (dynamic hash function)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#hashing#integer#data#types","text":"","title":"Hashing integer data types"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#identity#hash#function","text":"","title":"Identity hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#identity#function","text":"","title":"identity function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#trivial#hash#function","text":"","title":"Trivial hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#folding","text":"NOTE: \u5230\u5e95\u5728\u54ea\u4e2a\u7ea7\u522b\u8fdb\u884cfold\uff1f\u662f\u5728bit\u7ea7\u522b\u8fd8\u662fdigit\u7ea7\u522b\uff1f\u5728 What is Folding technique in hashing and how to implement it? \u4e2d\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u5728digit\u7ea7\u522b\u8fdb\u884cfold\u7684\u3002\u4f46\u662fWikipedia\u7ed9\u51fa\u7684\u89e3\u91ca\u662f\u5728bit\u7ea7\u522b\u8fdb\u884cfold\u7684\u3002 \u8fd9\u5176\u5b9e\u662f\u7531hash function\u7684domain\u51b3\u5b9a\u7684\uff0c\u5982\u679c\u8981hash\u7684\u662f\u6570\u5b57\uff0c\u5219\u5c31\u5728digit\u7ea7\u522b\u8fdb\u884cfold\uff1b\u5982\u679c\u8981hash\u7684\u662f\u5b57\u7b26\u4e32\uff0c\u5219\u5c31\u5728bit\u7ea7\u522b\u8fdb\u884cfold\u3002","title":"Folding"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Hash-function/#hashing#variable-length#dat","text":"","title":"Hashing variable-length dat"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Implementation-of-hash-function/","text":"Implementation of hash function \u975e\u5e38\u591a\u7684programming language\u4e2d\u90fd\u652f\u6301Customizing hash\uff0c\u6bd4\u5982 python object.__hash__ ( self ) \u00b6 java hashCode c# Object.GetHashCode Method \u6240\u4ee5\u4e86\u89e3\u4e00\u4e0b\u5b9e\u73b0hash function\u7684\u4e00\u4e9b\u7ec6\u8282\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002 Why is it best to use a prime number as a mod in a hashing function? A NOTE: \u201cbucket\u201d\u662f hash table \u4e2d\u7684\u672f\u8bed\uff0c\u5176\u5b9e\u5b83\u5c31\u662fslot Consider the set of keys K={0,1,...,100} and a hash table where the number of buckets is m=12 . Since 3 is a factor of 12, the keys that are multiples of 3 will be hashed to buckets that are multiples of 3: Keys {0,12,24,36,...} will be hashed to bucket 0. Keys {3,15,27,39,...} will be hashed to bucket 3. Keys {6,18,30,42,...} will be hashed to bucket 6. Keys {9,21,33,45,...} will be hashed to bucket 9. If K is uniformly distributed (i.e., every key in K is equally likely to occur), then the choice of m is not so critical. But, what happens if K is not uniformly distributed? Imagine that the keys that are most likely to occur are the multiples of 3. In this case, all of the buckets that are not multiples of 3 will be empty with high probability (which is really bad in terms of hash table performance). This situation is more common that it may seem. Imagine, for instance, that you are keeping track of objects based on where they are stored in memory. If your computer's word size is four bytes, then you will be hashing keys that are multiples of 4. Needless to say that choosing m to be a multiple of 4 would be a terrible choice: you would have 3m/4 buckets completely empty, and all of your keys colliding in the remaining m/4 buckets. NOTE: key\uff1a 0=4*0 \u3001 4=4*1 \u3001 8=4*2 \u3001 12=4*3 \u3001 16=4*4 \u3001 20=4*5 \u3001 24=4*6 \u3001 28=4*7 bracket\uff1a 12=4*3 key bracket 0 0 4 1 8 2 12 0 16 1 20 2 24 3 \u4e5f\u5c31\u662f\u8bf4\u63d0\u4f9b\u4e8612\u4e2abracket\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\u53ea\u4f1a\u6709 12/4=3 \u4e2abracket\u4f1a\u88ab\u4f7f\u7528\uff0c\u5269\u4f59\u7684 12-3=(12*4)/4 - 12/4= 12 * 3 /4 \u4e2abracket\u4e0d\u4f1a\u88ab\u4f7f\u7528\uff0c\u8fd9\u5c31\u662f\u4e0a\u8ff0\u7684 3m/4 buckets completely empty\u3002 In general: Every key in K that shares a common factor with the number of buckets m will be hashed to a bucket that is a multiple of this factor. Therefore, to minimize collisions, it is important to reduce the number of common factors between m and the elements of K. How can this be achieved? By choosing m to be a number that has very few factors: a prime number . SUMMARY : \u4e0a\u8ff0\u9a8c\u8bc1\u4e86\u5728 \u7efc\u8ff0 \u4e2d\u63d0\u51fa\u7684\uff1a \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u4e2d\u6570\u636e\u7684\u7279\u5f81 Why is XOR the default way to combine hashes? A Assuming uniformly random (1-bit) inputs, the AND function output probability distribution is 75% 0 and 25% 1 . Conversely, OR is 25% 0 and 75% 1 . The XOR function is 50% 0 and 50% 1 , therefore it is good for combining uniform probability distributions. This can be seen by writing out truth tables: a | b | a AND b ---+---+-------- 0 | 0 | 0 0 | 1 | 0 1 | 0 | 0 1 | 1 | 1 a | b | a OR b ---+---+-------- 0 | 0 | 0 0 | 1 | 1 1 | 0 | 1 1 | 1 | 1 a | b | a XOR b ---+---+-------- 0 | 0 | 0 0 | 1 | 1 1 | 0 | 1 1 | 1 | 0 Exercise: How many logical functions of two 1-bit inputs a and b have this uniform output distribution? Why is XOR the most suitable for the purpose stated in your question? What is the best algorithm for overriding GetHashCode? Writing a hash function in Java: a practical guide to implementing hashCode() CPython hashlib redis hash https://github.com/antirez/redis/blob/unstable/src/sha1.h https://github.com/antirez/redis/blob/unstable/src/sha256.h https://github.com/antirez/redis/blob/unstable/src/siphash.c","title":"Implementation-of-hash-function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Implementation-of-hash-function/#implementation#of#hash#function","text":"\u975e\u5e38\u591a\u7684programming language\u4e2d\u90fd\u652f\u6301Customizing hash\uff0c\u6bd4\u5982 python object.__hash__ ( self ) \u00b6 java hashCode c# Object.GetHashCode Method \u6240\u4ee5\u4e86\u89e3\u4e00\u4e0b\u5b9e\u73b0hash function\u7684\u4e00\u4e9b\u7ec6\u8282\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002","title":"Implementation of hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Implementation-of-hash-function/#why#is#it#best#to#use#a#prime#number#as#a#mod#in#a#hashing#function","text":"A NOTE: \u201cbucket\u201d\u662f hash table \u4e2d\u7684\u672f\u8bed\uff0c\u5176\u5b9e\u5b83\u5c31\u662fslot Consider the set of keys K={0,1,...,100} and a hash table where the number of buckets is m=12 . Since 3 is a factor of 12, the keys that are multiples of 3 will be hashed to buckets that are multiples of 3: Keys {0,12,24,36,...} will be hashed to bucket 0. Keys {3,15,27,39,...} will be hashed to bucket 3. Keys {6,18,30,42,...} will be hashed to bucket 6. Keys {9,21,33,45,...} will be hashed to bucket 9. If K is uniformly distributed (i.e., every key in K is equally likely to occur), then the choice of m is not so critical. But, what happens if K is not uniformly distributed? Imagine that the keys that are most likely to occur are the multiples of 3. In this case, all of the buckets that are not multiples of 3 will be empty with high probability (which is really bad in terms of hash table performance). This situation is more common that it may seem. Imagine, for instance, that you are keeping track of objects based on where they are stored in memory. If your computer's word size is four bytes, then you will be hashing keys that are multiples of 4. Needless to say that choosing m to be a multiple of 4 would be a terrible choice: you would have 3m/4 buckets completely empty, and all of your keys colliding in the remaining m/4 buckets. NOTE: key\uff1a 0=4*0 \u3001 4=4*1 \u3001 8=4*2 \u3001 12=4*3 \u3001 16=4*4 \u3001 20=4*5 \u3001 24=4*6 \u3001 28=4*7 bracket\uff1a 12=4*3 key bracket 0 0 4 1 8 2 12 0 16 1 20 2 24 3 \u4e5f\u5c31\u662f\u8bf4\u63d0\u4f9b\u4e8612\u4e2abracket\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\u53ea\u4f1a\u6709 12/4=3 \u4e2abracket\u4f1a\u88ab\u4f7f\u7528\uff0c\u5269\u4f59\u7684 12-3=(12*4)/4 - 12/4= 12 * 3 /4 \u4e2abracket\u4e0d\u4f1a\u88ab\u4f7f\u7528\uff0c\u8fd9\u5c31\u662f\u4e0a\u8ff0\u7684 3m/4 buckets completely empty\u3002 In general: Every key in K that shares a common factor with the number of buckets m will be hashed to a bucket that is a multiple of this factor. Therefore, to minimize collisions, it is important to reduce the number of common factors between m and the elements of K. How can this be achieved? By choosing m to be a number that has very few factors: a prime number . SUMMARY : \u4e0a\u8ff0\u9a8c\u8bc1\u4e86\u5728 \u7efc\u8ff0 \u4e2d\u63d0\u51fa\u7684\uff1a \u5728\u8bbe\u8ba1\u4e00\u4e2ahash function\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5b83\u7684 domain \u4e2d\u6570\u636e\u7684\u7279\u5f81","title":"Why is it best to use a prime number as a mod in a hashing function?"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Implementation-of-hash-function/#why#is#xor#the#default#way#to#combine#hashes","text":"A Assuming uniformly random (1-bit) inputs, the AND function output probability distribution is 75% 0 and 25% 1 . Conversely, OR is 25% 0 and 75% 1 . The XOR function is 50% 0 and 50% 1 , therefore it is good for combining uniform probability distributions. This can be seen by writing out truth tables: a | b | a AND b ---+---+-------- 0 | 0 | 0 0 | 1 | 0 1 | 0 | 0 1 | 1 | 1 a | b | a OR b ---+---+-------- 0 | 0 | 0 0 | 1 | 1 1 | 0 | 1 1 | 1 | 1 a | b | a XOR b ---+---+-------- 0 | 0 | 0 0 | 1 | 1 1 | 0 | 1 1 | 1 | 0 Exercise: How many logical functions of two 1-bit inputs a and b have this uniform output distribution? Why is XOR the most suitable for the purpose stated in your question?","title":"Why is XOR the default way to combine hashes?"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Implementation-of-hash-function/#what#is#the#best#algorithm#for#overriding#gethashcode","text":"","title":"What is the best algorithm for overriding GetHashCode?"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Implementation-of-hash-function/#writing#a#hash#function#in#java#a#practical#guide#to#implementing#hashcode","text":"","title":"Writing a hash function in Java: a practical guide to implementing hashCode()"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Implementation-of-hash-function/#cpython#hashlib","text":"","title":"CPython hashlib"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Implementation-of-hash-function/#redis#hash","text":"https://github.com/antirez/redis/blob/unstable/src/sha1.h https://github.com/antirez/redis/blob/unstable/src/sha256.h https://github.com/antirez/redis/blob/unstable/src/siphash.c","title":"redis hash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Perfect-hash-function/","text":"Perfect hash function \u7ef4\u57fa\u767e\u79d1 Perfect hash function","title":"Perfect-hash-function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Perfect-hash-function/#perfect#hash#function","text":"","title":"Perfect hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Perfect-hash-function/#perfect#hash#function_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Perfect hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Rolling-hash/","text":"Rolling hash A rolling hash (also known as recursive hashing or rolling checksum) is a hash function where the input is hashed in a window that moves through the input. A few hash functions allow a rolling hash to be computed very quickly\u2014the new hash value is rapidly calculated given only the old hash value, the old value removed from the window, and the new value added to the window\u2014similar to the way a moving average function can be computed much more quickly than other low-pass filters. One of the main applications is the Rabin\u2013Karp string search algorithm , which uses the rolling hash described below. Another popular application is the rsync program, which uses a checksum based on Mark Adler's adler-32 as its rolling hash. Low Bandwidth Network Filesystem (LBFS) uses a Rabin fingerprint as its rolling hash. At best, rolling hash values are pairwise independent [ 1] or strongly universal . They cannot be 3-wise independent , for example.","title":"Rolling-hash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Rolling-hash/#rolling#hash","text":"A rolling hash (also known as recursive hashing or rolling checksum) is a hash function where the input is hashed in a window that moves through the input. A few hash functions allow a rolling hash to be computed very quickly\u2014the new hash value is rapidly calculated given only the old hash value, the old value removed from the window, and the new value added to the window\u2014similar to the way a moving average function can be computed much more quickly than other low-pass filters. One of the main applications is the Rabin\u2013Karp string search algorithm , which uses the rolling hash described below. Another popular application is the rsync program, which uses a checksum based on Mark Adler's adler-32 as its rolling hash. Low Bandwidth Network Filesystem (LBFS) uses a Rabin fingerprint as its rolling hash. At best, rolling hash values are pairwise independent [ 1] or strongly universal . They cannot be 3-wise independent , for example.","title":"Rolling hash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/SipHash/","text":"SipHash SipHash","title":"SipHash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/SipHash/#siphash","text":"","title":"SipHash"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Category-Hashing/","text":"","title":"Category Hashing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Cipher/","text":"Cipher","title":"Cipher"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Cipher/#cipher","text":"","title":"Cipher"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/","text":"Cryptographic hash function Properties Applications Cryptographic hash algorithms Cryptographic hash function A cryptographic hash function ( CHF ) is a hash function that is suitable for use in cryptography . It is a mathematical algorithm that maps data of arbitrary size (often called the \"message\") to a bit string of a fixed size (the \"hash value\", \"hash\", or \"message digest\") and is a one-way function , that is, a function which is practically infeasible to invert. Ideally, the only way to find a message that produces a given hash is to attempt a brute-force search of possible inputs to see if they produce a match, or use a rainbow table of matched hashes. Cryptographic hash functions are a basic tool of modern cryptography. Properties The ideal cryptographic hash function has the following main properties: it is deterministic , meaning that the same message always results in the same hash it is quick to compute the hash value for any given message Note: The above two properties are the basic properties a good hash function should satisfy, which is introduced in Hash function . The below three properties are necessary for a ideal cryptographic hash function: it is infeasible to generate a message that yields a given hash value it is infeasible to find two different messages with the same hash value a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value ( avalanche effect ) A cryptographic hash function must be able to withstand all known types of cryptanalytic attack . In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance Given a hash value h it should be difficult to find any message m such that h = hash( m ). This concept is related to that of a one-way function . Functions that lack this property are vulnerable to preimage attacks . Second pre-image resistance Given an input m*1, it should be difficult to find a different input *m*2 such that hash(*m*1) = hash(*m*2). This property is sometimes referred to as *weak collision resistance . Functions that lack this property are vulnerable to second-preimage attacks . Collision resistance It should be difficult to find two different messages m*1 and *m*2 such that hash(*m*1) = hash(*m*2). Such a pair is called a cryptographic hash collision . This property is sometimes referred to as *strong collision resistance . It requires a hash value at least twice as long as that required for pre-image resistance; otherwise collisions may be found by a birthday attack . Collision resistance implies second pre-image resistance , but does not imply pre-image resistance . Informally, these properties mean that a malicious adversary cannot replace or modify the input data without changing its digest . Thus, if two strings have the same digest, one can be very confident that they are identical. Second pre-image resistance prevents an attacker from crafting a document with the same hash as a document the attacker cannot control. Collision resistance prevents an attacker from creating two distinct documents with the same hash. In practice, collision resistance is insufficient for many practical uses. In addition to collision resistance, it should be impossible for an adversary to find two messages with substantially similar digests; or to infer any useful information about the data, given only its digest. In particular, a hash function should behave as much as possible like a random function (often called a random oracle in proofs of security) while still being deterministic and efficiently computable. This rules out functions like the SWIFFT function, which can be rigorously proven to be collision resistant assuming that certain problems on ideal lattices are computationally difficult, but as a linear function, does not satisfy these additional properties. Checksum algorithms, such as CRC32 and other cyclic redundancy checks , are designed to meet much weaker requirements, and are generally unsuitable as cryptographic hash functions. For example, a CRC was used for message integrity in the WEP encryption standard, but an attack was readily discovered which exploited the linearity of the checksum. Applications Cryptographic hash functions have many information-security applications, notably in digital signatures , message authentication codes (MACs), and other forms of authentication . They can also be used as ordinary hash functions , to index data in hash tables , for fingerprinting , to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called ( digital ) fingerprints , checksums , or just hash values , even though all these terms stand for more general functions with rather different properties and purposes. Cryptographic hash algorithms MD5 SHA-1 RIPEMD-160 Bcrypt Whirlpool (cryptography) SHA-2 SHA-3 BLAKE2","title":"Cryptographic-hash-function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/#cryptographic#hash#function","text":"A cryptographic hash function ( CHF ) is a hash function that is suitable for use in cryptography . It is a mathematical algorithm that maps data of arbitrary size (often called the \"message\") to a bit string of a fixed size (the \"hash value\", \"hash\", or \"message digest\") and is a one-way function , that is, a function which is practically infeasible to invert. Ideally, the only way to find a message that produces a given hash is to attempt a brute-force search of possible inputs to see if they produce a match, or use a rainbow table of matched hashes. Cryptographic hash functions are a basic tool of modern cryptography.","title":"Cryptographic hash function"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/#properties","text":"The ideal cryptographic hash function has the following main properties: it is deterministic , meaning that the same message always results in the same hash it is quick to compute the hash value for any given message Note: The above two properties are the basic properties a good hash function should satisfy, which is introduced in Hash function . The below three properties are necessary for a ideal cryptographic hash function: it is infeasible to generate a message that yields a given hash value it is infeasible to find two different messages with the same hash value a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value ( avalanche effect ) A cryptographic hash function must be able to withstand all known types of cryptanalytic attack . In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance Given a hash value h it should be difficult to find any message m such that h = hash( m ). This concept is related to that of a one-way function . Functions that lack this property are vulnerable to preimage attacks . Second pre-image resistance Given an input m*1, it should be difficult to find a different input *m*2 such that hash(*m*1) = hash(*m*2). This property is sometimes referred to as *weak collision resistance . Functions that lack this property are vulnerable to second-preimage attacks . Collision resistance It should be difficult to find two different messages m*1 and *m*2 such that hash(*m*1) = hash(*m*2). Such a pair is called a cryptographic hash collision . This property is sometimes referred to as *strong collision resistance . It requires a hash value at least twice as long as that required for pre-image resistance; otherwise collisions may be found by a birthday attack . Collision resistance implies second pre-image resistance , but does not imply pre-image resistance . Informally, these properties mean that a malicious adversary cannot replace or modify the input data without changing its digest . Thus, if two strings have the same digest, one can be very confident that they are identical. Second pre-image resistance prevents an attacker from crafting a document with the same hash as a document the attacker cannot control. Collision resistance prevents an attacker from creating two distinct documents with the same hash. In practice, collision resistance is insufficient for many practical uses. In addition to collision resistance, it should be impossible for an adversary to find two messages with substantially similar digests; or to infer any useful information about the data, given only its digest. In particular, a hash function should behave as much as possible like a random function (often called a random oracle in proofs of security) while still being deterministic and efficiently computable. This rules out functions like the SWIFFT function, which can be rigorously proven to be collision resistant assuming that certain problems on ideal lattices are computationally difficult, but as a linear function, does not satisfy these additional properties. Checksum algorithms, such as CRC32 and other cyclic redundancy checks , are designed to meet much weaker requirements, and are generally unsuitable as cryptographic hash functions. For example, a CRC was used for message integrity in the WEP encryption standard, but an attack was readily discovered which exploited the linearity of the checksum.","title":"Properties"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/#applications","text":"Cryptographic hash functions have many information-security applications, notably in digital signatures , message authentication codes (MACs), and other forms of authentication . They can also be used as ordinary hash functions , to index data in hash tables , for fingerprinting , to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called ( digital ) fingerprints , checksums , or just hash values , even though all these terms stand for more general functions with rather different properties and purposes.","title":"Applications"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Cryptographic-hash-function/#cryptographic#hash#algorithms","text":"MD5 SHA-1 RIPEMD-160 Bcrypt Whirlpool (cryptography) SHA-2 SHA-3 BLAKE2","title":"Cryptographic hash algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Salt%28cryptography%29/","text":"Salt (cryptography) Application Python Reference the python doc: object.__hash__ ( self ) \u00b6","title":"Salt(cryptography)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Salt%28cryptography%29/#salt#cryptography","text":"","title":"Salt (cryptography)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Salt%28cryptography%29/#application","text":"","title":"Application"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Cryptographic-hash-functions/Salt%28cryptography%29/#python","text":"Reference the python doc: object.__hash__ ( self ) \u00b6","title":"Python"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Geometric-hashing/wikipedia-Geometric-hashing/","text":"Geometric hashing","title":"[Geometric hashing](https://en.wikipedia.org/wiki/Geometric_hashing)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Hash-function/Geometric-hashing/wikipedia-Geometric-hashing/#geometric#hashing","text":"","title":"Geometric hashing"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Randomization/wikipedia-Randomization-function/","text":"Randomization function","title":"[Randomization function](https://en.wikipedia.org/wiki/Randomization_function)"},{"location":"Relation-structure-computation/Structure/Data-structure/Hash/Randomization/wikipedia-Randomization-function/#randomization#function","text":"","title":"Randomization function"},{"location":"Relation-structure-computation/Structure/Data-structure/List/List/","text":"List (abstract data type) Linked list Related data structures Related data structures Unrolled linked list List (abstract data type) Linked list Related data structures Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list. A hash table may use linked lists to store the chains of items that hash to the same position in the hash table. A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index. A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list. Related data structures Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list. A hash table may use linked lists to store the chains of items that hash to the same position in the hash table. A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index. A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list. Unrolled linked list \u5c55\u5f00\u7684\u94fe\u8868 In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. It can dramatically increase cache performance, while decreasing the memory overhead associated with storing list metadata such as references . It is related to the B-tree .","title":"List"},{"location":"Relation-structure-computation/Structure/Data-structure/List/List/#list#abstract#data#type","text":"","title":"List (abstract data type)"},{"location":"Relation-structure-computation/Structure/Data-structure/List/List/#linked#list","text":"","title":"Linked list"},{"location":"Relation-structure-computation/Structure/Data-structure/List/List/#related#data#structures","text":"Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list. A hash table may use linked lists to store the chains of items that hash to the same position in the hash table. A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index. A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.","title":"Related data structures"},{"location":"Relation-structure-computation/Structure/Data-structure/List/List/#related#data#structures_1","text":"Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list. A hash table may use linked lists to store the chains of items that hash to the same position in the hash table. A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index. A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.","title":"Related data structures"},{"location":"Relation-structure-computation/Structure/Data-structure/List/List/#unrolled#linked#list","text":"\u5c55\u5f00\u7684\u94fe\u8868 In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. It can dramatically increase cache performance, while decreasing the memory overhead associated with storing list metadata such as references . It is related to the B-tree .","title":"Unrolled linked list"},{"location":"Relation-structure-computation/Structure/Data-structure/List/VS-Unrolled-linked%20list-VS-B-tree/","text":"\u5728 Unrolled linked list \u4e2d\u6709\u5982\u4e0b\u63cf\u8ff0 It is related to the B-tree","title":"VS-Unrolled-linked list-VS-B-tree"},{"location":"Relation-structure-computation/Structure/Data-structure/List/VS-double-linked-list-VS-binary-tree/","text":"double\u548cbinary\u90fd\u662f2\uff0c\u4e24\u8005\u7684\u8282\u70b9\u90fd\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0bstruct\u6765\u8868\u793a struct Node { void * value ; struct Node * next ; // left struct Node * prev ; // right } https://stackoverflow.com/questions/11392922/what-is-the-difference-between-node-structures-of-double-linked-list-and-binary https://cs.stackexchange.com/questions/74372/is-binary-tree-just-a-two-dimension-doubly-linked-list https://www.ritambhara.in/convert-a-binary-tree-to-a-doubly-linked-list/ \u5728https://en.wikipedia.org/wiki/Linked_list#Related_data_structures\u6709\u63d0\u53ca\u8fd9\u4e2a\u95ee\u9898 binary search tree \u9000\u5316\u4e3a linked list binary search tree\u662f\u53ef\u80fd\u9000\u5316\u4e3alinked list\u7684\uff1b","title":"VS-double-linked-list-VS-binary-tree"},{"location":"Relation-structure-computation/Structure/Data-structure/List/VS-double-linked-list-VS-binary-tree/#binary#search#tree#linked#list","text":"binary search tree\u662f\u53ef\u80fd\u9000\u5316\u4e3alinked list\u7684\uff1b","title":"binary search tree \u9000\u5316\u4e3a linked list"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Random-tree/","text":"Random tree","title":"Random-tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Random-tree/#random#tree","text":"","title":"Random tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Skip-list/","text":"Skip list Description Implementation details Skip list In computer science , a skip list is a data structure that allows {{O}}(\\log n) {{O}}(\\log n) search complexity as well as $ {{O}}(\\log n)$ insertion complexity within an ordered sequence of {n} {n} elements. Thus it can get the best of array (for searching) while maintaining a linked list -like structure that allows insertion- which is not possible in an array. Fast search is made possible by maintaining a linked hierarchy of subsequences , with each successive subsequence skipping over fewer elements than the previous one (see the picture below on the right). Searching starts in the sparsest subsequence until two consecutive elements have been found, one smaller and one larger than or equal to the element searched for. Via the linked hierarchy, these two elements link to elements of the next sparsest subsequence, where searching is continued until finally we are searching in the full sequence. The elements that are skipped over may be chosen probabilistically [ 2] or deterministically,[ 3] with the former being more common. SUMMARY : subsequence\u7684\u542b\u4e49\u662f\u5b50\u5e8f\u5217 Description A skip list is built in layers . The bottom layer is an ordinary ordered linked list . Each higher layer acts as an \"express lane\"\uff08\u5feb\u8f66\u9053\uff09 for the lists below, where an element in layer {i} {i} appears in layer {i+1} {i+1} with some fixed probability {p} {p} (two commonly used values for {p} {p} are {1/2} {1/2} or {1/4} {1/4} ). On average, each element appears in { 1/(1-p)} { 1/(1-p)} lists, and the tallest element (usually a special head element at the front of the skip list) in all the lists. The skip list contains {\\log _{1/p}n\\,} {\\log _{1/p}n\\,} (i.e. logarithm base { 1/p} { 1/p} of { n} { n} ) lists. SUMMARY : layers \u5bf9\u5e94\u7684\u662f\u4e0b\u9762\u7684Inserting elements to skip list\u56fe\u4e2d\u7684levels A schematic picture of the skip list data structure. Each box with an arrow represents a pointer and a row is a linked list giving a sparse subsequence; the numbered boxes (in yellow) at the bottom represent the ordered data sequence. Searching proceeds downwards from the sparsest subsequence at the top until consecutive elements bracketing the search element are found. Inserting elements to skip list A search for a target element begins at the head element in the top list , and proceeds horizontally\uff08\u6c34\u5e73\u7684\uff09 until the current element is greater than or equal to the target. If the current element is equal to the target, it has been found. If the current element is greater than the target, or the search reaches the end of the linked list, the procedure is repeated after returning to the previous element and dropping down vertically to the next lower list. The expected number of steps in each linked list is at most {\\displaystyle 1/p} {\\displaystyle 1/p} , which can be seen by tracing the search path backwards from the target until reaching an element that appears in the next higher list or reaching the beginning of the current list. Therefore, the total expected cost of a search is {\\displaystyle {\\tfrac {1}{p}}\\log _{1/p}n} {\\displaystyle {\\tfrac {1}{p}}\\log _{1/p}n} which is {\\displaystyle {\\mathcal {O}}(\\log n)\\,} {\\displaystyle {\\mathcal {O}}(\\log n)\\,} , when {\\displaystyle p} {\\displaystyle p} is a constant. By choosing different values of {\\displaystyle p} {\\displaystyle p} , it is possible to trade search costs against storage costs . Implementation details The elements used for a skip list can contain more than one pointer since they can participate in more than one list. Insertions and deletions are implemented much like the corresponding linked-list operations, except that \"tall\" elements must be inserted into or deleted from more than one linked list. ${\\displaystyle {\\mathcal {O}}(n)} $operations, which force us to visit every node in ascending order (such as printing the entire list), provide the opportunity to perform a behind-the-scenes derandomization of the level structure of the skip-list in an optimal way, bringing the skip list to {\\displaystyle {\\mathcal {O}}(\\log n)} {\\displaystyle {\\mathcal {O}}(\\log n)} search time. (Choose the level of the i'th finite node to be 1 plus the number of times we can repeatedly divide i by 2 before it becomes odd. Also, i=0 for the negative infinity header as we have the usual special case of choosing the highest possible level for negative and/or positive infinite nodes.) However this also allows someone to know where all of the higher-than-level 1 nodes are and delete them. Alternatively, we could make the level structure quasi-random in the following way:","title":"Skip-list"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Skip-list/#skip#list","text":"In computer science , a skip list is a data structure that allows {{O}}(\\log n) {{O}}(\\log n) search complexity as well as $ {{O}}(\\log n)$ insertion complexity within an ordered sequence of {n} {n} elements. Thus it can get the best of array (for searching) while maintaining a linked list -like structure that allows insertion- which is not possible in an array. Fast search is made possible by maintaining a linked hierarchy of subsequences , with each successive subsequence skipping over fewer elements than the previous one (see the picture below on the right). Searching starts in the sparsest subsequence until two consecutive elements have been found, one smaller and one larger than or equal to the element searched for. Via the linked hierarchy, these two elements link to elements of the next sparsest subsequence, where searching is continued until finally we are searching in the full sequence. The elements that are skipped over may be chosen probabilistically [ 2] or deterministically,[ 3] with the former being more common. SUMMARY : subsequence\u7684\u542b\u4e49\u662f\u5b50\u5e8f\u5217","title":"Skip list"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Skip-list/#description","text":"A skip list is built in layers . The bottom layer is an ordinary ordered linked list . Each higher layer acts as an \"express lane\"\uff08\u5feb\u8f66\u9053\uff09 for the lists below, where an element in layer {i} {i} appears in layer {i+1} {i+1} with some fixed probability {p} {p} (two commonly used values for {p} {p} are {1/2} {1/2} or {1/4} {1/4} ). On average, each element appears in { 1/(1-p)} { 1/(1-p)} lists, and the tallest element (usually a special head element at the front of the skip list) in all the lists. The skip list contains {\\log _{1/p}n\\,} {\\log _{1/p}n\\,} (i.e. logarithm base { 1/p} { 1/p} of { n} { n} ) lists. SUMMARY : layers \u5bf9\u5e94\u7684\u662f\u4e0b\u9762\u7684Inserting elements to skip list\u56fe\u4e2d\u7684levels A schematic picture of the skip list data structure. Each box with an arrow represents a pointer and a row is a linked list giving a sparse subsequence; the numbered boxes (in yellow) at the bottom represent the ordered data sequence. Searching proceeds downwards from the sparsest subsequence at the top until consecutive elements bracketing the search element are found. Inserting elements to skip list A search for a target element begins at the head element in the top list , and proceeds horizontally\uff08\u6c34\u5e73\u7684\uff09 until the current element is greater than or equal to the target. If the current element is equal to the target, it has been found. If the current element is greater than the target, or the search reaches the end of the linked list, the procedure is repeated after returning to the previous element and dropping down vertically to the next lower list. The expected number of steps in each linked list is at most {\\displaystyle 1/p} {\\displaystyle 1/p} , which can be seen by tracing the search path backwards from the target until reaching an element that appears in the next higher list or reaching the beginning of the current list. Therefore, the total expected cost of a search is {\\displaystyle {\\tfrac {1}{p}}\\log _{1/p}n} {\\displaystyle {\\tfrac {1}{p}}\\log _{1/p}n} which is {\\displaystyle {\\mathcal {O}}(\\log n)\\,} {\\displaystyle {\\mathcal {O}}(\\log n)\\,} , when {\\displaystyle p} {\\displaystyle p} is a constant. By choosing different values of {\\displaystyle p} {\\displaystyle p} , it is possible to trade search costs against storage costs .","title":"Description"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Skip-list/#implementation#details","text":"The elements used for a skip list can contain more than one pointer since they can participate in more than one list. Insertions and deletions are implemented much like the corresponding linked-list operations, except that \"tall\" elements must be inserted into or deleted from more than one linked list. ${\\displaystyle {\\mathcal {O}}(n)} $operations, which force us to visit every node in ascending order (such as printing the entire list), provide the opportunity to perform a behind-the-scenes derandomization of the level structure of the skip-list in an optimal way, bringing the skip list to {\\displaystyle {\\mathcal {O}}(\\log n)} {\\displaystyle {\\mathcal {O}}(\\log n)} search time. (Choose the level of the i'th finite node to be 1 plus the number of times we can repeatedly divide i by 2 before it becomes odd. Also, i=0 for the negative infinity header as we have the usual special case of choosing the highest possible level for negative and/or positive infinite nodes.) However this also allows someone to know where all of the higher-than-level 1 nodes are and delete them. Alternatively, we could make the level structure quasi-random in the following way:","title":"Implementation details"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Bloom-filter/","text":"Bloom filter developer.51cto \u9ad8\u6027\u80fd\u5f00\u53d1\u7684\u201c\u5341\u5927\u6b66\u5668\u201d\uff0c\u7231\u4e86\u7231\u4e86\uff01# \u7f13\u5b58\u6280\u672f&\u5e03\u9686\u8fc7\u6ee4\u5668 \u5728 developer.51cto \u9ad8\u6027\u80fd\u5f00\u53d1\u7684\u201c\u5341\u5927\u6b66\u5668\u201d\uff0c\u7231\u4e86\u7231\u4e86\uff01 \u4e2d\uff0c\u5bf9Bloom filter\u6709\u7740\u6bd4\u8f83\u597d\u7684\u4ecb\u7ecd: \u6709\u4e86**\u7f13\u5b58\u7cfb\u7edf**\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u5411\u6570\u636e\u5e93\u8bf7\u6c42\u4e4b\u524d\uff0c\u5148\u8be2\u95ee**\u7f13\u5b58\u7cfb\u7edf**\u662f\u5426\u6709\u6211\u4eec\u9700\u8981\u7684\u6570\u636e\uff0c\u5982\u679c\u6709\u4e14\u6ee1\u8db3\u9700\u8981\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u7701\u53bb\u4e00\u6b21\u6570\u636e\u5e93\u7684\u67e5\u8be2\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u6211\u4eec\u518d\u5411\u6570\u636e\u5e93\u8bf7\u6c42\u3002 \u6ce8\u610f\uff0c\u8fd9\u91cc\u6709\u4e00\u4e2a\u5173\u952e\u7684\u95ee\u9898\uff0c\u5982\u4f55\u5224\u65ad\u6211\u4eec\u8981\u7684\u6570\u636e\u662f\u4e0d\u662f\u5728**\u7f13\u5b58\u7cfb\u7edf**\u4e2d\u5462? \u8fdb\u4e00\u6b65\uff0c\u6211\u4eec\u628a\u8fd9\u4e2a\u95ee\u9898\u62bd\u8c61\u51fa\u6765\uff1a\u5982\u4f55\u5feb\u901f\u5224\u65ad\u4e00\u4e2a\u6570\u636e\u91cf\u5f88\u5927\u7684\u96c6\u5408\u4e2d\u662f\u5426\u5305\u542b\u6211\u4eec\u6307\u5b9a\u7684\u6570\u636e? \u8fd9\u4e2a\u65f6\u5019\uff0c\u5c31\u662f**\u5e03\u9686\u8fc7\u6ee4\u5668**\u5927\u663e\u8eab\u624b\u7684\u65f6\u5019\u4e86\uff0c\u5b83\u5c31\u662f\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u800c\u8bde\u751f\u7684\u3002\u90a3**\u5e03\u9686\u8fc7\u6ee4\u5668**\u662f\u5982\u4f55\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u5462? \u5148\u56de\u5230\u4e0a\u9762\u7684\u95ee\u9898\u4e2d\u6765\uff0c\u8fd9\u5176\u5b9e\u662f\u4e00\u4e2a**\u67e5\u627e\u95ee\u9898**\uff0c\u5bf9\u4e8e**\u67e5\u627e\u95ee\u9898**\uff0c\u6700\u5e38\u7528\u7684\u89e3\u51b3\u65b9\u6848\u662f**\u641c\u7d22\u6811**\u548c**\u54c8\u5e0c\u8868**\u4e24\u79cd\u65b9\u6848\u3002 \u56e0\u4e3a\u8fd9\u4e2a\u95ee\u9898\u6709\u4e24\u4e2a\u5173\u952e\u70b9\uff1a \u5feb\u901f \u3001**\u6570\u636e\u91cf**\u5f88\u5927\u3002 **\u6811\u7ed3\u6784**\u9996\u5148\u5f97\u6392\u9664\uff0c\u54c8\u5e0c\u8868\u5012\u662f\u53ef\u4ee5\u505a\u5230\u5e38\u6570\u9636\u7684\u6027\u80fd\uff0c\u4f46\u6570\u636e\u91cf\u5927\u4e86\u4ee5\u540e\uff0c\u4e00\u65b9\u9762\u5bf9**\u54c8\u5e0c\u8868**\u7684**\u5bb9\u91cf**\u8981\u6c42\u5de8\u5927\uff0c\u53e6\u4e00\u65b9\u9762\u5982\u4f55\u8bbe\u8ba1\u4e00\u4e2a\u597d\u7684**\u54c8\u5e0c\u7b97\u6cd5**\u80fd\u591f\u505a\u5230\u5982\u6b64\u5927\u91cf\u6570\u636e\u7684\u54c8\u5e0c\u6620\u5c04\u4e5f\u662f\u4e00\u4e2a\u96be\u9898\u3002 \u5bf9\u4e8e**\u5bb9\u91cf**\u7684\u95ee\u9898\uff0c\u8003\u8651\u5230\u53ea\u9700\u8981\u5224\u65ad\u5bf9\u8c61\u662f\u5426\u5b58\u5728\uff0c\u800c\u5e76\u975e\u62ff\u5230\u5bf9\u8c61\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06**\u54c8\u5e0c\u8868**\u7684**\u8868\u9879**\u5927\u5c0f\u8bbe\u7f6e\u4e3a 1 \u4e2a bit\uff0c1 \u8868\u793a\u5b58\u5728\uff0c0 \u8868\u793a\u4e0d\u5b58\u5728\uff0c\u8fd9\u6837\u5927\u5927\u7f29\u5c0f**\u54c8\u5e0c\u8868**\u7684\u5bb9\u91cf\u3002 \u800c\u5bf9\u4e8e**\u54c8\u5e0c\u7b97\u6cd5**\u7684\u95ee\u9898\uff0c\u5982\u679c\u6211\u4eec\u5bf9**\u54c8\u5e0c\u7b97\u6cd5**\u8981\u6c42\u4f4e\u4e00\u4e9b\uff0c\u90a3**\u54c8\u5e0c\u78b0\u649e**\u7684\u673a\u7387\u5c31\u4f1a\u589e\u52a0\u3002 \u90a3\u4e00\u4e2a**\u54c8\u5e0c\u7b97\u6cd5**\u5bb9\u6613\u51b2\u7a81\uff0c\u90a3\u5c31\u591a\u5f04\u51e0\u4e2a\uff0c\u591a\u4e2a\u54c8\u5e0c\u51fd\u6570\u540c\u65f6\u51b2\u7a81\u7684\u6982\u7387\u5c31\u5c0f\u7684\u591a\u3002 **\u5e03\u9686\u8fc7\u6ee4\u5668**\u5c31\u662f\u57fa\u4e8e\u8fd9\u6837\u7684\u8bbe\u8ba1\u601d\u8def\uff1a \u5f53\u8bbe\u7f6e\u5bf9\u5e94\u7684 key-value \u65f6\uff0c\u6309\u7167\u4e00\u7ec4\u54c8\u5e0c\u7b97\u6cd5\u7684\u8ba1\u7b97\uff0c\u5c06\u5bf9\u5e94\u6bd4\u7279\u4f4d\u7f6e 1\u3002 \u4f46\u5f53\u5bf9\u5e94\u7684 key-value \u5220\u9664\u65f6\uff0c\u5374\u4e0d\u80fd\u5c06\u5bf9\u5e94\u7684\u6bd4\u7279\u4f4d\u7f6e 0\uff0c\u56e0\u4e3a\u4fdd\u4e0d\u51c6\u5176\u4ed6\u67d0\u4e2a key \u7684\u67d0\u4e2a\u54c8\u5e0c\u7b97\u6cd5\u4e5f\u6620\u5c04\u5230\u4e86\u540c\u4e00\u4e2a\u4f4d\u7f6e\u3002 \u4e5f\u6b63\u662f\u56e0\u4e3a\u8fd9\u6837\uff0c\u5f15\u51fa\u4e86\u5e03\u9686\u8fc7\u6ee4\u5668\u7684\u53e6\u5916\u4e00\u4e2a\u91cd\u8981\u7279\u70b9\uff1a\u5e03\u9686\u8fc7\u6ee4\u5668\u5224\u5b9a\u5b58\u5728\u7684\u5b9e\u9645\u4e0a\u4e0d\u4e00\u5b9a\u5b58\u5728\uff0c\u4f46\u5224\u5b9a\u4e0d\u5b58\u5728\u7684\u5219\u4e00\u5b9a\u4e0d\u5b58\u5728\u3002 wikipedia Bloom filter","title":"Bloom-filter"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Bloom-filter/#bloom#filter","text":"","title":"Bloom filter"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Bloom-filter/#developer51cto","text":"\u5728 developer.51cto \u9ad8\u6027\u80fd\u5f00\u53d1\u7684\u201c\u5341\u5927\u6b66\u5668\u201d\uff0c\u7231\u4e86\u7231\u4e86\uff01 \u4e2d\uff0c\u5bf9Bloom filter\u6709\u7740\u6bd4\u8f83\u597d\u7684\u4ecb\u7ecd: \u6709\u4e86**\u7f13\u5b58\u7cfb\u7edf**\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u5411\u6570\u636e\u5e93\u8bf7\u6c42\u4e4b\u524d\uff0c\u5148\u8be2\u95ee**\u7f13\u5b58\u7cfb\u7edf**\u662f\u5426\u6709\u6211\u4eec\u9700\u8981\u7684\u6570\u636e\uff0c\u5982\u679c\u6709\u4e14\u6ee1\u8db3\u9700\u8981\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u7701\u53bb\u4e00\u6b21\u6570\u636e\u5e93\u7684\u67e5\u8be2\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u6211\u4eec\u518d\u5411\u6570\u636e\u5e93\u8bf7\u6c42\u3002 \u6ce8\u610f\uff0c\u8fd9\u91cc\u6709\u4e00\u4e2a\u5173\u952e\u7684\u95ee\u9898\uff0c\u5982\u4f55\u5224\u65ad\u6211\u4eec\u8981\u7684\u6570\u636e\u662f\u4e0d\u662f\u5728**\u7f13\u5b58\u7cfb\u7edf**\u4e2d\u5462? \u8fdb\u4e00\u6b65\uff0c\u6211\u4eec\u628a\u8fd9\u4e2a\u95ee\u9898\u62bd\u8c61\u51fa\u6765\uff1a\u5982\u4f55\u5feb\u901f\u5224\u65ad\u4e00\u4e2a\u6570\u636e\u91cf\u5f88\u5927\u7684\u96c6\u5408\u4e2d\u662f\u5426\u5305\u542b\u6211\u4eec\u6307\u5b9a\u7684\u6570\u636e? \u8fd9\u4e2a\u65f6\u5019\uff0c\u5c31\u662f**\u5e03\u9686\u8fc7\u6ee4\u5668**\u5927\u663e\u8eab\u624b\u7684\u65f6\u5019\u4e86\uff0c\u5b83\u5c31\u662f\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u800c\u8bde\u751f\u7684\u3002\u90a3**\u5e03\u9686\u8fc7\u6ee4\u5668**\u662f\u5982\u4f55\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u5462? \u5148\u56de\u5230\u4e0a\u9762\u7684\u95ee\u9898\u4e2d\u6765\uff0c\u8fd9\u5176\u5b9e\u662f\u4e00\u4e2a**\u67e5\u627e\u95ee\u9898**\uff0c\u5bf9\u4e8e**\u67e5\u627e\u95ee\u9898**\uff0c\u6700\u5e38\u7528\u7684\u89e3\u51b3\u65b9\u6848\u662f**\u641c\u7d22\u6811**\u548c**\u54c8\u5e0c\u8868**\u4e24\u79cd\u65b9\u6848\u3002 \u56e0\u4e3a\u8fd9\u4e2a\u95ee\u9898\u6709\u4e24\u4e2a\u5173\u952e\u70b9\uff1a \u5feb\u901f \u3001**\u6570\u636e\u91cf**\u5f88\u5927\u3002 **\u6811\u7ed3\u6784**\u9996\u5148\u5f97\u6392\u9664\uff0c\u54c8\u5e0c\u8868\u5012\u662f\u53ef\u4ee5\u505a\u5230\u5e38\u6570\u9636\u7684\u6027\u80fd\uff0c\u4f46\u6570\u636e\u91cf\u5927\u4e86\u4ee5\u540e\uff0c\u4e00\u65b9\u9762\u5bf9**\u54c8\u5e0c\u8868**\u7684**\u5bb9\u91cf**\u8981\u6c42\u5de8\u5927\uff0c\u53e6\u4e00\u65b9\u9762\u5982\u4f55\u8bbe\u8ba1\u4e00\u4e2a\u597d\u7684**\u54c8\u5e0c\u7b97\u6cd5**\u80fd\u591f\u505a\u5230\u5982\u6b64\u5927\u91cf\u6570\u636e\u7684\u54c8\u5e0c\u6620\u5c04\u4e5f\u662f\u4e00\u4e2a\u96be\u9898\u3002 \u5bf9\u4e8e**\u5bb9\u91cf**\u7684\u95ee\u9898\uff0c\u8003\u8651\u5230\u53ea\u9700\u8981\u5224\u65ad\u5bf9\u8c61\u662f\u5426\u5b58\u5728\uff0c\u800c\u5e76\u975e\u62ff\u5230\u5bf9\u8c61\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06**\u54c8\u5e0c\u8868**\u7684**\u8868\u9879**\u5927\u5c0f\u8bbe\u7f6e\u4e3a 1 \u4e2a bit\uff0c1 \u8868\u793a\u5b58\u5728\uff0c0 \u8868\u793a\u4e0d\u5b58\u5728\uff0c\u8fd9\u6837\u5927\u5927\u7f29\u5c0f**\u54c8\u5e0c\u8868**\u7684\u5bb9\u91cf\u3002 \u800c\u5bf9\u4e8e**\u54c8\u5e0c\u7b97\u6cd5**\u7684\u95ee\u9898\uff0c\u5982\u679c\u6211\u4eec\u5bf9**\u54c8\u5e0c\u7b97\u6cd5**\u8981\u6c42\u4f4e\u4e00\u4e9b\uff0c\u90a3**\u54c8\u5e0c\u78b0\u649e**\u7684\u673a\u7387\u5c31\u4f1a\u589e\u52a0\u3002 \u90a3\u4e00\u4e2a**\u54c8\u5e0c\u7b97\u6cd5**\u5bb9\u6613\u51b2\u7a81\uff0c\u90a3\u5c31\u591a\u5f04\u51e0\u4e2a\uff0c\u591a\u4e2a\u54c8\u5e0c\u51fd\u6570\u540c\u65f6\u51b2\u7a81\u7684\u6982\u7387\u5c31\u5c0f\u7684\u591a\u3002 **\u5e03\u9686\u8fc7\u6ee4\u5668**\u5c31\u662f\u57fa\u4e8e\u8fd9\u6837\u7684\u8bbe\u8ba1\u601d\u8def\uff1a \u5f53\u8bbe\u7f6e\u5bf9\u5e94\u7684 key-value \u65f6\uff0c\u6309\u7167\u4e00\u7ec4\u54c8\u5e0c\u7b97\u6cd5\u7684\u8ba1\u7b97\uff0c\u5c06\u5bf9\u5e94\u6bd4\u7279\u4f4d\u7f6e 1\u3002 \u4f46\u5f53\u5bf9\u5e94\u7684 key-value \u5220\u9664\u65f6\uff0c\u5374\u4e0d\u80fd\u5c06\u5bf9\u5e94\u7684\u6bd4\u7279\u4f4d\u7f6e 0\uff0c\u56e0\u4e3a\u4fdd\u4e0d\u51c6\u5176\u4ed6\u67d0\u4e2a key \u7684\u67d0\u4e2a\u54c8\u5e0c\u7b97\u6cd5\u4e5f\u6620\u5c04\u5230\u4e86\u540c\u4e00\u4e2a\u4f4d\u7f6e\u3002 \u4e5f\u6b63\u662f\u56e0\u4e3a\u8fd9\u6837\uff0c\u5f15\u51fa\u4e86\u5e03\u9686\u8fc7\u6ee4\u5668\u7684\u53e6\u5916\u4e00\u4e2a\u91cd\u8981\u7279\u70b9\uff1a\u5e03\u9686\u8fc7\u6ee4\u5668\u5224\u5b9a\u5b58\u5728\u7684\u5b9e\u9645\u4e0a\u4e0d\u4e00\u5b9a\u5b58\u5728\uff0c\u4f46\u5224\u5b9a\u4e0d\u5b58\u5728\u7684\u5219\u4e00\u5b9a\u4e0d\u5b58\u5728\u3002","title":"developer.51cto \u9ad8\u6027\u80fd\u5f00\u53d1\u7684\u201c\u5341\u5927\u6b66\u5668\u201d\uff0c\u7231\u4e86\u7231\u4e86\uff01#\u7f13\u5b58\u6280\u672f&amp;\u5e03\u9686\u8fc7\u6ee4\u5668"},{"location":"Relation-structure-computation/Structure/Data-structure/Probabilistic-data-structures/Bloom-filter/#wikipedia#bloom#filter","text":"","title":"wikipedia Bloom filter"},{"location":"Relation-structure-computation/Structure/Data-structure/Set/Set%28abstract-data-type%29/","text":"Set (abstract data type) Implementations Set (abstract data type) In computer science , a set is an abstract data type that can store unique values, without any particular order . It is a computer implementation of the mathematical concept of a finite set . Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set. Some set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements \u2014 such as checking whether a given value is in the set, or enumerating the values in some arbitrary order. Other variants, called dynamic or mutable sets , allow also the insertion and deletion of elements from the set. A multiset is a special kind of set in which an element can figure several times. Implementations Sets can be implemented using various data structures , which provide different time and space trade-offs for various operations. Some implementations are designed to improve the efficiency of very specialized operations, such as nearest or union . Implementations described as \"general use\" typically strive to optimize the element_of , add , and delete operations. A simple implementation is to use a list , ignoring the order of the elements and taking care to avoid repeated values. This is simple but inefficient, as operations like set membership or element deletion are O ( n ), as they require scanning the entire list.[ b] Sets are often instead implemented using more efficient data structures, particularly various flavors of trees , tries , or hash tables . As sets can be interpreted as a kind of map (by the indicator function), sets are commonly implemented in the same way as (partial) maps ( associative arrays ) \u2013 in this case in which the value of each key-value pair has the unit type or a sentinel value (like 1) \u2013 namely, a self-balancing binary search tree for sorted sets (which has O(log n) for most operations), or a hash table for unsorted sets (which has O(1) average-case, but O(n) worst-case, for most operations). A sorted linear hash table[ 8] may be used to provide deterministically ordered sets. Further, in languages that support maps but not sets, sets can be implemented in terms of maps. For example, a common programming idiom in Perl that converts an array to a hash whose values are the sentinel value 1, for use as a set, is: my %elements = map { $_ => 1 } @elements; Other popular methods include arrays . In particular a subset of the integers 1.. n can be implemented efficiently as an n -bit bit array , which also support very efficient union and intersection operations. A Bloom map implements a set probabilistically, using a very compact representation but risking a small chance of false positives on queries. The Boolean set operations can be implemented in terms of more elementary operations ( pop , clear , and add ), but specialized algorithms may yield lower asymptotic time bounds. If sets are implemented as sorted lists, for example, the naive algorithm for union(*S*,*T*) will take time proportional to the length m of S times the length n of T ; whereas a variant of the list merging algorithm will do the job in time proportional to m + n . Moreover, there are specialized set data structures (such as the union-find data structure ) that are optimized for one or more of these operations, at the expense of others. How is set() implemented in python ? What are the underlying data structures used for Redis? How is the Redis sorted set implemented?","title":"Set"},{"location":"Relation-structure-computation/Structure/Data-structure/Set/Set%28abstract-data-type%29/#set#abstract#data#type","text":"In computer science , a set is an abstract data type that can store unique values, without any particular order . It is a computer implementation of the mathematical concept of a finite set . Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set. Some set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements \u2014 such as checking whether a given value is in the set, or enumerating the values in some arbitrary order. Other variants, called dynamic or mutable sets , allow also the insertion and deletion of elements from the set. A multiset is a special kind of set in which an element can figure several times.","title":"Set (abstract data type)"},{"location":"Relation-structure-computation/Structure/Data-structure/Set/Set%28abstract-data-type%29/#implementations","text":"Sets can be implemented using various data structures , which provide different time and space trade-offs for various operations. Some implementations are designed to improve the efficiency of very specialized operations, such as nearest or union . Implementations described as \"general use\" typically strive to optimize the element_of , add , and delete operations. A simple implementation is to use a list , ignoring the order of the elements and taking care to avoid repeated values. This is simple but inefficient, as operations like set membership or element deletion are O ( n ), as they require scanning the entire list.[ b] Sets are often instead implemented using more efficient data structures, particularly various flavors of trees , tries , or hash tables . As sets can be interpreted as a kind of map (by the indicator function), sets are commonly implemented in the same way as (partial) maps ( associative arrays ) \u2013 in this case in which the value of each key-value pair has the unit type or a sentinel value (like 1) \u2013 namely, a self-balancing binary search tree for sorted sets (which has O(log n) for most operations), or a hash table for unsorted sets (which has O(1) average-case, but O(n) worst-case, for most operations). A sorted linear hash table[ 8] may be used to provide deterministically ordered sets. Further, in languages that support maps but not sets, sets can be implemented in terms of maps. For example, a common programming idiom in Perl that converts an array to a hash whose values are the sentinel value 1, for use as a set, is: my %elements = map { $_ => 1 } @elements; Other popular methods include arrays . In particular a subset of the integers 1.. n can be implemented efficiently as an n -bit bit array , which also support very efficient union and intersection operations. A Bloom map implements a set probabilistically, using a very compact representation but risking a small chance of false positives on queries. The Boolean set operations can be implemented in terms of more elementary operations ( pop , clear , and add ), but specialized algorithms may yield lower asymptotic time bounds. If sets are implemented as sorted lists, for example, the naive algorithm for union(*S*,*T*) will take time proportional to the length m of S times the length n of T ; whereas a variant of the list merging algorithm will do the job in time proportional to m + n . Moreover, there are specialized set data structures (such as the union-find data structure ) that are optimized for one or more of these operations, at the expense of others. How is set() implemented in python ? What are the underlying data structures used for Redis? How is the Redis sorted set implemented?","title":"Implementations"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/","text":"Application of stack \u6808\u6709\u7740\u975e\u5e38\u591a\u7684\u5e94\u7528\uff0c\u6211\u89c9\u5f97\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0b\u8fd9\u4e9b\u7eb7\u7e41\u590d\u6742\u7684\u5e94\u7528\u4e3b\u8981\u4f7f\u7528\u4e86\u6808\u7684\u54ea\u4e9b\u7279\u6027\u3002 \u6808\u7279\u6027\uff1a\u5148\u8fdb\u540e\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09 \u6808\u7684\u540e\u8fdb\u5148\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09\u7279\u6027\u5176\u5b9e\u5b58\u5728\u7740\u4e00\u79cd\u5929\u7136\u7684**\u9006\u5e8f**\uff08\u4e0e\u6b64\u76f8\u53cd\u7684\u662fqueue\u7684\u5148\u8fdb\u5148\u51fa\u5219\u662f\u5929\u7136\u7684**\u987a\u5e8f**\uff09 - \u9006\u5e8f \u6808\u7279\u6027\uff1a\u6709\u8fdb\u6709\u51fa \u62ec\u53f7\u5339\u914d\uff08\u6b63\u62ec\u53f7\u8fdb\u6808\uff0c\u53cd\u62ec\u53f7\u51fa\u6808\uff09\uff0c\u51fd\u6570\u6267\u884c\uff08\u8c03\u7528\u51fd\u6570\u8fdb\u6808\uff0c\u51fd\u6570\u8fd4\u56de\u51fa\u6808\uff09 \u6808\u7279\u6027\uff1a\u7ebf\u6027 \u6808\u662f\u4e00\u79cd**\u7ebf\u6027**\u7684\u6570\u636e\u7ed3\u6784\uff0c\u4e00\u4e9b**\u7ebf\u6027\u5e8f\u5217**\u53ef\u4ee5\u57fa\u4e8e\u6808\u6765\u505a\u4e00\u4e9b**\u76f8\u90bb\u5143\u7d20**\u7684\u57fa\u4e8e**\u67d0\u79cd\u5173\u7cfb**\u7684**\u805a\u5408**\u3002 - \u76f8\u90bb - \u57fa\u4e8e\u67d0\u79cd\u5173\u7cfb\u7684\u805a\u5408 All nearest smaller values \u4f7f\u7528\u6808\u6765\u8868\u793a**\u6700\u8fd1** \u62ec\u53f7\u5339\u914d\u95ee\u9898 \u5176\u5b9e\u62ec\u53f7\u5339\u914d\u95ee\u9898\u4e5f\u53ef\u4ee5\u4f7f\u7528\u8fd9\u79cd\u601d\u8def\u6765\u8fdb\u884c\u5206\u6790\uff1a\u5305\u542b\u6709\u62ec\u53f7\u5bf9\u7684\u5b57\u7b26\u4e32\u662f**\u7ebf\u6027**\u7684\uff0c\u5728\u8fdb\u884c\u62ec\u53f7\u5339\u914d\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4ec5\u4ec5\u5173\u6ce8\u7684\u662f\u62ec\u53f7\u5b57\u7b26\u800c\u5ffd\u89c6\u6240\u6709\u7684\u5176\u4ed6\u5b57\u7b26\uff0c\u6240\u4ee5\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0c\u5b57\u7b26\u4e32\u4ec5\u4ec5\u5305\u542b\u4e86\u62ec\u53f7\u5bf9\uff0c\u6240\u4ee5\u5176\u4e2d\u7684\u5143\u7d20\u90fd\u662f\u76f8\u90bb\u7684\uff1b \u6b63\u62ec\u53f7**\u4e0e**\u53cd\u62ec\u53f7**\u4e4b\u95f4\u5b58\u5728\u7684\u5173\u7cfb\u662f**\u5339\u914d\u5173\u7cfb \uff0c\u53ea\u6709\u5f53\u76f8\u90bb\u4e24\u4e2a\u62ec\u53f7\u5b57\u7b26\u4e4b\u95f4\u5b58\u5728\u7740**\u5339\u914d\u5173\u7cfb**\u7684\u65f6\u5019\uff0c\u6211\u4eec\u624d\u5c06\u5b83\u4eec**\u805a\u5408**\uff08\u5bf9\u5e94\u8fd9\u5c06\u51fa\u6808\uff09\uff1b\u5176\u5b9e\u8fd9\u79cd\u6846\u67b6\u662f\u80fd\u591f\u89e3\u51b3\u975e\u5e38\u591a\u7684\u7c7b\u4f3c\u8fd9\u6837\u7684\u95ee\u9898\u7684 \u663e\u793a\u6808\u548c\u9690\u5f0f\u6808 \u9690\u5f0f\u6808\u662f\u6307\u4f7f\u7528call stack\uff0c\u6bd4\u5982\uff1a python\u7684 pgen \u4f7f\u7528\u9690\u5f0f\u6808\u6765\u5b9e\u73b0\u62ec\u53f7\u7684\u5339\u914d \u603b\u7ed3 stack in automata theory Deterministic pushdown automaton Pushdown automaton Stack-oriented programming Stack machine","title":"Application-of-stack"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#application#of#stack","text":"\u6808\u6709\u7740\u975e\u5e38\u591a\u7684\u5e94\u7528\uff0c\u6211\u89c9\u5f97\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0b\u8fd9\u4e9b\u7eb7\u7e41\u590d\u6742\u7684\u5e94\u7528\u4e3b\u8981\u4f7f\u7528\u4e86\u6808\u7684\u54ea\u4e9b\u7279\u6027\u3002","title":"Application of stack"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#_1","text":"\u6808\u7684\u540e\u8fdb\u5148\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09\u7279\u6027\u5176\u5b9e\u5b58\u5728\u7740\u4e00\u79cd\u5929\u7136\u7684**\u9006\u5e8f**\uff08\u4e0e\u6b64\u76f8\u53cd\u7684\u662fqueue\u7684\u5148\u8fdb\u5148\u51fa\u5219\u662f\u5929\u7136\u7684**\u987a\u5e8f**\uff09 - \u9006\u5e8f","title":"\u6808\u7279\u6027\uff1a\u5148\u8fdb\u540e\u51fa\uff08\u5148\u8fdb\u540e\u51fa\uff09"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#_2","text":"\u62ec\u53f7\u5339\u914d\uff08\u6b63\u62ec\u53f7\u8fdb\u6808\uff0c\u53cd\u62ec\u53f7\u51fa\u6808\uff09\uff0c\u51fd\u6570\u6267\u884c\uff08\u8c03\u7528\u51fd\u6570\u8fdb\u6808\uff0c\u51fd\u6570\u8fd4\u56de\u51fa\u6808\uff09","title":"\u6808\u7279\u6027\uff1a\u6709\u8fdb\u6709\u51fa"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#_3","text":"\u6808\u662f\u4e00\u79cd**\u7ebf\u6027**\u7684\u6570\u636e\u7ed3\u6784\uff0c\u4e00\u4e9b**\u7ebf\u6027\u5e8f\u5217**\u53ef\u4ee5\u57fa\u4e8e\u6808\u6765\u505a\u4e00\u4e9b**\u76f8\u90bb\u5143\u7d20**\u7684\u57fa\u4e8e**\u67d0\u79cd\u5173\u7cfb**\u7684**\u805a\u5408**\u3002 - \u76f8\u90bb - \u57fa\u4e8e\u67d0\u79cd\u5173\u7cfb\u7684\u805a\u5408","title":"\u6808\u7279\u6027\uff1a\u7ebf\u6027"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#all#nearest#smaller#values","text":"\u4f7f\u7528\u6808\u6765\u8868\u793a**\u6700\u8fd1**","title":"All nearest smaller values"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#_4","text":"\u5176\u5b9e\u62ec\u53f7\u5339\u914d\u95ee\u9898\u4e5f\u53ef\u4ee5\u4f7f\u7528\u8fd9\u79cd\u601d\u8def\u6765\u8fdb\u884c\u5206\u6790\uff1a\u5305\u542b\u6709\u62ec\u53f7\u5bf9\u7684\u5b57\u7b26\u4e32\u662f**\u7ebf\u6027**\u7684\uff0c\u5728\u8fdb\u884c\u62ec\u53f7\u5339\u914d\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4ec5\u4ec5\u5173\u6ce8\u7684\u662f\u62ec\u53f7\u5b57\u7b26\u800c\u5ffd\u89c6\u6240\u6709\u7684\u5176\u4ed6\u5b57\u7b26\uff0c\u6240\u4ee5\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0c\u5b57\u7b26\u4e32\u4ec5\u4ec5\u5305\u542b\u4e86\u62ec\u53f7\u5bf9\uff0c\u6240\u4ee5\u5176\u4e2d\u7684\u5143\u7d20\u90fd\u662f\u76f8\u90bb\u7684\uff1b \u6b63\u62ec\u53f7**\u4e0e**\u53cd\u62ec\u53f7**\u4e4b\u95f4\u5b58\u5728\u7684\u5173\u7cfb\u662f**\u5339\u914d\u5173\u7cfb \uff0c\u53ea\u6709\u5f53\u76f8\u90bb\u4e24\u4e2a\u62ec\u53f7\u5b57\u7b26\u4e4b\u95f4\u5b58\u5728\u7740**\u5339\u914d\u5173\u7cfb**\u7684\u65f6\u5019\uff0c\u6211\u4eec\u624d\u5c06\u5b83\u4eec**\u805a\u5408**\uff08\u5bf9\u5e94\u8fd9\u5c06\u51fa\u6808\uff09\uff1b\u5176\u5b9e\u8fd9\u79cd\u6846\u67b6\u662f\u80fd\u591f\u89e3\u51b3\u975e\u5e38\u591a\u7684\u7c7b\u4f3c\u8fd9\u6837\u7684\u95ee\u9898\u7684","title":"\u62ec\u53f7\u5339\u914d\u95ee\u9898"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#_5","text":"\u9690\u5f0f\u6808\u662f\u6307\u4f7f\u7528call stack\uff0c\u6bd4\u5982\uff1a python\u7684 pgen \u4f7f\u7528\u9690\u5f0f\u6808\u6765\u5b9e\u73b0\u62ec\u53f7\u7684\u5339\u914d","title":"\u663e\u793a\u6808\u548c\u9690\u5f0f\u6808"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#_6","text":"","title":"\u603b\u7ed3"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#stack#in#automata#theory","text":"Deterministic pushdown automaton Pushdown automaton","title":"stack in automata theory"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#stack-oriented#programming","text":"","title":"Stack-oriented programming"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Application-of-stack/#stack#machine","text":"","title":"Stack machine"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack%28abstract-data-type%29/","text":"Stack (abstract data type) In computer science , a stack is an abstract data type that serves as a collection of elements, with two principal operations: push , which adds an element to the collection, and pop , which removes the most recently added element that was not yet removed. The order in which elements come off a stack gives rise to its alternative name, LIFO ( last in, first out ). Additionally, a peek operation may give access to the top without modifying the stack. The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first. Considered as a linear data structure , or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A stack is needed to implement depth-first search . Applications of stacks Expression evaluation and syntax parsing Calculators employing reverse Polish notation use a stack structure to hold values. Expressions can be represented in prefix, postfix or infix notations and conversion from one form to another may be accomplished using a stack. Many compilers use a stack for parsing the syntax of expressions, program blocks etc. before translating into low level code. Most programming languages are context-free languages , allowing them to be parsed with stack based machines. Backtracking Main article: Backtracking Another important application of stacks is backtracking . Consider a simple example of finding the correct path in a maze. There are a series of points, from the starting point to the destination. We start from one point. To reach the final destination, there are several paths. Suppose we choose a random path. After following a certain path, we realise that the path we have chosen is wrong. So we need to find a way by which we can return to the beginning of that path. This can be done with the use of stacks. With the help of stacks, we remember the point where we have reached. This is done by pushing that point into the stack. In case we end up on the wrong path, we can pop the last point from the stack and thus return to the last point and continue our quest to find the right path. This is called backtracking. The prototypical example of a backtracking algorithm is depth-first search , which finds all vertices of a graph that can be reached from a specified starting vertex. Other applications of backtracking involve searching through spaces that represent potential solutions to an optimization problem. Branch and bound is a technique for performing such backtracking searches without exhaustively searching all of the potential solutions in such a space. Compile time memory management Main articles: Stack-based memory allocation and Stack machine A number of programming languages are stack-oriented , meaning they define most basic operations (adding two numbers, printing a character) as taking their arguments from the stack, and placing any return values back on the stack. For example, PostScript has a return stack and an operand stack, and also has a graphics state stack and a dictionary stack. Many virtual machines are also stack-oriented, including the p-code machine and the Java Virtual Machine . Almost all calling conventions \u200d\u2014\u200cthe ways in which subroutines receive their parameters and return results\u200d\u2014\u200cuse a special stack (the \" call stack \") to hold information about procedure/function calling and nesting in order to switch to the context of the called function and restore to the caller function when the calling finishes. The functions follow a runtime protocol between caller and callee to save arguments and return value on the stack. Stacks are an important way of supporting nested or recursive function calls. This type of stack is used implicitly by the compiler to support CALL and RETURN statements (or their equivalents) and is not manipulated directly by the programmer. Some programming languages use the stack to store data that is local to a procedure. Space for local data items is allocated from the stack when the procedure is entered, and is deallocated when the procedure exits. The C programming language is typically implemented in this way. Using the same stack for both data and procedure calls has important security implications (see below) of which a programmer must be aware in order to avoid introducing serious security bugs into a program. Efficient algorithms Several algorithms use a stack (separate from the usual function call stack of most programming languages) as the principle data structure with which they organize their information. These include: Graham scan , an algorithm for the convex hull of a two-dimensional system of points. A convex hull of a subset of the input is maintained in a stack, which is used to find and remove concavities in the boundary when a new point is added to the hull.[ 10] Part of the SMAWK algorithm for finding the row minima of a monotone matrix uses stacks in a similar way to Graham scan.[ 11] All nearest smaller values , the problem of finding, for each number in an array, the closest preceding number that is smaller than it. One algorithm for this problem uses a stack to maintain a collection of candidates for the nearest smaller value. For each position in the array, the stack is popped until a smaller value is found on its top, and then the value in the new position is pushed onto the stack.[ 12] The nearest-neighbor chain algorithm , a method for agglomerative hierarchical clustering based on maintaining a stack of clusters, each of which is the nearest neighbor of its predecessor on the stack. When this method finds a pair of clusters that are mutual nearest neighbors, they are popped and merged.[ 13] SUMMARY : \u6808\u7684\u540e\u8fdb\u5148\u51fa\u7684\u7279\u6027\u662f\u4e00\u79cd\u5012\u5e8f\u7684\u7279\u6027\uff0c\u5b83\u975e\u5e38\u9002\u5408\u4e8e\u5b9e\u73b0\u4e00\u4e9b\u9700\u8981\u6267\u884c\u9006\u8f6c\u64cd\u4f5c\u7684\u7b97\u6cd5\uff1b Security Some computing environments use stacks in ways that may make them vulnerable to security breaches and attacks. Programmers working in such environments must take special care to avoid the pitfalls of these implementations. For example, some programming languages use a common stack to store both data local to a called procedure and the linking information that allows the procedure to return to its caller. This means that the program moves data into and out of the same stack that contains critical return addresses for the procedure calls. If data is moved to the wrong location on the stack, or an oversized data item is moved to a stack location that is not large enough to contain it, return information for procedure calls may be corrupted, causing the program to fail. Malicious parties may attempt a stack smashing attack that takes advantage of this type of implementation by providing oversized data input to a program that does not check the length of input. Such a program may copy the data in its entirety to a location on the stack, and in so doing it may change the return addresses for procedures that have called it. An attacker can experiment to find a specific type of data that can be provided to such a program such that the return address of the current procedure is reset to point to an area within the stack itself (and within the data provided by the attacker), which in turn contains instructions that carry out unauthorized operations. This type of attack is a variation on the buffer overflow attack and is an extremely frequent source of security breaches in software, mainly because some of the most popular compilers use a shared stack for both data and procedure calls, and do not verify the length of data items. Frequently programmers do not write code to verify the size of data items, either, and when an oversized or undersized data item is copied to the stack, a security breach may occur.","title":"Stack"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack%28abstract-data-type%29/#stack#abstract#data#type","text":"In computer science , a stack is an abstract data type that serves as a collection of elements, with two principal operations: push , which adds an element to the collection, and pop , which removes the most recently added element that was not yet removed. The order in which elements come off a stack gives rise to its alternative name, LIFO ( last in, first out ). Additionally, a peek operation may give access to the top without modifying the stack. The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first. Considered as a linear data structure , or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A stack is needed to implement depth-first search .","title":"Stack (abstract data type)"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack%28abstract-data-type%29/#applications#of#stacks","text":"","title":"Applications of stacks"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack%28abstract-data-type%29/#expression#evaluation#and#syntax#parsing","text":"Calculators employing reverse Polish notation use a stack structure to hold values. Expressions can be represented in prefix, postfix or infix notations and conversion from one form to another may be accomplished using a stack. Many compilers use a stack for parsing the syntax of expressions, program blocks etc. before translating into low level code. Most programming languages are context-free languages , allowing them to be parsed with stack based machines.","title":"Expression evaluation and syntax parsing"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack%28abstract-data-type%29/#backtracking","text":"Main article: Backtracking Another important application of stacks is backtracking . Consider a simple example of finding the correct path in a maze. There are a series of points, from the starting point to the destination. We start from one point. To reach the final destination, there are several paths. Suppose we choose a random path. After following a certain path, we realise that the path we have chosen is wrong. So we need to find a way by which we can return to the beginning of that path. This can be done with the use of stacks. With the help of stacks, we remember the point where we have reached. This is done by pushing that point into the stack. In case we end up on the wrong path, we can pop the last point from the stack and thus return to the last point and continue our quest to find the right path. This is called backtracking. The prototypical example of a backtracking algorithm is depth-first search , which finds all vertices of a graph that can be reached from a specified starting vertex. Other applications of backtracking involve searching through spaces that represent potential solutions to an optimization problem. Branch and bound is a technique for performing such backtracking searches without exhaustively searching all of the potential solutions in such a space.","title":"Backtracking"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack%28abstract-data-type%29/#compile#time#memory#management","text":"Main articles: Stack-based memory allocation and Stack machine A number of programming languages are stack-oriented , meaning they define most basic operations (adding two numbers, printing a character) as taking their arguments from the stack, and placing any return values back on the stack. For example, PostScript has a return stack and an operand stack, and also has a graphics state stack and a dictionary stack. Many virtual machines are also stack-oriented, including the p-code machine and the Java Virtual Machine . Almost all calling conventions \u200d\u2014\u200cthe ways in which subroutines receive their parameters and return results\u200d\u2014\u200cuse a special stack (the \" call stack \") to hold information about procedure/function calling and nesting in order to switch to the context of the called function and restore to the caller function when the calling finishes. The functions follow a runtime protocol between caller and callee to save arguments and return value on the stack. Stacks are an important way of supporting nested or recursive function calls. This type of stack is used implicitly by the compiler to support CALL and RETURN statements (or their equivalents) and is not manipulated directly by the programmer. Some programming languages use the stack to store data that is local to a procedure. Space for local data items is allocated from the stack when the procedure is entered, and is deallocated when the procedure exits. The C programming language is typically implemented in this way. Using the same stack for both data and procedure calls has important security implications (see below) of which a programmer must be aware in order to avoid introducing serious security bugs into a program.","title":"Compile time memory management"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack%28abstract-data-type%29/#efficient#algorithms","text":"Several algorithms use a stack (separate from the usual function call stack of most programming languages) as the principle data structure with which they organize their information. These include: Graham scan , an algorithm for the convex hull of a two-dimensional system of points. A convex hull of a subset of the input is maintained in a stack, which is used to find and remove concavities in the boundary when a new point is added to the hull.[ 10] Part of the SMAWK algorithm for finding the row minima of a monotone matrix uses stacks in a similar way to Graham scan.[ 11] All nearest smaller values , the problem of finding, for each number in an array, the closest preceding number that is smaller than it. One algorithm for this problem uses a stack to maintain a collection of candidates for the nearest smaller value. For each position in the array, the stack is popped until a smaller value is found on its top, and then the value in the new position is pushed onto the stack.[ 12] The nearest-neighbor chain algorithm , a method for agglomerative hierarchical clustering based on maintaining a stack of clusters, each of which is the nearest neighbor of its predecessor on the stack. When this method finds a pair of clusters that are mutual nearest neighbors, they are popped and merged.[ 13] SUMMARY : \u6808\u7684\u540e\u8fdb\u5148\u51fa\u7684\u7279\u6027\u662f\u4e00\u79cd\u5012\u5e8f\u7684\u7279\u6027\uff0c\u5b83\u975e\u5e38\u9002\u5408\u4e8e\u5b9e\u73b0\u4e00\u4e9b\u9700\u8981\u6267\u884c\u9006\u8f6c\u64cd\u4f5c\u7684\u7b97\u6cd5\uff1b","title":"Efficient algorithms"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack%28abstract-data-type%29/#security","text":"Some computing environments use stacks in ways that may make them vulnerable to security breaches and attacks. Programmers working in such environments must take special care to avoid the pitfalls of these implementations. For example, some programming languages use a common stack to store both data local to a called procedure and the linking information that allows the procedure to return to its caller. This means that the program moves data into and out of the same stack that contains critical return addresses for the procedure calls. If data is moved to the wrong location on the stack, or an oversized data item is moved to a stack location that is not large enough to contain it, return information for procedure calls may be corrupted, causing the program to fail. Malicious parties may attempt a stack smashing attack that takes advantage of this type of implementation by providing oversized data input to a program that does not check the length of input. Such a program may copy the data in its entirety to a location on the stack, and in so doing it may change the return addresses for procedures that have called it. An attacker can experiment to find a specific type of data that can be provided to such a program such that the return address of the current procedure is reset to point to an area within the stack itself (and within the data provided by the attacker), which in turn contains instructions that carry out unauthorized operations. This type of attack is a variation on the buffer overflow attack and is an extremely frequent source of security breaches in software, mainly because some of the most popular compilers use a shared stack for both data and procedure calls, and do not verify the length of data items. Frequently programmers do not write code to verify the size of data items, either, and when an oversized or undersized data item is copied to the stack, a security breach may occur.","title":"Security"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack-%E7%9B%B8%E5%85%B3/","text":"Stack machine","title":"[Stack machine](https://en.wikipedia.org/wiki/Stack_machine)"},{"location":"Relation-structure-computation/Structure/Data-structure/Stack/Stack-%E7%9B%B8%E5%85%B3/#stack#machine","text":"","title":"Stack machine"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/","text":"\u524d\u8a00 So how to choose data structure ? It is an art and worth learning\u3002 \u672c\u7ae0\u5bf9\u5404\u79cddata structure\u8fdb\u884c\u5bf9\u6bd4: 1) \u627e\u51fa\u5b83\u4eec\u7684\u5171\u6027\u3001\u5185\u5728\u5173\u8054 2) \u627e\u51fa\u5b83\u4eec\u7684\u4e2a\u6027 \u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u9009\u62e9data structure\u63d0\u4f9b\u53c2\u8003\u610f\u89c1\u3002\u6bd4\u8f83\u5404\u79cddata structure\uff0c Trade off \u4e0b\u9762\u8fd9\u6bb5\u8bdd\u6458\u53d6\u81ea\u7ef4\u57fa\u767e\u79d1 Hash function \uff1a In data storage and retrieval applications, use of a hash function is a trade off between search time and data storage space . If search time were unbounded, a very compact unordered linear list would be the best medium; if storage space were unbounded, a randomly accessible structure indexable by the key value would be very large, very sparse, but very fast. A hash function takes a finite amount of time to map a potentially large key space to a feasible amount of storage space searchable in a bounded amount of time regardless of the number of keys. \u5728\u9009\u62e9data structure\u7684\u65f6\u5019\uff0c\u6211\u4eec\u603b\u662f\u5728time\u548cspace\u4e0a\u8fdb\u884c\u6743\u8861\u3002 Classification \u672c\u8282\u8ba8\u8bba\u5bf9ADT\u7684\u5206\u7c7b: 1) sequence 2) mapping \u5e95\u5c42\u7684data structure\uff0c\u5219\u5206\u4e3a\u975e\u5e38\u591a\u7684\u7c7b\u522b\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/#_1","text":"So how to choose data structure ? It is an art and worth learning\u3002 \u672c\u7ae0\u5bf9\u5404\u79cddata structure\u8fdb\u884c\u5bf9\u6bd4: 1) \u627e\u51fa\u5b83\u4eec\u7684\u5171\u6027\u3001\u5185\u5728\u5173\u8054 2) \u627e\u51fa\u5b83\u4eec\u7684\u4e2a\u6027 \u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u9009\u62e9data structure\u63d0\u4f9b\u53c2\u8003\u610f\u89c1\u3002\u6bd4\u8f83\u5404\u79cddata structure\uff0c","title":"\u524d\u8a00"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/#trade#off","text":"\u4e0b\u9762\u8fd9\u6bb5\u8bdd\u6458\u53d6\u81ea\u7ef4\u57fa\u767e\u79d1 Hash function \uff1a In data storage and retrieval applications, use of a hash function is a trade off between search time and data storage space . If search time were unbounded, a very compact unordered linear list would be the best medium; if storage space were unbounded, a randomly accessible structure indexable by the key value would be very large, very sparse, but very fast. A hash function takes a finite amount of time to map a potentially large key space to a feasible amount of storage space searchable in a bounded amount of time regardless of the number of keys. \u5728\u9009\u62e9data structure\u7684\u65f6\u5019\uff0c\u6211\u4eec\u603b\u662f\u5728time\u548cspace\u4e0a\u8fdb\u884c\u6743\u8861\u3002","title":"Trade off"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/#classification","text":"\u672c\u8282\u8ba8\u8bba\u5bf9ADT\u7684\u5206\u7c7b: 1) sequence 2) mapping \u5e95\u5c42\u7684data structure\uff0c\u5219\u5206\u4e3a\u975e\u5e38\u591a\u7684\u7c7b\u522b\u3002","title":"Classification"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Graph-and-tree-and-list/","text":"Graph\u3001tree and list \u8fdb\u5316\uff1a\u4ecelist\u5230tree\uff0c\u4ecetree\u5230graph\uff1b \u9000\u5316\uff1agraph\u9000\u5316\u4e3atree\uff0ctree\u9000\u5316\u4e3alist\uff1b \u6811\u548c\u56fe\u90fd\u53ef\u80fd\u9000\u5316\u6210\u94fe\uff0c\u6240\u4ee5\u5176\u5b9e\u94fe\u4e5f\u5177\u5907\u90e8\u5206tree\u548cgraph\u7684\u5173\u7cfb\u3002\u53cd\u8fc7\u6765\u8bf4\u5176\u5b9e\u5c31\u662f\u6811\u548c\u56fe\u662f\u94fe\u7684\u6cdb\u5316\u3002 \u5176\u5b9e\u53ef\u4ee5\u770b\u5230\uff0c\u5f88\u591a\u57fa\u4e8egraph\u3001tree\u7684\u7ed3\u6784\u6216\u8bbe\u8ba1\u90fd\u6709\u5bf9\u5e94\u7684\u7ebf\u6027\u7248\u672c\uff0c\u6bd4\u5982 Merkle tree \uff0c\u5b83\u7684\u7ebf\u6027\u7248\u672c\u5c31\u662f Hash chain \u3002 Difference between Trees and Graphs | Trees vs. Graphs Trees Graphs Path Tree is special form of graph i.e. minimally connected graph and having only one path between any two vertices. In graph there can be more than one path i.e. graph can have uni-directional or bi-directional paths (edges) between nodes Loops Tree is a special case of graph having no loops , no circuits and no self-loops. Graph can have loops, circuits as well as can have self-loops . Root Node In tree there is exactly one root node and every child have only one parent . In graph there is no such concept of root node. NOTE: In graph, a node can be specified as the root node. Parent Child relationship In trees, there is parent child relationship so flow can be there with direction top to bottom or vice versa. In Graph there is no such parent child relationship. Complexity Trees are less complex then graphs as having no cycles, no self-loops and still connected. Graphs are more complex in compare to trees as it can have cycles, loops etc Types of Traversal Tree traversal is a kind of special case of traversal of graph. Tree is traversed in Pre-Order , In-Order and Post-Order (all three in DFS or in BFS algorithm) Graph is traversed by DFS: Depth First Search and in BFS : Breadth First Search algorithm Connection Rules In trees, there are many rules / restrictions for making connections between nodes through edges. In graphs no such rules/ restrictions are there for connecting the nodes through edges. DAG Trees come in the category of DAG : Directed Acyclic Graphs is a kind of directed graph that have no cycles. Graph can be Cyclic or Acyclic . Different Types Different types of trees are : Binary Tree , Binary Search Tree, AVL tree, Heaps . There are mainly two types of Graphs : Directed and Undirected graphs . Applications Tree applications : sorting and searching like Tree Traversal & Binary Search. Graph applications : Coloring of maps, in OR ( PERT & CPM ), algorithms, Graph coloring, job scheduling, etc. No. of edges Tree always has n-1 edges. In Graph, no. of edges depend on the graph. Model Tree is a hierarchical model . Graph is a network model . Figure \u6811\u4e2d\u6bcf\u4e2a\u8282\u70b9\u90fd\u53ea\u80fd\u6709\u4e00\u4e2a\u7236\u8282\u70b9\uff0c\u56fe\u4e2d\u4e00\u4e2a\u8282\u70b9\u53ef\u4ee5\u6709\u591a\u4e2a\u7236\u8282\u70b9\u3002 \u79bb\u6563\u6570\u5b66\u4e2d\u5bf9tree\u7684\u5b9a\u4e49\uff1a a tree is a connected undirected graph with no simple circuits \u8fd9\u8574\u542b\u7740 an undirected graph is a tree if and only if there is a unique simple path between any two of its vertices \u5982\u679c\u4e24\u4e2a\u70b9\u4e4b\u95f4\u6709\u591a\u6761path\u7684\u8bdd\uff0c\u5219\u5fc5\u7136\u5c31\u5f62\u6210\u4e86circuit\u4e86","title":"Graph-and-tree-and-list"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Graph-and-tree-and-list/#graphtree#and#list","text":"\u8fdb\u5316\uff1a\u4ecelist\u5230tree\uff0c\u4ecetree\u5230graph\uff1b \u9000\u5316\uff1agraph\u9000\u5316\u4e3atree\uff0ctree\u9000\u5316\u4e3alist\uff1b \u6811\u548c\u56fe\u90fd\u53ef\u80fd\u9000\u5316\u6210\u94fe\uff0c\u6240\u4ee5\u5176\u5b9e\u94fe\u4e5f\u5177\u5907\u90e8\u5206tree\u548cgraph\u7684\u5173\u7cfb\u3002\u53cd\u8fc7\u6765\u8bf4\u5176\u5b9e\u5c31\u662f\u6811\u548c\u56fe\u662f\u94fe\u7684\u6cdb\u5316\u3002 \u5176\u5b9e\u53ef\u4ee5\u770b\u5230\uff0c\u5f88\u591a\u57fa\u4e8egraph\u3001tree\u7684\u7ed3\u6784\u6216\u8bbe\u8ba1\u90fd\u6709\u5bf9\u5e94\u7684\u7ebf\u6027\u7248\u672c\uff0c\u6bd4\u5982 Merkle tree \uff0c\u5b83\u7684\u7ebf\u6027\u7248\u672c\u5c31\u662f Hash chain \u3002","title":"Graph\u3001tree and list"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Graph-and-tree-and-list/#difference#between#trees#and#graphs#trees#vs#graphs","text":"Trees Graphs Path Tree is special form of graph i.e. minimally connected graph and having only one path between any two vertices. In graph there can be more than one path i.e. graph can have uni-directional or bi-directional paths (edges) between nodes Loops Tree is a special case of graph having no loops , no circuits and no self-loops. Graph can have loops, circuits as well as can have self-loops . Root Node In tree there is exactly one root node and every child have only one parent . In graph there is no such concept of root node. NOTE: In graph, a node can be specified as the root node. Parent Child relationship In trees, there is parent child relationship so flow can be there with direction top to bottom or vice versa. In Graph there is no such parent child relationship. Complexity Trees are less complex then graphs as having no cycles, no self-loops and still connected. Graphs are more complex in compare to trees as it can have cycles, loops etc Types of Traversal Tree traversal is a kind of special case of traversal of graph. Tree is traversed in Pre-Order , In-Order and Post-Order (all three in DFS or in BFS algorithm) Graph is traversed by DFS: Depth First Search and in BFS : Breadth First Search algorithm Connection Rules In trees, there are many rules / restrictions for making connections between nodes through edges. In graphs no such rules/ restrictions are there for connecting the nodes through edges. DAG Trees come in the category of DAG : Directed Acyclic Graphs is a kind of directed graph that have no cycles. Graph can be Cyclic or Acyclic . Different Types Different types of trees are : Binary Tree , Binary Search Tree, AVL tree, Heaps . There are mainly two types of Graphs : Directed and Undirected graphs . Applications Tree applications : sorting and searching like Tree Traversal & Binary Search. Graph applications : Coloring of maps, in OR ( PERT & CPM ), algorithms, Graph coloring, job scheduling, etc. No. of edges Tree always has n-1 edges. In Graph, no. of edges depend on the graph. Model Tree is a hierarchical model . Graph is a network model . Figure \u6811\u4e2d\u6bcf\u4e2a\u8282\u70b9\u90fd\u53ea\u80fd\u6709\u4e00\u4e2a\u7236\u8282\u70b9\uff0c\u56fe\u4e2d\u4e00\u4e2a\u8282\u70b9\u53ef\u4ee5\u6709\u591a\u4e2a\u7236\u8282\u70b9\u3002 \u79bb\u6563\u6570\u5b66\u4e2d\u5bf9tree\u7684\u5b9a\u4e49\uff1a a tree is a connected undirected graph with no simple circuits \u8fd9\u8574\u542b\u7740 an undirected graph is a tree if and only if there is a unique simple path between any two of its vertices \u5982\u679c\u4e24\u4e2a\u70b9\u4e4b\u95f4\u6709\u591a\u6761path\u7684\u8bdd\uff0c\u5219\u5fc5\u7136\u5c31\u5f62\u6210\u4e86circuit\u4e86","title":"Difference between Trees and Graphs | Trees vs. Graphs"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Implementation-of-ADT/","text":"ADT\u7684\u5b9e\u73b0 Associative array \u7684\u5b9e\u73b0 The two major solutions to the dictionary problem are a hash table or a search tree .[ 1] [ 2] [ 4] [ 5] b set\u7684\u5b9e\u73b0 \u4f7f\u7528tree\u6765\u5b9e\u73b0 \u4f7f\u7528skip list\u6765\u5b9e\u73b0","title":"Implementation-of-ADT"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Implementation-of-ADT/#adt","text":"","title":"ADT\u7684\u5b9e\u73b0"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Implementation-of-ADT/#associative#array","text":"The two major solutions to the dictionary problem are a hash table or a search tree .[ 1] [ 2] [ 4] [ 5] b","title":"Associative array\u7684\u5b9e\u73b0"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Implementation-of-ADT/#set","text":"\u4f7f\u7528tree\u6765\u5b9e\u73b0 \u4f7f\u7528skip list\u6765\u5b9e\u73b0","title":"set\u7684\u5b9e\u73b0"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/List-of-data-structure/","text":"List of data structures linear structure VS nonlinear structure linear nonlinear Arrays \u3001 Lists Trees \u3001 Graphs \u3001 Hash-based structures","title":"List-of-data-structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/List-of-data-structure/#list#of#data#structures","text":"","title":"List of data structures"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/List-of-data-structure/#linear#structure#vs#nonlinear#structure","text":"linear nonlinear Arrays \u3001 Lists Trees \u3001 Graphs \u3001 Hash-based structures","title":"linear structure VS nonlinear structure"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-hash-table/","text":"binary search tree VS hash table \u5728 Binary search tree \u4e2d\uff0c\u6709\u5982\u4e0b\u5bf9\u6bd4\uff1a Binary search trees keep their keys in sorted order, so that lookup and other operations can use the principle of binary search : when looking for a key in a tree (or a place to insert a new key), they traverse the tree from root to leaf, making comparisons to keys stored in the nodes of the tree and deciding, on the basis of the comparison, to continue searching in the left or right subtrees. On average, this means that each comparison allows the operations to skip about half of the tree, so that each lookup, insertion or deletion takes time proportional to the logarithm of the number of items stored in the tree. This is much better than the linear time required to find items by key in an (unsorted) array, but slower than the corresponding operations on hash tables . B tree VS hash table Introduction to the B-Tree Why is a tree a good data structure for a database? Searching for a particular value is fast (logarithmic time) Inserting / deleting a value you\u2019ve already found is fast (constant-ish time to rebalance) Traversing a range of values is fast (unlike a hash map) Self-balancing binary search tree VS hash table Self-balancing binary search tree Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues . They can also be used for associative arrays ; key-value pairs are simply inserted with an ordering based on the key alone. In this capacity, self-balancing BSTs have a number of advantages and disadvantages over their main competitor, hash tables . One advantage of self-balancing BSTs is that they allow fast (indeed, asymptotically optimal) enumeration of the items in key order , which hash tables do not provide. One disadvantage is that their lookup algorithms get more complicated when there may be multiple items with the same key. Self-balancing BSTs have better worst-case lookup performance than hash tables (O(log n) compared to O(n)), but have worse average-case performance (O(log n) compared to O(1)).","title":"Search-tree-and-hash-table"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-hash-table/#binary#search#tree#vs#hash#table","text":"\u5728 Binary search tree \u4e2d\uff0c\u6709\u5982\u4e0b\u5bf9\u6bd4\uff1a Binary search trees keep their keys in sorted order, so that lookup and other operations can use the principle of binary search : when looking for a key in a tree (or a place to insert a new key), they traverse the tree from root to leaf, making comparisons to keys stored in the nodes of the tree and deciding, on the basis of the comparison, to continue searching in the left or right subtrees. On average, this means that each comparison allows the operations to skip about half of the tree, so that each lookup, insertion or deletion takes time proportional to the logarithm of the number of items stored in the tree. This is much better than the linear time required to find items by key in an (unsorted) array, but slower than the corresponding operations on hash tables .","title":"binary search tree VS hash table"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-hash-table/#b#tree#vs#hash#table","text":"","title":"B tree VS hash table"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-hash-table/#introduction#to#the#b-tree","text":"Why is a tree a good data structure for a database? Searching for a particular value is fast (logarithmic time) Inserting / deleting a value you\u2019ve already found is fast (constant-ish time to rebalance) Traversing a range of values is fast (unlike a hash map)","title":"Introduction to the B-Tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-hash-table/#self-balancing#binary#search#tree#vs#hash#table","text":"","title":"Self-balancing binary search tree VS hash table"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-hash-table/#self-balancing#binary#search#tree","text":"Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues . They can also be used for associative arrays ; key-value pairs are simply inserted with an ordering based on the key alone. In this capacity, self-balancing BSTs have a number of advantages and disadvantages over their main competitor, hash tables . One advantage of self-balancing BSTs is that they allow fast (indeed, asymptotically optimal) enumeration of the items in key order , which hash tables do not provide. One disadvantage is that their lookup algorithms get more complicated when there may be multiple items with the same key. Self-balancing BSTs have better worst-case lookup performance than hash tables (O(log n) compared to O(n)), but have worse average-case performance (O(log n) compared to O(1)).","title":"Self-balancing binary search tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-skip-list/","text":"Skip List vs. Binary Search Tree I recently came across the data structure known as a skip list . It seems to have very similar behavior to a binary search tree. Why would you ever want to use a skip list over a binary search tree? A Skip lists are more amenable to concurrent access/modification. Herb Sutter wrote an article about data structure in concurrent environments. It has more indepth information. The most frequently used implementation of a binary search tree is a red-black tree . The concurrent problems come in when the tree is modified it often needs to rebalance. The rebalance operation can affect large portions of the tree, which would require a mutex lock on many of the tree nodes. Inserting a node into a skip list is far more localized, only nodes directly linked to the affected node need to be locked. Update from Jon Harrops comments I read Fraser and Harris's latest paper Concurrent programming without locks . Really good stuff if you're interested in lock-free data structures. The paper focuses on Transactional Memory and a theoretical operation multiword-compare-and-swap MCAS. Both of these are simulated in software as no hardware supports them yet. I'm fairly impressed that they were able to build MCAS in software at all. I didn't find the transactional memory stuff particularly compelling as it requires a garbage collector. Also software transactional memory is plagued with performance issues. However, I'd be very excited if hardware transactional memory ever becomes common. In the end it's still research and won't be of use for production code for another decade or so. In section 8.2 they compare the performance of several concurrent tree implementations. I'll summarize their findings. It's worth it to download the pdf as it has some very informative graphs on pages 50, 53, and 54. Locking skip lists are insanely fast. They scale incredibly well with the number of concurrent accesses. This is what makes skip lists special, other lock based data structures tend to croak under pressure. Lock-free skip lists are consistently faster than locking skip lists but only barely. transactional skip lists are consistently 2-3 times slower than the locking and non-locking versions. locking red-black trees croak under concurrent access. Their performance degrades linearly with each new concurrent user. Of the two known locking red-black tree implementations, one essentially has a global lock during tree rebalancing. The other uses fancy (and complicated) lock escalation but still doesn't significantly out perform the global lock version. lock-free red-black trees don't exist (no longer true, see Update). transactional red-black trees are comparable with transactional skip-lists. That was very surprising and very promising. Transactional memory, though slower if far easier to write. It can be as easy as quick search and replace on the non-concurrent version. Update Here is paper about lock-free trees: Lock-Free Red-Black Trees Using CAS . I haven't looked into it deeply, but on the surface it seems solid. COMMENTS : 3 Not to mention that in a non-degenerate skiplist, about 50% of the nodes should only have a single link which makes insert and delete remarkably efficient. \u2013 Adisak Oct 30 '09 at 3:44 2 Rebalancing does not require a mutex lock. See cl.cam.ac.uk/research/srg/netos/lock-free \u2013 Jon Harrop May 20 '10 at 21:00 3 @Jon , yes and no. There are no known lock-free red-black tree implementations. Fraser and Harris show how a transactional memory based red-black tree is implemented and its performance. Transactional memory is still very much in the research arena, so in production code, a red-black tree will still need to lock large portions of the tree. \u2013 deft_code May 21 '10 at 16:20 1 I wanted to update this answer. There are currently two lock based efficient binary search trees. One is based on AVL trees ( dl.acm.org/citation.cfm?id=1693488 ) and the other (Warning! shameless plug) is based on red black trees. See actapress.com/Abstract.aspx?paperId=453069 \u2013 Juan Besa Mar 2 '12 at 20:01 @JuanBesa , \"14% better than the best known concurrent dictionary solutions\" . Are you comparing against skip-lists? The other paper inadvertently points out that lock based trees are O(n) for n < 90, whereas skiplists (also a dictionary) are O(1) ! 14% doesn't seem to be enough when the O is that disparate. \u2013 deft_code Mar 2 '12 at 22:08 That RB tree paper looks bloody good! \u2013 user82238 Jun 19 '12 at 10:22 Think I'm going to try to apply that basic mechanism to AVL. As I see it from a quick read, the basic solution to rotation (which is the fundamental problem) is to have a retry-block which is the raising of flags in the elements you need to control - if you can raise them all, then you're safe to proceed as other threads will fail and be retrying to get those flags. Simple genius! \u2013 user82238 Jun 19 '12 at 10:29 @BlankXavier , Hmmm, that sounds suspiciously like using a spinlock instead of a mutex for a regular lock based tree. It may be more performant, but I want to see some benchmarks. In particular against the a lock-free skiplist and a locking skiplist. \u2013 deft_code Jun 20 '12 at 23:01 All helper mechanisms are essentially spinning mechanisms - it's just that rather than dumbly spinning, which performs no work, by spinning on a helper mechanism which if it completes permits you to continue your own work , then you're doing something useful - you're lock-free, in fact... \u2013 user82238 Jun 21 '12 at 6:53 4 @deft_code : Intel recently announced an implementation of Transactional Memory via TSX on Haswell. This may prove interesting w.r.t those lock free data structures you mentioned. \u2013 Mike Bailey Oct 3 '12 at 5:07 1 Any comment on Respawned Fluff's recent answer ? \u2013 Claudiu Feb 2 '15 at 3:06 2 I think Fizz' answer is more up-to-date (from 2015) rather than this answer (2012) and therefore should probably be the preferred answer by now. \u2013 fnl Jul 11 '17 at 10:45 B-tree VS skip list \u770b\u4e86\u8fd9\u4e24\u79cdDS\u7684\u539f\u7406\uff0c\u53d1\u73b0\u4e24\u8005\u5176\u5b9e\u6709\u4e9b\u7c7b\u4f3c\uff1a\u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4\uff0c\u5373\u901a\u8fc7\u6784\u5efa\u6570\u636e\u4e4b\u95f4\u7684\u66f4\u591a\u5173\u7cfb\u6765\u52a0\u901f\u6570\u636e\u7684access\uff0c\u663e\u7136\u8fd9\u4e9b\u5173\u7cfb\u662f\u9700\u8981\u8017\u8d39\u7a7a\u95f4\u6765\u5b58\u50a8\u7684\uff0c\u6240\u4ee5\u5c31\u662f\u524d\u9762\u6240\u8ff0\u7684\u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4\uff0c\u5176\u5b9e\u5f53\u6211\u770b\u5b8c\u4e86\u4e24\u8005\u7684\u539f\u7406\u540e\uff0c\u89c9\u5f97\u5b83\u4eec\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\uff0c\u5c24\u5176\u662f\u770b\u5230 B-tree \u7684\u4e3b\u8981\u5e94\u7528\u662f\u5728DB\u6216file system\u4e2d\u5b9e\u73b0\u7d22\u5f15\u540e\uff0c\u6211\u53d1\u6398\u5176\u5b9e\u5b83\u4eec\u7684\u539f\u7406\u7684\u5171\u540c\u4e4b\u5904\u5176\u5b9e\u5c31\u662findex\uff1b\u901a\u8fc7\u91cd\u5efaindex\u6765\u52a0\u901f\u6570\u636e\u7684access\uff1b \u4e8e\u662f\u6211\u5c31\u60f3\uff0c\u65e2\u7136 B-tree \u7684\u4e3b\u8981\u5e94\u7528\u662f\u5728DB\u6216file system\u4e2d\u5b9e\u73b0\u7d22\u5f15\uff0c\u90a3\u4e48 skip list \u662f\u5426\u4e5f\u80fd\u591f\u5462\uff1f \u68c0\u7d22\u4e86\u4e00\u756a\u540e\uff0c\u53d1\u73b0\u5176\u5b9e\u662f\u6709\u5e94\u7528\u6848\u4f8b\u7684\uff1a MemSQL uses skip lists as its prime indexing structure for its database technology. The Story Behind MemSQL\u2019s Skiplist Indexes \u5176\u5b9e\u53d1\u73b0\uff0c\u5728\u6587\u4ef6\u7cfb\u7edf\u4e2d\u591a\u4f7f\u7528B-tree\uff0c\u800c\u5728\u5185\u5b58\u4e2d\u5219\u591a\u4f7f\u7528skip list","title":"Search-tree-and-skip-list"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-skip-list/#skip#list#vs#binary#search#tree","text":"I recently came across the data structure known as a skip list . It seems to have very similar behavior to a binary search tree. Why would you ever want to use a skip list over a binary search tree?","title":"Skip List vs. Binary Search Tree"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-skip-list/#a","text":"Skip lists are more amenable to concurrent access/modification. Herb Sutter wrote an article about data structure in concurrent environments. It has more indepth information. The most frequently used implementation of a binary search tree is a red-black tree . The concurrent problems come in when the tree is modified it often needs to rebalance. The rebalance operation can affect large portions of the tree, which would require a mutex lock on many of the tree nodes. Inserting a node into a skip list is far more localized, only nodes directly linked to the affected node need to be locked. Update from Jon Harrops comments I read Fraser and Harris's latest paper Concurrent programming without locks . Really good stuff if you're interested in lock-free data structures. The paper focuses on Transactional Memory and a theoretical operation multiword-compare-and-swap MCAS. Both of these are simulated in software as no hardware supports them yet. I'm fairly impressed that they were able to build MCAS in software at all. I didn't find the transactional memory stuff particularly compelling as it requires a garbage collector. Also software transactional memory is plagued with performance issues. However, I'd be very excited if hardware transactional memory ever becomes common. In the end it's still research and won't be of use for production code for another decade or so. In section 8.2 they compare the performance of several concurrent tree implementations. I'll summarize their findings. It's worth it to download the pdf as it has some very informative graphs on pages 50, 53, and 54. Locking skip lists are insanely fast. They scale incredibly well with the number of concurrent accesses. This is what makes skip lists special, other lock based data structures tend to croak under pressure. Lock-free skip lists are consistently faster than locking skip lists but only barely. transactional skip lists are consistently 2-3 times slower than the locking and non-locking versions. locking red-black trees croak under concurrent access. Their performance degrades linearly with each new concurrent user. Of the two known locking red-black tree implementations, one essentially has a global lock during tree rebalancing. The other uses fancy (and complicated) lock escalation but still doesn't significantly out perform the global lock version. lock-free red-black trees don't exist (no longer true, see Update). transactional red-black trees are comparable with transactional skip-lists. That was very surprising and very promising. Transactional memory, though slower if far easier to write. It can be as easy as quick search and replace on the non-concurrent version. Update Here is paper about lock-free trees: Lock-Free Red-Black Trees Using CAS . I haven't looked into it deeply, but on the surface it seems solid. COMMENTS : 3 Not to mention that in a non-degenerate skiplist, about 50% of the nodes should only have a single link which makes insert and delete remarkably efficient. \u2013 Adisak Oct 30 '09 at 3:44 2 Rebalancing does not require a mutex lock. See cl.cam.ac.uk/research/srg/netos/lock-free \u2013 Jon Harrop May 20 '10 at 21:00 3 @Jon , yes and no. There are no known lock-free red-black tree implementations. Fraser and Harris show how a transactional memory based red-black tree is implemented and its performance. Transactional memory is still very much in the research arena, so in production code, a red-black tree will still need to lock large portions of the tree. \u2013 deft_code May 21 '10 at 16:20 1 I wanted to update this answer. There are currently two lock based efficient binary search trees. One is based on AVL trees ( dl.acm.org/citation.cfm?id=1693488 ) and the other (Warning! shameless plug) is based on red black trees. See actapress.com/Abstract.aspx?paperId=453069 \u2013 Juan Besa Mar 2 '12 at 20:01 @JuanBesa , \"14% better than the best known concurrent dictionary solutions\" . Are you comparing against skip-lists? The other paper inadvertently points out that lock based trees are O(n) for n < 90, whereas skiplists (also a dictionary) are O(1) ! 14% doesn't seem to be enough when the O is that disparate. \u2013 deft_code Mar 2 '12 at 22:08 That RB tree paper looks bloody good! \u2013 user82238 Jun 19 '12 at 10:22 Think I'm going to try to apply that basic mechanism to AVL. As I see it from a quick read, the basic solution to rotation (which is the fundamental problem) is to have a retry-block which is the raising of flags in the elements you need to control - if you can raise them all, then you're safe to proceed as other threads will fail and be retrying to get those flags. Simple genius! \u2013 user82238 Jun 19 '12 at 10:29 @BlankXavier , Hmmm, that sounds suspiciously like using a spinlock instead of a mutex for a regular lock based tree. It may be more performant, but I want to see some benchmarks. In particular against the a lock-free skiplist and a locking skiplist. \u2013 deft_code Jun 20 '12 at 23:01 All helper mechanisms are essentially spinning mechanisms - it's just that rather than dumbly spinning, which performs no work, by spinning on a helper mechanism which if it completes permits you to continue your own work , then you're doing something useful - you're lock-free, in fact... \u2013 user82238 Jun 21 '12 at 6:53 4 @deft_code : Intel recently announced an implementation of Transactional Memory via TSX on Haswell. This may prove interesting w.r.t those lock free data structures you mentioned. \u2013 Mike Bailey Oct 3 '12 at 5:07 1 Any comment on Respawned Fluff's recent answer ? \u2013 Claudiu Feb 2 '15 at 3:06 2 I think Fizz' answer is more up-to-date (from 2015) rather than this answer (2012) and therefore should probably be the preferred answer by now. \u2013 fnl Jul 11 '17 at 10:45","title":"A"},{"location":"Relation-structure-computation/Structure/Data-structure/Summary/Search-tree-and-skip-list/#b-tree#vs#skip#list","text":"\u770b\u4e86\u8fd9\u4e24\u79cdDS\u7684\u539f\u7406\uff0c\u53d1\u73b0\u4e24\u8005\u5176\u5b9e\u6709\u4e9b\u7c7b\u4f3c\uff1a\u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4\uff0c\u5373\u901a\u8fc7\u6784\u5efa\u6570\u636e\u4e4b\u95f4\u7684\u66f4\u591a\u5173\u7cfb\u6765\u52a0\u901f\u6570\u636e\u7684access\uff0c\u663e\u7136\u8fd9\u4e9b\u5173\u7cfb\u662f\u9700\u8981\u8017\u8d39\u7a7a\u95f4\u6765\u5b58\u50a8\u7684\uff0c\u6240\u4ee5\u5c31\u662f\u524d\u9762\u6240\u8ff0\u7684\u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4\uff0c\u5176\u5b9e\u5f53\u6211\u770b\u5b8c\u4e86\u4e24\u8005\u7684\u539f\u7406\u540e\uff0c\u89c9\u5f97\u5b83\u4eec\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\uff0c\u5c24\u5176\u662f\u770b\u5230 B-tree \u7684\u4e3b\u8981\u5e94\u7528\u662f\u5728DB\u6216file system\u4e2d\u5b9e\u73b0\u7d22\u5f15\u540e\uff0c\u6211\u53d1\u6398\u5176\u5b9e\u5b83\u4eec\u7684\u539f\u7406\u7684\u5171\u540c\u4e4b\u5904\u5176\u5b9e\u5c31\u662findex\uff1b\u901a\u8fc7\u91cd\u5efaindex\u6765\u52a0\u901f\u6570\u636e\u7684access\uff1b \u4e8e\u662f\u6211\u5c31\u60f3\uff0c\u65e2\u7136 B-tree \u7684\u4e3b\u8981\u5e94\u7528\u662f\u5728DB\u6216file system\u4e2d\u5b9e\u73b0\u7d22\u5f15\uff0c\u90a3\u4e48 skip list \u662f\u5426\u4e5f\u80fd\u591f\u5462\uff1f \u68c0\u7d22\u4e86\u4e00\u756a\u540e\uff0c\u53d1\u73b0\u5176\u5b9e\u662f\u6709\u5e94\u7528\u6848\u4f8b\u7684\uff1a MemSQL uses skip lists as its prime indexing structure for its database technology. The Story Behind MemSQL\u2019s Skiplist Indexes \u5176\u5b9e\u53d1\u73b0\uff0c\u5728\u6587\u4ef6\u7cfb\u7edf\u4e2d\u591a\u4f7f\u7528B-tree\uff0c\u800c\u5728\u5185\u5b58\u4e2d\u5219\u591a\u4f7f\u7528skip list","title":"B-tree VS skip list"},{"location":"Relation-structure-computation/Structure/Structure/","text":"Structure \u201cstructure\u201d\u5373\u201c\u7ed3\u6784\u201d\uff0c\u5728\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1computer science\u3001\u6570\u5b66\u76f8\u5173\u7684\u6587\u7ae0\u4e2d\uff0c\u7ecf\u5e38\u78b0\u5230\u201cstructure\u201d\u6982\u5ff5\uff0c\u8fd9\u5f15\u8d77\u4e86\u6211\u601d\u8003\uff1awhat is structure\uff1f\u672c\u6587\u5c31\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u5256\u6790\u3002 Structure\u662f\u4e00\u4e2a\u5b8f\u5927\u7684\u8bdd\u9898\uff0cstructure\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u6211\u4eec\u8089\u773c\u53ef\u89c1\u7684\u7269\u7406structure\uff0c\u5b83\u8fd8\u5305\u62ec\u6211\u4eec\u8089\u773c\u65e0\u6cd5\u770b\u5230\u7684\u903b\u8f91\uff08\u62bd\u8c61\uff09structure(abstract structure)\u3002 What is structure? \u4e0b\u9762\u662f\u7ef4\u57fa\u767e\u79d1 Structure \u4e2d\u7ed9\u51fa\u7684\u5b9a\u4e49\uff1a A structure is an arrangement and organization of interrelated\uff08\u76f8\u4e92\u5173\u8054\uff09 elements in a material object or system , or the object or system so organized. Material structures include man-made objects such as buildings and machines and natural objects such as biological organisms , minerals and chemicals . Abstract structures include data structures in computer science and musical form . \u4e0a\u8ff0\u5b9a\u4e49\u4e2d\u7684\u201cmaterial structures\u201d\u662f\u6307\u7269\u7406\u7684\u3001\u8089\u773c\u53ef\u89c1\u7684\u7ed3\u6784\u3002\u4e0a\u8ff0\u5b9a\u4e49\u662fgeneral\u7684\uff0c\u4e0b\u9762\u770b\u770b\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u6700\u6700\u201d\u4eb2\u5bc6\u201c\u7684\u6570\u5b66\u4e2d\u5bf9structure\u7684\u5b9a\u4e49\uff1a Mathematical structure In mathematics , a structure is a set endowed\uff08\u8d4b\u4e88\uff09 with some additional features on the set (e.g., operation , relation , metric , topology ). Often, the additional features are attached or related to the set, so as to provide it with some additional meaning or significance. Structure (mathematical logic) \uff1a In universal algebra and in model theory , a structure consists of a set along with a collection of finitary operations and relations that are defined on it. Algebraic structure In mathematics , more specifically in abstract algebra and universal algebra , an algebraic structure consists of a set A (called the underlying set , carrier set or domain ), a collection of operations on A of finite arity (typically binary operations ), and a finite set of identities , known as axioms , that these operations must satisfy. \u7b80\u8a00\u4e4b\uff0c\u5728\u6570\u5b66\u4e2d\uff0c\u5f53\u8c08\u53castructure\u7684\u65f6\u5019\uff0c\u5b83\u8868\u793a\u7684\u662f\u4e00\u4e2aset\u4ee5\u53ca\u8d4b\u4e88\u7ed9\u5b83\u7684feature\u3002 \u6570\u5b66\u4e2d\u7684\u5b9a\u4e49\u6240\u4f7f\u7528\u7684\u662f \u6570\u5b66\u8bed\u8a00 \uff0cgeneral\u5b9a\u4e49\u4e2d\u7684\u201celements\u201d\u4f7f\u7528 \u6570\u5b66\u8bed\u8a00 \u6765\u63cf\u8ff0\u662f set \uff0cgeneral\u5b9a\u4e49\u4e2d\u7684\u201cinterrelated\u201d\u4f7f\u7528 \u6570\u5b66\u8bed\u8a00 \u6765\u63cf\u8ff0\u662f operations \u6216 relations \u3002 \u9605\u8bfb\u4e86\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u76f4\u89c2\u611f\u53d7\u5c31\u662f\uff1a **structure**\u6240\u63cf\u8ff0\u7684\u662f\u5143\u7d20\u4ee5\u53ca\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u76f8\u540c\u7684\u5143\u7d20\uff0c\u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7684**\u5173\u7cfb**\u6765\u8fdb\u884c\u7ec4\u7ec7\uff0c\u5c31\u662f\u4e0d\u540c\u7684**structure**\uff0c\u5c31\u5448\u73b0\u51fa\u4e0d\u540c\u7684\u89c6\u89c9**\u5f62\u6001**\uff08\u540e\u6587\u4e2d\uff0c\u6211\u4eec\u628a\u8fd9\u79f0\u4e4b\u4e3a\u201c \u5f62\u72b6 \u201d\uff09 \u6240\u4ee5\uff0c\u6211\u4eec\u8bf4\uff1a \u5173\u7cfb\u51b3\u5b9a\uff08\u6216\u8005\u8bf4\uff1a\u5b9a\u4e49\uff09\u4e86structure\uff0c\u8fdb\u800c\u51b3\u5b9a\u4e86\u5f62\u72b6 \uff0c\u6240\u4ee5\uff0c\u6211\u4eec\u5728\u7814\u7a76\u7ed3\u6784\u7684\u65f6\u5019\uff0c\u5207\u83ab\u5ffd\u89c6\u4e86\u5bf9**\u5173\u7cfb**\u5206\u6790\u3002 NOTE: \u8fd9\u4e2a\u601d\u60f3\uff0c\u5728 Relation-structure-computation\\index.md \u4e2d\u5df2\u7ecf\u7ed9\u51fa\uff0c\u672c\u6587\u53ea\u662f\u7ed9\u51fa\u5b83\u7684\u5f62\u6210\u8fc7\u7a0b\u3002 \u5728\u7ef4\u57fa\u767e\u79d1 Mathematical structure \u4e2d\uff0c\u5217\u4e3e\u4e86\u4e00\u4e9b\u6570\u5b66\u4e2d\u7684structure\uff1a A partial list of possible structures are measures , algebraic structures ( groups , fields , etc.), topologies , metric structures ( geometries ), orders , events , equivalence relations , differential structures , and categories . \u901a\u8fc7\u4e0a\u9762\u7684\u63cf\u8ff0\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u6765\u56de\u7b54\u672c\u8282\u6807\u9898\u4e2d\u7684\u95ee\u9898\uff1a structure\u662f\u6211\u4eec\u6309\u7167\u67d0\u79cd\u5173\u7cfb\u5bf9\u5143\u7d20\u8fdb\u884c\u7ec4\u7ec7\u540e\u5f62\u6210\u7684 \u3002 \u7ed3\u6784\u7684\u5f62\u72b6 \u5f53\u6211\u4eec\u6309\u7167\u4e00\u5b9a\u7684\u5173\u7cfb\u5bf9\u5143\u7d20\u8fdb\u884c\u7ec4\u7ec7\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528digraph\u5c06\u5b83\u4eec\u7ed9\u201c\u753b\u201d\u51fa\u6765\uff0c\u8fd9\u65f6\u4f1a\u5448\u73b0\u4e00\u5b9a\u7684\u201c\u5f62\u72b6\u201d\uff08\u6b64\u5904\u4f7f\u7528\u201c\u5f62\u72b6\u201d\u8fd9\u4e2a\u8bcd\uff0c\u662f\u4e3a\u4e86\u4e0e\"\u7ed3\u6784\"\"\u8fd9\u4e2a\u8bcd\u6709\u6240\u533a\u5206\uff0c\u5b83\u5f3a\u8c03\u7684\u662f\uff0c\u6211\u4eec\u8089\u773c\u53ef\u4ee5\u770b\u5230\u7684\u5f62\u6001\uff0c\u4f46\u662f\uff0c\u5e73\u65f6\uff0c\u6211\u4eec\u66f4\u591a\u7684\u8fd8\u662f\u4f7f\u7528\u7ed3\u6784\u8fd9\u4e2a\u8bcd\uff09\uff0c\u6bd4\u5982 \u6309\u7167parent-child\u5173\u7cfb\u6765\u7ec4\u7ec7process\uff0c\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811\u5f62\u7ed3\u6784 \u6309\u7167inheritance\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u4e0d\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f**tree**\uff1b\u5982\u679c\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f**hierarchy** \u6bd4\u8f83\u5178\u578b\u7684\u5f62\u72b6\u6709\uff1a Chain\uff0c\u7ebf\u6027\u7684 Hierarchy\uff0c\u975e\u7ebf\u6027\u7684\uff0c\u5448\u73b0\u51fa\u5c42\u6b21\u7684\u7ed3\u6784 Tree Network\uff0c \u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Network Lattice\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Lattice \u4e0a\u8bc9\u524d\u56db\u79cd**\u5f62\u72b6**\u662f\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u975e\u5e38\u5e38\u89c1\u7684\uff0c\u540e\u9762\u6211\u4eec\u4f1a\u5bf9\u5b83\u4eec\u8fdb\u884c\u5206\u6790\u3002 Relation\u51b3\u5b9astructure \u6309\u7167\u6709\u7684\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u4f1a\u5f62\u6210\u6811\u7ed3\u6784\uff08\u4e0d\u4f1a\u6210\u73af\uff09\uff0c\u6bd4\u5982parent-children\u5173\u7cfb\uff1b\u6309\u7167\u6709\u7684\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u4f1a\u5f62\u6210\u5c42\u6b21\u5316\u7ed3\u6784\uff08\u662f\u56fe\uff0c\u56e0\u4e3a\u5b83\u4f1a\u6210\u73af\uff09\uff0c\u8fd9\u5c31\u662f\u6211\u4eec\u5728\u4e0a\u4e00\u8282\u4e2d\u63d0\u51fa\u7684\u201c relation\u51b3\u5b9astructure \u201d\u7ed3\u8bba\uff0c\u5173\u4e8e\u6b64\uff0c\u6211\u4eec\u9700\u8981\u8fdb\u884c\u6df1\u5165\u601d\u8003: Relation\u7684\u54ea\u4e9b\u7279\u6027\u51b3\u5b9a\u4e86structure\uff0c\u6216\u8005\u8bf4\uff1a\u4ec0\u4e48\u6837\u7684relation\u53ef\u4ee5\u5f62\u6210\u4ec0\u4e48\u6837\u7684\u7ed3\u6784\uff0c\u6bd4\u5982chain\u3001tree\u3001graph\uff1f \u8981\u641e\u6e05\u695a\u8fd9\u4e2a\u95ee\u9898\uff0c\u9996\u5148\u6211\u4eec\u9700\u8981\u5bf9\u201cRelation\u201d\u7406\u8bba\u6709\u4e00\u5b9a\u7684\u4e86\u89e3\uff08\u53c2\u89c1 Relation-structure-computation\\Relation\\Relation \uff09\uff0c\u7136\u540e\u5728\u6df1\u5165\u5206\u6790\u6211\u4eec\u8089\u773c\u770b\u5230\u7684\u5404\u79cd\u5f62\u72b6\u80cc\u540e\u7684relation\u7684\u6027\u8d28\u3002\u6211\u76ee\u524d\u65e0\u6cd5\u7ed9\u51fa**\u901a\u7528\u7684\u89e3\u7b54**\uff0c\u53ea\u80fd\u591f\u7ed3\u5408\u5177\u4f53\u7684\u51e0\u79cd\u5f62\u72b6\u6765\u8fdb\u884c\u5206\u6790\uff0c\u5173\u4e8e\u8fd9\u51e0\u79cd\u7ed3\u6784\u7684\u5206\u6790\u53c2\u89c1\uff1a \u5f62\u72b6 \u63cf\u8ff0\u7ae0\u8282 \u8bf4\u660e Hierarchy Relation-structure-computation\\Model\\Hierarchy-relation-model Tree Relation-structure-computation\\Model\\Containing-relation-model Chain Relation-structure-computation\\Model\\Chain Network \u5176\u5b9e\u5c31\u662fgraph Discrete structure \u6709\u4e86 discrete \u6982\u5ff5\u548c structure \u6982\u5ff5\uff0c\u90a3\u4e48\u7406\u89e3what is discrete structure\u5c31\u6bd4\u8f83\u5bb9\u6613\u4e86\u3002\u4e0b\u9762\u662f\u4e00\u4e9b\u5173\u4e8ediscrete structure\u7684\u5185\u5bb9\uff1a quora Explain what is the role of Discrete Structures in Computer Science Discrete structures is study of mathematical structures that are fundamentally discrete (that is not continuous). This is usually the first mathematics course for computer science students; it helps them to start thinking about problems in a way computers can solve. ACM Discrete structures Discrete Mathematics and Its Applications \u5728\u8fd9\u672c\u4e66\u7684 Preface \u3001 chapter 2 \u3001 chapter 9 \u4e2d\u6709\u5bf9discrete structure\u7684\u63cf\u8ff0\u3002 Abstract structure \u5728 Relation-structure-computation\\Structuralization-and-formalization \u7ae0\u8282\u4e2d\u4ecb\u7ecd\u4e86abstract structure\u3002 Classification \u672c\u8282\u5bf9structure\u7684\u8fdb\u884c\u5206\u7c7b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u6b64\u5904\u7684structure\u66f4\u591a\u7684\u662f\u6307abstract structure\uff0c\u5f53\u7136\u4e5f\u5305\u542bdata structure\u3002 1) linear \u7ebf\u6027\u7ed3\u6784\u3002\u8fd9\u6837\u7684structure\u662f\u975e\u5e38\u666e\u904d\u7684\uff0c\u5e76\u4e14\u975e\u5e38\u5bb9\u6613computation\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50: Chomsky hierarchy \u4e2d\u7684Type-3 language\u5373 Regular \u5c31\u662flinear\u7ed3\u6784 Factorial\u662f\u5178\u578b\u7684\u7ebf\u6027\u7ed3\u6784\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1: https://infogalactic.com/info/Recursion_(computer_science ) 2) nonlinear \u975e\u7ebf\u6027\u7ed3\u6784 Thoughts \u4f7f\u7528structure\u6982\u5ff5\u80fd\u591f\u4f7f\u6211\u4eec\u7684\u63cf\u8ff0\u975e\u5e38\u5730\u4fbf\u5229\u3002 \u6211\u5bf9structure\u7684\u76f4\u89c2\u7406\u89e3\u662f\uff1astructure\u662f\u5c06set\u4e2d\u7684element\u6309\u7167relation\u8fdb\u884c\u7ec4\u7ec7\u540e\u5f62\u6210\u7684\u3002 \u6211\u89c9\u5f97structure\u662f\u6700\u6700\u5178\u578b\u7684\u5177\u5907discrete\u7279\u6027\u7684\uff0c\u6240\u4ee5\u5b83\u4eec\u5f80\u5f80\u662fcomputable\u7684\u3002","title":"Introduction"},{"location":"Relation-structure-computation/Structure/Structure/#structure","text":"\u201cstructure\u201d\u5373\u201c\u7ed3\u6784\u201d\uff0c\u5728\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1computer science\u3001\u6570\u5b66\u76f8\u5173\u7684\u6587\u7ae0\u4e2d\uff0c\u7ecf\u5e38\u78b0\u5230\u201cstructure\u201d\u6982\u5ff5\uff0c\u8fd9\u5f15\u8d77\u4e86\u6211\u601d\u8003\uff1awhat is structure\uff1f\u672c\u6587\u5c31\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u5256\u6790\u3002 Structure\u662f\u4e00\u4e2a\u5b8f\u5927\u7684\u8bdd\u9898\uff0cstructure\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u6211\u4eec\u8089\u773c\u53ef\u89c1\u7684\u7269\u7406structure\uff0c\u5b83\u8fd8\u5305\u62ec\u6211\u4eec\u8089\u773c\u65e0\u6cd5\u770b\u5230\u7684\u903b\u8f91\uff08\u62bd\u8c61\uff09structure(abstract structure)\u3002","title":"Structure"},{"location":"Relation-structure-computation/Structure/Structure/#what#is#structure","text":"\u4e0b\u9762\u662f\u7ef4\u57fa\u767e\u79d1 Structure \u4e2d\u7ed9\u51fa\u7684\u5b9a\u4e49\uff1a A structure is an arrangement and organization of interrelated\uff08\u76f8\u4e92\u5173\u8054\uff09 elements in a material object or system , or the object or system so organized. Material structures include man-made objects such as buildings and machines and natural objects such as biological organisms , minerals and chemicals . Abstract structures include data structures in computer science and musical form . \u4e0a\u8ff0\u5b9a\u4e49\u4e2d\u7684\u201cmaterial structures\u201d\u662f\u6307\u7269\u7406\u7684\u3001\u8089\u773c\u53ef\u89c1\u7684\u7ed3\u6784\u3002\u4e0a\u8ff0\u5b9a\u4e49\u662fgeneral\u7684\uff0c\u4e0b\u9762\u770b\u770b\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u6700\u6700\u201d\u4eb2\u5bc6\u201c\u7684\u6570\u5b66\u4e2d\u5bf9structure\u7684\u5b9a\u4e49\uff1a Mathematical structure In mathematics , a structure is a set endowed\uff08\u8d4b\u4e88\uff09 with some additional features on the set (e.g., operation , relation , metric , topology ). Often, the additional features are attached or related to the set, so as to provide it with some additional meaning or significance. Structure (mathematical logic) \uff1a In universal algebra and in model theory , a structure consists of a set along with a collection of finitary operations and relations that are defined on it. Algebraic structure In mathematics , more specifically in abstract algebra and universal algebra , an algebraic structure consists of a set A (called the underlying set , carrier set or domain ), a collection of operations on A of finite arity (typically binary operations ), and a finite set of identities , known as axioms , that these operations must satisfy. \u7b80\u8a00\u4e4b\uff0c\u5728\u6570\u5b66\u4e2d\uff0c\u5f53\u8c08\u53castructure\u7684\u65f6\u5019\uff0c\u5b83\u8868\u793a\u7684\u662f\u4e00\u4e2aset\u4ee5\u53ca\u8d4b\u4e88\u7ed9\u5b83\u7684feature\u3002 \u6570\u5b66\u4e2d\u7684\u5b9a\u4e49\u6240\u4f7f\u7528\u7684\u662f \u6570\u5b66\u8bed\u8a00 \uff0cgeneral\u5b9a\u4e49\u4e2d\u7684\u201celements\u201d\u4f7f\u7528 \u6570\u5b66\u8bed\u8a00 \u6765\u63cf\u8ff0\u662f set \uff0cgeneral\u5b9a\u4e49\u4e2d\u7684\u201cinterrelated\u201d\u4f7f\u7528 \u6570\u5b66\u8bed\u8a00 \u6765\u63cf\u8ff0\u662f operations \u6216 relations \u3002 \u9605\u8bfb\u4e86\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u76f4\u89c2\u611f\u53d7\u5c31\u662f\uff1a **structure**\u6240\u63cf\u8ff0\u7684\u662f\u5143\u7d20\u4ee5\u53ca\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u76f8\u540c\u7684\u5143\u7d20\uff0c\u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7684**\u5173\u7cfb**\u6765\u8fdb\u884c\u7ec4\u7ec7\uff0c\u5c31\u662f\u4e0d\u540c\u7684**structure**\uff0c\u5c31\u5448\u73b0\u51fa\u4e0d\u540c\u7684\u89c6\u89c9**\u5f62\u6001**\uff08\u540e\u6587\u4e2d\uff0c\u6211\u4eec\u628a\u8fd9\u79f0\u4e4b\u4e3a\u201c \u5f62\u72b6 \u201d\uff09 \u6240\u4ee5\uff0c\u6211\u4eec\u8bf4\uff1a \u5173\u7cfb\u51b3\u5b9a\uff08\u6216\u8005\u8bf4\uff1a\u5b9a\u4e49\uff09\u4e86structure\uff0c\u8fdb\u800c\u51b3\u5b9a\u4e86\u5f62\u72b6 \uff0c\u6240\u4ee5\uff0c\u6211\u4eec\u5728\u7814\u7a76\u7ed3\u6784\u7684\u65f6\u5019\uff0c\u5207\u83ab\u5ffd\u89c6\u4e86\u5bf9**\u5173\u7cfb**\u5206\u6790\u3002 NOTE: \u8fd9\u4e2a\u601d\u60f3\uff0c\u5728 Relation-structure-computation\\index.md \u4e2d\u5df2\u7ecf\u7ed9\u51fa\uff0c\u672c\u6587\u53ea\u662f\u7ed9\u51fa\u5b83\u7684\u5f62\u6210\u8fc7\u7a0b\u3002 \u5728\u7ef4\u57fa\u767e\u79d1 Mathematical structure \u4e2d\uff0c\u5217\u4e3e\u4e86\u4e00\u4e9b\u6570\u5b66\u4e2d\u7684structure\uff1a A partial list of possible structures are measures , algebraic structures ( groups , fields , etc.), topologies , metric structures ( geometries ), orders , events , equivalence relations , differential structures , and categories . \u901a\u8fc7\u4e0a\u9762\u7684\u63cf\u8ff0\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u6765\u56de\u7b54\u672c\u8282\u6807\u9898\u4e2d\u7684\u95ee\u9898\uff1a structure\u662f\u6211\u4eec\u6309\u7167\u67d0\u79cd\u5173\u7cfb\u5bf9\u5143\u7d20\u8fdb\u884c\u7ec4\u7ec7\u540e\u5f62\u6210\u7684 \u3002","title":"What is structure?"},{"location":"Relation-structure-computation/Structure/Structure/#_1","text":"\u5f53\u6211\u4eec\u6309\u7167\u4e00\u5b9a\u7684\u5173\u7cfb\u5bf9\u5143\u7d20\u8fdb\u884c\u7ec4\u7ec7\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528digraph\u5c06\u5b83\u4eec\u7ed9\u201c\u753b\u201d\u51fa\u6765\uff0c\u8fd9\u65f6\u4f1a\u5448\u73b0\u4e00\u5b9a\u7684\u201c\u5f62\u72b6\u201d\uff08\u6b64\u5904\u4f7f\u7528\u201c\u5f62\u72b6\u201d\u8fd9\u4e2a\u8bcd\uff0c\u662f\u4e3a\u4e86\u4e0e\"\u7ed3\u6784\"\"\u8fd9\u4e2a\u8bcd\u6709\u6240\u533a\u5206\uff0c\u5b83\u5f3a\u8c03\u7684\u662f\uff0c\u6211\u4eec\u8089\u773c\u53ef\u4ee5\u770b\u5230\u7684\u5f62\u6001\uff0c\u4f46\u662f\uff0c\u5e73\u65f6\uff0c\u6211\u4eec\u66f4\u591a\u7684\u8fd8\u662f\u4f7f\u7528\u7ed3\u6784\u8fd9\u4e2a\u8bcd\uff09\uff0c\u6bd4\u5982 \u6309\u7167parent-child\u5173\u7cfb\u6765\u7ec4\u7ec7process\uff0c\u6700\u7ec8\u5f62\u6210\u7684\u662f\u6811\u5f62\u7ed3\u6784 \u6309\u7167inheritance\u6765\u7ec4\u7ec7\u7c7b\uff0c\u5982\u679c\u4e0d\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f**tree**\uff1b\u5982\u679c\u5141\u8bb8\u591a\u7ee7\u627f\u7684\u8bdd\uff0c\u5219\u6700\u7ec8\u5f62\u6210\u7684\u662f**hierarchy** \u6bd4\u8f83\u5178\u578b\u7684\u5f62\u72b6\u6709\uff1a Chain\uff0c\u7ebf\u6027\u7684 Hierarchy\uff0c\u975e\u7ebf\u6027\u7684\uff0c\u5448\u73b0\u51fa\u5c42\u6b21\u7684\u7ed3\u6784 Tree Network\uff0c \u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Network Lattice\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Lattice \u4e0a\u8bc9\u524d\u56db\u79cd**\u5f62\u72b6**\u662f\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u975e\u5e38\u5e38\u89c1\u7684\uff0c\u540e\u9762\u6211\u4eec\u4f1a\u5bf9\u5b83\u4eec\u8fdb\u884c\u5206\u6790\u3002","title":"\u7ed3\u6784\u7684\u5f62\u72b6"},{"location":"Relation-structure-computation/Structure/Structure/#relationstructure","text":"\u6309\u7167\u6709\u7684\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u4f1a\u5f62\u6210\u6811\u7ed3\u6784\uff08\u4e0d\u4f1a\u6210\u73af\uff09\uff0c\u6bd4\u5982parent-children\u5173\u7cfb\uff1b\u6309\u7167\u6709\u7684\u5173\u7cfb\u6765\u7ec4\u7ec7\u6570\u636e\uff0c\u5b83\u4eec\u4f1a\u5f62\u6210\u5c42\u6b21\u5316\u7ed3\u6784\uff08\u662f\u56fe\uff0c\u56e0\u4e3a\u5b83\u4f1a\u6210\u73af\uff09\uff0c\u8fd9\u5c31\u662f\u6211\u4eec\u5728\u4e0a\u4e00\u8282\u4e2d\u63d0\u51fa\u7684\u201c relation\u51b3\u5b9astructure \u201d\u7ed3\u8bba\uff0c\u5173\u4e8e\u6b64\uff0c\u6211\u4eec\u9700\u8981\u8fdb\u884c\u6df1\u5165\u601d\u8003: Relation\u7684\u54ea\u4e9b\u7279\u6027\u51b3\u5b9a\u4e86structure\uff0c\u6216\u8005\u8bf4\uff1a\u4ec0\u4e48\u6837\u7684relation\u53ef\u4ee5\u5f62\u6210\u4ec0\u4e48\u6837\u7684\u7ed3\u6784\uff0c\u6bd4\u5982chain\u3001tree\u3001graph\uff1f \u8981\u641e\u6e05\u695a\u8fd9\u4e2a\u95ee\u9898\uff0c\u9996\u5148\u6211\u4eec\u9700\u8981\u5bf9\u201cRelation\u201d\u7406\u8bba\u6709\u4e00\u5b9a\u7684\u4e86\u89e3\uff08\u53c2\u89c1 Relation-structure-computation\\Relation\\Relation \uff09\uff0c\u7136\u540e\u5728\u6df1\u5165\u5206\u6790\u6211\u4eec\u8089\u773c\u770b\u5230\u7684\u5404\u79cd\u5f62\u72b6\u80cc\u540e\u7684relation\u7684\u6027\u8d28\u3002\u6211\u76ee\u524d\u65e0\u6cd5\u7ed9\u51fa**\u901a\u7528\u7684\u89e3\u7b54**\uff0c\u53ea\u80fd\u591f\u7ed3\u5408\u5177\u4f53\u7684\u51e0\u79cd\u5f62\u72b6\u6765\u8fdb\u884c\u5206\u6790\uff0c\u5173\u4e8e\u8fd9\u51e0\u79cd\u7ed3\u6784\u7684\u5206\u6790\u53c2\u89c1\uff1a \u5f62\u72b6 \u63cf\u8ff0\u7ae0\u8282 \u8bf4\u660e Hierarchy Relation-structure-computation\\Model\\Hierarchy-relation-model Tree Relation-structure-computation\\Model\\Containing-relation-model Chain Relation-structure-computation\\Model\\Chain Network \u5176\u5b9e\u5c31\u662fgraph","title":"Relation\u51b3\u5b9astructure"},{"location":"Relation-structure-computation/Structure/Structure/#discrete#structure","text":"\u6709\u4e86 discrete \u6982\u5ff5\u548c structure \u6982\u5ff5\uff0c\u90a3\u4e48\u7406\u89e3what is discrete structure\u5c31\u6bd4\u8f83\u5bb9\u6613\u4e86\u3002\u4e0b\u9762\u662f\u4e00\u4e9b\u5173\u4e8ediscrete structure\u7684\u5185\u5bb9\uff1a","title":"Discrete structure"},{"location":"Relation-structure-computation/Structure/Structure/#quora#explain#what#is#the#role#of#discrete#structures#in#computer#science","text":"Discrete structures is study of mathematical structures that are fundamentally discrete (that is not continuous). This is usually the first mathematics course for computer science students; it helps them to start thinking about problems in a way computers can solve.","title":"quora Explain what is the role of Discrete Structures in Computer Science"},{"location":"Relation-structure-computation/Structure/Structure/#acm#discrete#structures","text":"","title":"ACM Discrete structures"},{"location":"Relation-structure-computation/Structure/Structure/#discrete#mathematics#and#its#applications","text":"\u5728\u8fd9\u672c\u4e66\u7684 Preface \u3001 chapter 2 \u3001 chapter 9 \u4e2d\u6709\u5bf9discrete structure\u7684\u63cf\u8ff0\u3002","title":"Discrete Mathematics and Its Applications"},{"location":"Relation-structure-computation/Structure/Structure/#abstract#structure","text":"\u5728 Relation-structure-computation\\Structuralization-and-formalization \u7ae0\u8282\u4e2d\u4ecb\u7ecd\u4e86abstract structure\u3002","title":"Abstract structure"},{"location":"Relation-structure-computation/Structure/Structure/#classification","text":"\u672c\u8282\u5bf9structure\u7684\u8fdb\u884c\u5206\u7c7b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u6b64\u5904\u7684structure\u66f4\u591a\u7684\u662f\u6307abstract structure\uff0c\u5f53\u7136\u4e5f\u5305\u542bdata structure\u3002","title":"Classification"},{"location":"Relation-structure-computation/Structure/Structure/#1#linear","text":"\u7ebf\u6027\u7ed3\u6784\u3002\u8fd9\u6837\u7684structure\u662f\u975e\u5e38\u666e\u904d\u7684\uff0c\u5e76\u4e14\u975e\u5e38\u5bb9\u6613computation\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50: Chomsky hierarchy \u4e2d\u7684Type-3 language\u5373 Regular \u5c31\u662flinear\u7ed3\u6784 Factorial\u662f\u5178\u578b\u7684\u7ebf\u6027\u7ed3\u6784\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1: https://infogalactic.com/info/Recursion_(computer_science )","title":"1) linear"},{"location":"Relation-structure-computation/Structure/Structure/#2#nonlinear","text":"\u975e\u7ebf\u6027\u7ed3\u6784","title":"2) nonlinear"},{"location":"Relation-structure-computation/Structure/Structure/#thoughts","text":"\u4f7f\u7528structure\u6982\u5ff5\u80fd\u591f\u4f7f\u6211\u4eec\u7684\u63cf\u8ff0\u975e\u5e38\u5730\u4fbf\u5229\u3002 \u6211\u5bf9structure\u7684\u76f4\u89c2\u7406\u89e3\u662f\uff1astructure\u662f\u5c06set\u4e2d\u7684element\u6309\u7167relation\u8fdb\u884c\u7ec4\u7ec7\u540e\u5f62\u6210\u7684\u3002 \u6211\u89c9\u5f97structure\u662f\u6700\u6700\u5178\u578b\u7684\u5177\u5907discrete\u7279\u6027\u7684\uff0c\u6240\u4ee5\u5b83\u4eec\u5f80\u5f80\u662fcomputable\u7684\u3002","title":"Thoughts"},{"location":"TODO/20201028-thoughts/","text":"Thoughts \u7ed3\u6784\uff1a\u4ea7\u751f\u5f0f\u662fcontaining\u5173\u7cfb\uff0c\u662f\u6811\u7ed3\u6784\uff1b\u51fd\u6570\u662fcomputation graph \u4e0d\u540c\u7c7b\u578b\u7684graph\u652f\u6301\u4e0d\u540c\u7684\u64cd\u4f5c\uff0c\u4f46\u662f\u6709\u4e00\u4e9b\u57fa\u672c\u64cd\u4f5c\u662f\u5168\u90e8\u90fd\u8981\u652f\u6301\u7684\uff0c\u6bd4\u5982\u67e5\u8be2\u4e00\u4e2anode\u7684adjacent node \u56fe\uff0c\u6392\u5e8f\uff0c\u5173\u7cfb\uff0c\u6709\u5e8f\u6027\uff0c\u65b9\u5411 \u7b80\u5355\u4ec5\u4ec5\u662f\u590d\u6742\u7684\u4e00\u79cd\u7b80\u5316\uff0c\u6bd4\u5982\uff1achain\u300a-tree\u300a-graph\uff1bbinary-search\u5176\u5b9e\u662f\u4e00\u79cddeep-first-search","title":"20201028-thoughts"},{"location":"TODO/20201028-thoughts/#thoughts","text":"\u7ed3\u6784\uff1a\u4ea7\u751f\u5f0f\u662fcontaining\u5173\u7cfb\uff0c\u662f\u6811\u7ed3\u6784\uff1b\u51fd\u6570\u662fcomputation graph \u4e0d\u540c\u7c7b\u578b\u7684graph\u652f\u6301\u4e0d\u540c\u7684\u64cd\u4f5c\uff0c\u4f46\u662f\u6709\u4e00\u4e9b\u57fa\u672c\u64cd\u4f5c\u662f\u5168\u90e8\u90fd\u8981\u652f\u6301\u7684\uff0c\u6bd4\u5982\u67e5\u8be2\u4e00\u4e2anode\u7684adjacent node \u56fe\uff0c\u6392\u5e8f\uff0c\u5173\u7cfb\uff0c\u6709\u5e8f\u6027\uff0c\u65b9\u5411 \u7b80\u5355\u4ec5\u4ec5\u662f\u590d\u6742\u7684\u4e00\u79cd\u7b80\u5316\uff0c\u6bd4\u5982\uff1achain\u300a-tree\u300a-graph\uff1bbinary-search\u5176\u5b9e\u662f\u4e00\u79cddeep-first-search","title":"Thoughts"},{"location":"TODO/stack-order/","text":"Stack order stack order: \u5bf9\u4e8e\u6240\u6709\u9700\u8981\u4e00\u5bf9\u76f8\u53cd\u7684\u64cd\u4f5c\uff0c\u90fd\u662f\u53ef\u4ee5\u4f7f\u7528stack\u6765\u8fdb\u884c\u6a21\u62df\u7684\u3002\u6bd4\u5982open/close\uff0cconnect/disconnect\uff0cnew/delete\uff1b \u8fd9\u975e\u5e38\u7c7b\u4f3c\u4e8e\u62ec\u53f7\u5339\u914d\uff0c\u8fd9\u5c31\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u62ec\u53f7\u5339\u914d\u95ee\u9898\u3002 \u8fd9\u5f15\u53d1\u4e86\u6211\u5bf9RAII\u7684\u601d\u8003: RAII\uff0c\u62ec\u53f7\uff0cstack\uff0c\u5176\u5b9e\u5b83\u5df2\u7ecf\u5c55\u793a\u4e86\u4e09\u79cd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u662fstack order\u5728C++\u4e2d\uff0c\u975e\u5e38\u663e\u8457\u7684application\u3002","title":"stack-order"},{"location":"TODO/stack-order/#stack#order","text":"stack order: \u5bf9\u4e8e\u6240\u6709\u9700\u8981\u4e00\u5bf9\u76f8\u53cd\u7684\u64cd\u4f5c\uff0c\u90fd\u662f\u53ef\u4ee5\u4f7f\u7528stack\u6765\u8fdb\u884c\u6a21\u62df\u7684\u3002\u6bd4\u5982open/close\uff0cconnect/disconnect\uff0cnew/delete\uff1b \u8fd9\u975e\u5e38\u7c7b\u4f3c\u4e8e\u62ec\u53f7\u5339\u914d\uff0c\u8fd9\u5c31\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u62ec\u53f7\u5339\u914d\u95ee\u9898\u3002 \u8fd9\u5f15\u53d1\u4e86\u6211\u5bf9RAII\u7684\u601d\u8003: RAII\uff0c\u62ec\u53f7\uff0cstack\uff0c\u5176\u5b9e\u5b83\u5df2\u7ecf\u5c55\u793a\u4e86\u4e09\u79cd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u662fstack order\u5728C++\u4e2d\uff0c\u975e\u5e38\u663e\u8457\u7684application\u3002","title":"Stack order"}]}